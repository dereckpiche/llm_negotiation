/home/mila/d/dereck.piche/llm_negotiation/src/run.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="default")
INFO 02-10 14:40:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:40:36 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:40:36 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:40:36,881][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:40:38,241][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-02-10 14:40:38,639][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-02-10 14:40:39,931][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:40:41,269][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-02-10 14:40:41,269][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 02-10 14:40:41 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 02-10 14:40:43 gpu_executor.py:122] # GPU blocks: 27321, # CPU blocks: 2048
INFO 02-10 14:40:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:40:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:41:05 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-02-10 14:41:05,534][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:41:06,444][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:00<00:28,  1.10it/s, est. speed input: 212.17 toks/s, output: 31.88 toks/s]
[2025-02-10 14:41:06,445][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:00<00:00, 35.14it/s, est. speed input: 6782.90 toks/s, output: 1019.18 toks/s]
[2025-02-10 14:41:06,448][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:41:06,867][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.39it/s, est. speed input: 623.74 toks/s, output: 71.69 toks/s]
[2025-02-10 14:41:06,867][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.39it/s, est. speed input: 623.74 toks/s, output: 71.69 toks/s]
[2025-02-10 14:41:06,881][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:41:07,820][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:00<00:28,  1.07it/s, est. speed input: 237.59 toks/s, output: 30.90 toks/s]
[2025-02-10 14:41:07,847][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:00<00:00, 32.12it/s, est. speed input: 7162.52 toks/s, output: 933.51 toks/s]
[2025-02-10 14:41:08,086][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:41:08,480][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 457.84 toks/s, output: 73.76 toks/s]
[2025-02-10 14:41:08,480][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 457.84 toks/s, output: 73.76 toks/s]
[2025-02-10 14:41:09,174][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:41:09,228][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:41:11,719][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.49s/it]
[2025-02-10 14:41:13,367][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:03,  1.99s/it]
[2025-02-10 14:41:14,943][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.80s/it]
[2025-02-10 14:41:15,460][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.30s/it]
[2025-02-10 14:41:15,460][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.56s/it]
[2025-02-10 14:41:23,037][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:41:23,089][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:41:24,758][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-02-10 14:41:26,375][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-02-10 14:41:27,972][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-02-10 14:41:28,500][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-02-10 14:41:28,500][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
INFO 02-10 14:41:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:41:36 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:41:36 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:41:36,552][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:41:37,876][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-02-10 14:41:38,266][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-02-10 14:41:39,578][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:41:40,928][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-02-10 14:41:40,928][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 02-10 14:41:41 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:41:42 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:41:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:41:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:42:04 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:42:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:42:04,371][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:42:08,439][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.07s/it, est. speed input: 47.45 toks/s, output: 7.13 toks/s]
[2025-02-10 14:42:08,439][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.87it/s, est. speed input: 1518.01 toks/s, output: 228.10 toks/s]
WARNING 02-10 14:42:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:42:08,444][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:42:09,593][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.15s/it, est. speed input: 227.20 toks/s, output: 47.01 toks/s]
[2025-02-10 14:42:09,593][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.15s/it, est. speed input: 227.20 toks/s, output: 47.01 toks/s]
WARNING 02-10 14:42:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:42:09,612][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:42:12,440][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:24,  2.83s/it, est. speed input: 78.87 toks/s, output: 9.20 toks/s]
[2025-02-10 14:42:12,612][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:36,  1.27s/it, est. speed input: 148.65 toks/s, output: 18.33 toks/s]
[2025-02-10 14:42:12,613][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00, 10.33it/s, est. speed input: 2303.56 toks/s, output: 298.57 toks/s]
WARNING 02-10 14:42:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:42:12,849][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:42:13,629][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.28it/s, est. speed input: 386.09 toks/s, output: 37.20 toks/s]
[2025-02-10 14:42:13,629][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.56it/s, est. speed input: 616.68 toks/s, output: 74.36 toks/s]
[2025-02-10 14:42:14,939][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:42:14,991][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:42:17,458][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.47s/it]
[2025-02-10 14:42:19,099][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:03,  1.98s/it]
[2025-02-10 14:42:20,684][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.80s/it]
[2025-02-10 14:42:21,214][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.30s/it]
[2025-02-10 14:42:21,215][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.56s/it]
[2025-02-10 14:42:29,358][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:42:29,411][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:42:31,124][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:42:32,849][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-02-10 14:42:34,432][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:42:34,978][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:42:34,979][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:42:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:42:43 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:42:43 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:42:43,665][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:42:44,975][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:42:45,357][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:42:46,646][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:42:47,984][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:42:47,984][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:42:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:42:49 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:42:49 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:42:49 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:43:11 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:43:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:43:11,291][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:43:13,942][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 72.80 toks/s, output: 10.94 toks/s]
[2025-02-10 14:43:13,943][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.07it/s, est. speed input: 2328.94 toks/s, output: 349.94 toks/s]
WARNING 02-10 14:43:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:43:13,947][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:43:15,138][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 219.25 toks/s, output: 46.20 toks/s]
[2025-02-10 14:43:15,138][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 219.25 toks/s, output: 46.20 toks/s]
WARNING 02-10 14:43:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:43:15,157][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:43:17,752][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:17,  2.59s/it, est. speed input: 85.94 toks/s, output: 11.18 toks/s]
[2025-02-10 14:43:17,753][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:02<00:00, 11.94it/s, est. speed input: 2663.12 toks/s, output: 346.32 toks/s]
WARNING 02-10 14:43:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:43:17,999][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:43:18,733][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 245.31 toks/s, output: 39.52 toks/s]
[2025-02-10 14:43:18,733][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 245.31 toks/s, output: 39.52 toks/s]
[2025-02-10 14:43:19,991][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:43:20,043][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:43:22,588][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.55s/it]
[2025-02-10 14:43:24,224][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.01s/it]
[2025-02-10 14:43:25,856][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.84s/it]
[2025-02-10 14:43:26,375][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:43:26,376][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.58s/it]
[2025-02-10 14:43:34,051][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:43:34,102][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:43:35,846][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.74s/it]
[2025-02-10 14:43:37,528][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-02-10 14:43:39,145][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-02-10 14:43:39,669][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:43:39,669][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:43:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:43:47 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:43:47 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:43:47,942][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:43:49,242][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:43:49,624][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[2025-02-10 14:43:50,912][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:43:52,247][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-02-10 14:43:52,248][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:43:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:43:53 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:43:53 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:43:53 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:44:15 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 02-10 14:44:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:44:15,395][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:44:18,009][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.61s/it, est. speed input: 73.84 toks/s, output: 11.10 toks/s]
[2025-02-10 14:44:18,010][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.24it/s, est. speed input: 2362.14 toks/s, output: 354.93 toks/s]
WARNING 02-10 14:44:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:44:18,015][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:44:19,195][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 221.25 toks/s, output: 47.47 toks/s]
[2025-02-10 14:44:19,195][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 221.25 toks/s, output: 47.47 toks/s]
WARNING 02-10 14:44:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:44:19,214][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:44:21,808][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:17,  2.59s/it, est. speed input: 85.98 toks/s, output: 11.18 toks/s]
[2025-02-10 14:44:21,809][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:02<00:00, 11.95it/s, est. speed input: 2664.48 toks/s, output: 346.50 toks/s]
WARNING 02-10 14:44:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:44:22,045][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:44:22,826][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.28it/s, est. speed input: 389.21 toks/s, output: 37.13 toks/s]
[2025-02-10 14:44:22,826][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.56it/s, est. speed input: 619.36 toks/s, output: 74.22 toks/s]
[2025-02-10 14:44:24,034][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:44:24,086][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:44:26,668][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.58s/it]
[2025-02-10 14:44:28,326][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.04s/it]
[2025-02-10 14:44:29,937][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.84s/it]
[2025-02-10 14:44:30,450][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:44:30,451][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.59s/it]
[2025-02-10 14:44:38,637][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:44:38,688][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:44:40,345][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-02-10 14:44:42,005][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-10 14:44:43,616][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-02-10 14:44:44,140][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-10 14:44:44,140][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
INFO 02-10 14:44:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:44:52 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:44:52 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:44:52,370][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:44:53,672][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:44:54,054][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:44:55,355][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-02-10 14:44:56,695][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:44:56,695][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:44:56 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:44:58 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:44:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:44:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:45:19 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:45:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:45:19,929][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:45:22,566][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.64s/it, est. speed input: 73.20 toks/s, output: 11.00 toks/s]
[2025-02-10 14:45:22,567][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.13it/s, est. speed input: 2341.56 toks/s, output: 351.84 toks/s]
WARNING 02-10 14:45:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:45:22,588][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:45:25,229][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.64s/it, est. speed input: 84.45 toks/s, output: 10.98 toks/s]
[2025-02-10 14:45:25,230][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.11it/s, est. speed input: 2701.53 toks/s, output: 351.32 toks/s]
[2025-02-10 14:45:26,665][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:45:26,717][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:45:29,294][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.58s/it]
[2025-02-10 14:45:30,944][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.03s/it]
[2025-02-10 14:45:32,547][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.84s/it]
[2025-02-10 14:45:33,071][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:45:33,072][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.59s/it]
[2025-02-10 14:45:40,758][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:45:40,810][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:45:42,541][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.73s/it]
[2025-02-10 14:45:44,219][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-02-10 14:45:45,831][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:45:46,381][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:45:46,382][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:45:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:45:54 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:45:54 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:45:54,643][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:45:55,961][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-02-10 14:45:56,345][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-02-10 14:45:57,664][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:45:59,016][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-02-10 14:45:59,016][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 02-10 14:45:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:46:00 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:46:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:46:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:46:22 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:46:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:46:22,632][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:46:25,264][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.63s/it, est. speed input: 73.33 toks/s, output: 11.02 toks/s]
[2025-02-10 14:46:25,265][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.15it/s, est. speed input: 2345.68 toks/s, output: 352.46 toks/s]
WARNING 02-10 14:46:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:46:25,285][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:46:27,925][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.64s/it, est. speed input: 84.47 toks/s, output: 10.98 toks/s]
[2025-02-10 14:46:27,926][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.12it/s, est. speed input: 2701.98 toks/s, output: 351.38 toks/s]
[2025-02-10 14:46:29,424][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:46:29,475][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:46:31,997][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.52s/it]
[2025-02-10 14:46:33,637][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.00s/it]
[2025-02-10 14:46:35,236][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.82s/it]
[2025-02-10 14:46:35,758][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-10 14:46:35,759][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.57s/it]
[2025-02-10 14:46:43,699][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:46:43,751][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:46:45,429][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.68s/it]
[2025-02-10 14:46:47,077][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-10 14:46:48,675][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-02-10 14:46:49,200][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-02-10 14:46:49,200][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
INFO 02-10 14:46:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:46:57 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:46:57 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:46:57,477][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:46:58,781][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:46:59,166][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:47:00,487][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:47:01,852][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-02-10 14:47:01,852][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 02-10 14:47:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:47:03 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:47:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:47:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:47:25 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:47:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:47:25,215][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:47:27,854][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.64s/it, est. speed input: 73.13 toks/s, output: 10.99 toks/s]
[2025-02-10 14:47:27,855][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.12it/s, est. speed input: 2339.28 toks/s, output: 351.50 toks/s]
WARNING 02-10 14:47:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:47:27,877][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:47:30,552][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.68s/it, est. speed input: 83.35 toks/s, output: 10.84 toks/s]
[2025-02-10 14:47:30,553][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 11.96it/s, est. speed input: 2666.36 toks/s, output: 346.75 toks/s]
[2025-02-10 14:47:31,974][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:47:32,026][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:47:34,623][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.60s/it]
[2025-02-10 14:47:36,295][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.05s/it]
[2025-02-10 14:47:37,952][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.87s/it]
[2025-02-10 14:47:38,476][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.34s/it]
[2025-02-10 14:47:38,476][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.61s/it]
[2025-02-10 14:47:46,554][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:47:46,606][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:47:48,341][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.74s/it]
[2025-02-10 14:47:49,989][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-02-10 14:47:51,621][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:47:52,145][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-02-10 14:47:52,145][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
INFO 02-10 14:48:00 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:48:00 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:48:00 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:48:00,741][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:48:02,055][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:48:02,441][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-02-10 14:48:03,763][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:48:05,131][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-02-10 14:48:05,131][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 02-10 14:48:05 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:48:06 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:48:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:48:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:48:28 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:48:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:48:28,693][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:48:31,309][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.62s/it, est. speed input: 73.79 toks/s, output: 11.09 toks/s]
[2025-02-10 14:48:31,310][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.23it/s, est. speed input: 2360.47 toks/s, output: 354.68 toks/s]
WARNING 02-10 14:48:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:48:31,331][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:48:33,991][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.66s/it, est. speed input: 83.82 toks/s, output: 10.90 toks/s]
[2025-02-10 14:48:33,992][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.02it/s, est. speed input: 2681.51 toks/s, output: 348.72 toks/s]
[2025-02-10 14:48:35,500][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:48:35,552][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:48:38,089][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.54s/it]
[2025-02-10 14:48:39,738][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.01s/it]
[2025-02-10 14:48:41,325][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.82s/it]
[2025-02-10 14:48:41,849][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-10 14:48:41,849][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.57s/it]
[2025-02-10 14:48:49,503][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:48:49,555][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:48:51,268][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:48:52,923][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-02-10 14:48:54,558][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:48:55,096][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:48:55,097][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:49:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:49:03 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:49:03 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:49:03,677][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:49:04,984][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:49:05,369][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:49:06,658][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:49:08,004][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:49:08,004][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:49:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:49:09 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:49:09 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:49:09 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:49:31 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:49:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:49:31,524][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:49:34,129][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:20,  2.60s/it, est. speed input: 74.10 toks/s, output: 11.13 toks/s]
[2025-02-10 14:49:34,130][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.28it/s, est. speed input: 2370.33 toks/s, output: 356.16 toks/s]
WARNING 02-10 14:49:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:49:34,150][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:49:36,804][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.04 toks/s, output: 10.93 toks/s]
[2025-02-10 14:49:36,805][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.06it/s, est. speed input: 2688.36 toks/s, output: 349.61 toks/s]
[2025-02-10 14:49:38,346][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:49:38,398][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:49:40,959][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-02-10 14:49:42,613][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.03s/it]
[2025-02-10 14:49:44,201][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.83s/it]
[2025-02-10 14:49:44,731][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-10 14:49:44,731][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.58s/it]
[2025-02-10 14:49:52,553][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:49:52,605][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:49:54,319][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:49:55,948][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-10 14:49:57,561][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-02-10 14:49:58,084][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-10 14:49:58,084][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
INFO 02-10 14:50:05 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:50:06 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:50:06 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:50:06,507][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:50:07,816][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:50:08,203][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-02-10 14:50:09,492][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:50:10,828][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:50:10,828][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:50:11 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:50:12 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:50:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:50:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:50:34 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:50:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:50:34,234][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:50:36,870][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.64s/it, est. speed input: 73.22 toks/s, output: 11.00 toks/s]
[2025-02-10 14:50:36,871][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.14it/s, est. speed input: 2342.28 toks/s, output: 351.95 toks/s]
WARNING 02-10 14:50:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:50:36,891][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:50:39,546][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.00 toks/s, output: 10.92 toks/s]
[2025-02-10 14:50:39,547][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.05it/s, est. speed input: 2687.24 toks/s, output: 349.46 toks/s]
[2025-02-10 14:50:40,999][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:50:41,050][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:50:43,579][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.53s/it]
[2025-02-10 14:50:45,207][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:03,  2.00s/it]
[2025-02-10 14:50:46,812][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.82s/it]
[2025-02-10 14:50:47,337][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-10 14:50:47,337][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.57s/it]
[2025-02-10 14:50:55,135][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:50:55,186][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:50:56,848][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-02-10 14:50:58,511][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-10 14:51:00,112][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-02-10 14:51:00,635][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-10 14:51:00,635][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
INFO 02-10 14:51:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:51:08 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:51:08 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:51:09,215][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:51:10,524][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:51:10,908][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:51:12,205][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-02-10 14:51:13,547][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:51:13,547][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:51:13 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:51:15 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:51:15 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:51:15 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:51:37 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:51:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:51:37,105][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:51:39,775][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.67s/it, est. speed input: 72.29 toks/s, output: 10.86 toks/s]
[2025-02-10 14:51:39,776][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 11.98it/s, est. speed input: 2312.44 toks/s, output: 347.46 toks/s]
WARNING 02-10 14:51:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:51:39,796][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:51:42,462][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.66s/it, est. speed input: 83.68 toks/s, output: 10.88 toks/s]
[2025-02-10 14:51:42,462][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.00it/s, est. speed input: 2676.86 toks/s, output: 348.11 toks/s]
[2025-02-10 14:51:43,881][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:51:43,933][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:51:46,551][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.62s/it]
[2025-02-10 14:51:48,208][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.05s/it]
[2025-02-10 14:51:49,810][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.85s/it]
[2025-02-10 14:51:50,333][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:51:50,333][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.60s/it]
[2025-02-10 14:51:58,418][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:51:58,470][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:52:00,181][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:52:01,891][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-02-10 14:52:03,485][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:52:04,007][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-02-10 14:52:04,007][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
INFO 02-10 14:52:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:52:12 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:52:12 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:52:12,965][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:52:14,271][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:52:14,657][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:52:15,948][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:52:17,286][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:52:17,287][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:52:17 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:52:18 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:52:18 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:52:18 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:52:40 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:52:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:52:40,710][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:52:43,338][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.63s/it, est. speed input: 73.47 toks/s, output: 11.04 toks/s]
[2025-02-10 14:52:43,338][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.18it/s, est. speed input: 2350.15 toks/s, output: 353.13 toks/s]
WARNING 02-10 14:52:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:52:43,359][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:52:46,017][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.66s/it, est. speed input: 83.92 toks/s, output: 10.91 toks/s]
[2025-02-10 14:52:46,018][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.04it/s, est. speed input: 2684.40 toks/s, output: 349.09 toks/s]
[2025-02-10 14:52:47,442][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:52:47,493][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:52:50,057][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-02-10 14:52:51,739][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.04s/it]
[2025-02-10 14:52:53,339][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.84s/it]
[2025-02-10 14:52:53,866][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:52:53,867][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.59s/it]
[2025-02-10 14:53:21,598][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:53:21,650][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:53:23,357][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:53:24,993][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-10 14:53:26,609][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-02-10 14:53:27,148][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-02-10 14:53:27,148][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
INFO 02-10 14:53:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:53:35 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:53:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:53:35,382][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:53:36,681][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:53:37,063][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[2025-02-10 14:53:38,349][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
[2025-02-10 14:53:39,690][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-02-10 14:53:39,690][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:53:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:53:41 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:53:41 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:53:41 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:54:03 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:54:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:54:03,142][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:54:05,770][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.63s/it, est. speed input: 73.45 toks/s, output: 11.04 toks/s]
[2025-02-10 14:54:05,771][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.17it/s, est. speed input: 2349.61 toks/s, output: 353.05 toks/s]
WARNING 02-10 14:54:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:54:05,791][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:54:08,443][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.12 toks/s, output: 10.94 toks/s]
[2025-02-10 14:54:08,444][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.07it/s, est. speed input: 2690.57 toks/s, output: 349.89 toks/s]
[2025-02-10 14:54:09,936][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:54:09,988][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:54:12,541][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.55s/it]
[2025-02-10 14:54:14,174][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.01s/it]
[2025-02-10 14:54:15,756][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.82s/it]
[2025-02-10 14:54:16,282][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-10 14:54:16,282][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.57s/it]
[2025-02-10 14:54:24,207][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:54:24,259][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:54:25,956][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-02-10 14:54:27,607][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-02-10 14:54:29,198][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-02-10 14:54:29,728][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-10 14:54:29,728][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
INFO 02-10 14:54:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:54:37 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:54:38 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:54:38,120][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:54:39,420][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:54:39,806][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:54:41,093][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:54:42,426][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-02-10 14:54:42,426][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:54:42 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:54:43 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:54:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:54:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:55:05 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:55:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:55:05,844][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:55:08,471][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.63s/it, est. speed input: 73.46 toks/s, output: 11.04 toks/s]
[2025-02-10 14:55:08,472][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.18it/s, est. speed input: 2350.02 toks/s, output: 353.11 toks/s]
WARNING 02-10 14:55:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:55:08,493][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:55:11,142][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.19 toks/s, output: 10.95 toks/s]
[2025-02-10 14:55:11,143][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.08it/s, est. speed input: 2693.26 toks/s, output: 350.24 toks/s]
[2025-02-10 14:55:12,600][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:55:12,652][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:55:15,212][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-02-10 14:55:16,869][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.03s/it]
[2025-02-10 14:55:18,523][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.86s/it]
[2025-02-10 14:55:19,056][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.33s/it]
[2025-02-10 14:55:19,057][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.60s/it]
[2025-02-10 14:55:26,954][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:55:27,006][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:55:28,674][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-02-10 14:55:30,315][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-02-10 14:55:31,894][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-02-10 14:55:32,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-02-10 14:55:32,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
INFO 02-10 14:55:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:55:40 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:55:40 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:55:40,677][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:55:41,982][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:55:42,370][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:55:43,696][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:55:45,065][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-02-10 14:55:45,065][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 02-10 14:55:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:55:46 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:55:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:55:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:56:08 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:56:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:56:08,494][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:56:11,106][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:20,  2.61s/it, est. speed input: 73.89 toks/s, output: 11.10 toks/s]
[2025-02-10 14:56:11,107][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.25it/s, est. speed input: 2363.71 toks/s, output: 355.17 toks/s]
WARNING 02-10 14:56:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:56:11,128][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:56:13,782][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.01 toks/s, output: 10.93 toks/s]
[2025-02-10 14:56:13,783][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.05it/s, est. speed input: 2687.43 toks/s, output: 349.49 toks/s]
[2025-02-10 14:56:15,318][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:56:15,370][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:56:17,990][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.62s/it]
[2025-02-10 14:56:19,648][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.05s/it]
[2025-02-10 14:56:21,245][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.85s/it]
[2025-02-10 14:56:21,780][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.33s/it]
[2025-02-10 14:56:21,781][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.60s/it]
[2025-02-10 14:56:29,441][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:56:29,496][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:56:31,203][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-02-10 14:56:32,935][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-02-10 14:56:34,528][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:56:35,060][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:56:35,061][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:56:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:56:43 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:56:43 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:56:43,600][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:56:44,923][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-02-10 14:56:45,321][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-02-10 14:56:46,635][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-02-10 14:56:47,996][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-02-10 14:56:47,996][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 02-10 14:56:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:56:49 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:56:49 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:56:49 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:57:11 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:57:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:57:11,583][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:57:14,188][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:20,  2.61s/it, est. speed input: 74.07 toks/s, output: 11.13 toks/s]
[2025-02-10 14:57:14,189][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.28it/s, est. speed input: 2369.42 toks/s, output: 356.03 toks/s]
WARNING 02-10 14:57:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:57:14,210][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:57:16,861][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.13 toks/s, output: 10.94 toks/s]
[2025-02-10 14:57:16,862][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.07it/s, est. speed input: 2691.10 toks/s, output: 349.96 toks/s]
[2025-02-10 14:57:18,390][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:57:18,442][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:57:21,006][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-02-10 14:57:22,707][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.06s/it]
[2025-02-10 14:57:24,313][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.85s/it]
[2025-02-10 14:57:24,867][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.34s/it]
[2025-02-10 14:57:24,868][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.61s/it]
[2025-02-10 14:57:32,737][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:57:32,790][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:57:34,490][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-02-10 14:57:36,218][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-02-10 14:57:37,864][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-02-10 14:57:38,399][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-02-10 14:57:38,400][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
INFO 02-10 14:57:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:57:46 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:57:46 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:57:46,830][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:57:48,137][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-10 14:57:48,525][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-02-10 14:57:49,831][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-02-10 14:57:51,172][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:57:51,172][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 02-10 14:57:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:57:52 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:57:52 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:57:52 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:58:14 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:58:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:58:14,758][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:58:17,359][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:20,  2.60s/it, est. speed input: 74.21 toks/s, output: 11.15 toks/s]
[2025-02-10 14:58:17,360][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.30it/s, est. speed input: 2373.88 toks/s, output: 356.70 toks/s]
WARNING 02-10 14:58:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:58:17,380][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:58:20,047][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.67s/it, est. speed input: 83.61 toks/s, output: 10.87 toks/s]
[2025-02-10 14:58:20,048][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 11.99it/s, est. speed input: 2674.53 toks/s, output: 347.81 toks/s]
[2025-02-10 14:58:21,801][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:58:21,853][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:58:24,346][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.49s/it]
[2025-02-10 14:58:26,026][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.01s/it]
[2025-02-10 14:58:27,629][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.83s/it]
[2025-02-10 14:58:28,181][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 14:58:28,182][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.58s/it]
[2025-02-10 14:58:35,922][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:58:35,974][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:58:37,732][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.76s/it]
[2025-02-10 14:58:39,401][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-02-10 14:58:41,005][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-02-10 14:58:41,544][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-02-10 14:58:41,545][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
INFO 02-10 14:58:49 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:58:49 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:58:50 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:58:50,129][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:58:51,447][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-02-10 14:58:51,874][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-02-10 14:58:53,206][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-02-10 14:58:54,578][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
[2025-02-10 14:58:54,578][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 02-10 14:58:54 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:58:56 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 14:58:56 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 14:58:56 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 14:59:18 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 14:59:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:59:18,428][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:59:21,053][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.62s/it, est. speed input: 73.54 toks/s, output: 11.05 toks/s]
[2025-02-10 14:59:21,054][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.19it/s, est. speed input: 2352.50 toks/s, output: 353.48 toks/s]
WARNING 02-10 14:59:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 14:59:21,074][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 14:59:23,729][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.66s/it, est. speed input: 83.99 toks/s, output: 10.92 toks/s]
[2025-02-10 14:59:23,730][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.05it/s, est. speed input: 2686.73 toks/s, output: 349.39 toks/s]
[2025-02-10 14:59:25,245][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:59:25,297][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:59:27,876][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.58s/it]
[2025-02-10 14:59:29,525][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.03s/it]
[2025-02-10 14:59:31,152][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.85s/it]
[2025-02-10 14:59:31,678][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.33s/it]
[2025-02-10 14:59:31,678][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.60s/it]
[2025-02-10 14:59:39,857][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 14:59:39,909][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:59:41,637][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.73s/it]
[2025-02-10 14:59:43,273][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-02-10 14:59:44,868][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-02-10 14:59:45,397][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-10 14:59:45,398][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
INFO 02-10 14:59:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-10 14:59:53 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-10 14:59:54 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-10 14:59:54,135][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-10 14:59:55,436][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[2025-02-10 14:59:55,825][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-10 14:59:57,114][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-10 14:59:58,453][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-10 14:59:58,453][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-10 14:59:58 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-10 14:59:59 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-10 15:00:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-10 15:00:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-10 15:00:22 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 02-10 15:00:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 15:00:22,121][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 15:00:24,747][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:21,  2.62s/it, est. speed input: 73.53 toks/s, output: 11.05 toks/s]
[2025-02-10 15:00:24,747][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.19it/s, est. speed input: 2352.00 toks/s, output: 353.41 toks/s]
WARNING 02-10 15:00:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-10/14-40-34/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-10 15:00:24,768][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-10 15:00:27,421][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:22,  2.65s/it, est. speed input: 84.04 toks/s, output: 10.93 toks/s]
[2025-02-10 15:00:27,422][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 12.06it/s, est. speed input: 2688.56 toks/s, output: 349.63 toks/s]
[2025-02-10 15:00:28,983][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-10 15:00:29,036][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-10 15:00:31,595][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-02-10 15:00:33,269][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.04s/it]
[2025-02-10 15:00:34,872][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.84s/it]
[2025-02-10 15:00:35,407][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.32s/it]
[2025-02-10 15:00:35,407][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.59s/it]
