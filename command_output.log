/home/mila/d/dereck.piche/llm_negotiation/src/run.py:10: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="default")
INFO 02-11 09:47:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-11 09:47:47 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-11 09:47:47 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-11 09:47:47,559][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:47:48,875][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-02-11 09:47:49,253][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-02-11 09:47:50,544][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-02-11 09:47:51,885][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-11 09:47:51,885][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-11 09:47:52 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 02-11 09:47:53 gpu_executor.py:122] # GPU blocks: 27321, # CPU blocks: 2048
INFO 02-11 09:47:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-11 09:47:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-11 09:48:15 model_runner.py:1335] Graph capturing finished in 20 secs.
[2025-02-11 09:48:15,277][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-11 09:48:15,703][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.35it/s, est. speed input: 437.37 toks/s, output: 68.19 toks/s]
[2025-02-11 09:48:15,703][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.35it/s, est. speed input: 437.37 toks/s, output: 68.19 toks/s]
[2025-02-11 09:48:15,705][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-11 09:48:16,107][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.49it/s, est. speed input: 537.63 toks/s, output: 72.18 toks/s]
[2025-02-11 09:48:16,107][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.49it/s, est. speed input: 537.63 toks/s, output: 72.18 toks/s]
[2025-02-11 09:48:16,702][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-11 09:48:16,755][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:48:19,208][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.45s/it]
[2025-02-11 09:48:20,874][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:03,  1.99s/it]
[2025-02-11 09:48:22,469][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.81s/it]
[2025-02-11 09:48:23,011][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.31s/it]
[2025-02-11 09:48:23,011][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.56s/it]
[2025-02-11 09:48:26,495][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-11 09:48:26,547][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:48:28,221][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-02-11 09:48:29,864][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-11 09:48:31,513][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-02-11 09:48:32,018][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-02-11 09:48:32,018][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
INFO 02-11 09:48:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=500, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 02-11 09:48:35 model_runner.py:915] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 02-11 09:48:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-02-11 09:48:35,915][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:48:37,227][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-02-11 09:48:37,616][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-02-11 09:48:38,906][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-02-11 09:48:40,242][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-02-11 09:48:40,242][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 02-11 09:48:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 02-11 09:48:41 gpu_executor.py:122] # GPU blocks: 27392, # CPU blocks: 2048
INFO 02-11 09:48:41 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-11 09:48:41 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-11 09:49:02 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 02-11 09:49:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-11 09:49:02,540][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-11 09:49:04,741][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 84.52 toks/s, output: 13.18 toks/s]
[2025-02-11 09:49:04,741][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 84.52 toks/s, output: 13.18 toks/s]
WARNING 02-11 09:49:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-02-11/09-47-45/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-02-11 09:49:04,745][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-02-11 09:49:05,782][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 208.29 toks/s, output: 27.96 toks/s]
[2025-02-11 09:49:05,783][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 208.29 toks/s, output: 27.96 toks/s]
[2025-02-11 09:49:06,931][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-11 09:49:06,983][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:49:09,428][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.44s/it]
[2025-02-11 09:49:11,038][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:03,  1.95s/it]
[2025-02-11 09:49:12,596][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.77s/it]
[2025-02-11 09:49:13,102][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.27s/it]
[2025-02-11 09:49:13,102][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.53s/it]
[2025-02-11 09:49:16,603][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-02-11 09:49:16,655][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-02-11 09:49:18,350][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-02-11 09:49:19,986][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-02-11 09:49:21,551][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-02-11 09:49:22,055][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-02-11 09:49:22,056][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
