defaults:
  - dereck_base_config.yaml

experiment:
  name: "COOPERATIVE/sum_rewards_big_adapter"
  description: "Sum rewards with a big adapter finetune"

matches:
  agents:
    alice:
      kwargs:
        agent_name: "alice"
        policy_id: "llama/fulldude"
    bob:
      kwargs:
        agent_name: "bob"
        policy_id: "llama/fulldude"

temperature: 0.7


models:
  llama:
    class: LocalLLM
    init_args:
      name: 'llama'
      vllm_params:
        max_lora_rank: 512
      adapter_configs:
        fulldude:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 1e-6
            weight_decay: 0.0
          lora_kwargs:
            r: 512
            lora_alpha: 1024
            lora_dropout: 0.0



####################################################################################################
# TRAINING
####################################################################################################
default_kwargs: &default_kwargs
  train_func_args:
    gradient_checkpointing: true
    temperature: ${temperature}
    entropy_coef: 0.0
    kl_loss_coef: 0.0
    debug_enabled: True
    debug_log_path: "${oc.env:SCRATCH}/${experiment.name}/trainer_debugger"
  train_data_args:
    average_score_over_message: False

training:
  agents:
    alice:
      training_data_func_args:
        score_method: sum_rloo_scores
        score_method_kwargs:
          discount_factor: 1.0
    bob:
      training_data_func_args:
        score_method: sum_rloo_scores
        score_method_kwargs:
          discount_factor: 1.0
  llama:
    adapters:
      fulldude:
        train_func: train_reinforce_main
        <<: *default_kwargs
