defaults:
  - dereck_base_config.yaml

experiment:
  name: "COOPERATIVE/qwen_sum_rewards_big_adapter_test2"
  description: "Sum rewards with a big adapter finetune"
  seed: 200

matches:
  agents:
    alice:
      kwargs:
        agent_name: "alice"
        policy_id: "llama/fulldude"
    bob:
      kwargs:
        agent_name: "bob"
        policy_id: "llama/fulldude"

temperature: 0.7


models:
  llama:
    class: local_llm_v2
    init_args:
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      bits_and_bytes_args: null
      vllm_params:
        gpu_memory_utilization: 0.6
        max_lora_rank: 64
        dtype: "bfloat16"
      hf_model_init_kwargs:
        torch_dtype: "bfloat16"
      adapter_configs:
        fulldude:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 1e-5
            weight_decay: 0.0
          lora_kwargs:
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: ['k_proj', 'q_proj', 'v_proj', 'o_proj', "gate_proj", "down_proj", "up_proj"]



####################################################################################################
# TRAINING
####################################################################################################
default_kwargs: &default_kwargs
  train_func_args:
    gradient_checkpointing: true
    temperature: ${temperature}
    entropy_coef: 0.0
    kl_loss_coef: 0.0
    debug_enabled: True
    debug_log_path: "${oc.env:SCRATCH}/${experiment.name}/trainer_debugger"
  train_data_args:
    average_score_over_message: False

training:
  agents:
    alice:
      training_data_func_args:
        score_method: sum_rloo_scores
        score_method_kwargs:
          discount_factor: 1.0
    bob:
      training_data_func_args:
        score_method: sum_rloo_scores
        score_method_kwargs:
          discount_factor: 1.0
  llama:
    adapters:
      fulldude:
        train_func: train_reinforce_main
        <<: *default_kwargs
