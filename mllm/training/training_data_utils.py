import torch
from dataclasses import dataclass
import torch.nested as tn
from typing import Literal


def get_tokenwise_credits(
    # B := batch size, S := number of tokens / seq. length, T := number of states. `j` stands for jagged (see pytorch nested tensors.)
    batch_timesteps:      torch.IntTensor, # (B, jS),
    batch_credits:        torch.FloatTensor # (B, jT)
    ) -> torch.FloatTensor: # (B, jS)
    """
    TOWRITE
    """
    #TODO vectorize this code
    batch_token_credits = []
    for credits, timesteps in zip(batch_credits, batch_timesteps):
        token_credits = torch.zeros_like(timesteps)
        for idx, credit in enumerate(credits):
            token_credits[timesteps == idx] = credit
        batch_token_credits.append(token_credits)
    batch_token_credits = tn.nested_tensor(batch_token_credits, layout=torch.jagged)
    return batch_token_credits



@dataclass
class TrajectoryBatch:
    """
    Tensorized batch of trajectories.
    """
    # B := batch size, S := number of tokens / seq. length, T := number of states. `j` stands for jagged (see pytorch nested tensors.)
    rollout_ids: torch.IntTensor # (B,)
    batch_input_ids:      torch.LongTensor # (B, jS)
    batch_action_mask:    torch.BoolTensor # (B, jS)
    batch_timesteps:      torch.IntTensor # (B, jS)
    batch_state_ends_idx: torch.IntTensor # (B, jT)
    batch_rewards:        torch.FloatTensor # (B, jT)

    def __post_init__(self):
        B = self.rollout_ids.shape[0]
        for b in range(B):
            nb_rewards = self.batch_rewards[b].shape[0]-1
            nb_timesteps = torch.max(self.batch_timesteps[b]).item()
            assert nb_rewards == nb_timesteps, "Number of rewards and timesteps mismatch."
            assert self.batch_input_ids[b].shape[0] == self.batch_action_mask[b].shape[0] == self.batch_timesteps[b].shape[0], "Tensors must have the same shape along the jagged dimension."
            assert self.batch_state_ends_idx[b].shape[0] == self.batch_rewards[b].shape[0], "Tensors must have the same shape along the jagged dimension."


    """
    Entries:
        Here, we ignore the batch dimension.
        input_ids:
            All of the tokens of both the user and the assistant, flattened.
        action_mask:
            Set to true on the tokens of the assistant (tokens generated by the model).
        timesteps:
            Therefore, max(timesteps) = Ns - 1.
        state_ends_idx:
            Indices of the tokens at which state descriptions end.
        rewards:
            rewards[t] := R_t(s_t, a_t)
    Example:
        position:       "0  1  2  3  4  5  6  7  8  9  10 11 12 13 14"
        input_ids:      "U  U  U  a  a  a  U  a  U  a  a  a  U  U  U" (U := User, a := Assistant)
        action_mask:    "x  x  x  ✓  ✓  ✓  x  ✓  x  ✓  ✓  ✓  x  x  x"
        timestep:       "0  0  0  0  0  0  1  1  1  1  1  1  2  2  2"
        state_ends_dx:  [2, 6, 14]
        rewards:        [r0, r1, r2]
    """

    def __getitem__(self, key) -> "TrajectoryBatch":
        if isinstance(key, slice):
            ret = TrajectoryBatch(
                rollout_ids = self.rollout_ids,
                batch_input_ids = tn.nested_tensor(self.batch_input_ids.unbind().__getitem__(key), layout=torch.jagged),
                batch_action_mask = tn.nested_tensor(self.batch_action_mask.unbind().__getitem__(key), layout=torch.jagged),
                batch_timesteps = tn.nested_tensor(self.batch_timesteps.unbind().__getitem__(key), layout=torch.jagged),
                batch_state_ends_idx = tn.nested_tensor(self.batch_state_ends_idx.unbind().__getitem__(key), layout=torch.jagged),
                batch_rewards = tn.nested_tensor(self.batch_state_ends_idx.unbind().__getitem__(key), layout=torch.jagged)
            )
            return ret

    def __len__(self):
        return self.batch_input_ids.shape[0]

    def to(self, device):
        self.rollout_ids.to(device)
        self.batch_input_ids.to(device)
        self.batch_action_mask.to(device)
        self.batch_timesteps.to(device)
        self.batch_state_ends_idx.to(device)
        self.batch_rewards.to(device)

    def get_padded_input_ids(self, padding: float):
        """
        TOWRITE
        """
        padded_batch_input_ids = tn.to_padded_tensor(self.batch_input_ids, padding=padding)
        return padded_batch_input_ids

timestep = int


@dataclass
class PaddedTensorTrainingBatch:
    batch_input_ids:      torch.LongTensor
    batch_action_mask:    torch.BoolTensor
    batch_credits:        torch.FloatTensor
    def __len__(self):
        return self.batch_input_ids.shape[0]
    def to(self, device):
        self.batch_input_ids = self.batch_input_ids.to(device)
        self.batch_action_mask = self.batch_action_mask.to(device)
        self.batch_credits = self.batch_credits.to(device)

@dataclass
class TrainingBatch:
    rollout_ids: torch.IntTensor # (B,)
    batch_input_ids:      torch.LongTensor # (B, jS)
    batch_action_mask:    torch.FloatTensor # (B, jS)
    batch_credits: torch.FloatTensor # (B, jS)

    def __post_init__(self):
        # Ensure batch dimension is present
        if self.batch_input_ids.dim() == 1:
            self.batch_input_ids = self.batch_input_ids.unsqueeze(0)
            self.batch_action_mask = self.batch_action_mask.unsqueeze(0)
            self.batch_credits = self.batch_credits.unsqueeze(0)
        B = self.rollout_ids.shape[0]
        for b in range(B):
            assert self.batch_input_ids[b].shape[0] == self.batch_action_mask[b].shape[0] == self.batch_credits[b].shape[0], "Tensors must have the same shape along the jagged dimension."

    def __getitem__(self, key) -> "TrainingBatch":
        if isinstance(key, slice):
            ret = TrainingBatch(
                rollout_ids = self.rollout_ids.__getitem__(key),
                batch_input_ids = tn.nested_tensor(self.batch_input_ids.unbind().__getitem__(key), layout=torch.jagged),
                batch_action_mask = tn.nested_tensor(self.batch_action_mask.unbind().__getitem__(key), layout=torch.jagged),
                batch_credits = tn.nested_tensor(self.batch_credits.unbind().__getitem__(key), layout=torch.jagged)
            )
            return ret

    def __len__(self):
        return self.batch_input_ids.shape[0]

    def to(self, device):
        self.rollout_ids = self.rollout_ids.to(device)
        self.batch_input_ids = self.batch_input_ids.to(device)
        self.batch_action_mask = self.batch_action_mask.to(device)
        self.batch_credits = self.batch_credits.to(device)

    def get_padded_tensors(self, padding: float = 0.0):
        """
        TOWRITE
        Always pad to the right.
        """
        padded_batch_input_ids = tn.to_padded_tensor(self.batch_input_ids, padding=padding)
        padded_batch_action_mask = tn.to_padded_tensor(self.batch_action_mask, padding=padding)
        padded_batch_credits = tn.to_padded_tensor(self.batch_credits, padding=padding)



        return PaddedTensorTrainingBatch(padded_batch_input_ids, padded_batch_action_mask, padded_batch_credits)

timestep = int
