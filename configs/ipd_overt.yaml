hydra:
  run:
    dir: "${oc.env:SCRATCH}/llm_negotiation/${experiment.name}"
  job:
    chdir: false

experiment:
  name: "ipd_overt"
  # name: "${now:%Y-%m-%d___%H-%M-%S}_IPD"
  method: generate_and_train
  description: "no description"
  nb_epochs: 2000
  nb_matches_per_iteration: 64
  reinit_matches_each_it: true
  checkpoint_every_n_iterations: 25
  start_epoch: 0
  resume_experiment: true
  base_seed: 1000

####################################################################################################
#                                           GENERATION
####################################################################################################

common_agent_kwargs: &common_agent_kwargs
  max_errors: 0
  allow_reasoning: false
  max_reasoning_chars: 0

  intro_prompt: |
    Welcome to Iterated Prisoner's Dilemma. You are playing against another agent.

    Game Rules:
    1. In each round, both agents simultaneously choose to either play Cooperate or Defect.
    2. The payoffs for each round are as follows:
       - If both agents play Cooperate, both receive 3 points.
       - If both agents play Defect, both receive 1 points.
       - If one agent plays Cooperate and the other plays Defect, the player who plays Cooperate receives 0 points and the player who plays Defect receives 5 points.
    3. The goal is to maximize your total points over all rounds.
    4. You will see the previous play of the other player after the completion of a round.

    In order to play Cooperate, you must write "<Cooperate>" and nothing else.
    In order to play Defect, you must write "<Defect>" and nothing else.

    During the entire game, you must only write "<Cooperate>" or "<Defect>". If you write anything else, your response will be considered as an error.

    The game starts now:

  goal_prompt: |
    Your goal is to maximize your utility.

matches:
  env_class: "IPDEnv"
  agent_class: "IPDAgent"

  max_length: 30
  continuation_prob: 0.85
  same_length_batch: True

  # If set to 0, existing setup will run where utilities are randomly sampled at the beginning of each round.
  # If set to 1, all utilities will be calculated beforehand but rloo rewards will also be only calculated per game.
  nb_matches_with_same_roundwise_utilities: 0

  log_func: log_ipd_match

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1


  env_kwargs:
    agents: ['Alice', 'Bob']
    reward: 3
    punishment: 1
    temptation: 5
    sucker: 0


  agents:
    Alice:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Alice"
        policy_id: "qwen/sp_adapter"
    Bob:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Bob"
        policy_id: "qwen/sp_adapter"

temperature: 0.7


models:
  qwen:
    name: 'qwen'
    class: local_llm_v2
    init_args:
      max_model_length: 10000
      device: "cuda"
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      bits_and_bytes_args: null
      generation_args:
        max_new_tokens: 120
        do_sample: True
        temperature: ${temperature}
        top_k: 0
        top_p: 1.0
        repetition_penalty: 1
      vllm_params:
        max_model_len : 13e3
        gpu_memory_utilization: 0.45
        enable_lora: True
        enable_prefix_caching: True
        enable_sleep_mode : True
        max_lora_rank: 64
        dtype: "bfloat16"
      hf_model_init_kwargs:
        torch_dtype: "bfloat16"
        device_map: "auto"
        attn_implementation: "flash_attention_2"
      eval_with: "vllm"
      train_with: "hf"
      optimizer_on_gpu_during_training: True
      fully_switch_vllm_weights_after_training: False
      keep_vllm_during_training: True
      keep_vllm_during_eval: True
      sleep_vllm_during_training: True
      wake_vllm_during_eval: True
      export_trained_parameters: True
      export_optimizer: True
      adapter_configs:
        sp_adapter:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 1e-5
            weight_decay: 0.0
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"


####################################################################################################
# TRAINING
####################################################################################################


training:

  agents:

    Alice:
      training_data_func: ipd_generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        debug_output: True
        score_method: rloo_scores
        score_method_kwargs:
          discount_factor: 1.0

    Bob:
      training_data_func: ipd_generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        debug_output: True
        score_method: rloo_scores
        score_method_kwargs:
          discount_factor: 1.0

  qwen:
    adapters:
      sp_adapter:
        train_func: train_reinforce_main
        train_func_args:
          gradient_checkpointing: true
          temperature: ${temperature}
          entropy_coef: 0.1
          kl_loss_coef: 0.0
          debug_enabled: True
          debug_log_path: "${oc.env:SCRATCH}/${experiment.name}/trainer_debugger"
        train_data_args:
          average_score_over_message: False
