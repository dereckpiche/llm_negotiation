hydra:
  run:
    dir: "${oc.env:SCRATCH}/llm_negotiation/${experiment.name}"
  job:
    chdir: false

experiment:
  name: "gpt_vs_qwen_${now:%Y-%m-%d___%H-%M-%S}"
  method: generate_and_train
  description: "Agents trained on negotiation game with regular RL."
  nb_epochs: 2000
  nb_matches_per_iteration: 64
  checkpoint_every_n_iterations: 25
  reinit_matches_each_it: true
  start_epoch: 0
  resume_experiment: true
  base_seed: 344

####################################################################################################
#                                           GENERATION
####################################################################################################


common_agent_kwargs: &common_agent_kwargs
  max_errors: 0
  allow_reasoning: false
  max_reasoning_chars: 0
  message_parser: regular_message_parser
  finalization_parser: regular_finalization_parser
  finalization_parser_kwargs:
    attribution_map:
      Alice:
        i_take_key: "Alice"
        other_takes_key: "Bob"
      Bob:
        i_take_key: "Bob"
        other_takes_key: "Alice"

  intro_prompt: |
      Welcome, you are participating in a negotiation game. You are playing as the agent named {agent_name}. The other agent with whom you are playing is named {coagent_name}. In this game, two agents bargain over how to divide items over multiple rounds. The number of rounds is incredibly high. Each agent only knows their own item values. Each item has different values for each agent. Each agent submits their own proposal for how to divide the items.

  dond_version_specificities: |
    Each round is played as follows:
    1) One agent is designated as the starting agent
    2) The starting agent sends a message
    3) The second agent sends a message
    4) The starting agent sends a finalization
    5) The second agent sends a finalization

    Each agent can only send 1 message per round.
    The starting agent alternates each round.

    Each agent only knows the values of items from their own perspective. You cannot see the other agent's item values unless they explicitly share this information in their messages. And vice versa. You are strongly encouraged to use your messages to discuss item values with the other agent.

    The items obtained by each agent at the end of a round are proportional to the number of items they proposed for themselves in their finalization. First example: If both players propose to take all the items, they both get half. Second example: If one agent proposes to take all the items and the other agent proposes to take none, the first agent gets all the items and the second agent gets nothing.

    Your round points are calculated by multiplying the number of each item you get by how much you value that item. For example, if you get 7 coins and you value each coin at 5 points, you'll earn 35 points for the round.

  message_mechanics_prompt: |
    Your messages must be sent in this exact format:
    <message>
    your-message-here
    </message>
    There must be no extraneous text outside the tags. They should be concise - less than 400 characters. Finalization is not allowed in messages.

  finalization_mechanics_prompt: |
    Your finalizations must be sent in this exact format:
    <finalize>
    {finalize_sample}
    </finalize>
    There must be no extraneous text outside the tags. Your proposal should be consistent with itself - the sum of items that you propose for yourself and the items that you propose for the other agent should equal the total available items.

  reasoning_mechanics_prompt: |
    In order to think before you act, you are allowed to use {max_reasoning_chars} characters per round. When reasoning, please follow these rules; 1) Enclose your thought process within <think> and </think> tags. 2) Do not include extraneous content outside these tags. 3) Your reasoning is limited to {max_reasoning_chars} characters per response. 4) You are not obligated to reason at each response.

  goal_prompt: |
    Your goal is to maximize the sum of the points you will accumulate over the course of the game, not the points of a single round.

  first_round_prompt: |
    For this round, the items available are {quantities} and your values for each item category are {values}.

  new_round_prompt: |
    The previous round has ended. In the previous round, the values of {coagent_name} were {last_round_coagent_values}.

    A new round has started. For this round, the items available are {quantities} and your values for each item category are {values}.
  agent_with_first_move_prompt: |
   You are the starting agent.
  agent_with_second_move_prompt: |
    You are the second agent.
  received_message_prompt: |
    The other agent sent the following message: {last_message}.
  other_agent_finalized_prompt: |
    The other agent has finalized.
  time_to_finalize_prompt: |
    (You must now finalize in the correct format.)
  time_to_send_message_prompt: |
    (You must now send a message in the correct format.)

matches:
  env_class: "DondEnv"
  agent_class: "DondAgent"

  max_length: 30
  continuation_prob: 0.85
  same_length_batch: True

  # If set to 0, existing setup will run where utilities are randomly sampled at the beginning of each round.
  # If set to 1, all utilities will be calculated beforehand but rloo rewards will also be only calculated per game.
  nb_matches_with_same_roundwise_utilities: 8

  log_func: dond_log_match
  log_func_args:
    metrics_func: gather_dond_statistics
    metrics_func_args:
      format_options: ["by_agent"]
      stats_to_log: [
        "agreement_percentage",
        "round_values",
        "round_points",
        "optimal_points_difference",
        "items_given_to_self_percentage",
        "more_items_to_value_more_percentage",
        "more_items_to_value_more_percentage",

        "total_agreement_percentage",
        "total_points_difference",
        "total_optimal_points_difference",
        "total_imbalance",
        "total_average_imbalance",
        "total_sum_points_percentage_of_max",
        "total_more_items_to_value_more_percentage",
        "total_items_given_to_self_percentage",
        "number_of_rounds"
      ]

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1


  env_kwargs:
    agents: ['Alice', 'Bob']
    max_messages: 1
    min_messages: 1
    max_chars_per_message: 450
    mode: basic
    random_setup_func: independent_random_vals
    random_setup_kwargs:
      items: ["coins"]
      min_quant: 10
      max_quant: 10
      min_val: 1
      max_val: 20
    role_assignator_func: alternating_role_assignator
    role_assignator_func_kwargs: {}
    points_attribution_method: negotiation_payoff
    points_attributions_kwargs: {}

  agents:
    Alice:
      kwargs:
        <<: *common_agent_kwargs
        agent_name: "Alice"
        policy_id: "gpt/ad_alice"
    Bob:
      kwargs:
        <<: *common_agent_kwargs
        agent_name: "Bob"
        policy_id: "qwen/ad_bob"

temperature: 0.7


models:
  qwen:
    name: 'qwen'
    class: local_llm_v2
    init_args:
      max_model_length: 10000
      device: "cuda"
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      bits_and_bytes_args: null
      generation_args:
        max_new_tokens: 120
        do_sample: True
        temperature: ${temperature}
        top_k: 0
        top_p: 1.0
        repetition_penalty: 1
      vllm_params:
        max_model_len : 13e3
        gpu_memory_utilization: 0.6
        enable_lora: True
        enable_prefix_caching: True
        enable_sleep_mode : True
        max_lora_rank: 64
        dtype: "bfloat16"
      hf_model_init_kwargs:
        torch_dtype: "bfloat16"
        device_map: "auto"
        attn_implementation: "flash_attention_2"
      eval_with: "vllm"
      train_with: "hf"
      optimizer_on_gpu_during_training: True
      fully_switch_vllm_weights_after_training: False
      keep_vllm_during_training: True
      keep_vllm_during_eval: True
      sleep_vllm_during_training: True
      wake_vllm_during_eval: True
      export_trained_parameters: True
      export_optimizer: True
      adapter_configs:
        ad_bob:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 1e-5
            weight_decay: 0.0
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"
  gpt:
    class: server_llm
    init_args:
      api_key: <api-key>
      model: gpt-4o


####################################################################################################
# TRAINING
####################################################################################################


training:

  agents:

    Alice:
      training_data_func: generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        debug_output: True
        score_method: rloo_scores
        score_method_kwargs:
          discount_factor: 1.0

    Bob:
      training_data_func: generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        debug_output: True
        score_method: rloo_scores
        score_method_kwargs:
          discount_factor: 1.0

  qwen:
    adapters:
      ad_bob:
        train_func: train_reinforce_main
        train_func_args:
          gradient_checkpointing: true
          temperature: ${temperature}
          entropy_coef: 0.0
          kl_loss_coef: 0.0
          debug_enabled: True
          debug_log_path: "${hydra:run.dir}/trainer_debugger"
        train_data_args:
          average_score_over_message: False
