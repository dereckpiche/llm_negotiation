hydra:
    run:
        dir: "${oc.env:SCRATCH}/llm_negotiation/${now:%Y_%m}/${experiment.name}"

    job:
        chdir: false

experiment:
    name: "ipd"
    method: generate_and_train
    description: "no description"
    nb_epochs: 10
    nb_matches_per_iteration: 16
    reinit_matches_each_it: true
    checkpoint_every_n_iterations: 25
    start_epoch: 0
    resume_experiment: true
    base_seed: 1000

####################################################################################################
#                                           GENERATION
####################################################################################################

common_agent_kwargs: &common_agent_kwargs
    max_errors: 0
    allow_reasoning: false
    max_reasoning_chars: 0
    cooperate_strings: ["C"]
    defect_strings: ["D"]
    strategy_prompt: ""
    intro_prompt: |
        Welcome to an iterated game. You are playing against another agent.

        Game Rules:
        1. In each round, both agents simultaneously choose to either play A or B.
        2. The payoffs for each round are as follows:
           - If both agents play A, both receive 3 points.
           - If both agents play B, both receive 1 points.
           - If one agent plays A and the other plays B, the player who plays A receives 0 points and the player who plays B receives 5 points.
        3. The goal is to maximize your total points over all rounds.
        4. You will see the previous play of the other player after the completion of a round.

        In order to play A, you must write "<A>" and nothing else.
        In order to play B, you must write "<B>" and nothing else.

        During the entire game, you must only write "<A>" or "<B>". If you write anything else, your response will be considered as an error.

        The game starts now:

    goal_prompt: |
        Your goal is to maximize your utility.

agent_0_id: "Alice"
agent_1_id: "Bob"
agent_ids: ["Alice", "Bob"]

markov_games:
    runner_method_name: "LinearRunner"
    simulation_class_name: "IPD"
    simulation_init_args:
        rounds_per_game: 16
        cooperate_actions: ["C"]
        defect_actions: ["D"]
        agent_ids: ${agent_ids}
        reward: 3
        punishment: 1
        temptation: 5
        sucker: 0
    agents:
        0:
            agent_id: ${agent_0_id}
            agent_class_name: "IPDAgent"
            policy_id: "qwen/self_play_agent"
            init_kwargs:
                <<: *common_agent_kwargs

        1:
            agent_id: ${agent_1_id}
            agent_class_name: "IPDAgent"
            policy_id: "qwen/self_play_agent"
            init_kwargs:
                <<: *common_agent_kwargs

    log_func: log_ipd_match

    run_batched_matches_args:
        nb_parallel_matches: -1

temperature: 1.0

models:
    qwen:
        class: DummyLocalLLM
        init_args:
            name: "qwen"
            model_name: Qwen/Qwen2.5-0.5B-Instruct # HuggingFaceTB/SmolLM-135M # "Qwen/Qwen2.5-7B-Instruct"
            adapter_configs:
                self_play_agent:
                    task_type: "CAUSAL_LM"
                    r: 32
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"
                self_play_critic:
                    task_type: "CAUSAL_LM"
                    r: 32
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"

critics:
    self_play_agent_critic:
        pointer: ["qwen", "self_play_critic"]

optimizers:
    self_play_agent_optimizer:
        pointer: ["qwen", "self_play_agent"]
        optimizer_class: torch.optim.Adam
        init_args:
            lr: 1e-6
            weight_decay: 0.0

    self_play_agent_critic_optimizer:
        pointer: self_play_agent_critic
        optimizer_class: torch.optim.Adam
        init_args:
            lr: 1e-6
            weight_decay: 0.0

common_trainer_kwargs: &common_trainer_kwargs
    entropy_coeff: 0.0
    kl_coeff: 0.0
    gradient_clipping: 1.0
    restrict_tokens: null
    mini_batch_size: 4
    use_gradient_checkpointing: True
    temperature: ${temperature}
    device: "cuda:0"

    discount_factor: 1.0
    use_gae: True
    gae_lambda_for_credits: 0.92
    gae_lambda_for_targets: 0.92
    end_at_last_state_flag: True # if true, will not train on last round actions

    # Reward Shaping
    use_sum_credits: False
    use_advantage_alignment: False
    ad_align_normalize_advantages: False
    ad_align_force_coop_first_step: False
    use_sign_in_ad_align: False
    ad_align_clipping: null
    use_time_regularization_in_ad_align: False
    use_variance_regularization_in_ad_align: False
    ad_align_beta: 1.0

trainers:
    self_play_trainer:
        pointers:
            model: ["qwen", "self_play_agent"]
            optimizer: "self_play_agent_optimizer"
            critic: "self_play_agent_critic"
            critic_optimizer: "self_play_agent_critic_optimizer"
        kwargs:
            <<: *common_trainer_kwargs

train_on_which_data:
    self_play_trainer: ${agent_ids} # take gradient steps on the reinforce data of both agents at once
