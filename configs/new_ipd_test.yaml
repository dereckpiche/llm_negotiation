hydra:
    run:
        dir: "${oc.env:SCRATCH}/llm_negotiation/${now:%Y_%m}/${experiment.name}"

    job:
        chdir: false

experiment:
    name: "ipd"
    method: generate_and_train
    description: "no description"
    nb_epochs: 1
    nb_matches_per_iteration: 2
    reinit_matches_each_it: true
    checkpoint_every_n_iterations: 25
    start_epoch: 0
    resume_experiment: true
    base_seed: 1000

####################################################################################################
#                                           GENERATION
####################################################################################################

common_agent_kwargs: &common_agent_kwargs
    max_errors: 0
    allow_reasoning: false
    max_reasoning_chars: 0
    cooperate_strings: ["C"]
    defect_strings: ["D"]
    strategy_prompt: ""
    intro_prompt: |
        Welcome to an iterated game. You are playing against another agent.

        Game Rules:
        1. In each round, both agents simultaneously choose to either play A or B.
        2. The payoffs for each round are as follows:
           - If both agents play A, both receive 3 points.
           - If both agents play B, both receive 1 points.
           - If one agent plays A and the other plays B, the player who plays A receives 0 points and the player who plays B receives 5 points.
        3. The goal is to maximize your total points over all rounds.
        4. You will see the previous play of the other player after the completion of a round.

        In order to play A, you must write "<A>" and nothing else.
        In order to play B, you must write "<B>" and nothing else.

        During the entire game, you must only write "<A>" or "<B>". If you write anything else, your response will be considered as an error.

        The game starts now:

    goal_prompt: |
        Your goal is to maximize your utility.

agent_0_id: "Alice"
agent_1_id: "Bob"
agent_ids: ["Alice", "Bob"]

markov_games:
    runner_method_name: "AlternativeActionsRunner"
    simulation_class_name: "IPD"
    simulation_init_args:
        rounds_per_game: 5
        cooperate_actions: ["C"]
        defect_actions: ["D"]
        agent_ids: ${agent_ids}
        reward: 3
        punishment: 1
        temptation: 5
        sucker: 0
    agents:
        0:
            agent_id: ${agent_0_id}
            agent_class_name: "IPDAgent"
            policy_id: "qwen/self_play_agent"
            init_kwargs:
                <<: *common_agent_kwargs

        1:
            agent_id: ${agent_1_id}
            agent_class_name: "IPDAgent"
            policy_id: "qwen/self_play_agent"
            init_kwargs:
                <<: *common_agent_kwargs

    max_length: 30
    continuation_prob: 0.85
    same_length_batch: True

    log_func: log_ipd_match

    run_batched_matches_args:
        nb_parallel_matches: -1

temperature: 1.0

models:
    qwen:
        class: DummyLocalLLM
        init_args:
            name: "qwen"
            max_model_length: 1e4
            device: "cuda"
            model_name: "Qwen/Qwen2.5-7B-Instruct"
            generation_args:
                max_new_tokens: 120
                do_sample: True
                temperature: ${temperature}
                top_k: 0
                top_p: 1.0
                repetition_penalty: 1
            vllm_params:
                max_model_len: 13e3
                gpu_memory_utilization: 0.45
                enable_lora: True
                enable_prefix_caching: True
                enable_sleep_mode: True
                max_lora_rank: 64
                dtype: "bfloat16"
            shared_hf_llm_init_kwargs:
                torch_dtype: "bfloat16"
                device_map: "auto"
                attn_implementation: "flash_attention_2"
            adapter_configs:
                self_play_agent:
                    task_type: "CAUSAL_LM"
                    r: 64
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"
                self_play_critic:
                    task_type: "CAUSAL_LM"
                    r: 64
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"

critics: {}
#     self_play_agent_critic:
#         pointer: ["qwen", "self_play_critic"]

optimizers: {}
#     self_play_agent_optimizer:
#         pointer: ["qwen", "self_play_agent"]
#         optimizer_class: torch.optim.Adam
#         init_args:
#             lr: 1e-6
#             weight_decay: 0.0

#     self_play_agent_critic_optimizer:
#         pointer: self_play_agent_critic
#         optimizer_class: torch.optim.Adam
#         init_args:
#             lr: 1e-6
#             weight_decay: 0.0

trainers: {}
    # qwen_trainer:
    #     pointers:
    #         model: ["qwen", "self_play_agent"]
    #         optimizer: "self_play_agent_optimizer"
    #         critic: "self_play_agent_critic"
    #         critic_optimizer: "self_play_agent_critic_optimizer"

    #     init_args:
    #         entropy_coeff: 0.0
    #         kl_coeff: 0.0
    #         gradient_clipping: 1.0
    #         restrict_tokens: null
    #         mini_batch_size: 1
    #         use_gradient_checkpointing: True
    #         temperature: ${temperature}
    #         device: "cuda:0"

    #         discount_factor: 1.0
    #         use_gae: False
    #         gae_lambda_for_credits: 0.92
    #         gae_lambda_for_targets: 0.92
    #         end_at_last_state_flag: True # if true, will not train on last round actions

    #         # Reward Shaping
    #         use_sum_credits: False
    #         use_advantage_alignment: False
    #         ad_align_normalize_advantages: False
    #         ad_align_force_coop_first_step: False
    #         use_sign_in_ad_align: False
    #         ad_align_clipping: null
    #         use_time_regularization_in_ad_align: False
    #         use_variance_regularization_in_ad_align: False
    #         ad_align_beta: 1.0

    #         log_ctz_next_token: True
    #         log_ctz_next_token_credit: True
    #         log_ctz_next_token_log_prob: True
    #         log_ctz_next_token_prob: True
    #         log_ctz_top_clogÏ€: True
    #         log_ctz_entropy: True
    #         log_ctz_kl: True

training:
    agents:
        ${agent_id_0}:
            training_data_func: ipd_generate_training_data_from_raw
            training_data_func_args:
                exclude_errors: False
                normalize_round_points: True

        ${agent_id_1}:
            training_data_func: ipd_generate_training_data_from_raw
            training_data_func_args:
                exclude_errors: False
                normalize_round_points: True
