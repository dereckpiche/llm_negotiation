hydra:
    run:
        dir: "${oc.env:SCRATCH}/llm_negotiation/${now:%Y_%m}/${experiment.name}"
    job:
        chdir: false

experiment:
    name: "trust_and_split"
    method: generate_and_train
    description: "Trust-and-Split negotiation game"
    nb_epochs: 10
    nb_matches_per_iteration: 32
    reinit_matches_each_it: true
    checkpoint_every_n_iterations: 25
    start_epoch: 0
    resume_experiment: true
    base_seed: 1001

# Top-level agent ids for convenience
agent_0_id: Alice
agent_1_id: Bob
agent_ids: ["Alice", "Bob"]

####################################################################################################
#                                           GENERATION
####################################################################################################

common_agent_kwargs: &common_agent_kwargs
    goal: "Maximize your total points over the whole game."

markov_games:
    runner_method_name: LinearRunner
    runner_kwargs: {}

    simulation_class_name: TrustAndSplitSimulation
    simulation_init_args:
        agent_ids: ${agent_ids}
        rounds_per_game: 10
        quota_messages_per_agent_per_round: 1
        max_coins: 10

    agents:
        0:
            agent_id: ${agent_0_id}
            agent_class_name: TrustAndSplitAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

        1:
            agent_id: ${agent_1_id}
            agent_class_name: TrustAndSplitAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

    run_batched_matches_args:
        nb_parallel_matches: -1

temperature: 1.0

models:
    base_llm:
        class: LeanLocalLLM
        init_args:
            llm_id: base_llm
            model_name: Qwen/Qwen3-4B-Instruct-2507
            inference_backend: vllm
            hf_kwargs:
                device_map: "auto"
                torch_dtype: "bfloat16"
                max_memory: {0: "15GiB"}
                attn_implementation: "flash_attention_2"
            inference_backend_init_kwargs:
                enable_prefix_caching: True
                max_model_len: 1e4
                gpu_memory_utilization: 0.5
                dtype: "bfloat16"
                trust_remote_code: True
                max_lora_rank: 32
            inference_backend_sampling_params:
                temperature: ${temperature}
                top_p: 1.0
                max_tokens: 400
                top_k: 0
            adapter_configs:
                agent_adapter:
                    task_type: CAUSAL_LM
                    r: 32
                    lora_alpha: 64
                    lora_dropout: 0.0
                    target_modules: all-linear
                critic_adapter:
                    task_type: CAUSAL_LM
                    r: 32
                    lora_alpha: 64
                    lora_dropout: 0.0
                    target_modules: all-linear

critics:
    agent_critic:
        module_pointer: ["base_llm", "critic_adapter"]

optimizers:
    agent_optimizer:
        module_pointer: ["base_llm", "agent_adapter"]
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 1e-6
            weight_decay: 0.0

    critic_optimizer:
        module_pointer: agent_critic
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 1e-6
            weight_decay: 0.0

trainers:
    agent_trainer:
        class: TrainerNaive
        module_pointers:
            policy: ["base_llm", "agent_adapter"]
            policy_optimizer: agent_optimizer
            critic: agent_critic
            critic_optimizer: critic_optimizer
        kwargs:
            entropy_coeff: 0.0
            kl_coeff: 0.0
            gradient_clipping: 1.0
            restrict_tokens: null
            mini_batch_size: 1
            use_gradient_checkpointing: True
            temperature: ${temperature}
            device: "cuda:0"
            use_gae: False
            skip_discounted_state_visitation: True
            gae_lambda_for_credits: 0.92
            gae_lambda_for_targets: 0.92
            discount_factor: 0.9
            use_rloo: True
            enable_tokenwise_logging: False
            pg_loss_normalization: "batch"

train_on_which_data:
    agent_trainer: ${agent_ids}
