hydra:
  run:
    dir: "${oc.env:SCRATCH}/${experiment.name}"
  job:
    chdir: false

experiment:
  name: "${now:%Y-%m-%d___%H-%M-%S}_no_name"
  method: generate_and_train
  description: "no description"
  nb_epochs: 2000
  nb_matches_per_iteration: 64
  reinit_matches_each_it: true
  start_epoch: 0
  resume_experiment: true
  base_seed: 1000

####################################################################################################
#                                           GENERATION
####################################################################################################


common_agent_kwargs: &common_agent_kwargs
  max_errors: 0
  allow_reasoning: false
  max_reasoning_chars: 0

  intro_prompt: |
    Welcome to the Iterated Prisoner's Dilemma game! You are playing against another agent.


    Game Rules:
    1. You and another agent will play multiple rounds of the Prisoner's Dilemma.
    2. In each round, both agents simultaneously choose to either Cooperate (C) or Defect (D).
    3. The payoffs for each round are as follows:
       - If both agents cooperate, both receive 3 points.
       - If both agents defect, both receive 1 point.
       - If one agent cooperates and the other defects, the cooperator receives 0 points and the defector receives 5 points.
    4. The goal is to maximize your total points over all rounds.
    5. You will see the history of previous rounds, including both agents' choices and points earned.

    In order to cooperate, you must write "<Cooperate>" and nothing else.
    In order to defect, you must write "<Defect>" and nothing else.

    During the entire game, you must only write "<Cooperate>" or "<Defect>". If you write anything else, your response will be considered as an error.

    The game starts now:

  goal_prompt: |
    Your goal is to maximize your utility.

matches:
  env_class: "IPDEnv"
  agent_class: "IPDAgent"

  max_length: 1
  continuation_prob: 1
  same_length_batch: True

  # If set to 0, existing setup will run where utilities are randomly sampled at the beginning of each round.
  # If set to 1, all utilities will be calculated beforehand but rloo rewards will also be only calculated per game.
  nb_matches_with_same_roundwise_utilities: False

  log_func: log_ipd_match
  log_func_args:
    metrics_func: gather_ipd_statistics
    metrics_func_args:
      format_options: ["by_agent"]
      stats_to_log: [
        "agreement_percentage",
        "round_values",
        "round_points",
        "optimal_points_difference",
        "items_given_to_self_percentage",
        "more_items_to_value_more_percentage",
        "more_items_to_value_more_percentage",

        "total_agreement_percentage",
        "total_points_difference",
        "total_optimal_points_difference",
        "total_imbalance",
        "total_average_imbalance",
        "total_sum_points_percentage_of_max",
        "total_more_items_to_value_more_percentage",
        "total_items_given_to_self_percentage",
        "number_of_rounds"
      ]

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1


  env_kwargs:
    agents: ['Alice', 'Bob']


  agents:
    Alice:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Alice"
        policy_id: "qwen/sp_adapter"
    Bob:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Bob"
        policy_id: "qwen/sp_adapter"

temperature: 0.7


models:
  qwen:
    name: 'qwen'
    class: local_llm_v2
    init_args:
      max_model_length: 10000
      device: "cuda"
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      bits_and_bytes_args: null
      generation_args:
        max_new_tokens: 120
        do_sample: True
        temperature: ${temperature}
        top_k: 0
        top_p: 1.0
        repetition_penalty: 1
      vllm_params:
        max_model_len : 13e3
        gpu_memory_utilization: 0.45
        enable_lora: True
        enable_prefix_caching: True
        enable_sleep_mode : True
        max_lora_rank: 64
        dtype: "bfloat16"
      hf_model_init_kwargs:
        torch_dtype: "bfloat16"
        device_map: "auto"
        attn_implementation: "flash_attention_2"
      eval_with: "vllm"
      train_with: "hf"
      optimizer_on_gpu_during_training: True
      fully_switch_vllm_weights_after_training: False
      keep_vllm_during_training: True
      keep_vllm_during_eval: True
      sleep_vllm_during_training: True
      wake_vllm_during_eval: True
      export_trained_parameters: True
      export_optimizer: True
      adapter_configs:
        sp_adapter:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 1e-5
            weight_decay: 0.0
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"


####################################################################################################
# TRAINING
####################################################################################################

common_training_agent_kwargs: &common_training_agent_kwargs
  training_data_func: generate_training_data_from_raw
  training_data_func_args:
    exclude_errors: False
    debug_output: True
    score_method: rloo_advantage_alignment_scores
    score_method_kwargs:
      discount_factor: 1.0


training:

  agents:
    Alice:
      <<: *common_training_agent_kwargs
    Bob:
      <<: *common_training_agent_kwargs

  qwen:
    adapters:
      sp_adapter:
        train_func: train_reinforce_main
        train_func_args:
          gradient_checkpointing: true
          temperature: ${temperature}
          entropy_coef: 0.0
          kl_loss_coef: 0.0
          debug_enabled: True
          debug_log_path: "${oc.env:SCRATCH}/${experiment.name}/trainer_debugger"
        train_data_args:
          average_score_over_message: False
