# To quickly check if code still runs
experiment:
    name: "debug_nego_${now:%Y-%m-%d___%H-%M-%S}"
    nb_epochs: 10
    nb_matches_per_iteration: 8
    train: true

defaults:
    - tas_rps_ad_align.yaml

optimizers:
    agent_optimizer:
        module_pointer: ["base_llm", "agent_adapter"]
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 1e-2
            weight_decay: 0.0

    critic_optimizer:
        module_pointer: agent_critic
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 3e-6
            weight_decay: 0.0

common_agent_kwargs: &common_agent_kwargs
    goal: "Maximize your total points over the whole game."
    exploration_prompts: [
        "In this round, be a cooperative agent. Maximize the sum of your points and the other agent's points.",
        "In this round, be a greedy agent. Maximize your own points regardless of the other agent's points."
    ]
    exploration_prompt_probs: [0.25, 0.25]

models:
    base_llm:
        class: LeanLocalLLM
        init_args:
            llm_id: base_llm
            enable_thinking: False
            model_name: "Qwen/Qwen3-8B" # Qwen/Qwen3-4B-Instruct-2507
            inference_backend: vllm

markov_games:
    agents:
        0:
            agent_id: ${agent_0_id}
            agent_name: Alice
            agent_class_name: NoPressAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

        1:
            agent_id: ${agent_1_id}
            agent_name: Bob
            agent_class_name: NoPressAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

    simulation_init_args:
        nb_of_rounds: 10
trainers:
    agent_trainer:
        kwargs:
            kl_coeff: 1e5
            enable_tokenwise_logging: True
            use_gradient_checkpointing: True
            
