defaults:
    - base.yaml

lora_count: 30
experiment:
    nb_epochs: 2000
    name: "tas_classic_ad_align"
    agent_buffer: true
    keep_agent_buffer_count: ${lora_count}
    agent_buffer_recent_k: -1
    checkpoint_every_n_iterations: 10
    seed_group_size: 8

common_agent_kwargs: &common_agent_kwargs
    goal: "Maximize your total points over the whole game."
    num_message_chars: 500

agent_0_id: Alice
agent_1_id: Bob
agent_ids: ["Alice", "Bob"]

markov_games:
    group_by_round: True # This will merge all timesteps of a round into a single timestep
    simulation_class_name: TrustAndSplitSimulation
    simulation_init_args:
        nb_of_rounds: 10
        quota_messages_per_agent_per_round: 1

    agents:
        0:
            agent_id: ${agent_0_id}
            agent_name: Alice
            agent_class_name: TrustAndSplitAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

        1:
            agent_id: ${agent_1_id}
            agent_name: Bob
            agent_class_name: TrustAndSplitAgent
            policy_id: base_llm/agent_adapter
            init_kwargs:
                <<: *common_agent_kwargs

temperature: 1.0

models:
    base_llm:
        init_args:
            model_name: Qwen/Qwen3-4B-Instruct-2507
            # enable_thiking: False
            regex_max_attempts: 3
            inference_backend_init_kwargs:
                enforce_eager: False
                gpu_memory_utilization: 0.5
                max_loras: ${lora_count}
                max_cpu_loras : ${lora_count}
                enable_sleep_mode: True
                enable_lora: True

optimizers:
    agent_optimizer:
        module_pointer: ["base_llm", "agent_adapter"]
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 1e-6
            weight_decay: 0.0

    critic_optimizer:
        module_pointer: agent_critic
        optimizer_class_name: torch.optim.Adam
        init_args:
            lr: 3e-6
            weight_decay: 0.0

trainers:
    agent_trainer:
        class: TrainerAdAlign
        kwargs:
            discount_factor: 1.0
            use_gradient_checkpointing: False
            mini_batch_size: 1

            # ad align
            ad_align_force_coop_first_step: False
            ad_align_clipping: null
            ad_align_gamma: 1.0
            ad_align_exclude_k_equals_t: False
            ad_align_use_sign: False
            ad_align_beta: 1.0
            use_old_ad_align: True
            use_time_regularization: True
            use_rloo: True
            rloo_branch: False
            reuse_baseline: False
            reward_normalizing_constant: 200.0
