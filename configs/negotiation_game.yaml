hydra:
    run:
        dir: "${oc.env:SCRATCH}/llm_negotiation/${experiment.name}"
    job:
        chdir: false

experiment:
    name: "negotiation_game"
    method: generate_and_train
    description: "Agents trained on negotiation game with regular RL."
    nb_epochs: 500
    nb_matches_per_iteration: 32
    checkpoint_every_n_iterations: 25
    reinit_matches_each_it: true
    start_epoch: 0
    resume_experiment: true
    base_seed: 1

common_agent_kwargs: &common_agent_kwargs
    max_errors: 0
    allow_reasoning: false
    max_reasoning_chars: 0
    message_parser: regular_message_parser
    finalization_parser: regular_finalization_parser
    finalization_parser_kwargs:
        attribution_map:
            Alice:
                i_take_key: "Alice"
                other_takes_key: "Bob"
            Bob:
                i_take_key: "Bob"
                other_takes_key: "Alice"

    intro_prompt: |
        Welcome, you are participating in a negotiation game. You are playing as the agent named {agent_name}. The other agent with whom you are playing is named {coagent_name}. In this game, two agents bargain over how to divide items over multiple rounds. The number of rounds is incredibly high. Each agent only knows their own item values. Each item has different values for each agent. Each agent submits their own proposal for how to divide the items.

    dond_version_specificities: |
        Each round is played as follows:
        1) One agent is designated as the starting agent
        2) The starting agent sends a message
        3) The second agent sends a message
        4) The starting agent sends a finalization
        5) The second agent sends a finalization

        Each agent can only send 1 message per round.
        The starting agent alternates each round.

        Each agent only knows the values of items from their own perspective. You cannot see the other agent's item values unless they explicitly share this information in their messages. And vice versa. You are strongly encouraged to use your messages to discuss item values with the other agent.

        The items obtained by each agent at the end of a round are proportional to the number of items they proposed for themselves in their finalization. First example: If both players propose to take all the items, they both get half. Second example: If one agent proposes to take all the items and the other agent proposes to take none, the first agent gets all the items and the second agent gets nothing.

        Your round points are calculated by multiplying the number of each item you get by how much you value that item. For example, if you get 7 coins and you value each coin at 5 points, you'll earn 35 points for the round.

    message_mechanics_prompt: |
        Your messages must be sent in this exact format:
        <message>
        your-message-here
        </message>
        There must be no extraneous text outside the tags. They should be concise - less than 400 characters. Finalization is not allowed in messages.

    finalization_mechanics_prompt: |
        Your finalizations must be sent in this exact format:
        <finalize>
        {finalize_sample}
        </finalize>
        There must be no extraneous text outside the tags. Your proposal should be consistent with itself - the sum of items that you propose for yourself and the items that you propose for the other agent should equal the total available items.

    reasoning_mechanics_prompt: |
        In order to think before you act, you are allowed to use {max_reasoning_chars} characters per round. When reasoning, please follow these rules; 1) Enclose your thought process within <think> and </think> tags. 2) Do not include extraneous content outside these tags. 3) Your reasoning is limited to {max_reasoning_chars} characters per response. 4) You are not obligated to reason at each response.

    goal_prompt: |
        Your goal is to maximize the sum of the points you will accumulate over the course of the game, not the points of a single round.

    first_round_prompt: |
        For this round, the items available are {quantities} and your values for each item category are {values}.

    new_round_prompt: |
        The previous round has ended. In the previous round, you proposed {last_round_agent_finalization} for yourself and had values {last_round_agent_values}. {coagent_name} proposed {last_round_coagent_finalization} for themselves and had values {last_round_coagent_values}.

        A new round has started. For this round, the items available are {quantities} and your values for each item category are {values}.
    agent_with_first_move_prompt: |
        You are the starting agent.
    agent_with_second_move_prompt: |
        You are the second agent.
    received_message_prompt: |
        The other agent sent the following message: {last_message}.
    other_agent_finalized_prompt: |
        The other agent has finalized.
    time_to_finalize_prompt: |
        (You must now finalize in the correct format.)
    time_to_send_message_prompt: |
        (You must now send a message in the correct format.)

matches:
    env_class: "DondEnv"

    agent_class: "DondAgent"

    max_length: 10
    continuation_prob: 1.0
    same_length_batch: True

    nb_matches_with_same_roundwise_utilities: 1

    log_func: dond_log_match

    stop_condition: game_over_condition
    stop_condition_kwargs: {}

    run_batched_matches_args:
        nb_parallel_matches: -1

    env_kwargs:
        agents: ["Alice", "Bob"]
        max_messages: 1
        min_messages: 1
        max_chars_per_message: 450
        mode: basic
        random_setup_func: independent_random_vals
        random_setup_kwargs:
            items: ["coins"]
            min_quant: 10
            max_quant: 10
            min_val: 1
            max_val: 20
        role_assignator_func: alternating_role_assignator
        role_assignator_func_kwargs: {}
        points_attribution_method: negotiation_payoff
        points_attributions_kwargs: {}

    agents:
        Alice:
            kwargs:
                <<: *common_agent_kwargs
                agent_name: "Alice"
                policy_id: "qwen/self_play_agent"
        Bob:
            kwargs:
                <<: *common_agent_kwargs
                agent_name: "Bob"
                policy_id: "qwen/self_play_agent"

temperature: 1.0

models:
    qwen:
        class: LeanLocalLLM
        init_args:
            max_model_length: 1e4
            device: "cuda"
            model_name: "Qwen/Qwen2.5-7B-Instruct"
            generation_args:
                max_new_tokens: 120
                do_sample: True
                temperature: ${temperature}
                top_k: 0
                top_p: 1.0
                repetition_penalty: 1
            vllm_params:
                max_model_len: 13e3
                gpu_memory_utilization: 0.45
                enable_lora: True
                enable_prefix_caching: True
                enable_sleep_mode: True
                max_lora_rank: 64
                dtype: "bfloat16"
            shared_hf_llm_init_kwargs:
                torch_dtype: "bfloat16"
                device_map: "auto"
                attn_implementation: "flash_attention_2"
            adapter_configs:
                self_play_agent:
                    task_type: "CAUSAL_LM"
                    r: 64
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"
                self_play_critic:
                    task_type: "CAUSAL_LM"
                    r: 64
                    lora_alpha: 128
                    lora_dropout: 0.0
                    target_modules: "all-linear"

critics:
    self_play_agent_critic:
        pointer: ["qwen", "self_play_critic"]

optimizers:
    self_play_agent_optimizer:
        pointer: ["qwen", "self_play_agent"]
        optimizer_class: torch.optim.Adam
        init_args:
            lr: 1e-5
            weight_decay: 0.0

    self_play_agent_critic_optimizer:
        pointer: self_play_agent_critic
        optimizer_class: torch.optim.Adam
        init_args:
            lr: 1e-5
            weight_decay: 0.0

trainers:
    qwen_trainer:
        pointers:
            model: ["qwen", "self_play_agent"]
            optimizer: "self_play_agent_optimizer"
            critic: "self_play_agent_critic"
            critic_optimizer: "self_play_agent_critic_optimizer"

        init_args:
            entropy_coeff: 0.0
            kl_coeff: 0.0
            gradient_clipping: 1.0
            restrict_tokens: null
            mini_batch_size: 1
            use_gradient_checkpointing: True
            temperature: ${temperature}
            device: "cuda:0"

            discount_factor: 0.9
            use_gae: True
            gae_lambda_for_credits: 0.92
            gae_lambda_for_targets: 0.92
            end_at_last_state_flag: True # if true, will not train on last round actions

            # Reward Shaping
            use_sum_credits: False
            use_advantage_alignment: False
            ad_align_normalize_advantages: False
            ad_align_force_coop_first_step: False
            use_sign_in_ad_align: False
            ad_align_clipping: null
            use_time_regularization_in_ad_align: False
            use_variance_regularization_in_ad_align: False
            ad_align_beta: 3.0

            log_ctz_next_token: True
            log_ctz_next_token_credit: True
            log_ctz_next_token_log_prob: True
            log_ctz_next_token_prob: True
            log_ctz_top_clogÏ€: True
            log_ctz_entropy: True
            log_ctz_kl: True

training:
    agents:
        Alice:
            training_data_func: dond_generate_training_data_from_raw
            training_data_func_args:
                exclude_errors: False
                debug_output: True
                substract_group_wise_loo_mean_rewards: False
                substract_loo_mean_rewards: False

        Bob:
            training_data_func: dond_generate_training_data_from_raw
            training_data_func_args:
                exclude_errors: False
                debug_output: True
                substract_group_wise_loo_mean_rewards: False
                substract_loo_mean_rewards: False
