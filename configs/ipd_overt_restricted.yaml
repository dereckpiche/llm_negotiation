hydra:
  run:
    dir: "${oc.env:SCRATCH}/llm_negotiation/${experiment.name}"
  job:
    chdir: false

experiment:
  name: "ipd_overt_restricted_${now:%Y-%m-%d___%H-%M-%S}"
  # name: "${now:%Y-%m-%d___%H-%M-%S}_IPD"
  method: generate_and_train
  description: "no description"
  nb_epochs: 2000
  nb_matches_per_iteration: 64
  reinit_matches_each_it: true
  checkpoint_every_n_iterations: 25
  start_epoch: 0
  resume_experiment: true
  base_seed: 948

####################################################################################################
#                                           GENERATION
####################################################################################################

common_agent_kwargs: &common_agent_kwargs
  max_errors: 3
  allow_reasoning: false
  max_reasoning_chars: 0

  intro_prompt: |
    Welcome to Iterated Prisoner's Dilemma. You are playing against another agent.

    Game Rules:
    1. In each round, both agents simultaneously choose to either play Cooperate or Defect.
    2. The payoffs for each round are as follows:
       - If both agents play Cooperate, both receive 3 points.
       - If both agents play Defect, both receive 1 points.
       - If one agent plays Cooperate and the other plays Defect, the player who plays Cooperate receives 0 points and the player who plays Defect receives 5 points.
    3. The goal is to maximize your total points over all rounds.
    4. You will see the previous play of the other player after the completion of a round.

    In order to play Cooperate, you must write "C" and nothing else.
    In order to play Defect, you must write "D" and nothing else.

    During the entire game, you must only write "C" or "D". If you write anything else, your response will be considered as an error.

    The game starts now:

  goal_prompt: |
    Your goal is to maximize your utility.

matches:
  env_class: "IPDEnv"
  agent_class: "IPDAgent"

  max_length: 30
  continuation_prob: 0.85
  same_length_batch: True

  # If set to 0, existing setup will run where utilities are randomly sampled at the beginning of each round.
  # If set to 1, all utilities will be calculated beforehand but rloo rewards will also be only calculated per game.
  nb_matches_with_same_roundwise_utilities: 0

  log_func: log_ipd_match

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1


  env_kwargs:
    cooperate_actions: ["C"]
    defect_actions: ["D"]
    agents: ['Alice', 'Bob']
    reward: 3
    punishment: 1
    temptation: 5
    sucker: 0


  agents:
    Alice:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Alice"
        policy_id: "qwen/sp_adapter"
    Bob:
      kwargs:
        <<: *common_agent_kwargs
        agent_id: "Bob"
        policy_id: "qwen/sp_adapter"

temperature: 2.0

models:
  qwen:
    name: 'qwen'
    class: local_llm_v2 
    init_args:
      max_model_length: 10000
      restrict_tokens: ["C", "D"]
      device: "cuda"
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      bits_and_bytes_args: null
      generation_args:
        max_new_tokens: 1
        do_sample: True
        temperature: ${temperature}
        top_k: 0
        top_p: 1.0
        repetition_penalty: 1
      vllm_params:
        max_model_len : 13e3
        gpu_memory_utilization: 0.45
        enable_lora: True
        enable_prefix_caching: True
        enable_sleep_mode : True
        max_lora_rank: 64
        dtype: "bfloat16"
      hf_model_init_kwargs:
        torch_dtype: "bfloat16"
        device_map: "auto"
        attn_implementation: "flash_attention_2"
      eval_with: "vllm"
      train_with: "hf"
      optimizer_on_gpu_during_training: True
      fully_switch_vllm_weights_after_training: False
      keep_vllm_during_training: True
      keep_vllm_during_eval: True
      sleep_vllm_during_training: True
      wake_vllm_during_eval: True
      export_trained_parameters: True
      export_optimizer: True
      adapter_configs:
        sp_adapter:
          type: "lora"
          optimizer_method: "Adam"
          optimizer_kwargs:
            lr: 3e-6
            weight_decay: 0.0
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"


####################################################################################################
# TRAINING
####################################################################################################


training:

  agents:

    Alice:
      training_data_func: ipd_generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        normalize_round_points: True

    Bob:
      training_data_func: ipd_generate_training_data_from_raw
      training_data_func_args:
        exclude_errors: False
        normalize_round_points: True

  qwen:
    adapters:
      sp_adapter:
        trainer_config:
          entropy_coeff: 0.1
          kl_coeff: 0.0
          gradient_clipping: null
          restrict_tokens: ["C", "D"]
          mini_batch_size: 1
          use_gradient_checkpointing: true
          top_k_for_logging: 1
          temperature: ${temperature}
          discount_factor: 1.0
          use_sum_rewards: false
          use_advantage_alignment: false
          use_variance_regularization_in_ad_align: null
          use_time_regularization_in_ad_align: null
          ad_align_beta: null
          device: "cuda:0"
