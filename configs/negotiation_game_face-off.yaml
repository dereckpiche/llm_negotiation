defaults:
  - negotiation_game.yaml

experiment:
  name: "negotiation_game_faceoff_${now:%Y-%m-%d___%H-%M-%S}"
  # name: "${now:%Y-%m-%d___%H-%M-%S}_no_name" (having timestamp is good for debugging, but it is difficult to restart with preemption)
  description: "Agents facing off."
  nb_epochs: 1
  nb_matches_per_iteration: 5
  checkpoint_every_n_iterations: -1 # don't save checkpoint
  base_seed: 344

####################################################################################################
#                                           GENERATION
####################################################################################################

matches:
  max_length: 15
  continuation_prob: 1.0
  same_length_batch: True
  nb_matches_with_same_roundwise_utilities: 0
  log_func: dond_log_match
  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1


  agents:
    Alice:
      kwargs:
        agent_name: "Alice"
        policy_id: "qwen/adapter_alice"
    Bob:
      kwargs:
        agent_name: "Bob"
        policy_id: "qwen/adapter_bob"

temperature: 0.7



models:
  qwen:
    init_args:
      adapter_configs:
        adapter_alice:
          type: "lora"
          hf_server_import_adapter_path: dereckpichemila/greedy_qwen # path to hf folder that contains configs and .safetensors
          optimizer_method: "SGD"
          optimizer_kwargs: {}
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"
        adapter_bob:
          type: "lora"
          # hf_server_import_adapter_path: dereckpichemila/greedy_qwen # path to hf folder that contains configs and .safetensors
          optimizer_method: "SGD"
          optimizer_kwargs: {}
          lora_kwargs:
            task_type: "CAUSAL_LM"
            r: 64
            lora_alpha: 128
            lora_dropout: 0.0
            target_modules: "all-linear"
  # 4o:
  #   class: server_llm
  #   init_args:
  #     model_name: "gpt-4o"
  #     api_key: None
