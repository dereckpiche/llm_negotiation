hydra:
  job:
    chdir: false
    name: ultimatum
  run:
    dir: ${oc.env:EXP_OUT}/${hydra.job.name}/

experiment:
  method: generate_and_train
  description: "no description"
  nb_epochs: 2000
  nb_matches_per_iteration: 32
  # TODO (Dereck): reinit_matches_each_it is not used anywhere in the code
  reinit_matches_each_it: true
  start_epoch: 0
  resume_experiment: true
  base_seed: 33
  # random_seeds: [33, 53, 97, 157, 468, 484, 2959, 5145, 6368, 8602]

####################################################################################################
#                                           GENERATION
####################################################################################################

matches:
  env_class: "DondEnv"
  agent_class: "DondAgent"
  log_func: dond_log_match
  log_func_args:
    metrics_func: gather_dond_statistics
    metrics_func_args:
      stats_to_log: [
      "agreement_percentage",
      "points",
      "points_on_agreement",
      "items_given_to_self"
      ]

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_batched_matches_args:
    nb_parallel_matches: -1

  env_kwargs:
    agents: ['alice', 'bob']
    rounds_per_game: 1
    max_messages: 0
    min_messages: 0
    max_chars_per_message: 1000
    mode: basic
    random_setup_func: fixed_manual
    random_setup_kwargs:
      items: ['coins']
      quantities: [10]
      val_starting_negotiator: [1]
      val_responding_negotiator: [1]
    role_assignator_func: fixed_role_assignator
    role_assignator_func_kwargs: {}

  agents:
    alice:
      kwargs:
        agent_name: "alice"
        policy_id: 'llama/ad_alice'
        allow_reasoning: false
        max_errors: 1
        max_reasoning_chars: 0
        intro_prompt: ${prompt_blocks.barebone_ultimatum}
        goal_prompt: ${prompt_blocks.fair_goal}
        new_round_prompt: null
        agent_with_first_move_prompt: "You are the first agent. It is your turn to play."
        received_message_prompt: null
        first_round_prompt: null
        other_agent_finalized_prompt: ${prompt_blocks.visible_finalization_prompt}

    bob:
      kwargs:
        agent_name: "bob"
        policy_id: 'llama/ad_bob'
        allow_reasoning: false
        max_errors: 1
        max_reasoning_chars: 0
        intro_prompt: ${prompt_blocks.barebone_ultimatum}
        goal_prompt: ${prompt_blocks.fair_goal}
        new_round_prompt: null
        agent_with_first_move_prompt: null
        received_message_prompt: null
        first_round_prompt: null
        other_agent_finalized_prompt: ${prompt_blocks.visible_finalization_prompt}

common_models_init_args: &common_models_init_args
  max_model_length: 10000
  include_value_head: false
  device: "cuda"
  model_name: "/network/weights/llama.var/llama_3.1/Meta-Llama-3.1-8B-Instruct"
  pretrained_args:
    pretrained_model_name_or_path: ${models.llama.init_args.model_name}
    torch_dtype: "bfloat16"
    device_map: "auto"
    attn_implementation: "flash_attention_2"
  bits_and_bytes_args: null
    #load_in_8bit: False
    #load_in_4bit: true
  lora_args:
    task_type: CAUSAL_LM
    r: 64
    lora_alpha: 32
    lora_dropout: 0.0
    target_modules: "all-linear"

  generation_args:
    max_new_tokens: 120
    do_sample: True
    temperature: 1.0
    top_k: 0.0
    top_p: 1.0
    repetition_penalty: 0.0

  vllm_params:
    max_model_len : 13e3
    gpu_memory_utilization: 0.45
    enable_prefix_caching: True

  keep_vllm_during_training: False
  keep_hf_during_training: True
  keep_hf_during_eval: False
  keep_vllm_during_eval: True
  eval_with: "vllm"
  train_with: "hf"

models:
  llama:
    class: local_llm
    init_args:
      <<: *common_models_init_args
      name: 'llama'
      adapter_names: ['ad_alice', 'ad_bob']

####################################################################################################
# TRAINING
####################################################################################################

common_training_agent_kwargs: &common_training_agent_kwargs
  training_data_func: generate_training_data_from_raw
  training_data_func_args:
    exclude_errors: True
    debug_output: True
    score_method: rloo_scores
    score_method_kwargs:
      discount_factor: 0.9

common_adapter_kwargs: &common_adapter_kwargs
  train_func_args:
    gradient_checkpointing: true
    temperature: 1
    learning_rate: 3e-6
    entropy_coef: 0.01
    kl_loss_coef: 0.01
  train_data_args:
    average_score_over_message: True

training:

  agents:
    alice:
      <<: *common_training_agent_kwargs
    bob:
      <<: *common_training_agent_kwargs

  llama:
    adapters:
      ad_alice:
        train_func: train_reinforce_main
        <<: *common_adapter_kwargs
      ad_bob:
        train_func: train_reinforce_main
        <<: *common_adapter_kwargs


####################################################################################################
# PROMPT DEFINITIONS
####################################################################################################

prompt_blocks:

  initial_play_prompt: |
    You are the first agent. It is your turn to play.

  ongoing_play_prompt: |
    The other agent sent you this message: <message>{last_message}</message>

  visible_finalization_prompt: |
    The other agent's finalization was {other_agent_finalization}.
    You are the second agent. It is your turn to play.

  second_agent_finalization_prompt: |
    The other agent's finalization was {other_agent_finalization}.
    It is your turn to play.

  unbiased_goal: null

  normal_goal: |
    Your points will be the number of coins you get at the end of the game.

  agressive_goal: |
    Your goal is to get as many coins as possible.

  agressive_pair_goal: |
    Your goal is to get as many coins as possible.
    This is also the goal of the other agent.

  fair_goal: |
    Goal: Aim to be fair with both the other agent and yourself.

  submitted_goal: |
    Goal: Your goal is to do exactly what the other agent wants. You must make the other agent happy.


############################################################
# Ultimatum Game Prompts
############################################################

  barebone_ultimatum: |

    Welcome to the Splitting Game.

    Game Overview:

      1) Two agents divide 10 coins.

      2) Each agent proposes a division of the coins.

      3) Both agents must agree on a division to receive the coins. If not, both receive zero coins.

      4) The game is played only once.

    Response Format:

      1) Responses must be within <finalize> </finalize> tags.

      2) Use JSON format: <finalize> {"i_take": {"coins": x}, "other_agent_gets": {"coins": y}} </finalize>.