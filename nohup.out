/home/mila/d/dereck.piche/llm_negotiation/src/run.py:14: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="ultimatum")
[2025-01-06 10:43:31,950][root][INFO] - Loading VLLM model.
WARNING 01-06 10:43:32 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:43:32 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:43:33 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:43:33 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:23,  7.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:07,  3.71s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:16<00:05,  5.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:24<00:00,  6.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:24<00:00,  6.01s/it]

INFO 01-06 10:43:58 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-06 10:44:13 gpu_executor.py:122] # GPU blocks: 19859, # CPU blocks: 2048
INFO 01-06 10:44:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:44:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:44:33 model_runner.py:1335] Graph capturing finished in 19 secs.
[2025-01-06 10:44:33,800][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:01<00:56,  1.96s/it, est. speed input: 257.42 toks/s, output: 29.05 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:02<00:10,  2.37it/s, est. speed input: 939.51 toks/s, output: 113.02 toks/s]Processed prompts:  20%|██        | 6/30 [00:02<00:06,  3.65it/s, est. speed input: 1309.17 toks/s, output: 166.78 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:02<00:02,  8.21it/s, est. speed input: 2281.59 toks/s, output: 321.60 toks/s]Processed prompts:  50%|█████     | 15/30 [00:02<00:01, 12.17it/s, est. speed input: 2987.65 toks/s, output: 444.89 toks/s]Processed prompts:  60%|██████    | 18/30 [00:02<00:00, 14.66it/s, est. speed input: 3436.89 toks/s, output: 532.35 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:02<00:00, 17.18it/s, est. speed input: 3955.26 toks/s, output: 641.88 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:03<00:00, 11.79it/s, est. speed input: 3865.61 toks/s, output: 669.63 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:03<00:00, 12.20it/s, est. speed input: 4051.00 toks/s, output: 753.76 toks/s]Processed prompts: 100%|██████████| 30/30 [00:03<00:00,  9.66it/s, est. speed input: 3929.62 toks/s, output: 772.95 toks/s]Processed prompts: 100%|██████████| 30/30 [00:03<00:00,  7.78it/s, est. speed input: 3929.62 toks/s, output: 772.95 toks/s]
[2025-01-06 10:44:37,881][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, est. speed input: 860.51 toks/s, output: 30.28 toks/s]Processed prompts: 100%|██████████| 11/11 [00:00<00:00, 16.27it/s, est. speed input: 8028.13 toks/s, output: 351.76 toks/s]Processed prompts: 100%|██████████| 11/11 [00:00<00:00, 12.28it/s, est. speed input: 8028.13 toks/s, output: 351.76 toks/s]
[2025-01-06 10:44:38,964][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:01<00:22,  1.26s/it, est. speed input: 553.02 toks/s, output: 22.94 toks/s]Processed prompts:  11%|█         | 2/19 [00:01<00:14,  1.20it/s, est. speed input: 776.77 toks/s, output: 49.45 toks/s]Processed prompts:  26%|██▋       | 5/19 [00:02<00:04,  3.48it/s, est. speed input: 1739.87 toks/s, output: 143.37 toks/s]Processed prompts:  42%|████▏     | 8/19 [00:02<00:01,  5.59it/s, est. speed input: 2510.35 toks/s, output: 237.92 toks/s]Processed prompts:  58%|█████▊    | 11/19 [00:02<00:01,  7.86it/s, est. speed input: 3209.95 toks/s, output: 337.32 toks/s]Processed prompts:  68%|██████▊   | 13/19 [00:02<00:00,  7.42it/s, est. speed input: 3362.61 toks/s, output: 384.48 toks/s]Processed prompts:  79%|███████▉  | 15/19 [00:03<00:00,  6.22it/s, est. speed input: 3327.38 toks/s, output: 417.31 toks/s]Processed prompts:  84%|████████▍ | 16/19 [00:03<00:00,  6.56it/s, est. speed input: 3426.49 toks/s, output: 451.29 toks/s]Processed prompts:  89%|████████▉ | 17/19 [00:03<00:00,  4.94it/s, est. speed input: 3237.73 toks/s, output: 452.29 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:03<00:00,  5.11it/s, est. speed input: 3274.96 toks/s, output: 484.14 toks/s]Processed prompts: 100%|██████████| 19/19 [00:03<00:00,  4.95it/s, est. speed input: 3456.78 toks/s, output: 536.17 toks/s]
[2025-01-06 10:44:43,118][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:01<00:12,  1.02s/it, est. speed input: 683.68 toks/s, output: 31.30 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:01<00:05,  1.95it/s, est. speed input: 1187.52 toks/s, output: 62.86 toks/s]Processed prompts:  31%|███       | 4/13 [00:01<00:02,  4.10it/s, est. speed input: 2435.16 toks/s, output: 127.07 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:01<00:01,  4.66it/s, est. speed input: 2659.57 toks/s, output: 156.52 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:01<00:00,  6.19it/s, est. speed input: 3164.17 toks/s, output: 221.97 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:01<00:00,  6.58it/s, est. speed input: 3335.93 toks/s, output: 281.75 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:02<00:00,  4.31it/s, est. speed input: 2868.79 toks/s, output: 296.74 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:03<00:00,  3.45it/s, est. speed input: 2628.38 toks/s, output: 307.69 toks/s]Processed prompts: 100%|██████████| 13/13 [00:03<00:00,  3.85it/s, est. speed input: 2712.71 toks/s, output: 352.44 toks/s]Processed prompts: 100%|██████████| 13/13 [00:03<00:00,  3.84it/s, est. speed input: 2712.71 toks/s, output: 352.44 toks/s]
[2025-01-06 10:44:46,762][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:00<00:01,  1.64it/s, est. speed input: 1479.56 toks/s, output: 47.57 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  4.19it/s, est. speed input: 3119.17 toks/s, output: 141.65 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.75it/s, est. speed input: 3119.17 toks/s, output: 141.65 toks/s]
[2025-01-06 10:44:52,744][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:05,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]
[2025-01-06 10:45:03,010][root][INFO] - Adapter 'ad_alice' added to HF.
[2025-01-06 10:45:03,010][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.56 GB
[2025-01-06 10:45:03,313][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.33 GB
[2025-01-06 10:45:14,169][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:45:14,170][root][INFO] - Before destroying HF.: GPU memory allocated: 15.34 GB
[2025-01-06 10:45:14,436][root][INFO] - After destroying HF.: GPU memory allocated: 0.06 GB
[2025-01-06 10:45:14,595][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]
[2025-01-06 10:45:21,305][root][INFO] - Adapter 'ad_bob' added to HF.
[2025-01-06 10:45:34,561][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:45:34,562][root][INFO] - Iteration 1 took 2m 2s. Generation: 65.65%, Training: 34.35%. Estimated time remaining: 1h 40m 16s. Estimated total time for complete run: 1h 42m 19s.
[2025-01-06 10:45:34,845][root][INFO] - Loading VLLM model.
WARNING 01-06 10:45:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:45:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:45:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:45:35 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]

INFO 01-06 10:45:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:45:54 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:45:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:45:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:46:15 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:46:15,833][root][INFO] - Before destroying HF.: GPU memory allocated: 70.70 GB
[2025-01-06 10:46:16,093][root][INFO] - After destroying HF.: GPU memory allocated: 55.41 GB
[2025-01-06 10:46:16,094][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:46:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:06<03:10,  6.58s/it, est. speed input: 76.72 toks/s, output: 9.72 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:18,  2.81s/it, est. speed input: 149.52 toks/s, output: 19.39 toks/s]Processed prompts:  10%|█         | 3/30 [00:07<00:45,  1.68s/it, est. speed input: 213.70 toks/s, output: 28.77 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:07<00:19,  1.29it/s, est. speed input: 348.29 toks/s, output: 48.83 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:07<00:10,  2.16it/s, est. speed input: 477.68 toks/s, output: 68.78 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:06,  3.13it/s, est. speed input: 598.67 toks/s, output: 88.65 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:07<00:04,  4.08it/s, est. speed input: 710.87 toks/s, output: 108.13 toks/s]Processed prompts:  40%|████      | 12/30 [00:07<00:03,  4.52it/s, est. speed input: 762.85 toks/s, output: 117.83 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:08<00:03,  5.01it/s, est. speed input: 855.06 toks/s, output: 136.30 toks/s]Processed prompts:  50%|█████     | 15/30 [00:08<00:03,  4.69it/s, est. speed input: 887.71 toks/s, output: 144.49 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:08<00:02,  5.28it/s, est. speed input: 934.93 toks/s, output: 155.28 toks/s]Processed prompts:  60%|██████    | 18/30 [00:08<00:01,  7.40it/s, est. speed input: 1039.22 toks/s, output: 178.92 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:08<00:01,  9.10it/s, est. speed input: 1137.72 toks/s, output: 202.42 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:09<00:00,  9.36it/s, est. speed input: 1223.63 toks/s, output: 224.68 toks/s]Processed prompts:  80%|████████  | 24/30 [00:09<00:00,  8.35it/s, est. speed input: 1293.25 toks/s, output: 246.27 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:09<00:00,  9.91it/s, est. speed input: 1421.22 toks/s, output: 284.03 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  8.38it/s, est. speed input: 1475.33 toks/s, output: 306.65 toks/s]Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  4.81it/s, est. speed input: 1432.65 toks/s, output: 306.77 toks/s]Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.84it/s, est. speed input: 1432.65 toks/s, output: 306.77 toks/s]
[2025-01-06 10:46:27,064][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:46:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.51s/it, est. speed input: 414.10 toks/s, output: 17.89 toks/s]Processed prompts: 100%|██████████| 9/9 [00:01<00:00,  7.29it/s, est. speed input: 3642.86 toks/s, output: 160.31 toks/s]Processed prompts: 100%|██████████| 9/9 [00:01<00:00,  5.46it/s, est. speed input: 3642.86 toks/s, output: 160.31 toks/s]
[2025-01-06 10:46:29,102][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:46:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:02<00:55,  2.77s/it, est. speed input: 252.55 toks/s, output: 6.50 toks/s]Processed prompts:  10%|▉         | 2/21 [00:03<00:29,  1.56s/it, est. speed input: 402.14 toks/s, output: 14.96 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:04<00:23,  1.28s/it, est. speed input: 473.61 toks/s, output: 24.39 toks/s]Processed prompts:  19%|█▉        | 4/21 [00:04<00:14,  1.14it/s, est. speed input: 597.68 toks/s, output: 36.34 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:04<00:07,  2.07it/s, est. speed input: 847.34 toks/s, output: 61.22 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:05<00:05,  2.30it/s, est. speed input: 981.65 toks/s, output: 81.28 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:05<00:04,  2.74it/s, est. speed input: 1078.60 toks/s, output: 95.50 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:06<00:03,  2.81it/s, est. speed input: 1134.50 toks/s, output: 107.28 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:06<00:03,  2.90it/s, est. speed input: 1187.72 toks/s, output: 119.71 toks/s]Processed prompts:  62%|██████▏   | 13/21 [00:06<00:02,  3.98it/s, est. speed input: 1350.55 toks/s, output: 150.41 toks/s]Processed prompts:  67%|██████▋   | 14/21 [00:07<00:01,  3.65it/s, est. speed input: 1382.92 toks/s, output: 162.23 toks/s]Processed prompts:  76%|███████▌  | 16/21 [00:07<00:00,  5.29it/s, est. speed input: 1553.02 toks/s, output: 198.15 toks/s]Processed prompts:  86%|████████▌ | 18/21 [00:07<00:00,  5.81it/s, est. speed input: 1680.55 toks/s, output: 230.54 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:08<00:00,  3.97it/s, est. speed input: 1684.86 toks/s, output: 250.68 toks/s]Processed prompts: 100%|██████████| 21/21 [00:08<00:00,  2.52it/s, est. speed input: 1761.89 toks/s, output: 273.66 toks/s]
[2025-01-06 10:46:37,951][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:46:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:32,  2.34s/it, est. speed input: 407.72 toks/s, output: 12.39 toks/s]Processed prompts:  27%|██▋       | 4/15 [00:02<00:06,  1.82it/s, est. speed input: 1247.97 toks/s, output: 46.84 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:03<00:05,  1.93it/s, est. speed input: 1296.89 toks/s, output: 57.76 toks/s]Processed prompts:  40%|████      | 6/15 [00:03<00:03,  2.48it/s, est. speed input: 1502.25 toks/s, output: 73.64 toks/s]Processed prompts:  47%|████▋     | 7/15 [00:03<00:02,  3.05it/s, est. speed input: 1643.89 toks/s, output: 89.25 toks/s]Processed prompts:  60%|██████    | 9/15 [00:03<00:01,  4.52it/s, est. speed input: 1895.19 toks/s, output: 122.38 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:03<00:00,  6.02it/s, est. speed input: 2231.05 toks/s, output: 157.13 toks/s]Processed prompts:  87%|████████▋ | 13/15 [00:04<00:00,  3.18it/s, est. speed input: 1991.51 toks/s, output: 164.10 toks/s]Processed prompts:  93%|█████████▎| 14/15 [00:05<00:00,  3.19it/s, est. speed input: 2006.28 toks/s, output: 183.52 toks/s]Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  3.71it/s, est. speed input: 2092.91 toks/s, output: 209.37 toks/s]Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  2.80it/s, est. speed input: 2092.91 toks/s, output: 209.37 toks/s]
[2025-01-06 10:46:43,772][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:46:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s, est. speed input: 1077.84 toks/s, output: 35.16 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s, est. speed input: 2056.91 toks/s, output: 70.30 toks/s]
[2025-01-06 10:46:49,152][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 10:46:55,935][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:46:55,936][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.70 GB
[2025-01-06 10:46:56,286][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.35 GB
[2025-01-06 10:47:06,728][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:47:06,729][root][INFO] - Before destroying HF.: GPU memory allocated: 15.35 GB
[2025-01-06 10:47:07,016][root][INFO] - After destroying HF.: GPU memory allocated: 0.06 GB
[2025-01-06 10:47:07,163][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 10:47:14,013][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:47:27,206][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:47:27,207][root][INFO] - Iteration 2 took 1m 52s. Generation: 66.09%, Training: 33.91%. Estimated time remaining: 1h 29m 56s. Estimated total time for complete run: 1h 33m 52s.
[2025-01-06 10:47:27,483][root][INFO] - Loading VLLM model.
WARNING 01-06 10:47:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:47:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:47:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:47:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 10:47:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:47:46 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:47:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:47:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:48:08 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:48:08,738][root][INFO] - Before destroying HF.: GPU memory allocated: 70.71 GB
[2025-01-06 10:48:08,969][root][INFO] - After destroying HF.: GPU memory allocated: 55.42 GB
[2025-01-06 10:48:08,970][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:48:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:44,  5.68s/it, est. speed input: 88.85 toks/s, output: 11.26 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:08,  2.44s/it, est. speed input: 172.49 toks/s, output: 22.37 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:26,  1.01s/it, est. speed input: 326.82 toks/s, output: 44.01 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:13,  1.75it/s, est. speed input: 478.26 toks/s, output: 66.45 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:05,  3.70it/s, est. speed input: 774.03 toks/s, output: 112.35 toks/s]Processed prompts:  40%|████      | 12/30 [00:06<00:04,  4.48it/s, est. speed input: 898.74 toks/s, output: 133.62 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:07<00:03,  5.00it/s, est. speed input: 1005.70 toks/s, output: 153.91 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:02,  5.29it/s, est. speed input: 1098.52 toks/s, output: 173.34 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  5.19it/s, est. speed input: 1134.64 toks/s, output: 182.65 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:01,  7.27it/s, est. speed input: 1301.94 toks/s, output: 220.81 toks/s]Processed prompts:  80%|████████  | 24/30 [00:08<00:00,  9.25it/s, est. speed input: 1506.39 toks/s, output: 270.95 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  9.03it/s, est. speed input: 1624.44 toks/s, output: 306.54 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00, 11.02it/s, est. speed input: 1774.71 toks/s, output: 352.13 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.51it/s, est. speed input: 1774.71 toks/s, output: 352.13 toks/s]
[2025-01-06 10:48:17,944][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:48:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:09,  1.38s/it, est. speed input: 471.64 toks/s, output: 19.50 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  7.04it/s, est. speed input: 3417.43 toks/s, output: 157.08 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  5.30it/s, est. speed input: 3417.43 toks/s, output: 157.08 toks/s]
[2025-01-06 10:48:19,844][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:48:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:04,  3.08s/it, est. speed input: 227.26 toks/s, output: 9.43 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:03<00:17,  1.10it/s, est. speed input: 619.31 toks/s, output: 27.76 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:04<00:16,  1.08it/s, est. speed input: 644.74 toks/s, output: 35.05 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:04<00:12,  1.34it/s, est. speed input: 741.69 toks/s, output: 46.48 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:04<00:09,  1.75it/s, est. speed input: 853.17 toks/s, output: 59.20 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:04,  3.16it/s, est. speed input: 1184.99 toks/s, output: 97.95 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:05<00:03,  3.71it/s, est. speed input: 1291.81 toks/s, output: 111.99 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:01,  5.08it/s, est. speed input: 1506.06 toks/s, output: 140.77 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:05<00:01,  5.36it/s, est. speed input: 1589.26 toks/s, output: 153.91 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:05<00:01,  5.90it/s, est. speed input: 1678.20 toks/s, output: 168.06 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:05<00:01,  6.50it/s, est. speed input: 1765.56 toks/s, output: 182.53 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:01,  5.95it/s, est. speed input: 1819.58 toks/s, output: 194.58 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:06<00:00,  8.27it/s, est. speed input: 2080.36 toks/s, output: 241.70 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:06<00:00,  6.22it/s, est. speed input: 2085.92 toks/s, output: 250.97 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:06<00:00,  5.25it/s, est. speed input: 2098.10 toks/s, output: 262.57 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  3.14it/s, est. speed input: 2192.63 toks/s, output: 284.17 toks/s]
[2025-01-06 10:48:27,465][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:48:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:02<00:16,  2.05s/it, est. speed input: 340.16 toks/s, output: 21.90 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:02<00:03,  1.75it/s, est. speed input: 1054.63 toks/s, output: 64.81 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:03<00:02,  1.68it/s, est. speed input: 1080.62 toks/s, output: 87.31 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:03<00:01,  2.12it/s, est. speed input: 1238.80 toks/s, output: 115.24 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:03<00:00,  2.19it/s, est. speed input: 1285.96 toks/s, output: 136.35 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:05<00:00,  1.58it/s, est. speed input: 1110.24 toks/s, output: 144.74 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  2.00it/s, est. speed input: 1208.97 toks/s, output: 178.56 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  1.73it/s, est. speed input: 1208.97 toks/s, output: 178.56 toks/s]
[2025-01-06 10:48:33,098][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:48:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:00<00:01,  1.04it/s, est. speed input: 833.66 toks/s, output: 30.14 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.87it/s, est. speed input: 2470.75 toks/s, output: 87.97 toks/s]
[2025-01-06 10:48:38,828][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]
[2025-01-06 10:48:45,734][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:48:45,734][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.71 GB
[2025-01-06 10:48:46,066][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.36 GB
[2025-01-06 10:48:56,517][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:48:56,518][root][INFO] - Before destroying HF.: GPU memory allocated: 15.36 GB
[2025-01-06 10:48:56,797][root][INFO] - After destroying HF.: GPU memory allocated: 0.07 GB
[2025-01-06 10:48:56,980][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 10:49:03,710][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:49:16,271][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:49:16,272][root][INFO] - Iteration 3 took 1m 49s. Generation: 65.52%, Training: 34.48%. Estimated time remaining: 1h 25m 8s. Estimated total time for complete run: 1h 30m 53s.
[2025-01-06 10:49:16,629][root][INFO] - Loading VLLM model.
WARNING 01-06 10:49:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:49:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:49:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:49:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 10:49:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:49:36 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:49:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:49:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:49:57 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:49:57,880][root][INFO] - Before destroying HF.: GPU memory allocated: 70.72 GB
[2025-01-06 10:49:58,104][root][INFO] - After destroying HF.: GPU memory allocated: 55.43 GB
[2025-01-06 10:49:58,105][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:49:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:06<02:58,  6.16s/it, est. speed input: 81.96 toks/s, output: 10.39 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:19,  2.84s/it, est. speed input: 151.28 toks/s, output: 20.52 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:28,  1.11s/it, est. speed input: 295.30 toks/s, output: 41.96 toks/s]Processed prompts:  20%|██        | 6/30 [00:07<00:15,  1.58it/s, est. speed input: 430.08 toks/s, output: 62.88 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:07<00:04,  3.94it/s, est. speed input: 772.81 toks/s, output: 118.53 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  4.65it/s, est. speed input: 886.93 toks/s, output: 138.75 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  5.03it/s, est. speed input: 982.17 toks/s, output: 157.41 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  5.77it/s, est. speed input: 1082.77 toks/s, output: 178.72 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:08<00:01,  7.02it/s, est. speed input: 1190.40 toks/s, output: 201.73 toks/s]Processed prompts:  70%|███████   | 21/30 [00:08<00:01,  6.89it/s, est. speed input: 1268.05 toks/s, output: 221.68 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  8.14it/s, est. speed input: 1366.23 toks/s, output: 246.31 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  8.61it/s, est. speed input: 1450.79 toks/s, output: 269.93 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  9.85it/s, est. speed input: 1543.17 toks/s, output: 296.18 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  8.59it/s, est. speed input: 1602.47 toks/s, output: 318.30 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.26it/s, est. speed input: 1645.70 toks/s, output: 332.83 toks/s]
[2025-01-06 10:50:07,738][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:50:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s, est. speed input: 652.91 toks/s, output: 29.26 toks/s]Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.04it/s, est. speed input: 2555.44 toks/s, output: 117.03 toks/s]
[2025-01-06 10:50:09,119][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:50:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:20,  3.20s/it, est. speed input: 218.22 toks/s, output: 7.18 toks/s]Processed prompts:   8%|▊         | 2/26 [00:03<00:36,  1.50s/it, est. speed input: 398.12 toks/s, output: 14.81 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:04<00:28,  1.23s/it, est. speed input: 475.26 toks/s, output: 22.44 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:04<00:18,  1.17it/s, est. speed input: 594.47 toks/s, output: 32.32 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:04<00:12,  1.62it/s, est. speed input: 714.01 toks/s, output: 42.70 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:05<00:06,  2.77it/s, est. speed input: 955.64 toks/s, output: 64.45 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:05<00:04,  4.24it/s, est. speed input: 1199.31 toks/s, output: 87.12 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:05<00:03,  4.57it/s, est. speed input: 1292.47 toks/s, output: 97.26 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:05<00:03,  4.17it/s, est. speed input: 1345.97 toks/s, output: 105.56 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:05<00:03,  4.26it/s, est. speed input: 1414.11 toks/s, output: 115.65 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:06<00:01,  7.33it/s, est. speed input: 1870.96 toks/s, output: 176.03 toks/s]Processed prompts:  73%|███████▎  | 19/26 [00:06<00:01,  6.50it/s, est. speed input: 1966.76 toks/s, output: 196.51 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:07<00:00,  7.78it/s, est. speed input: 2194.63 toks/s, output: 239.33 toks/s]Processed prompts:  88%|████████▊ | 23/26 [00:07<00:00,  4.38it/s, est. speed input: 2059.79 toks/s, output: 235.36 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:08<00:00,  4.41it/s, est. speed input: 2090.75 toks/s, output: 250.25 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  4.30it/s, est. speed input: 2134.53 toks/s, output: 279.88 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  3.05it/s, est. speed input: 2134.53 toks/s, output: 279.88 toks/s]
[2025-01-06 10:50:18,223][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:50:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:13,  1.67s/it, est. speed input: 570.68 toks/s, output: 17.35 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:02<00:00,  3.52it/s, est. speed input: 2317.69 toks/s, output: 91.91 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:02<00:00,  3.40it/s, est. speed input: 2286.45 toks/s, output: 106.67 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:02<00:00,  3.82it/s, est. speed input: 2425.56 toks/s, output: 129.44 toks/s]Processed prompts: 100%|██████████| 9/9 [00:02<00:00,  3.53it/s, est. speed input: 2374.79 toks/s, output: 146.60 toks/s]Processed prompts: 100%|██████████| 9/9 [00:02<00:00,  3.05it/s, est. speed input: 2374.79 toks/s, output: 146.60 toks/s]
[2025-01-06 10:50:26,047][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 10:50:32,581][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:50:32,581][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.72 GB
[2025-01-06 10:50:32,941][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.37 GB
[2025-01-06 10:50:43,197][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:50:43,198][root][INFO] - Before destroying HF.: GPU memory allocated: 15.37 GB
[2025-01-06 10:50:43,491][root][INFO] - After destroying HF.: GPU memory allocated: 0.08 GB
[2025-01-06 10:50:43,680][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 10:50:50,199][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:51:03,439][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:51:03,440][root][INFO] - Iteration 4 took 1m 47s. Generation: 64.97%, Training: 35.03%. Estimated time remaining: 1h 21m 46s. Estimated total time for complete run: 1h 29m 18s.
[2025-01-06 10:51:03,689][root][INFO] - Loading VLLM model.
WARNING 01-06 10:51:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:51:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:51:04 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:51:04 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 10:51:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:51:23 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:51:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:51:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:51:44 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:51:45,003][root][INFO] - Before destroying HF.: GPU memory allocated: 70.73 GB
[2025-01-06 10:51:45,274][root][INFO] - After destroying HF.: GPU memory allocated: 55.44 GB
[2025-01-06 10:51:45,276][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:51:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:51,  5.91s/it, est. speed input: 85.48 toks/s, output: 9.82 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:16,  2.74s/it, est. speed input: 157.27 toks/s, output: 19.46 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:28,  1.11s/it, est. speed input: 301.78 toks/s, output: 39.59 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:14,  1.65it/s, est. speed input: 445.72 toks/s, output: 60.61 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:05,  3.58it/s, est. speed input: 727.31 toks/s, output: 103.12 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  5.28it/s, est. speed input: 928.20 toks/s, output: 134.74 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  6.05it/s, est. speed input: 1041.91 toks/s, output: 154.19 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  6.12it/s, est. speed input: 1131.64 toks/s, output: 171.36 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:07<00:01,  5.92it/s, est. speed input: 1206.39 toks/s, output: 189.60 toks/s]Processed prompts:  70%|███████   | 21/30 [00:08<00:01,  6.80it/s, est. speed input: 1302.93 toks/s, output: 212.55 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  8.08it/s, est. speed input: 1404.03 toks/s, output: 236.56 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  6.62it/s, est. speed input: 1450.21 toks/s, output: 255.12 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  8.14it/s, est. speed input: 1546.31 toks/s, output: 283.06 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  7.34it/s, est. speed input: 1600.02 toks/s, output: 305.15 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.27it/s, est. speed input: 1652.11 toks/s, output: 321.70 toks/s]
[2025-01-06 10:51:54,887][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:51:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:04,  1.05s/it, est. speed input: 643.74 toks/s, output: 25.75 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.59it/s, est. speed input: 2931.64 toks/s, output: 129.54 toks/s]
[2025-01-06 10:51:56,359][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:51:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:22,  3.42s/it, est. speed input: 204.20 toks/s, output: 8.47 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:04<00:28,  1.30s/it, est. speed input: 461.90 toks/s, output: 24.45 toks/s]Processed prompts:  20%|██        | 5/25 [00:04<00:13,  1.50it/s, est. speed input: 746.59 toks/s, output: 47.00 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:04<00:09,  1.91it/s, est. speed input: 871.12 toks/s, output: 57.74 toks/s]Processed prompts:  32%|███▏      | 8/25 [00:05<00:06,  2.58it/s, est. speed input: 1068.99 toks/s, output: 77.23 toks/s]Processed prompts:  40%|████      | 10/25 [00:05<00:04,  3.20it/s, est. speed input: 1245.03 toks/s, output: 98.14 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:05<00:02,  4.40it/s, est. speed input: 1457.11 toks/s, output: 123.51 toks/s]Processed prompts:  56%|█████▌    | 14/25 [00:05<00:01,  5.77it/s, est. speed input: 1661.59 toks/s, output: 149.42 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:06<00:01,  6.24it/s, est. speed input: 1881.58 toks/s, output: 182.73 toks/s]Processed prompts:  72%|███████▏  | 18/25 [00:06<00:01,  6.42it/s, est. speed input: 1950.76 toks/s, output: 195.35 toks/s]Processed prompts:  76%|███████▌  | 19/25 [00:07<00:01,  3.63it/s, est. speed input: 1837.40 toks/s, output: 193.13 toks/s]Processed prompts:  80%|████████  | 20/25 [00:07<00:01,  3.66it/s, est. speed input: 1865.43 toks/s, output: 205.89 toks/s]Processed prompts:  92%|█████████▏| 23/25 [00:08<00:00,  4.33it/s, est. speed input: 1997.11 toks/s, output: 249.93 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:08<00:00,  4.30it/s, est. speed input: 2023.81 toks/s, output: 265.40 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  4.43it/s, est. speed input: 2058.30 toks/s, output: 282.68 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  2.94it/s, est. speed input: 2058.30 toks/s, output: 282.68 toks/s]
[2025-01-06 10:52:05,444][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:52:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:19,  1.94s/it, est. speed input: 492.71 toks/s, output: 14.98 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:02<00:01,  3.09it/s, est. speed input: 2142.00 toks/s, output: 86.90 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:03<00:00,  3.02it/s, est. speed input: 2114.73 toks/s, output: 103.01 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:03<00:00,  2.87it/s, est. speed input: 2063.17 toks/s, output: 120.01 toks/s]Processed prompts: 100%|██████████| 11/11 [00:04<00:00,  3.25it/s, est. speed input: 2167.75 toks/s, output: 165.19 toks/s]Processed prompts: 100%|██████████| 11/11 [00:04<00:00,  2.74it/s, est. speed input: 2167.75 toks/s, output: 165.19 toks/s]
[2025-01-06 10:52:14,511][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 10:52:21,066][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:52:21,067][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.73 GB
[2025-01-06 10:52:21,409][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.38 GB
[2025-01-06 10:52:31,671][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:52:31,672][root][INFO] - Before destroying HF.: GPU memory allocated: 15.38 GB
[2025-01-06 10:52:31,976][root][INFO] - After destroying HF.: GPU memory allocated: 0.09 GB
[2025-01-06 10:52:32,130][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 10:52:38,943][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:52:51,839][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:52:51,840][root][INFO] - Iteration 5 took 1m 48s. Generation: 65.38%, Training: 34.62%. Estimated time remaining: 1h 20m 59s. Estimated total time for complete run: 1h 30m 20s.
[2025-01-06 10:52:52,142][root][INFO] - Loading VLLM model.
WARNING 01-06 10:52:52 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:52:52 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:52:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:52:52 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 10:52:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:53:11 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:53:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:53:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:53:33 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:53:33,181][root][INFO] - Before destroying HF.: GPU memory allocated: 70.73 GB
[2025-01-06 10:53:33,407][root][INFO] - After destroying HF.: GPU memory allocated: 55.45 GB
[2025-01-06 10:53:33,409][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:53:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:44,  5.68s/it, est. speed input: 88.95 toks/s, output: 11.27 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:08,  2.44s/it, est. speed input: 172.67 toks/s, output: 22.40 toks/s]Processed prompts:  10%|█         | 3/30 [00:06<00:39,  1.48s/it, est. speed input: 245.03 toks/s, output: 32.99 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:17,  1.46it/s, est. speed input: 398.09 toks/s, output: 55.81 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.49it/s, est. speed input: 548.64 toks/s, output: 78.84 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.33it/s, est. speed input: 675.61 toks/s, output: 99.74 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.45it/s, est. speed input: 804.64 toks/s, output: 122.11 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  5.04it/s, est. speed input: 912.16 toks/s, output: 142.83 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:01,  7.34it/s, est. speed input: 1099.52 toks/s, output: 179.62 toks/s]Processed prompts:  60%|██████    | 18/30 [00:07<00:01,  8.55it/s, est. speed input: 1214.19 toks/s, output: 203.57 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:00,  9.35it/s, est. speed input: 1366.88 toks/s, output: 238.06 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:07<00:00,  9.95it/s, est. speed input: 1465.92 toks/s, output: 262.51 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  9.05it/s, est. speed input: 1540.23 toks/s, output: 284.62 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  7.80it/s, est. speed input: 1595.86 toks/s, output: 305.59 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  7.19it/s, est. speed input: 1680.58 toks/s, output: 341.33 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.33it/s, est. speed input: 1680.58 toks/s, output: 341.33 toks/s]
[2025-01-06 10:53:42,841][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:53:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:07,  1.28s/it, est. speed input: 526.98 toks/s, output: 21.02 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  5.25it/s, est. speed input: 3448.61 toks/s, output: 150.75 toks/s]
[2025-01-06 10:53:44,555][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:53:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:20,  3.66s/it, est. speed input: 191.21 toks/s, output: 10.67 toks/s]Processed prompts:   9%|▊         | 2/23 [00:04<00:45,  2.15s/it, est. speed input: 294.62 toks/s, output: 21.28 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:04<00:16,  1.19it/s, est. speed input: 572.90 toks/s, output: 46.92 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:05<00:11,  1.54it/s, est. speed input: 685.67 toks/s, output: 58.66 toks/s]Processed prompts:  30%|███       | 7/23 [00:05<00:05,  2.68it/s, est. speed input: 937.40 toks/s, output: 84.68 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:05<00:02,  4.74it/s, est. speed input: 1302.26 toks/s, output: 124.64 toks/s]Processed prompts:  57%|█████▋    | 13/23 [00:05<00:01,  6.71it/s, est. speed input: 1635.68 toks/s, output: 163.08 toks/s]Processed prompts:  65%|██████▌   | 15/23 [00:06<00:01,  5.42it/s, est. speed input: 1718.18 toks/s, output: 179.93 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:06<00:01,  4.03it/s, est. speed input: 1718.65 toks/s, output: 193.95 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:07<00:00,  4.78it/s, est. speed input: 1859.79 toks/s, output: 226.43 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:07<00:00,  4.93it/s, est. speed input: 1911.33 toks/s, output: 241.72 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:08<00:00,  3.58it/s, est. speed input: 1879.69 toks/s, output: 259.01 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.80it/s, est. speed input: 1956.95 toks/s, output: 282.28 toks/s]
[2025-01-06 10:53:53,313][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:53:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.75s/it, est. speed input: 545.06 toks/s, output: 16.57 toks/s]Processed prompts:  40%|████      | 4/10 [00:02<00:03,  1.58it/s, est. speed input: 1173.78 toks/s, output: 55.59 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:01,  2.72it/s, est. speed input: 1663.13 toks/s, output: 122.28 toks/s]Processed prompts:  80%|████████  | 8/10 [00:03<00:00,  3.00it/s, est. speed input: 1769.02 toks/s, output: 144.33 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  2.67it/s, est. speed input: 1716.02 toks/s, output: 158.03 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s, est. speed input: 1655.79 toks/s, output: 174.11 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s, est. speed input: 1655.79 toks/s, output: 174.11 toks/s]
[2025-01-06 10:54:03,010][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 10:54:09,619][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:54:09,619][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.73 GB
[2025-01-06 10:54:09,963][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.38 GB
[2025-01-06 10:54:20,453][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:54:20,455][root][INFO] - Before destroying HF.: GPU memory allocated: 15.38 GB
[2025-01-06 10:54:20,752][root][INFO] - After destroying HF.: GPU memory allocated: 0.09 GB
[2025-01-06 10:54:20,952][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 10:54:27,711][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:54:40,664][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:54:40,665][root][INFO] - Iteration 6 took 1m 48s. Generation: 65.26%, Training: 34.73%. Estimated time remaining: 1h 19m 31s. Estimated total time for complete run: 1h 30m 41s.
[2025-01-06 10:54:40,907][root][INFO] - Loading VLLM model.
WARNING 01-06 10:54:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:54:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:54:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:54:41 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 10:54:46 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:55:00 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:55:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:55:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:55:22 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:55:22,254][root][INFO] - Before destroying HF.: GPU memory allocated: 70.74 GB
[2025-01-06 10:55:22,523][root][INFO] - After destroying HF.: GPU memory allocated: 55.45 GB
[2025-01-06 10:55:22,524][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:55:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:06<03:02,  6.30s/it, est. speed input: 80.13 toks/s, output: 10.15 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:15,  2.70s/it, est. speed input: 155.99 toks/s, output: 20.23 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:28,  1.09s/it, est. speed input: 299.42 toks/s, output: 40.17 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:11,  2.06it/s, est. speed input: 516.04 toks/s, output: 71.68 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:05,  3.47it/s, est. speed input: 722.12 toks/s, output: 102.96 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  5.20it/s, est. speed input: 921.69 toks/s, output: 134.36 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  6.41it/s, est. speed input: 1046.27 toks/s, output: 154.83 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  5.83it/s, est. speed input: 1120.20 toks/s, output: 169.76 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:00,  9.14it/s, est. speed input: 1361.46 toks/s, output: 216.96 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  8.39it/s, est. speed input: 1435.75 toks/s, output: 235.36 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  6.68it/s, est. speed input: 1473.70 toks/s, output: 250.27 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:09<00:00,  5.84it/s, est. speed input: 1510.58 toks/s, output: 269.54 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  5.25it/s, est. speed input: 1540.12 toks/s, output: 290.67 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  4.50it/s, est. speed input: 1531.64 toks/s, output: 299.66 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.03it/s, est. speed input: 1531.64 toks/s, output: 299.66 toks/s]
[2025-01-06 10:55:32,893][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:55:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:09,  1.38s/it, est. speed input: 453.08 toks/s, output: 19.51 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  5.44it/s, est. speed input: 3541.57 toks/s, output: 157.85 toks/s]
[2025-01-06 10:55:34,758][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:55:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:04,  3.07s/it, est. speed input: 227.36 toks/s, output: 9.43 toks/s]Processed prompts:   9%|▉         | 2/22 [00:03<00:32,  1.62s/it, est. speed input: 380.83 toks/s, output: 19.34 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:22,  1.18s/it, est. speed input: 483.63 toks/s, output: 29.52 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:04<00:09,  1.75it/s, est. speed input: 768.72 toks/s, output: 54.55 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:04<00:07,  2.04it/s, est. speed input: 868.14 toks/s, output: 65.62 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:04<00:05,  2.63it/s, est. speed input: 989.40 toks/s, output: 78.66 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:03,  4.03it/s, est. speed input: 1227.97 toks/s, output: 105.01 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:01,  6.38it/s, est. speed input: 1578.85 toks/s, output: 146.06 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:05<00:01,  6.95it/s, est. speed input: 1764.56 toks/s, output: 172.20 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:05<00:00,  8.08it/s, est. speed input: 1960.45 toks/s, output: 200.88 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:05<00:00, 10.46it/s, est. speed input: 2264.17 toks/s, output: 247.37 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:06<00:00, 10.79it/s, est. speed input: 2431.78 toks/s, output: 276.82 toks/s]Processed prompts: 100%|██████████| 22/22 [00:06<00:00,  3.36it/s, est. speed input: 2346.25 toks/s, output: 276.92 toks/s]
[2025-01-06 10:55:41,857][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:55:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:21,  1.98s/it, est. speed input: 417.02 toks/s, output: 14.66 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:02<00:03,  2.21it/s, est. speed input: 1480.55 toks/s, output: 63.53 toks/s]Processed prompts:  50%|█████     | 6/12 [00:02<00:02,  2.65it/s, est. speed input: 1658.47 toks/s, output: 82.38 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:03<00:01,  2.57it/s, est. speed input: 1673.24 toks/s, output: 108.11 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:03<00:00,  3.28it/s, est. speed input: 1891.80 toks/s, output: 152.81 toks/s]Processed prompts:  92%|█████████▏| 11/12 [00:05<00:00,  2.08it/s, est. speed input: 1599.76 toks/s, output: 152.66 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.31it/s, est. speed input: 1614.66 toks/s, output: 180.76 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.22it/s, est. speed input: 1614.66 toks/s, output: 180.76 toks/s]
[2025-01-06 10:55:47,774][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:55:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1127.06 toks/s, output: 38.41 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1127.06 toks/s, output: 38.41 toks/s]
[2025-01-06 10:55:53,750][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 10:56:00,331][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:56:00,332][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.74 GB
[2025-01-06 10:56:00,681][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.39 GB
[2025-01-06 10:56:10,690][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:56:10,691][root][INFO] - Before destroying HF.: GPU memory allocated: 15.39 GB
[2025-01-06 10:56:11,015][root][INFO] - After destroying HF.: GPU memory allocated: 0.10 GB
[2025-01-06 10:56:11,160][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 10:56:17,849][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:56:30,754][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:56:30,755][root][INFO] - Iteration 7 took 1m 50s. Generation: 66.25%, Training: 33.75%. Estimated time remaining: 1h 18m 45s. Estimated total time for complete run: 1h 31m 44s.
[2025-01-06 10:56:31,047][root][INFO] - Loading VLLM model.
WARNING 01-06 10:56:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:56:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:56:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:56:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 10:56:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:56:50 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:56:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:56:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:57:12 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:57:12,308][root][INFO] - Before destroying HF.: GPU memory allocated: 70.75 GB
[2025-01-06 10:57:12,538][root][INFO] - After destroying HF.: GPU memory allocated: 55.46 GB
[2025-01-06 10:57:12,539][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:57:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:32,  5.27s/it, est. speed input: 95.75 toks/s, output: 10.81 toks/s]Processed prompts:  10%|█         | 3/30 [00:05<00:39,  1.45s/it, est. speed input: 275.50 toks/s, output: 32.01 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:05<00:27,  1.06s/it, est. speed input: 346.93 toks/s, output: 41.73 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:14,  1.64it/s, est. speed input: 494.33 toks/s, output: 62.49 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:11,  2.04it/s, est. speed input: 562.89 toks/s, output: 73.09 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.22it/s, est. speed input: 707.59 toks/s, output: 95.75 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:05,  3.56it/s, est. speed input: 764.46 toks/s, output: 105.66 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:06<00:02,  5.99it/s, est. speed input: 1019.54 toks/s, output: 150.26 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:01,  7.42it/s, est. speed input: 1146.77 toks/s, output: 174.29 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:07<00:01, 10.43it/s, est. speed input: 1342.70 toks/s, output: 212.01 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:00,  9.34it/s, est. speed input: 1428.99 toks/s, output: 232.44 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:07<00:00,  9.56it/s, est. speed input: 1524.68 toks/s, output: 256.10 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:07<00:00, 10.39it/s, est. speed input: 1625.25 toks/s, output: 281.41 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:07<00:00, 11.00it/s, est. speed input: 1720.79 toks/s, output: 307.43 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:08<00:00,  7.01it/s, est. speed input: 1730.89 toks/s, output: 321.48 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.53it/s, est. speed input: 1783.44 toks/s, output: 338.68 toks/s]
[2025-01-06 10:57:21,443][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:57:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:10,  1.45s/it, est. speed input: 465.05 toks/s, output: 20.04 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  5.53it/s, est. speed input: 3567.34 toks/s, output: 160.27 toks/s]
[2025-01-06 10:57:23,278][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:57:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:04,  3.07s/it, est. speed input: 227.38 toks/s, output: 9.43 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:03<00:14,  1.24it/s, est. speed input: 717.80 toks/s, output: 34.66 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:04<00:10,  1.60it/s, est. speed input: 860.36 toks/s, output: 46.03 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:04<00:08,  1.89it/s, est. speed input: 964.90 toks/s, output: 56.60 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:04<00:06,  2.44it/s, est. speed input: 1096.87 toks/s, output: 69.04 toks/s]Processed prompts:  41%|████      | 9/22 [00:04<00:04,  3.04it/s, est. speed input: 1277.92 toks/s, output: 90.60 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:05<00:03,  3.27it/s, est. speed input: 1354.65 toks/s, output: 102.33 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:02,  3.87it/s, est. speed input: 1514.06 toks/s, output: 127.62 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:05<00:02,  4.28it/s, est. speed input: 1597.55 toks/s, output: 141.70 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:02,  3.86it/s, est. speed input: 1623.52 toks/s, output: 152.13 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:06<00:02,  3.38it/s, est. speed input: 1630.77 toks/s, output: 162.22 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  6.06it/s, est. speed input: 1915.21 toks/s, output: 217.52 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:06<00:00,  5.57it/s, est. speed input: 1951.07 toks/s, output: 230.94 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:06<00:00,  6.02it/s, est. speed input: 2018.21 toks/s, output: 248.45 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:07<00:00,  2.84it/s, est. speed input: 1866.63 toks/s, output: 244.28 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  2.80it/s, est. speed input: 1955.47 toks/s, output: 269.71 toks/s]
[2025-01-06 10:57:31,696][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:57:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:19,  1.75s/it, est. speed input: 546.20 toks/s, output: 12.02 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:01<00:08,  1.15it/s, est. speed input: 955.68 toks/s, output: 25.04 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:03<00:03,  2.05it/s, est. speed input: 1407.32 toks/s, output: 58.17 toks/s]Processed prompts:  50%|█████     | 6/12 [00:03<00:03,  1.87it/s, est. speed input: 1339.33 toks/s, output: 72.76 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:04<00:01,  2.86it/s, est. speed input: 1664.32 toks/s, output: 135.65 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:04<00:00,  3.13it/s, est. speed input: 1747.57 toks/s, output: 158.50 toks/s]Processed prompts:  92%|█████████▏| 11/12 [00:05<00:00,  2.58it/s, est. speed input: 1669.65 toks/s, output: 170.97 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.28it/s, est. speed input: 1615.73 toks/s, output: 187.83 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.12it/s, est. speed input: 1615.73 toks/s, output: 187.83 toks/s]
[2025-01-06 10:57:37,819][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:57:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s, est. speed input: 1077.92 toks/s, output: 34.16 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s, est. speed input: 1014.65 toks/s, output: 63.95 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s, est. speed input: 1014.65 toks/s, output: 63.95 toks/s]
[2025-01-06 10:57:44,822][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 10:57:51,277][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:57:51,277][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.75 GB
[2025-01-06 10:57:51,642][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.40 GB
[2025-01-06 10:58:01,959][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:58:01,961][root][INFO] - Before destroying HF.: GPU memory allocated: 15.40 GB
[2025-01-06 10:58:02,285][root][INFO] - After destroying HF.: GPU memory allocated: 0.11 GB
[2025-01-06 10:58:02,436][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 10:58:09,046][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 10:58:22,295][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 10:58:22,296][root][INFO] - Iteration 8 took 1m 51s. Generation: 66.27%, Training: 33.73%. Estimated time remaining: 1h 18m 6s. Estimated total time for complete run: 1h 32m 57s.
[2025-01-06 10:58:22,599][root][INFO] - Loading VLLM model.
WARNING 01-06 10:58:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 10:58:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 10:58:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 10:58:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 10:58:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 10:58:41 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 10:58:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 10:58:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 10:59:03 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 10:59:03,770][root][INFO] - Before destroying HF.: GPU memory allocated: 70.76 GB
[2025-01-06 10:59:04,017][root][INFO] - After destroying HF.: GPU memory allocated: 55.47 GB
[2025-01-06 10:59:04,018][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:59:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:34,  5.31s/it, est. speed input: 95.02 toks/s, output: 10.91 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:09,  2.49s/it, est. speed input: 173.27 toks/s, output: 21.44 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:26,  1.03s/it, est. speed input: 328.22 toks/s, output: 43.22 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:19,  1.31it/s, est. speed input: 400.00 toks/s, output: 54.18 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.34it/s, est. speed input: 551.23 toks/s, output: 77.34 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:05,  3.51it/s, est. speed input: 693.13 toks/s, output: 100.19 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.35it/s, est. speed input: 813.89 toks/s, output: 121.17 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  4.32it/s, est. speed input: 899.87 toks/s, output: 139.95 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  5.52it/s, est. speed input: 1016.78 toks/s, output: 164.03 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  5.76it/s, est. speed input: 1105.43 toks/s, output: 184.77 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:00,  9.43it/s, est. speed input: 1343.49 toks/s, output: 239.31 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00, 10.39it/s, est. speed input: 1446.68 toks/s, output: 264.67 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00, 11.06it/s, est. speed input: 1543.82 toks/s, output: 290.30 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00, 10.07it/s, est. speed input: 1618.82 toks/s, output: 314.15 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:08<00:00,  8.69it/s, est. speed input: 1676.88 toks/s, output: 337.55 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.26it/s, est. speed input: 1644.14 toks/s, output: 339.68 toks/s]
[2025-01-06 10:59:13,661][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 10:59:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:05,  1.19s/it, est. speed input: 523.18 toks/s, output: 22.60 toks/s]Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.84it/s, est. speed input: 3171.05 toks/s, output: 138.68 toks/s]
[2025-01-06 10:59:15,288][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:59:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:15,  3.30s/it, est. speed input: 211.90 toks/s, output: 8.79 toks/s]Processed prompts:   8%|▊         | 2/24 [00:04<00:39,  1.79s/it, est. speed input: 347.17 toks/s, output: 18.13 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:04<00:24,  1.14s/it, est. speed input: 475.93 toks/s, output: 28.37 toks/s]Processed prompts:  17%|█▋        | 4/24 [00:04<00:15,  1.31it/s, est. speed input: 609.12 toks/s, output: 39.43 toks/s]Processed prompts:  21%|██        | 5/24 [00:04<00:10,  1.76it/s, est. speed input: 726.28 toks/s, output: 50.29 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:05<00:08,  2.07it/s, est. speed input: 872.52 toks/s, output: 68.47 toks/s]Processed prompts:  33%|███▎      | 8/24 [00:06<00:07,  2.11it/s, est. speed input: 923.35 toks/s, output: 78.43 toks/s]Processed prompts:  38%|███▊      | 9/24 [00:06<00:05,  2.60it/s, est. speed input: 1013.24 toks/s, output: 91.80 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:06<00:04,  3.05it/s, est. speed input: 1093.60 toks/s, output: 104.82 toks/s]Processed prompts:  46%|████▌     | 11/24 [00:06<00:03,  3.28it/s, est. speed input: 1157.89 toks/s, output: 117.01 toks/s]Processed prompts:  50%|█████     | 12/24 [00:06<00:03,  3.76it/s, est. speed input: 1231.68 toks/s, output: 130.54 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:07<00:02,  3.78it/s, est. speed input: 1284.76 toks/s, output: 142.66 toks/s]Processed prompts:  58%|█████▊    | 14/24 [00:07<00:02,  4.13it/s, est. speed input: 1347.78 toks/s, output: 156.32 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:07<00:01,  6.11it/s, est. speed input: 1510.78 toks/s, output: 188.04 toks/s]Processed prompts:  71%|███████   | 17/24 [00:07<00:01,  5.10it/s, est. speed input: 1543.69 toks/s, output: 199.28 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:07<00:00,  5.85it/s, est. speed input: 1666.72 toks/s, output: 229.78 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  8.15it/s, est. speed input: 1992.88 toks/s, output: 312.55 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  2.85it/s, est. speed input: 1992.88 toks/s, output: 312.55 toks/s]
[2025-01-06 10:59:24,278][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:59:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.56s/it, est. speed input: 449.25 toks/s, output: 16.07 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:01<00:04,  1.42it/s, est. speed input: 965.76 toks/s, output: 32.45 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:03<00:02,  1.91it/s, est. speed input: 1296.46 toks/s, output: 66.63 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:03<00:01,  2.30it/s, est. speed input: 1445.16 toks/s, output: 93.42 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:03<00:00,  2.77it/s, est. speed input: 1584.91 toks/s, output: 120.31 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:03<00:00,  2.48it/s, est. speed input: 1554.57 toks/s, output: 138.69 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.30it/s, est. speed input: 1759.71 toks/s, output: 171.63 toks/s]
[2025-01-06 10:59:28,655][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 10:59:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 1136.25 toks/s, output: 42.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 1136.25 toks/s, output: 42.35 toks/s]
[2025-01-06 10:59:34,810][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 10:59:41,387][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 10:59:41,387][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.76 GB
[2025-01-06 10:59:41,744][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.41 GB
[2025-01-06 10:59:52,227][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 10:59:52,228][root][INFO] - Before destroying HF.: GPU memory allocated: 15.41 GB
[2025-01-06 10:59:52,596][root][INFO] - After destroying HF.: GPU memory allocated: 0.12 GB
[2025-01-06 10:59:52,743][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 10:59:59,443][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:00:12,319][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:00:12,320][root][INFO] - Iteration 9 took 1m 50s. Generation: 65.69%, Training: 34.31%. Estimated time remaining: 1h 15m 0s. Estimated total time for complete run: 1h 31m 41s.
[2025-01-06 11:00:12,583][root][INFO] - Loading VLLM model.
WARNING 01-06 11:00:12 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:00:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:00:13 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:00:13 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 11:00:18 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:00:31 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:00:32 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:00:32 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:00:53 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:00:53,954][root][INFO] - Before destroying HF.: GPU memory allocated: 70.77 GB
[2025-01-06 11:00:54,189][root][INFO] - After destroying HF.: GPU memory allocated: 55.48 GB
[2025-01-06 11:00:54,190][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:00:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:37,  5.44s/it, est. speed input: 92.75 toks/s, output: 10.65 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:11,  2.54s/it, est. speed input: 169.48 toks/s, output: 20.97 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:27,  1.05s/it, est. speed input: 321.39 toks/s, output: 42.32 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:19,  1.29it/s, est. speed input: 391.87 toks/s, output: 53.08 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.22it/s, est. speed input: 684.92 toks/s, output: 98.25 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.14it/s, est. speed input: 815.02 toks/s, output: 120.01 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  4.69it/s, est. speed input: 922.70 toks/s, output: 140.27 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  5.65it/s, est. speed input: 1037.02 toks/s, output: 162.23 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:01,  6.66it/s, est. speed input: 1147.41 toks/s, output: 184.84 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:07<00:01,  8.34it/s, est. speed input: 1265.32 toks/s, output: 209.81 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:00,  9.05it/s, est. speed input: 1366.48 toks/s, output: 232.71 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:00, 11.81it/s, est. speed input: 1535.17 toks/s, output: 271.69 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:08<00:00,  7.70it/s, est. speed input: 1563.78 toks/s, output: 287.86 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  7.17it/s, est. speed input: 1620.63 toks/s, output: 310.83 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  4.57it/s, est. speed input: 1585.04 toks/s, output: 322.66 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.14it/s, est. speed input: 1585.04 toks/s, output: 322.66 toks/s]
[2025-01-06 11:01:04,200][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:01:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:08,  1.34s/it, est. speed input: 465.33 toks/s, output: 20.88 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  5.12it/s, est. speed input: 3387.75 toks/s, output: 147.90 toks/s]
[2025-01-06 11:01:05,974][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:01:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:04<01:39,  4.55s/it, est. speed input: 153.78 toks/s, output: 12.54 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:04<00:24,  1.23s/it, est. speed input: 447.41 toks/s, output: 37.55 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:04<00:17,  1.11it/s, est. speed input: 564.49 toks/s, output: 48.86 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:05<00:08,  1.91it/s, est. speed input: 799.35 toks/s, output: 72.62 toks/s]Processed prompts:  30%|███       | 7/23 [00:05<00:08,  1.94it/s, est. speed input: 853.08 toks/s, output: 81.25 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:05<00:06,  2.40it/s, est. speed input: 949.69 toks/s, output: 94.25 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:06<00:04,  2.85it/s, est. speed input: 1036.24 toks/s, output: 106.90 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:06<00:03,  3.43it/s, est. speed input: 1125.00 toks/s, output: 120.22 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:06<00:02,  4.89it/s, est. speed input: 1308.44 toks/s, output: 148.03 toks/s]Processed prompts:  61%|██████    | 14/23 [00:06<00:01,  6.75it/s, est. speed input: 1498.31 toks/s, output: 177.91 toks/s]Processed prompts:  70%|██████▉   | 16/23 [00:07<00:01,  4.34it/s, est. speed input: 1535.18 toks/s, output: 193.27 toks/s]Processed prompts:  78%|███████▊  | 18/23 [00:07<00:00,  5.05it/s, est. speed input: 1665.28 toks/s, output: 224.07 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:08<00:01,  3.90it/s, est. speed input: 1648.88 toks/s, output: 231.17 toks/s]Processed prompts:  91%|█████████▏| 21/23 [00:08<00:00,  4.56it/s, est. speed input: 1756.17 toks/s, output: 265.36 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:08<00:00,  4.98it/s, est. speed input: 1811.79 toks/s, output: 283.82 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  5.27it/s, est. speed input: 1861.11 toks/s, output: 302.02 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.66it/s, est. speed input: 1861.11 toks/s, output: 302.02 toks/s]
[2025-01-06 11:01:15,140][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:01:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:32,  2.35s/it, est. speed input: 343.98 toks/s, output: 11.89 toks/s]Processed prompts:  60%|██████    | 9/15 [00:02<00:01,  3.95it/s, est. speed input: 2645.58 toks/s, output: 95.88 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:03<00:00,  4.09it/s, est. speed input: 2720.05 toks/s, output: 119.61 toks/s]Processed prompts:  80%|████████  | 12/15 [00:04<00:01,  2.97it/s, est. speed input: 2328.38 toks/s, output: 120.76 toks/s]Processed prompts:  87%|████████▋ | 13/15 [00:04<00:00,  3.20it/s, est. speed input: 2382.63 toks/s, output: 142.13 toks/s]Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.93it/s, est. speed input: 2499.81 toks/s, output: 187.97 toks/s]Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.20it/s, est. speed input: 2499.81 toks/s, output: 187.97 toks/s]
[2025-01-06 11:01:20,331][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:01:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 1001.00 toks/s, output: 43.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 1001.00 toks/s, output: 43.39 toks/s]
[2025-01-06 11:01:26,489][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 11:01:32,998][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:01:32,998][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.77 GB
[2025-01-06 11:01:33,366][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.41 GB
[2025-01-06 11:01:43,601][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:01:43,602][root][INFO] - Before destroying HF.: GPU memory allocated: 15.41 GB
[2025-01-06 11:01:43,874][root][INFO] - After destroying HF.: GPU memory allocated: 0.13 GB
[2025-01-06 11:01:44,028][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:01:50,535][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:02:03,973][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:02:03,974][root][INFO] - Iteration 10 took 1m 51s. Generation: 66.30%, Training: 33.70%. Estimated time remaining: 1h 14m 30s. Estimated total time for complete run: 1h 33m 2s.
[2025-01-06 11:02:04,266][root][INFO] - Loading VLLM model.
WARNING 01-06 11:02:04 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:02:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:02:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:02:05 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:02:10 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:02:24 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:02:24 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:02:24 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:02:45 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:02:45,905][root][INFO] - Before destroying HF.: GPU memory allocated: 70.77 GB
[2025-01-06 11:02:46,144][root][INFO] - After destroying HF.: GPU memory allocated: 55.49 GB
[2025-01-06 11:02:46,145][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:02:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:44,  5.67s/it, est. speed input: 89.11 toks/s, output: 11.29 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:08,  2.43s/it, est. speed input: 172.96 toks/s, output: 22.43 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:26,  1.01s/it, est. speed input: 327.66 toks/s, output: 44.12 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:10,  2.17it/s, est. speed input: 559.62 toks/s, output: 78.36 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:05,  3.56it/s, est. speed input: 776.57 toks/s, output: 112.10 toks/s]Processed prompts:  40%|████      | 12/30 [00:06<00:04,  4.49it/s, est. speed input: 907.38 toks/s, output: 134.16 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:07<00:04,  3.92it/s, est. speed input: 963.56 toks/s, output: 148.83 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:03,  4.11it/s, est. speed input: 1006.45 toks/s, output: 159.04 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:03,  4.63it/s, est. speed input: 1058.27 toks/s, output: 170.92 toks/s]Processed prompts:  60%|██████    | 18/30 [00:07<00:01,  6.18it/s, est. speed input: 1169.97 toks/s, output: 196.28 toks/s]Processed prompts:  70%|███████   | 21/30 [00:07<00:01,  8.83it/s, est. speed input: 1338.52 toks/s, output: 236.02 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  9.94it/s, est. speed input: 1440.43 toks/s, output: 262.17 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:08<00:00, 10.62it/s, est. speed input: 1578.92 toks/s, output: 299.91 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  7.72it/s, est. speed input: 1612.86 toks/s, output: 318.46 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  4.95it/s, est. speed input: 1586.99 toks/s, output: 331.54 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.14it/s, est. speed input: 1586.99 toks/s, output: 331.54 toks/s]
[2025-01-06 11:02:56,118][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:02:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:05,  1.19s/it, est. speed input: 549.70 toks/s, output: 22.76 toks/s]Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.74it/s, est. speed input: 3165.39 toks/s, output: 136.80 toks/s]
[2025-01-06 11:02:57,788][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:02:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:15,  3.30s/it, est. speed input: 211.97 toks/s, output: 8.79 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:04<00:23,  1.14s/it, est. speed input: 517.03 toks/s, output: 25.40 toks/s]Processed prompts:  21%|██        | 5/24 [00:04<00:13,  1.45it/s, est. speed input: 761.25 toks/s, output: 45.09 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:04<00:09,  1.82it/s, est. speed input: 880.26 toks/s, output: 56.25 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:05<00:08,  1.99it/s, est. speed input: 951.93 toks/s, output: 65.76 toks/s]Processed prompts:  33%|███▎      | 8/24 [00:05<00:07,  2.05it/s, est. speed input: 1000.66 toks/s, output: 74.98 toks/s]Processed prompts:  38%|███▊      | 9/24 [00:05<00:06,  2.42it/s, est. speed input: 1081.50 toks/s, output: 86.99 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:05<00:04,  2.90it/s, est. speed input: 1165.05 toks/s, output: 99.67 toks/s]Processed prompts:  46%|████▌     | 11/24 [00:06<00:04,  2.70it/s, est. speed input: 1196.02 toks/s, output: 109.20 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:06<00:02,  4.32it/s, est. speed input: 1384.82 toks/s, output: 139.44 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:06<00:01,  5.45it/s, est. speed input: 1605.86 toks/s, output: 180.49 toks/s]Processed prompts:  71%|███████   | 17/24 [00:07<00:01,  5.28it/s, est. speed input: 1655.20 toks/s, output: 193.20 toks/s]Processed prompts:  75%|███████▌  | 18/24 [00:07<00:01,  5.84it/s, est. speed input: 1727.49 toks/s, output: 208.83 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:07<00:01,  4.00it/s, est. speed input: 1704.62 toks/s, output: 215.12 toks/s]Processed prompts:  83%|████████▎ | 20/24 [00:07<00:00,  4.63it/s, est. speed input: 1768.52 toks/s, output: 232.26 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:08<00:00,  4.00it/s, est. speed input: 1779.17 toks/s, output: 243.86 toks/s]Processed prompts:  92%|█████████▏| 22/24 [00:08<00:00,  4.44it/s, est. speed input: 1828.58 toks/s, output: 261.12 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:08<00:00,  4.09it/s, est. speed input: 1846.96 toks/s, output: 275.26 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  2.76it/s, est. speed input: 1927.23 toks/s, output: 298.23 toks/s]
[2025-01-06 11:03:07,077][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:03:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.56s/it, est. speed input: 560.14 toks/s, output: 16.00 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:01<00:04,  1.42it/s, est. speed input: 1095.30 toks/s, output: 32.34 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:02<00:03,  1.67it/s, est. speed input: 1182.57 toks/s, output: 47.25 toks/s]Processed prompts:  44%|████▍     | 4/9 [00:02<00:02,  1.87it/s, est. speed input: 1254.01 toks/s, output: 64.51 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:02<00:01,  2.19it/s, est. speed input: 1357.42 toks/s, output: 84.71 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:03<00:01,  2.61it/s, est. speed input: 1476.86 toks/s, output: 106.97 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:03<00:00,  2.92it/s, est. speed input: 1644.71 toks/s, output: 129.12 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:04<00:00,  1.94it/s, est. speed input: 1468.79 toks/s, output: 137.80 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  1.64it/s, est. speed input: 1370.08 toks/s, output: 154.94 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  1.77it/s, est. speed input: 1370.08 toks/s, output: 154.94 toks/s]
[2025-01-06 11:03:12,630][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:03:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 1188.36 toks/s, output: 42.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 1188.36 toks/s, output: 42.13 toks/s]
[2025-01-06 11:03:18,654][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:03:25,299][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:03:25,299][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.77 GB
[2025-01-06 11:03:25,694][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.42 GB
[2025-01-06 11:03:36,092][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:03:36,093][root][INFO] - Before destroying HF.: GPU memory allocated: 15.42 GB
[2025-01-06 11:03:36,474][root][INFO] - After destroying HF.: GPU memory allocated: 0.13 GB
[2025-01-06 11:03:36,630][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 11:03:43,224][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:03:56,347][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:03:56,348][root][INFO] - Iteration 11 took 1m 52s. Generation: 66.33%, Training: 33.67%. Estimated time remaining: 1h 13m 13s. Estimated total time for complete run: 1h 33m 38s.
[2025-01-06 11:03:56,615][root][INFO] - Loading VLLM model.
WARNING 01-06 11:03:56 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:03:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:03:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:03:57 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:04:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:04:16 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:04:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:04:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:04:38 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:04:38,215][root][INFO] - Before destroying HF.: GPU memory allocated: 70.78 GB
[2025-01-06 11:04:38,444][root][INFO] - After destroying HF.: GPU memory allocated: 55.49 GB
[2025-01-06 11:04:38,445][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:04:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:40,  5.52s/it, est. speed input: 91.50 toks/s, output: 10.51 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:12,  2.58s/it, est. speed input: 167.38 toks/s, output: 20.72 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:28,  1.11s/it, est. speed input: 309.76 toks/s, output: 41.25 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:15,  1.57it/s, est. speed input: 450.50 toks/s, output: 63.19 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:11,  1.95it/s, est. speed input: 514.09 toks/s, output: 73.88 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:07,  2.87it/s, est. speed input: 638.83 toks/s, output: 95.72 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:07<00:02,  6.28it/s, est. speed input: 971.10 toks/s, output: 154.94 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:01,  7.70it/s, est. speed input: 1145.07 toks/s, output: 188.60 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:07<00:01,  6.50it/s, est. speed input: 1206.43 toks/s, output: 204.32 toks/s]Processed prompts:  70%|███████   | 21/30 [00:08<00:01,  6.21it/s, est. speed input: 1275.23 toks/s, output: 224.62 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:08<00:01,  5.57it/s, est. speed input: 1292.27 toks/s, output: 232.86 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:01,  5.84it/s, est. speed input: 1330.26 toks/s, output: 245.21 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:08<00:00,  8.54it/s, est. speed input: 1479.64 toks/s, output: 289.39 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:09<00:00,  5.92it/s, est. speed input: 1494.73 toks/s, output: 307.19 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  4.62it/s, est. speed input: 1479.06 toks/s, output: 313.38 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.01it/s, est. speed input: 1522.31 toks/s, output: 331.89 toks/s]
[2025-01-06 11:04:48,810][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:04:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:06,  1.16s/it, est. speed input: 542.62 toks/s, output: 18.17 toks/s]Processed prompts:  29%|██▊       | 2/7 [00:01<00:02,  1.70it/s, est. speed input: 974.09 toks/s, output: 37.09 toks/s]Processed prompts: 100%|██████████| 7/7 [00:02<00:00,  4.42it/s, est. speed input: 2298.60 toks/s, output: 115.71 toks/s]Processed prompts: 100%|██████████| 7/7 [00:02<00:00,  3.42it/s, est. speed input: 2298.60 toks/s, output: 115.71 toks/s]
[2025-01-06 11:04:51,247][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:04:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:09,  3.17s/it, est. speed input: 220.22 toks/s, output: 9.14 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:04<00:28,  1.40s/it, est. speed input: 443.02 toks/s, output: 25.56 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:05<00:19,  1.01s/it, est. speed input: 559.16 toks/s, output: 38.00 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:05<00:05,  2.73it/s, est. speed input: 1083.09 toks/s, output: 92.00 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:05<00:03,  3.54it/s, est. speed input: 1299.84 toks/s, output: 116.78 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:05<00:02,  4.23it/s, est. speed input: 1484.96 toks/s, output: 141.98 toks/s]Processed prompts:  61%|██████    | 14/23 [00:05<00:01,  5.00it/s, est. speed input: 1662.12 toks/s, output: 167.30 toks/s]Processed prompts:  70%|██████▉   | 16/23 [00:06<00:01,  6.26it/s, est. speed input: 1856.17 toks/s, output: 196.17 toks/s]Processed prompts:  78%|███████▊  | 18/23 [00:06<00:01,  4.74it/s, est. speed input: 1883.24 toks/s, output: 212.99 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:07<00:01,  3.59it/s, est. speed input: 1826.73 toks/s, output: 216.63 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:08<00:01,  2.66it/s, est. speed input: 1741.91 toks/s, output: 219.54 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:08<00:00,  3.54it/s, est. speed input: 1859.25 toks/s, output: 260.30 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.78it/s, est. speed input: 1943.73 toks/s, output: 284.48 toks/s]
[2025-01-06 11:05:00,089][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:05:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:14,  1.61s/it, est. speed input: 547.17 toks/s, output: 15.56 toks/s]Processed prompts:  20%|██        | 2/10 [00:01<00:05,  1.37it/s, est. speed input: 917.29 toks/s, output: 31.39 toks/s]Processed prompts:  50%|█████     | 5/10 [00:02<00:01,  2.76it/s, est. speed input: 1534.48 toks/s, output: 70.92 toks/s]Processed prompts:  70%|███████   | 7/10 [00:02<00:00,  3.30it/s, est. speed input: 1819.17 toks/s, output: 108.46 toks/s]Processed prompts:  80%|████████  | 8/10 [00:03<00:00,  3.58it/s, est. speed input: 1931.66 toks/s, output: 129.93 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  3.42it/s, est. speed input: 1949.47 toks/s, output: 148.23 toks/s]Processed prompts: 100%|██████████| 10/10 [00:03<00:00,  2.85it/s, est. speed input: 1868.76 toks/s, output: 163.35 toks/s]Processed prompts: 100%|██████████| 10/10 [00:03<00:00,  2.60it/s, est. speed input: 1868.76 toks/s, output: 163.35 toks/s]
[2025-01-06 11:05:04,402][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:05:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 1225.99 toks/s, output: 41.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 1225.99 toks/s, output: 41.49 toks/s]
[2025-01-06 11:05:10,587][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 11:05:17,209][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:05:17,209][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.78 GB
[2025-01-06 11:05:17,603][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.43 GB
[2025-01-06 11:05:27,889][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:05:27,890][root][INFO] - Before destroying HF.: GPU memory allocated: 15.43 GB
[2025-01-06 11:05:28,185][root][INFO] - After destroying HF.: GPU memory allocated: 0.14 GB
[2025-01-06 11:05:28,361][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 11:05:34,917][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:05:47,911][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:05:47,912][root][INFO] - Iteration 12 took 1m 51s. Generation: 66.40%, Training: 33.60%. Estimated time remaining: 1h 10m 41s. Estimated total time for complete run: 1h 32m 58s.
[2025-01-06 11:05:48,217][root][INFO] - Loading VLLM model.
WARNING 01-06 11:05:48 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:05:48 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:05:49 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:05:49 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:05:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:06:07 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:06:08 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:06:08 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:06:29 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:06:29,532][root][INFO] - Before destroying HF.: GPU memory allocated: 70.79 GB
[2025-01-06 11:06:29,758][root][INFO] - After destroying HF.: GPU memory allocated: 55.50 GB
[2025-01-06 11:06:29,759][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:06:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:34,  5.33s/it, est. speed input: 94.79 toks/s, output: 10.89 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:09,  2.50s/it, est. speed input: 172.88 toks/s, output: 21.40 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:25,  1.00it/s, est. speed input: 333.34 toks/s, output: 43.56 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:18,  1.38it/s, est. speed input: 409.53 toks/s, output: 54.66 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.38it/s, est. speed input: 559.55 toks/s, output: 77.09 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.46it/s, est. speed input: 837.69 toks/s, output: 120.79 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:06<00:02,  6.24it/s, est. speed input: 1039.91 toks/s, output: 155.62 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:06<00:01,  7.02it/s, est. speed input: 1156.50 toks/s, output: 177.63 toks/s]Processed prompts:  60%|██████    | 18/30 [00:07<00:01,  7.58it/s, est. speed input: 1263.42 toks/s, output: 199.45 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:01,  8.89it/s, est. speed input: 1379.67 toks/s, output: 223.48 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:07<00:00,  9.89it/s, est. speed input: 1488.03 toks/s, output: 247.65 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:00,  7.30it/s, est. speed input: 1531.13 toks/s, output: 264.66 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  9.11it/s, est. speed input: 1680.53 toks/s, output: 305.66 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:08<00:00,  6.36it/s, est. speed input: 1685.57 toks/s, output: 320.08 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  6.30it/s, est. speed input: 1710.95 toks/s, output: 333.72 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.39it/s, est. speed input: 1710.95 toks/s, output: 333.72 toks/s]
[2025-01-06 11:06:39,039][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:06:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:11,  1.41s/it, est. speed input: 484.17 toks/s, output: 17.06 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:01<00:02,  2.42it/s, est. speed input: 1321.01 toks/s, output: 50.13 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  3.37it/s, est. speed input: 1927.84 toks/s, output: 111.28 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.90it/s, est. speed input: 1927.84 toks/s, output: 111.28 toks/s]
[2025-01-06 11:06:42,530][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:06:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:04<01:25,  4.29s/it, est. speed input: 162.91 toks/s, output: 13.52 toks/s]Processed prompts:  10%|▉         | 2/21 [00:04<00:36,  1.92s/it, est. speed input: 306.76 toks/s, output: 26.77 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:04<00:20,  1.12s/it, est. speed input: 443.29 toks/s, output: 40.16 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:05<00:07,  2.11it/s, est. speed input: 813.63 toks/s, output: 78.96 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:05<00:05,  2.54it/s, est. speed input: 982.92 toks/s, output: 102.30 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:05<00:04,  2.72it/s, est. speed input: 1055.46 toks/s, output: 114.76 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:06<00:02,  3.89it/s, est. speed input: 1256.99 toks/s, output: 145.99 toks/s]Processed prompts:  57%|█████▋    | 12/21 [00:06<00:02,  4.28it/s, est. speed input: 1338.80 toks/s, output: 160.41 toks/s]Processed prompts:  62%|██████▏   | 13/21 [00:06<00:01,  4.57it/s, est. speed input: 1412.03 toks/s, output: 174.50 toks/s]Processed prompts:  76%|███████▌  | 16/21 [00:06<00:00,  5.13it/s, est. speed input: 1609.38 toks/s, output: 215.99 toks/s]Processed prompts:  86%|████████▌ | 18/21 [00:07<00:00,  4.86it/s, est. speed input: 1699.12 toks/s, output: 243.35 toks/s]Processed prompts:  90%|█████████ | 19/21 [00:07<00:00,  4.72it/s, est. speed input: 1737.56 toks/s, output: 258.26 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:08<00:00,  3.68it/s, est. speed input: 1717.44 toks/s, output: 266.95 toks/s]Processed prompts: 100%|██████████| 21/21 [00:08<00:00,  2.57it/s, est. speed input: 1799.52 toks/s, output: 290.91 toks/s]
[2025-01-06 11:06:51,248][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:06:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:16,  1.65s/it, est. speed input: 309.72 toks/s, output: 15.18 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:01<00:03,  2.13it/s, est. speed input: 974.52 toks/s, output: 46.49 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:02<00:01,  3.30it/s, est. speed input: 1691.39 toks/s, output: 76.39 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:02<00:01,  3.84it/s, est. speed input: 1898.48 toks/s, output: 92.31 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:02<00:01,  2.91it/s, est. speed input: 1697.38 toks/s, output: 99.27 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:03<00:01,  2.80it/s, est. speed input: 1649.93 toks/s, output: 115.29 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:03<00:00,  2.26it/s, est. speed input: 1549.62 toks/s, output: 127.51 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:04<00:00,  2.11it/s, est. speed input: 1513.89 toks/s, output: 146.27 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  1.75it/s, est. speed input: 1414.55 toks/s, output: 162.37 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  2.13it/s, est. speed input: 1414.55 toks/s, output: 162.37 toks/s]
[2025-01-06 11:06:56,862][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:06:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it, est. speed input: 564.97 toks/s, output: 28.54 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.54it/s, est. speed input: 1558.29 toks/s, output: 93.73 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.27it/s, est. speed input: 1558.29 toks/s, output: 93.73 toks/s]
[2025-01-06 11:07:04,223][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 11:07:10,737][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:07:10,738][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.79 GB
[2025-01-06 11:07:11,120][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.44 GB
[2025-01-06 11:07:21,252][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:07:21,253][root][INFO] - Before destroying HF.: GPU memory allocated: 15.44 GB
[2025-01-06 11:07:21,557][root][INFO] - After destroying HF.: GPU memory allocated: 0.15 GB
[2025-01-06 11:07:21,724][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 11:07:28,263][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:07:40,951][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:07:40,952][root][INFO] - Iteration 13 took 1m 53s. Generation: 67.38%, Training: 32.62%. Estimated time remaining: 1h 10m 2s. Estimated total time for complete run: 1h 34m 11s.
[2025-01-06 11:07:41,279][root][INFO] - Loading VLLM model.
WARNING 01-06 11:07:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:07:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:07:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:07:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:07:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:08:00 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:08:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:08:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:08:22 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:08:22,743][root][INFO] - Before destroying HF.: GPU memory allocated: 70.80 GB
[2025-01-06 11:08:22,972][root][INFO] - After destroying HF.: GPU memory allocated: 55.51 GB
[2025-01-06 11:08:22,973][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:08:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:53,  5.97s/it, est. speed input: 84.60 toks/s, output: 9.72 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:16,  2.73s/it, est. speed input: 157.16 toks/s, output: 19.29 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:30,  1.19s/it, est. speed input: 289.93 toks/s, output: 38.32 toks/s]Processed prompts:  20%|██        | 6/30 [00:07<00:15,  1.54it/s, est. speed input: 428.47 toks/s, output: 59.67 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:07<00:09,  2.29it/s, est. speed input: 552.39 toks/s, output: 79.71 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:07<00:07,  2.83it/s, est. speed input: 653.62 toks/s, output: 98.24 toks/s]Processed prompts:  40%|████      | 12/30 [00:07<00:04,  3.93it/s, est. speed input: 771.19 toks/s, output: 120.64 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  6.12it/s, est. speed input: 949.76 toks/s, output: 155.22 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:08<00:01,  6.78it/s, est. speed input: 1047.98 toks/s, output: 176.39 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:08<00:01,  7.50it/s, est. speed input: 1143.55 toks/s, output: 198.08 toks/s]Processed prompts:  70%|███████   | 21/30 [00:08<00:01,  8.33it/s, est. speed input: 1237.98 toks/s, output: 220.40 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  9.54it/s, est. speed input: 1334.56 toks/s, output: 244.39 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00, 10.50it/s, est. speed input: 1426.76 toks/s, output: 268.51 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:09<00:00,  7.97it/s, est. speed input: 1475.30 toks/s, output: 287.38 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  6.66it/s, est. speed input: 1516.39 toks/s, output: 308.14 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  6.30it/s, est. speed input: 1536.90 toks/s, output: 320.06 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.04it/s, est. speed input: 1536.90 toks/s, output: 320.06 toks/s]
[2025-01-06 11:08:33,265][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:08:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:09,  1.39s/it, est. speed input: 461.97 toks/s, output: 19.49 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  6.48it/s, est. speed input: 3279.10 toks/s, output: 149.61 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  4.99it/s, est. speed input: 3279.10 toks/s, output: 149.61 toks/s]
[2025-01-06 11:08:35,263][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:08:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:04,  3.09s/it, est. speed input: 226.51 toks/s, output: 9.40 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:27,  1.46s/it, est. speed input: 431.60 toks/s, output: 26.14 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:05<00:13,  1.28it/s, est. speed input: 678.16 toks/s, output: 52.97 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:05<00:07,  2.00it/s, est. speed input: 909.09 toks/s, output: 80.08 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:05<00:05,  2.44it/s, est. speed input: 1018.16 toks/s, output: 93.95 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:05<00:03,  3.48it/s, est. speed input: 1226.46 toks/s, output: 121.77 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:02,  4.84it/s, est. speed input: 1439.20 toks/s, output: 151.33 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:01,  5.89it/s, est. speed input: 1623.75 toks/s, output: 179.03 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:01,  4.30it/s, est. speed input: 1656.90 toks/s, output: 198.07 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  5.53it/s, est. speed input: 1826.20 toks/s, output: 232.67 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:07<00:00,  4.48it/s, est. speed input: 1819.46 toks/s, output: 240.70 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  5.08it/s, est. speed input: 1973.81 toks/s, output: 289.05 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  2.82it/s, est. speed input: 1973.81 toks/s, output: 289.05 toks/s]
[2025-01-06 11:08:43,594][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:08:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:18,  1.87s/it, est. speed input: 447.69 toks/s, output: 15.55 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:02<00:04,  1.67it/s, est. speed input: 1178.12 toks/s, output: 54.14 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:02<00:01,  2.60it/s, est. speed input: 1559.38 toks/s, output: 95.66 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:03<00:01,  2.88it/s, est. speed input: 1666.54 toks/s, output: 114.40 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:03<00:00,  3.24it/s, est. speed input: 1776.40 toks/s, output: 134.55 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:03<00:00,  2.79it/s, est. speed input: 1729.74 toks/s, output: 147.15 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:04<00:00,  2.21it/s, est. speed input: 1619.21 toks/s, output: 158.27 toks/s]Processed prompts: 100%|██████████| 11/11 [00:04<00:00,  2.36it/s, est. speed input: 1751.76 toks/s, output: 190.27 toks/s]
[2025-01-06 11:08:54,026][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 11:09:00,482][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:09:00,483][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.80 GB
[2025-01-06 11:09:00,848][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.45 GB
[2025-01-06 11:09:11,295][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:09:11,296][root][INFO] - Before destroying HF.: GPU memory allocated: 15.45 GB
[2025-01-06 11:09:11,599][root][INFO] - After destroying HF.: GPU memory allocated: 0.16 GB
[2025-01-06 11:09:12,158][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
[2025-01-06 11:09:18,397][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:09:31,203][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:09:31,204][root][INFO] - Iteration 14 took 1m 50s. Generation: 66.15%, Training: 33.85%. Estimated time remaining: 1h 5m 52s. Estimated total time for complete run: 1h 31m 52s.
[2025-01-06 11:09:31,471][root][INFO] - Loading VLLM model.
WARNING 01-06 11:09:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:09:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:09:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:09:32 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:09:37 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:09:51 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:09:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:09:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:10:13 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:10:13,451][root][INFO] - Before destroying HF.: GPU memory allocated: 70.80 GB
[2025-01-06 11:10:13,698][root][INFO] - After destroying HF.: GPU memory allocated: 55.52 GB
[2025-01-06 11:10:13,699][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:10:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:53,  5.98s/it, est. speed input: 84.42 toks/s, output: 9.70 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:17,  2.77s/it, est. speed input: 155.47 toks/s, output: 19.24 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:29,  1.13s/it, est. speed input: 296.11 toks/s, output: 38.99 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:20,  1.20it/s, est. speed input: 361.75 toks/s, output: 49.00 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:07<00:10,  2.15it/s, est. speed input: 499.26 toks/s, output: 70.05 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:07,  2.74it/s, est. speed input: 605.08 toks/s, output: 88.80 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:07<00:05,  3.67it/s, est. speed input: 718.28 toks/s, output: 109.52 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:03,  4.97it/s, est. speed input: 835.06 toks/s, output: 131.78 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  6.50it/s, est. speed input: 949.80 toks/s, output: 154.10 toks/s]Processed prompts:  60%|██████    | 18/30 [00:08<00:01,  9.56it/s, est. speed input: 1125.08 toks/s, output: 189.12 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:08<00:00, 13.94it/s, est. speed input: 1354.94 toks/s, output: 236.72 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  9.69it/s, est. speed input: 1449.60 toks/s, output: 263.74 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:09<00:00,  6.84it/s, est. speed input: 1468.74 toks/s, output: 278.99 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  6.42it/s, est. speed input: 1517.16 toks/s, output: 301.77 toks/s]Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.96it/s, est. speed input: 1494.86 toks/s, output: 307.16 toks/s]
[2025-01-06 11:10:24,266][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:10:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:04,  1.11s/it, est. speed input: 579.77 toks/s, output: 26.15 toks/s]Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  4.37it/s, est. speed input: 2924.86 toks/s, output: 128.61 toks/s]
[2025-01-06 11:10:25,806][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:10:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:21,  3.41s/it, est. speed input: 205.24 toks/s, output: 8.52 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:04<00:29,  1.33s/it, est. speed input: 454.01 toks/s, output: 24.25 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:04<00:20,  1.05it/s, est. speed input: 575.76 toks/s, output: 35.21 toks/s]Processed prompts:  28%|██▊       | 7/25 [00:05<00:07,  2.29it/s, est. speed input: 955.82 toks/s, output: 69.54 toks/s]Processed prompts:  36%|███▌      | 9/25 [00:05<00:05,  2.90it/s, est. speed input: 1146.23 toks/s, output: 90.55 toks/s]Processed prompts:  40%|████      | 10/25 [00:06<00:06,  2.28it/s, est. speed input: 1111.67 toks/s, output: 94.15 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:06<00:04,  2.76it/s, est. speed input: 1242.31 toks/s, output: 118.04 toks/s]Processed prompts:  52%|█████▏    | 13/25 [00:06<00:03,  3.04it/s, est. speed input: 1306.47 toks/s, output: 130.98 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:07<00:01,  5.87it/s, est. speed input: 1672.38 toks/s, output: 193.79 toks/s]Processed prompts:  76%|███████▌  | 19/25 [00:07<00:01,  5.50it/s, est. speed input: 1764.22 toks/s, output: 217.72 toks/s]Processed prompts:  80%|████████  | 20/25 [00:07<00:01,  4.90it/s, est. speed input: 1782.59 toks/s, output: 227.86 toks/s]Processed prompts:  84%|████████▍ | 21/25 [00:08<00:01,  3.84it/s, est. speed input: 1759.80 toks/s, output: 234.62 toks/s]Processed prompts:  88%|████████▊ | 22/25 [00:08<00:01,  2.91it/s, est. speed input: 1710.70 toks/s, output: 239.95 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  2.78it/s, est. speed input: 1943.94 toks/s, output: 306.69 toks/s]
[2025-01-06 11:10:35,364][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:10:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.69s/it, est. speed input: 301.29 toks/s, output: 14.77 toks/s]Processed prompts:  30%|███       | 3/10 [00:01<00:03,  2.08it/s, est. speed input: 1323.03 toks/s, output: 45.41 toks/s]Processed prompts:  50%|█████     | 5/10 [00:02<00:02,  2.41it/s, est. speed input: 1684.64 toks/s, output: 67.39 toks/s]Processed prompts:  60%|██████    | 6/10 [00:02<00:01,  2.62it/s, est. speed input: 1757.91 toks/s, output: 85.68 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:01,  2.97it/s, est. speed input: 1862.53 toks/s, output: 106.33 toks/s]Processed prompts:  80%|████████  | 8/10 [00:03<00:00,  3.30it/s, est. speed input: 1952.21 toks/s, output: 127.53 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:04<00:00,  2.08it/s, est. speed input: 1749.97 toks/s, output: 133.48 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s, est. speed input: 1749.66 toks/s, output: 158.18 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s, est. speed input: 1749.66 toks/s, output: 158.18 toks/s]
[2025-01-06 11:10:40,403][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:10:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 862.50 toks/s, output: 43.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 862.50 toks/s, output: 43.57 toks/s]
[2025-01-06 11:10:46,792][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 11:10:53,241][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:10:53,242][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.81 GB
[2025-01-06 11:10:53,606][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.45 GB
[2025-01-06 11:11:03,646][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:11:03,647][root][INFO] - Before destroying HF.: GPU memory allocated: 15.45 GB
[2025-01-06 11:11:03,964][root][INFO] - After destroying HF.: GPU memory allocated: 0.17 GB
[2025-01-06 11:11:04,111][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 11:11:10,703][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:11:23,851][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:11:23,852][root][INFO] - Iteration 15 took 1m 52s. Generation: 66.97%, Training: 33.03%. Estimated time remaining: 1h 5m 59s. Estimated total time for complete run: 1h 33m 52s.
[2025-01-06 11:11:24,117][root][INFO] - Loading VLLM model.
WARNING 01-06 11:11:24 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:11:24 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:11:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:11:24 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:11:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:11:43 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:11:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:11:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:12:05 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:12:05,638][root][INFO] - Before destroying HF.: GPU memory allocated: 70.81 GB
[2025-01-06 11:12:05,871][root][INFO] - After destroying HF.: GPU memory allocated: 55.52 GB
[2025-01-06 11:12:05,872][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:12:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:21,  4.88s/it, est. speed input: 103.58 toks/s, output: 10.26 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:07,  2.41s/it, est. speed input: 181.61 toks/s, output: 20.14 toks/s]Processed prompts:  10%|█         | 3/30 [00:05<00:38,  1.41s/it, est. speed input: 261.93 toks/s, output: 30.77 toks/s]Processed prompts:  20%|██        | 6/30 [00:05<00:12,  1.96it/s, est. speed input: 510.28 toks/s, output: 63.83 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:10,  2.29it/s, est. speed input: 575.93 toks/s, output: 73.64 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.47it/s, est. speed input: 723.59 toks/s, output: 95.84 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.64it/s, est. speed input: 859.75 toks/s, output: 117.63 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:06<00:02,  5.86it/s, est. speed input: 989.87 toks/s, output: 139.77 toks/s]Processed prompts:  50%|█████     | 15/30 [00:06<00:02,  6.29it/s, est. speed input: 1097.47 toks/s, output: 159.95 toks/s]Processed prompts:  60%|██████    | 18/30 [00:07<00:01,  6.92it/s, est. speed input: 1249.67 toks/s, output: 191.09 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:01,  8.22it/s, est. speed input: 1364.60 toks/s, output: 216.17 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:07<00:00,  8.61it/s, est. speed input: 1460.64 toks/s, output: 240.07 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:00,  8.88it/s, est. speed input: 1551.02 toks/s, output: 263.88 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:07<00:00,  9.62it/s, est. speed input: 1645.30 toks/s, output: 290.34 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:08<00:00,  9.29it/s, est. speed input: 1760.06 toks/s, output: 327.49 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.27it/s, est. speed input: 1652.49 toks/s, output: 318.72 toks/s]
[2025-01-06 11:12:15,493][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:12:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:04,  1.10s/it, est. speed input: 673.49 toks/s, output: 26.36 toks/s]Processed prompts: 100%|██████████| 5/5 [00:02<00:00,  1.88it/s, est. speed input: 1152.03 toks/s, output: 88.64 toks/s]Processed prompts: 100%|██████████| 5/5 [00:02<00:00,  1.77it/s, est. speed input: 1152.03 toks/s, output: 88.64 toks/s]
[2025-01-06 11:12:18,712][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:12:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:22,  3.42s/it, est. speed input: 204.46 toks/s, output: 8.48 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:03<00:23,  1.07s/it, est. speed input: 537.07 toks/s, output: 24.84 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:05<00:23,  1.11s/it, est. speed input: 549.42 toks/s, output: 31.64 toks/s]Processed prompts:  20%|██        | 5/25 [00:05<00:16,  1.20it/s, est. speed input: 651.43 toks/s, output: 43.06 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:05<00:11,  1.64it/s, est. speed input: 762.71 toks/s, output: 55.28 toks/s]Processed prompts:  32%|███▏      | 8/25 [00:05<00:06,  2.79it/s, est. speed input: 986.25 toks/s, output: 80.42 toks/s]Processed prompts:  40%|████      | 10/25 [00:05<00:03,  4.22it/s, est. speed input: 1207.19 toks/s, output: 106.21 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:06<00:02,  5.25it/s, est. speed input: 1395.79 toks/s, output: 130.96 toks/s]Processed prompts:  64%|██████▍   | 16/25 [00:06<00:01,  6.99it/s, est. speed input: 1747.33 toks/s, output: 179.36 toks/s]Processed prompts:  76%|███████▌  | 19/25 [00:06<00:00,  9.52it/s, est. speed input: 2040.76 toks/s, output: 223.11 toks/s]Processed prompts:  84%|████████▍ | 21/25 [00:06<00:00, 10.34it/s, est. speed input: 2206.69 toks/s, output: 250.90 toks/s]Processed prompts:  92%|█████████▏| 23/25 [00:07<00:00,  4.21it/s, est. speed input: 2033.17 toks/s, output: 252.30 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  3.99it/s, est. speed input: 2061.34 toks/s, output: 281.21 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  2.95it/s, est. speed input: 2061.34 toks/s, output: 281.21 toks/s]
[2025-01-06 11:12:27,751][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:12:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:08,  1.39s/it, est. speed input: 614.61 toks/s, output: 20.85 toks/s]Processed prompts:  29%|██▊       | 2/7 [00:01<00:03,  1.37it/s, est. speed input: 1092.73 toks/s, output: 41.68 toks/s]Processed prompts:  43%|████▎     | 3/7 [00:02<00:02,  1.72it/s, est. speed input: 1218.01 toks/s, output: 61.68 toks/s]Processed prompts:  57%|█████▋    | 4/7 [00:02<00:02,  1.47it/s, est. speed input: 1106.79 toks/s, output: 77.31 toks/s]Processed prompts:  71%|███████▏  | 5/7 [00:03<00:01,  1.42it/s, est. speed input: 1019.12 toks/s, output: 98.43 toks/s]Processed prompts:  86%|████████▌ | 6/7 [00:03<00:00,  1.89it/s, est. speed input: 1152.56 toks/s, output: 131.54 toks/s]Processed prompts: 100%|██████████| 7/7 [00:04<00:00,  2.15it/s, est. speed input: 1227.92 toks/s, output: 160.60 toks/s]Processed prompts: 100%|██████████| 7/7 [00:04<00:00,  1.68it/s, est. speed input: 1227.92 toks/s, output: 160.60 toks/s]
[2025-01-06 11:12:37,862][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:12:44,357][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:12:44,357][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.81 GB
[2025-01-06 11:12:44,722][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.46 GB
[2025-01-06 11:12:55,214][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:12:55,215][root][INFO] - Before destroying HF.: GPU memory allocated: 15.46 GB
[2025-01-06 11:12:55,520][root][INFO] - After destroying HF.: GPU memory allocated: 0.17 GB
[2025-01-06 11:12:55,702][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:13:02,194][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:13:15,124][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:13:15,125][root][INFO] - Iteration 16 took 1m 51s. Generation: 66.37%, Training: 33.63%. Estimated time remaining: 1h 2m 59s. Estimated total time for complete run: 1h 32m 43s.
[2025-01-06 11:13:15,442][root][INFO] - Loading VLLM model.
WARNING 01-06 11:13:15 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:13:15 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:13:16 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:13:16 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:13:20 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:13:34 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:13:35 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:13:35 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:13:56 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 11:13:56,683][root][INFO] - Before destroying HF.: GPU memory allocated: 70.82 GB
[2025-01-06 11:13:56,916][root][INFO] - After destroying HF.: GPU memory allocated: 55.53 GB
[2025-01-06 11:13:56,917][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:13:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:38,  5.46s/it, est. speed input: 92.51 toks/s, output: 10.99 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:09,  2.48s/it, est. speed input: 172.35 toks/s, output: 21.67 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:05<00:24,  1.04it/s, est. speed input: 338.34 toks/s, output: 44.22 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:17,  1.44it/s, est. speed input: 415.57 toks/s, output: 55.13 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:12,  1.94it/s, est. speed input: 490.40 toks/s, output: 66.03 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.48it/s, est. speed input: 558.51 toks/s, output: 76.47 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:04,  4.76it/s, est. speed input: 775.23 toks/s, output: 110.07 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:06<00:02,  7.07it/s, est. speed input: 982.23 toks/s, output: 144.08 toks/s]Processed prompts:  50%|█████     | 15/30 [00:06<00:01,  8.57it/s, est. speed input: 1113.83 toks/s, output: 166.89 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:01,  8.06it/s, est. speed input: 1211.98 toks/s, output: 186.07 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:00, 10.42it/s, est. speed input: 1394.97 toks/s, output: 222.23 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:07<00:00,  9.57it/s, est. speed input: 1482.38 toks/s, output: 242.70 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:00,  9.23it/s, est. speed input: 1567.57 toks/s, output: 265.40 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:07<00:00,  8.76it/s, est. speed input: 1643.44 toks/s, output: 288.63 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  8.37it/s, est. speed input: 1713.01 toks/s, output: 312.80 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  8.95it/s, est. speed input: 1795.01 toks/s, output: 340.99 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.55it/s, est. speed input: 1795.01 toks/s, output: 340.99 toks/s]
[2025-01-06 11:14:05,768][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:14:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it, est. speed input: 624.61 toks/s, output: 28.93 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.99it/s, est. speed input: 2545.69 toks/s, output: 115.71 toks/s]
[2025-01-06 11:14:07,157][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:14:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:26,  3.47s/it, est. speed input: 201.40 toks/s, output: 8.07 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:03<00:09,  2.21it/s, est. speed input: 1160.26 toks/s, output: 48.41 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:05<00:07,  2.13it/s, est. speed input: 1236.61 toks/s, output: 61.53 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:05<00:05,  2.64it/s, est. speed input: 1421.06 toks/s, output: 85.02 toks/s]Processed prompts:  50%|█████     | 13/26 [00:05<00:03,  3.46it/s, est. speed input: 1636.42 toks/s, output: 110.21 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:05<00:02,  4.96it/s, est. speed input: 1948.13 toks/s, output: 148.41 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:05<00:01,  5.73it/s, est. speed input: 2117.45 toks/s, output: 173.17 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:06<00:00,  6.74it/s, est. speed input: 2291.16 toks/s, output: 200.11 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:06<00:00,  5.83it/s, est. speed input: 2343.14 toks/s, output: 219.41 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:07<00:00,  4.79it/s, est. speed input: 2342.13 toks/s, output: 238.18 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:07<00:00,  3.83it/s, est. speed input: 2276.09 toks/s, output: 245.00 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  3.43it/s, est. speed input: 2245.00 toks/s, output: 257.06 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  3.21it/s, est. speed input: 2245.00 toks/s, output: 257.06 toks/s]
[2025-01-06 11:14:16,250][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:14:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:16,  1.80s/it, est. speed input: 434.45 toks/s, output: 16.13 toks/s]Processed prompts:  40%|████      | 4/10 [00:01<00:02,  2.57it/s, est. speed input: 1620.73 toks/s, output: 62.12 toks/s]Processed prompts:  50%|█████     | 5/10 [00:02<00:02,  2.04it/s, est. speed input: 1524.50 toks/s, output: 69.58 toks/s]Processed prompts:  60%|██████    | 6/10 [00:02<00:01,  2.53it/s, est. speed input: 1717.08 toks/s, output: 91.37 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:00,  3.02it/s, est. speed input: 1848.40 toks/s, output: 112.99 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  4.65it/s, est. speed input: 2198.02 toks/s, output: 161.63 toks/s]Processed prompts: 100%|██████████| 10/10 [00:03<00:00,  4.09it/s, est. speed input: 2187.91 toks/s, output: 177.41 toks/s]Processed prompts: 100%|██████████| 10/10 [00:03<00:00,  2.82it/s, est. speed input: 2187.91 toks/s, output: 177.41 toks/s]
[2025-01-06 11:14:20,254][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:14:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it, est. speed input: 547.76 toks/s, output: 52.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it, est. speed input: 547.76 toks/s, output: 52.05 toks/s]
[2025-01-06 11:14:27,578][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 11:14:34,038][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:14:34,039][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.82 GB
[2025-01-06 11:14:34,383][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.47 GB
[2025-01-06 11:14:44,496][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:14:44,497][root][INFO] - Before destroying HF.: GPU memory allocated: 15.47 GB
[2025-01-06 11:14:44,809][root][INFO] - After destroying HF.: GPU memory allocated: 0.18 GB
[2025-01-06 11:14:44,963][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:14:51,487][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:15:04,144][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:15:04,145][root][INFO] - Iteration 17 took 1m 49s. Generation: 66.33%, Training: 33.67%. Estimated time remaining: 59m 18s. Estimated total time for complete run: 1h 30m 50s.
[2025-01-06 11:15:04,434][root][INFO] - Loading VLLM model.
WARNING 01-06 11:15:04 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:15:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:15:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:15:05 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:15:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:15:23 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:15:24 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:15:24 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:15:45 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:15:45,775][root][INFO] - Before destroying HF.: GPU memory allocated: 70.83 GB
[2025-01-06 11:15:46,038][root][INFO] - After destroying HF.: GPU memory allocated: 55.54 GB
[2025-01-06 11:15:46,039][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:15:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:53,  5.99s/it, est. speed input: 84.37 toks/s, output: 9.69 toks/s]Processed prompts:   7%|▋         | 2/30 [00:06<01:13,  2.63s/it, est. speed input: 161.04 toks/s, output: 19.29 toks/s]Processed prompts:  10%|█         | 3/30 [00:06<00:40,  1.48s/it, est. speed input: 237.33 toks/s, output: 29.14 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:24,  1.06it/s, est. speed input: 311.17 toks/s, output: 38.97 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:15,  1.57it/s, est. speed input: 382.74 toks/s, output: 48.81 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:10,  2.19it/s, est. speed input: 452.25 toks/s, output: 58.66 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.47it/s, est. speed input: 505.06 toks/s, output: 67.15 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:07<00:07,  3.10it/s, est. speed input: 565.47 toks/s, output: 76.98 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:05,  3.76it/s, est. speed input: 623.78 toks/s, output: 86.88 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:07<00:02,  8.12it/s, est. speed input: 880.16 toks/s, output: 130.45 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  7.19it/s, est. speed input: 970.30 toks/s, output: 148.46 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:01,  8.88it/s, est. speed input: 1084.78 toks/s, output: 171.21 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:08<00:00, 11.24it/s, est. speed input: 1250.95 toks/s, output: 205.60 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00, 13.04it/s, est. speed input: 1409.68 toks/s, output: 240.55 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  9.53it/s, est. speed input: 1545.45 toks/s, output: 278.26 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:09<00:00,  6.46it/s, est. speed input: 1545.74 toks/s, output: 292.36 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  5.05it/s, est. speed input: 1524.14 toks/s, output: 298.79 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.02it/s, est. speed input: 1524.14 toks/s, output: 298.79 toks/s]
[2025-01-06 11:15:56,426][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:15:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it, est. speed input: 673.72 toks/s, output: 28.69 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.96it/s, est. speed input: 2551.76 toks/s, output: 114.73 toks/s]
[2025-01-06 11:15:57,829][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:15:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:27,  3.52s/it, est. speed input: 198.62 toks/s, output: 8.24 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:03<00:21,  1.05it/s, est. speed input: 579.30 toks/s, output: 24.59 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:04<00:16,  1.29it/s, est. speed input: 723.71 toks/s, output: 40.79 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:05<00:14,  1.43it/s, est. speed input: 786.15 toks/s, output: 49.49 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:05<00:10,  1.84it/s, est. speed input: 894.79 toks/s, output: 61.08 toks/s]Processed prompts:  31%|███       | 8/26 [00:05<00:07,  2.28it/s, est. speed input: 991.11 toks/s, output: 72.31 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:06<00:05,  2.70it/s, est. speed input: 1125.12 toks/s, output: 91.75 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:06<00:04,  3.15it/s, est. speed input: 1207.95 toks/s, output: 104.00 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:06<00:02,  5.76it/s, est. speed input: 1512.28 toks/s, output: 145.73 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:06<00:01,  6.52it/s, est. speed input: 1670.47 toks/s, output: 170.72 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:06<00:01,  7.55it/s, est. speed input: 1831.61 toks/s, output: 197.40 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:07<00:00,  6.37it/s, est. speed input: 1917.38 toks/s, output: 218.76 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:07<00:00,  7.66it/s, est. speed input: 2068.61 toks/s, output: 249.26 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:07<00:00,  6.98it/s, est. speed input: 2156.93 toks/s, output: 274.24 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:08<00:00,  4.01it/s, est. speed input: 2052.66 toks/s, output: 272.51 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  4.12it/s, est. speed input: 2081.56 toks/s, output: 288.63 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  2.98it/s, est. speed input: 2081.56 toks/s, output: 288.63 toks/s]
[2025-01-06 11:16:07,117][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:16:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:13,  1.68s/it, est. speed input: 468.06 toks/s, output: 17.31 toks/s]Processed prompts:  44%|████▍     | 4/9 [00:02<00:02,  2.05it/s, est. speed input: 1481.03 toks/s, output: 61.60 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:02<00:01,  2.57it/s, est. speed input: 1684.49 toks/s, output: 83.14 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:02<00:00,  3.17it/s, est. speed input: 1871.48 toks/s, output: 105.01 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:03<00:00,  2.52it/s, est. speed input: 1789.50 toks/s, output: 115.83 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  3.39it/s, est. speed input: 2012.87 toks/s, output: 166.88 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.56it/s, est. speed input: 2012.87 toks/s, output: 166.88 toks/s]
[2025-01-06 11:16:16,688][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 11:16:23,183][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:16:23,184][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.83 GB
[2025-01-06 11:16:23,532][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.48 GB
[2025-01-06 11:16:33,365][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:16:33,366][root][INFO] - Before destroying HF.: GPU memory allocated: 15.48 GB
[2025-01-06 11:16:33,686][root][INFO] - After destroying HF.: GPU memory allocated: 0.19 GB
[2025-01-06 11:16:33,857][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 11:16:40,407][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:16:53,308][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:16:53,309][root][INFO] - Iteration 18 took 1m 49s. Generation: 66.26%, Training: 33.74%. Estimated time remaining: 57m 36s. Estimated total time for complete run: 1h 30m 58s.
[2025-01-06 11:16:53,589][root][INFO] - Loading VLLM model.
WARNING 01-06 11:16:53 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:16:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:16:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:16:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 11:16:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:17:13 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:17:13 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:17:13 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:17:35 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:17:35,338][root][INFO] - Before destroying HF.: GPU memory allocated: 70.84 GB
[2025-01-06 11:17:35,584][root][INFO] - After destroying HF.: GPU memory allocated: 55.55 GB
[2025-01-06 11:17:35,585][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:17:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:09,  4.46s/it, est. speed input: 113.34 toks/s, output: 8.08 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:57,  2.07s/it, est. speed input: 207.98 toks/s, output: 16.27 toks/s]Processed prompts:  10%|█         | 3/30 [00:05<00:44,  1.63s/it, est. speed input: 253.83 toks/s, output: 23.79 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:26,  1.03s/it, est. speed input: 332.41 toks/s, output: 34.06 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:17,  1.43it/s, est. speed input: 408.41 toks/s, output: 44.32 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:11,  2.02it/s, est. speed input: 482.09 toks/s, output: 54.57 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:08,  2.61it/s, est. speed input: 549.28 toks/s, output: 64.48 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:06<00:06,  3.26it/s, est. speed input: 613.92 toks/s, output: 74.46 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:06<00:03,  5.15it/s, est. speed input: 751.33 toks/s, output: 95.96 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:03,  4.98it/s, est. speed input: 800.21 toks/s, output: 104.87 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:07<00:02,  6.54it/s, est. speed input: 972.75 toks/s, output: 135.80 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:01,  7.88it/s, est. speed input: 1089.38 toks/s, output: 158.42 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:07<00:01, 10.28it/s, est. speed input: 1265.12 toks/s, output: 193.56 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:07<00:00, 13.29it/s, est. speed input: 1443.14 toks/s, output: 230.30 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:00, 12.60it/s, est. speed input: 1537.97 toks/s, output: 252.39 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:08<00:00,  8.86it/s, est. speed input: 1584.81 toks/s, output: 269.16 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  9.40it/s, est. speed input: 1670.45 toks/s, output: 295.10 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  4.27it/s, est. speed input: 1582.86 toks/s, output: 300.80 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.13it/s, est. speed input: 1582.86 toks/s, output: 300.80 toks/s]
[2025-01-06 11:17:45,589][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:17:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:06,  1.23s/it, est. speed input: 548.96 toks/s, output: 23.62 toks/s]Processed prompts: 100%|██████████| 6/6 [00:02<00:00,  2.47it/s, est. speed input: 1473.03 toks/s, output: 97.63 toks/s]Processed prompts: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, est. speed input: 1473.03 toks/s, output: 97.63 toks/s]
[2025-01-06 11:17:48,692][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:17:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:02<01:06,  2.90s/it, est. speed input: 241.29 toks/s, output: 7.25 toks/s]Processed prompts:   8%|▊         | 2/24 [00:03<00:31,  1.42s/it, est. speed input: 425.50 toks/s, output: 15.22 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:05<00:11,  1.52it/s, est. speed input: 836.12 toks/s, output: 41.07 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:05<00:09,  1.78it/s, est. speed input: 936.38 toks/s, output: 53.58 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:05<00:04,  3.04it/s, est. speed input: 1281.65 toks/s, output: 93.88 toks/s]Processed prompts:  50%|█████     | 12/24 [00:05<00:03,  3.80it/s, est. speed input: 1473.30 toks/s, output: 119.44 toks/s]Processed prompts:  71%|███████   | 17/24 [00:05<00:00,  7.01it/s, est. speed input: 2024.24 toks/s, output: 192.66 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:06<00:00,  7.07it/s, est. speed input: 2161.62 toks/s, output: 216.80 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:06<00:00,  6.14it/s, est. speed input: 2224.11 toks/s, output: 236.82 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:06<00:00,  6.79it/s, est. speed input: 2362.23 toks/s, output: 268.30 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  3.42it/s, est. speed input: 2130.80 toks/s, output: 257.33 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  3.05it/s, est. speed input: 2130.80 toks/s, output: 257.33 toks/s]
[2025-01-06 11:17:57,136][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:17:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.51s/it, est. speed input: 464.27 toks/s, output: 16.60 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:01<00:02,  2.33it/s, est. speed input: 1423.25 toks/s, output: 50.85 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:01<00:01,  3.38it/s, est. speed input: 1959.64 toks/s, output: 79.47 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:02<00:00,  3.30it/s, est. speed input: 1990.01 toks/s, output: 94.20 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:02<00:00,  3.89it/s, est. speed input: 2165.82 toks/s, output: 116.17 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:02<00:00,  4.64it/s, est. speed input: 2348.03 toks/s, output: 139.64 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.39it/s, est. speed input: 1925.06 toks/s, output: 139.33 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.62it/s, est. speed input: 1925.06 toks/s, output: 139.33 toks/s]
[2025-01-06 11:18:01,028][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:18:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 1136.71 toks/s, output: 42.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 1136.71 toks/s, output: 42.37 toks/s]
[2025-01-06 11:18:07,603][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 11:18:14,095][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:18:14,096][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.84 GB
[2025-01-06 11:18:14,440][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.49 GB
[2025-01-06 11:18:24,369][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:18:24,370][root][INFO] - Before destroying HF.: GPU memory allocated: 15.49 GB
[2025-01-06 11:18:24,657][root][INFO] - After destroying HF.: GPU memory allocated: 0.20 GB
[2025-01-06 11:18:24,906][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 11:18:31,543][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:18:43,984][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:18:43,992][root][INFO] - Iteration 19 took 1m 50s. Generation: 66.99%, Training: 33.01%. Estimated time remaining: 57m 1s. Estimated total time for complete run: 1h 32m 14s.
[2025-01-06 11:18:44,338][root][INFO] - Loading VLLM model.
WARNING 01-06 11:18:44 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:18:44 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:18:45 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:18:45 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:18:50 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:19:03 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:19:04 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:19:04 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:19:26 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:19:26,129][root][INFO] - Before destroying HF.: GPU memory allocated: 70.84 GB
[2025-01-06 11:19:26,370][root][INFO] - After destroying HF.: GPU memory allocated: 55.56 GB
[2025-01-06 11:19:26,371][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:19:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:17,  4.74s/it, est. speed input: 106.63 toks/s, output: 7.60 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:01,  2.19s/it, est. speed input: 196.61 toks/s, output: 15.38 toks/s]Processed prompts:  10%|█         | 3/30 [00:06<00:45,  1.70s/it, est. speed input: 242.42 toks/s, output: 22.72 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:06<00:27,  1.07s/it, est. speed input: 317.73 toks/s, output: 32.56 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:12,  1.89it/s, est. speed input: 468.88 toks/s, output: 52.61 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:09,  2.43it/s, est. speed input: 538.65 toks/s, output: 62.32 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.29it/s, est. speed input: 659.26 toks/s, output: 80.50 toks/s]Processed prompts:  40%|████      | 12/30 [00:07<00:03,  5.02it/s, est. speed input: 846.60 toks/s, output: 110.78 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:01,  9.20it/s, est. speed input: 1174.38 toks/s, output: 165.93 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:00, 10.25it/s, est. speed input: 1341.20 toks/s, output: 196.53 toks/s]Processed prompts:  80%|████████  | 24/30 [00:08<00:00,  9.31it/s, est. speed input: 1510.05 toks/s, output: 233.86 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:08<00:00, 10.01it/s, est. speed input: 1607.17 toks/s, output: 258.64 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  9.62it/s, est. speed input: 1682.48 toks/s, output: 282.12 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  9.69it/s, est. speed input: 1760.29 toks/s, output: 306.98 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.49it/s, est. speed input: 1760.29 toks/s, output: 306.98 toks/s]
[2025-01-06 11:19:35,416][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:19:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:06,  1.15s/it, est. speed input: 548.37 toks/s, output: 19.09 toks/s]Processed prompts:  29%|██▊       | 2/7 [00:01<00:02,  1.74it/s, est. speed input: 961.45 toks/s, output: 38.61 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  4.65it/s, est. speed input: 2225.37 toks/s, output: 119.37 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  3.56it/s, est. speed input: 2225.37 toks/s, output: 119.37 toks/s]
[2025-01-06 11:19:37,789][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:19:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:09,  3.18s/it, est. speed input: 219.84 toks/s, output: 9.12 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:03<00:12,  1.57it/s, est. speed input: 843.95 toks/s, output: 35.92 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:05<00:12,  1.31it/s, est. speed input: 812.81 toks/s, output: 50.39 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:05<00:06,  2.33it/s, est. speed input: 1169.07 toks/s, output: 91.43 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:05<00:03,  3.06it/s, est. speed input: 1375.92 toks/s, output: 118.46 toks/s]Processed prompts:  61%|██████    | 14/23 [00:05<00:02,  4.32it/s, est. speed input: 1670.84 toks/s, output: 159.30 toks/s]Processed prompts:  70%|██████▉   | 16/23 [00:06<00:01,  4.37it/s, est. speed input: 1774.49 toks/s, output: 182.78 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:06<00:01,  4.27it/s, est. speed input: 1810.61 toks/s, output: 194.12 toks/s]Processed prompts:  78%|███████▊  | 18/23 [00:07<00:01,  3.61it/s, est. speed input: 1791.65 toks/s, output: 201.49 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:07<00:01,  3.82it/s, est. speed input: 1837.69 toks/s, output: 216.55 toks/s]Processed prompts:  91%|█████████▏| 21/23 [00:07<00:00,  5.41it/s, est. speed input: 1998.03 toks/s, output: 254.67 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:07<00:00,  5.65it/s, est. speed input: 2051.98 toks/s, output: 271.54 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  3.62it/s, est. speed input: 1986.00 toks/s, output: 276.09 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.84it/s, est. speed input: 1986.00 toks/s, output: 276.09 toks/s]
[2025-01-06 11:19:46,445][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:19:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:21,  1.98s/it, est. speed input: 352.93 toks/s, output: 14.14 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:02<00:03,  2.09it/s, est. speed input: 1504.38 toks/s, output: 61.57 toks/s]Processed prompts:  50%|█████     | 6/12 [00:03<00:02,  2.44it/s, est. speed input: 1645.05 toks/s, output: 80.02 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:03<00:01,  3.71it/s, est. speed input: 2069.10 toks/s, output: 122.18 toks/s]Processed prompts:  92%|█████████▏| 11/12 [00:03<00:00,  5.92it/s, est. speed input: 2607.13 toks/s, output: 185.83 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  2.82it/s, est. speed input: 2137.70 toks/s, output: 176.22 toks/s]
[2025-01-06 11:19:51,196][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:19:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s, est. speed input: 998.72 toks/s, output: 35.80 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s, est. speed input: 1837.01 toks/s, output: 71.33 toks/s]
[2025-01-06 11:19:57,899][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 11:20:04,335][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:20:04,335][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.84 GB
[2025-01-06 11:20:04,720][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.49 GB
[2025-01-06 11:20:14,765][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:20:14,766][root][INFO] - Before destroying HF.: GPU memory allocated: 15.49 GB
[2025-01-06 11:20:15,123][root][INFO] - After destroying HF.: GPU memory allocated: 0.21 GB
[2025-01-06 11:20:15,278][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 11:20:21,841][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:20:34,653][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:20:34,654][root][INFO] - Iteration 20 took 1m 50s. Generation: 66.64%, Training: 33.36%. Estimated time remaining: 55m 9s. Estimated total time for complete run: 1h 32m 13s.
[2025-01-06 11:20:34,921][root][INFO] - Loading VLLM model.
WARNING 01-06 11:20:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:20:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:20:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:20:35 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 11:20:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 11:20:54 gpu_executor.py:122] # GPU blocks: 19921, # CPU blocks: 2048
INFO 01-06 11:20:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 11:20:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 11:21:16 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 11:21:16,682][root][INFO] - Before destroying HF.: GPU memory allocated: 70.85 GB
[2025-01-06 11:21:16,943][root][INFO] - After destroying HF.: GPU memory allocated: 55.56 GB
[2025-01-06 11:21:16,944][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:21:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:12,  4.55s/it, est. speed input: 110.94 toks/s, output: 7.91 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:59,  2.11s/it, est. speed input: 203.94 toks/s, output: 15.95 toks/s]Processed prompts:  10%|█         | 3/30 [00:06<00:44,  1.65s/it, est. speed input: 249.84 toks/s, output: 23.42 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:18,  1.34it/s, est. speed input: 409.26 toks/s, output: 43.76 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:10,  2.29it/s, est. speed input: 563.65 toks/s, output: 64.26 toks/s]Processed prompts:  30%|███       | 9/30 [00:06<00:06,  3.40it/s, est. speed input: 708.36 toks/s, output: 84.47 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:04,  4.69it/s, est. speed input: 847.80 toks/s, output: 104.85 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:06<00:03,  4.70it/s, est. speed input: 941.28 toks/s, output: 121.58 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:02,  5.02it/s, est. speed input: 1035.22 toks/s, output: 139.94 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:07<00:02,  6.09it/s, est. speed input: 1145.34 toks/s, output: 162.09 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:07<00:01,  6.22it/s, est. speed input: 1268.50 toks/s, output: 191.28 toks/s]Processed prompts:  70%|███████   | 21/30 [00:08<00:01,  6.51it/s, est. speed input: 1312.59 toks/s, output: 202.86 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:08<00:00,  8.14it/s, est. speed input: 1417.97 toks/s, output: 229.27 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:08<00:00,  8.99it/s, est. speed input: 1509.73 toks/s, output: 254.35 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  9.89it/s, est. speed input: 1600.57 toks/s, output: 281.02 toks/s]Processed prompts:  97%|█████████▋| 29/30 [00:08<00:00,  9.76it/s, est. speed input: 1677.58 toks/s, output: 306.88 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.25it/s, est. speed input: 1641.69 toks/s, output: 309.26 toks/s]
[2025-01-06 11:21:26,621][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice)
WARNING 01-06 11:21:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s, est. speed input: 627.90 toks/s, output: 29.04 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.42it/s, est. speed input: 1839.39 toks/s, output: 100.66 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.90it/s, est. speed input: 1839.39 toks/s, output: 100.66 toks/s]
[2025-01-06 11:21:28,386][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:21:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:20,  3.20s/it, est. speed input: 218.10 toks/s, output: 7.18 toks/s]Processed prompts:   8%|▊         | 2/26 [00:03<00:35,  1.47s/it, est. speed input: 403.81 toks/s, output: 14.73 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:03<00:11,  1.84it/s, est. speed input: 876.75 toks/s, output: 37.13 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:05<00:13,  1.44it/s, est. speed input: 816.85 toks/s, output: 41.29 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:05<00:10,  1.83it/s, est. speed input: 928.87 toks/s, output: 52.96 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:05<00:06,  2.74it/s, est. speed input: 1139.57 toks/s, output: 76.08 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:05<00:04,  3.25it/s, est. speed input: 1238.68 toks/s, output: 87.89 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:06<00:03,  3.67it/s, est. speed input: 1379.29 toks/s, output: 108.53 toks/s]Processed prompts:  50%|█████     | 13/26 [00:06<00:03,  4.11it/s, est. speed input: 1460.04 toks/s, output: 120.83 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:06<00:04,  2.87it/s, est. speed input: 1418.00 toks/s, output: 125.19 toks/s]Processed prompts:  58%|█████▊    | 15/26 [00:07<00:03,  3.32it/s, est. speed input: 1483.88 toks/s, output: 138.83 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:07<00:01,  5.57it/s, est. speed input: 1729.80 toks/s, output: 184.91 toks/s]Processed prompts:  73%|███████▎  | 19/26 [00:07<00:01,  5.86it/s, est. speed input: 1792.74 toks/s, output: 198.97 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:07<00:01,  5.10it/s, est. speed input: 1816.96 toks/s, output: 209.77 toks/s]Processed prompts:  81%|████████  | 21/26 [00:07<00:00,  5.61it/s, est. speed input: 1878.20 toks/s, output: 225.07 toks/s]Processed prompts:  88%|████████▊ | 23/26 [00:07<00:00,  7.07it/s, est. speed input: 2011.84 toks/s, output: 257.66 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:08<00:00,  5.66it/s, est. speed input: 2023.75 toks/s, output: 268.65 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:08<00:00,  3.61it/s, est. speed input: 1968.39 toks/s, output: 273.38 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  2.93it/s, est. speed input: 2047.09 toks/s, output: 295.90 toks/s]
[2025-01-06 11:21:37,780][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:21:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/14 [00:02<00:29,  2.29s/it, est. speed input: 358.19 toks/s, output: 12.23 toks/s]Processed prompts:  64%|██████▍   | 9/14 [00:02<00:01,  4.24it/s, est. speed input: 2779.64 toks/s, output: 101.23 toks/s]Processed prompts:  79%|███████▊  | 11/14 [00:03<00:00,  3.11it/s, est. speed input: 2260.64 toks/s, output: 117.97 toks/s]Processed prompts:  86%|████████▌ | 12/14 [00:04<00:00,  3.44it/s, est. speed input: 2367.03 toks/s, output: 140.32 toks/s]Processed prompts:  93%|█████████▎| 13/14 [00:04<00:00,  3.51it/s, est. speed input: 2387.80 toks/s, output: 159.45 toks/s]Processed prompts: 100%|██████████| 14/14 [00:04<00:00,  3.98it/s, est. speed input: 2517.54 toks/s, output: 183.63 toks/s]Processed prompts: 100%|██████████| 14/14 [00:04<00:00,  3.19it/s, est. speed input: 2517.54 toks/s, output: 183.63 toks/s]
[2025-01-06 11:21:42,682][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob)
WARNING 01-06 11:21:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s, est. speed input: 794.66 toks/s, output: 36.35 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s, est. speed input: 1556.20 toks/s, output: 70.64 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s, est. speed input: 1556.20 toks/s, output: 70.64 toks/s]
[2025-01-06 11:21:49,650][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 11:21:56,116][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice.
[2025-01-06 11:21:56,116][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.85 GB
[2025-01-06 11:21:56,471][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.50 GB
[2025-01-06 11:22:06,493][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_alice
[2025-01-06 11:22:06,495][root][INFO] - Before destroying HF.: GPU memory allocated: 15.50 GB
[2025-01-06 11:22:06,766][root][INFO] - After destroying HF.: GPU memory allocated: 0.21 GB
[2025-01-06 11:22:06,923][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 11:22:13,536][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob.
[2025-01-06 11:22:26,927][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/10-43-31/ad_bob
[2025-01-06 11:22:26,928][root][INFO] - Iteration 21 took 1m 52s. Generation: 66.66%, Training: 33.34%. Estimated time remaining: 54m 38s. Estimated total time for complete run: 1h 33m 33s.
[2025-01-06 11:22:27,270][root][INFO] - Loading VLLM model.
WARNING 01-06 11:22:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 11:22:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 11:22:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 11:22:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 11:22:32 model_runner.py:926] Loading model weights took 14.9614 GB
