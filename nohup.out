/home/mila/d/dereck.piche/llm_negotiation/src/run.py:14: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="config")
[2025-01-06 15:33:46,925][root][INFO] - Loading VLLM model.
WARNING 01-06 15:33:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:33:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:33:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:33:49 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.60s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:07,  3.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:17<00:05,  5.94s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.65s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.34s/it]

INFO 01-06 15:34:15 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-06 15:34:29 gpu_executor.py:122] # GPU blocks: 19859, # CPU blocks: 2048
INFO 01-06 15:34:31 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:34:31 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:34:50 model_runner.py:1335] Graph capturing finished in 19 secs.
[2025-01-06 15:34:50,516][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:01<00:40,  1.31s/it, est. speed input: 384.57 toks/s, output: 15.23 toks/s]Processed prompts:   6%|▋         | 2/32 [00:01<00:23,  1.25it/s, est. speed input: 577.02 toks/s, output: 35.99 toks/s]Processed prompts:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s, est. speed input: 773.11 toks/s, output: 59.70 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:02<00:10,  2.74it/s, est. speed input: 974.43 toks/s, output: 85.38 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:02<00:07,  3.40it/s, est. speed input: 1126.08 toks/s, output: 109.71 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:02<00:01, 11.65it/s, est. speed input: 2361.45 toks/s, output: 290.34 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:02<00:00, 16.07it/s, est. speed input: 3308.81 toks/s, output: 447.08 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:02<00:00, 18.10it/s, est. speed input: 3735.52 toks/s, output: 535.18 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:02<00:00, 17.99it/s, est. speed input: 4138.12 toks/s, output: 636.08 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:03<00:00, 17.42it/s, est. speed input: 4375.96 toks/s, output: 716.00 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:03<00:00, 10.34it/s, est. speed input: 4078.04 toks/s, output: 719.24 toks/s]Processed prompts: 100%|██████████| 32/32 [00:04<00:00,  8.28it/s, est. speed input: 3903.92 toks/s, output: 734.64 toks/s]Processed prompts: 100%|██████████| 32/32 [00:04<00:00,  7.73it/s, est. speed input: 3903.92 toks/s, output: 734.64 toks/s]
[2025-01-06 15:34:54,876][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:00<00:09,  1.12it/s, est. speed input: 799.92 toks/s, output: 31.46 toks/s]Processed prompts: 100%|██████████| 12/12 [00:01<00:00,  9.78it/s, est. speed input: 5294.21 toks/s, output: 266.55 toks/s]Processed prompts: 100%|██████████| 12/12 [00:01<00:00,  8.20it/s, est. speed input: 5294.21 toks/s, output: 266.55 toks/s]
[2025-01-06 15:34:56,538][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:01<00:32,  1.73s/it, est. speed input: 403.79 toks/s, output: 30.62 toks/s]Processed prompts:  10%|█         | 2/20 [00:01<00:14,  1.27it/s, est. speed input: 754.45 toks/s, output: 60.98 toks/s]Processed prompts:  20%|██        | 4/20 [00:02<00:05,  2.93it/s, est. speed input: 1392.05 toks/s, output: 120.48 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:02<00:04,  3.70it/s, est. speed input: 1649.03 toks/s, output: 150.04 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:02<00:02,  5.66it/s, est. speed input: 2163.98 toks/s, output: 212.73 toks/s]Processed prompts:  50%|█████     | 10/20 [00:02<00:01,  9.40it/s, est. speed input: 2948.51 toks/s, output: 315.94 toks/s]Processed prompts:  65%|██████▌   | 13/20 [00:02<00:00,  9.17it/s, est. speed input: 3354.69 toks/s, output: 388.37 toks/s]Processed prompts:  75%|███████▌  | 15/20 [00:03<00:00,  7.97it/s, est. speed input: 3445.75 toks/s, output: 429.85 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:03<00:00,  7.79it/s, est. speed input: 3586.24 toks/s, output: 485.89 toks/s]Processed prompts:  95%|█████████▌| 19/20 [00:03<00:00,  6.78it/s, est. speed input: 3587.59 toks/s, output: 533.50 toks/s]Processed prompts: 100%|██████████| 20/20 [00:03<00:00,  6.41it/s, est. speed input: 3586.72 toks/s, output: 558.02 toks/s]Processed prompts: 100%|██████████| 20/20 [00:03<00:00,  5.13it/s, est. speed input: 3586.72 toks/s, output: 558.02 toks/s]
[2025-01-06 15:35:00,742][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:01<00:15,  1.08s/it, est. speed input: 783.38 toks/s, output: 26.95 toks/s]Processed prompts:  20%|██        | 3/15 [00:01<00:04,  2.65it/s, est. speed input: 1873.58 toks/s, output: 80.58 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:01<00:02,  4.00it/s, est. speed input: 2322.79 toks/s, output: 139.24 toks/s]Processed prompts:  53%|█████▎    | 8/15 [00:01<00:01,  5.61it/s, est. speed input: 2996.90 toks/s, output: 227.71 toks/s]Processed prompts:  60%|██████    | 9/15 [00:02<00:01,  5.92it/s, est. speed input: 3143.86 toks/s, output: 259.09 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:02<00:00,  6.35it/s, est. speed input: 3371.08 toks/s, output: 318.19 toks/s]Processed prompts:  93%|█████████▎| 14/15 [00:02<00:00,  7.93it/s, est. speed input: 3842.91 toks/s, output: 429.55 toks/s]Processed prompts: 100%|██████████| 15/15 [00:03<00:00,  4.03it/s, est. speed input: 3114.17 toks/s, output: 382.65 toks/s]Processed prompts: 100%|██████████| 15/15 [00:03<00:00,  4.36it/s, est. speed input: 3114.17 toks/s, output: 382.65 toks/s]
[2025-01-06 15:35:04,467][root][INFO] - Generating using VLLM (without LoRA)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 866.21 toks/s, output: 71.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 866.21 toks/s, output: 71.36 toks/s]
[2025-01-06 15:35:10,612][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:05,  2.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]
[2025-01-06 15:35:21,555][root][INFO] - Adapter 'ad_alice' added to HF.
[2025-01-06 15:35:21,555][root][INFO] - Before destroying VLLM: GPU memory allocated: 70.87 GB
[2025-01-06 15:35:21,881][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.65 GB
[2025-01-06 15:35:34,214][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:35:34,215][root][INFO] - Before destroying HF.: GPU memory allocated: 15.65 GB
[2025-01-06 15:35:34,532][root][INFO] - After destroying HF.: GPU memory allocated: 0.06 GB
[2025-01-06 15:35:34,706][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
[2025-01-06 15:35:42,276][root][INFO] - Adapter 'ad_bob' added to HF.
[2025-01-06 15:35:57,395][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:35:57,396][root][INFO] - Iteration 1 took 2m 10s. Generation: 63.89%, Training: 36.11%. Estimated time remaining: 4h 36m 30s. Estimated total time for complete run: 4h 38m 41s.
[2025-01-06 15:35:57,730][root][INFO] - Loading VLLM model.
WARNING 01-06 15:35:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:35:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:35:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:35:58 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]

INFO 01-06 15:36:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:36:17 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:36:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:36:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:36:38 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:36:38,754][root][INFO] - Before destroying HF.: GPU memory allocated: 71.03 GB
[2025-01-06 15:36:38,987][root][INFO] - After destroying HF.: GPU memory allocated: 55.43 GB
[2025-01-06 15:36:38,988][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:36:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:16,  4.40s/it, est. speed input: 114.85 toks/s, output: 4.55 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:18,  2.62s/it, est. speed input: 174.91 toks/s, output: 10.91 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:49,  1.72s/it, est. speed input: 236.11 toks/s, output: 18.23 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:31,  1.11s/it, est. speed input: 306.60 toks/s, output: 26.41 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:15,  1.73it/s, est. speed input: 445.17 toks/s, output: 43.05 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:12,  1.98it/s, est. speed input: 496.32 toks/s, output: 50.54 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:07,  3.05it/s, est. speed input: 620.68 toks/s, output: 68.01 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:03,  5.02it/s, est. speed input: 806.88 toks/s, output: 95.60 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:02,  6.38it/s, est. speed input: 925.25 toks/s, output: 114.12 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:01,  9.13it/s, est. speed input: 1106.22 toks/s, output: 143.16 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  6.94it/s, est. speed input: 1166.14 toks/s, output: 157.51 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  6.47it/s, est. speed input: 1234.49 toks/s, output: 174.49 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  6.72it/s, est. speed input: 1311.00 toks/s, output: 194.25 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00,  8.01it/s, est. speed input: 1403.84 toks/s, output: 217.28 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  7.74it/s, est. speed input: 1504.05 toks/s, output: 248.05 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  8.84it/s, est. speed input: 1588.03 toks/s, output: 273.89 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  4.46it/s, est. speed input: 1529.07 toks/s, output: 285.09 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.03it/s, est. speed input: 1529.07 toks/s, output: 285.09 toks/s]
[2025-01-06 15:36:49,968][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:36:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:05,  1.14s/it, est. speed input: 496.33 toks/s, output: 20.24 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:01<00:02,  1.83it/s, est. speed input: 961.05 toks/s, output: 40.90 toks/s]Processed prompts:  83%|████████▎ | 5/6 [00:01<00:00,  5.37it/s, est. speed input: 2241.38 toks/s, output: 104.25 toks/s]Processed prompts: 100%|██████████| 6/6 [00:02<00:00,  2.84it/s, est. speed input: 1774.35 toks/s, output: 106.35 toks/s]
[2025-01-06 15:36:52,454][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:36:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:36,  3.86s/it, est. speed input: 181.14 toks/s, output: 7.00 toks/s]Processed prompts:   8%|▊         | 2/26 [00:05<01:03,  2.65s/it, est. speed input: 247.23 toks/s, output: 15.74 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:05<00:34,  1.51s/it, est. speed input: 361.21 toks/s, output: 26.53 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:05<00:14,  1.41it/s, est. speed input: 582.96 toks/s, output: 48.04 toks/s]Processed prompts:  31%|███       | 8/26 [00:06<00:06,  2.84it/s, est. speed input: 906.03 toks/s, output: 81.17 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:06<00:04,  3.96it/s, est. speed input: 1110.08 toks/s, output: 103.54 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:06<00:02,  5.15it/s, est. speed input: 1301.13 toks/s, output: 125.65 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:06<00:02,  5.50it/s, est. speed input: 1448.31 toks/s, output: 145.33 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:07<00:02,  4.86it/s, est. speed input: 1537.75 toks/s, output: 163.07 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:07<00:01,  5.27it/s, est. speed input: 1607.64 toks/s, output: 175.20 toks/s]Processed prompts:  73%|███████▎  | 19/26 [00:07<00:01,  5.68it/s, est. speed input: 1726.70 toks/s, output: 198.53 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:07<00:01,  5.33it/s, est. speed input: 1763.87 toks/s, output: 208.94 toks/s]Processed prompts:  81%|████████  | 21/26 [00:08<00:01,  4.76it/s, est. speed input: 1786.59 toks/s, output: 218.71 toks/s]Processed prompts:  88%|████████▊ | 23/26 [00:08<00:00,  4.00it/s, est. speed input: 1817.83 toks/s, output: 238.47 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:09<00:00,  3.08it/s, est. speed input: 1776.98 toks/s, output: 244.58 toks/s]Processed prompts: 100%|██████████| 26/26 [00:09<00:00,  2.75it/s, est. speed input: 1925.02 toks/s, output: 286.94 toks/s]
[2025-01-06 15:37:02,435][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:37:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:02<00:23,  2.12s/it, est. speed input: 448.93 toks/s, output: 13.66 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:03<00:03,  1.98it/s, est. speed input: 1469.57 toks/s, output: 59.11 toks/s]Processed prompts:  50%|█████     | 6/12 [00:03<00:02,  2.09it/s, est. speed input: 1506.42 toks/s, output: 74.97 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:03<00:02,  2.08it/s, est. speed input: 1498.49 toks/s, output: 90.58 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:04<00:01,  2.57it/s, est. speed input: 1621.41 toks/s, output: 113.12 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:04<00:01,  2.97it/s, est. speed input: 1711.71 toks/s, output: 134.40 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:04<00:00,  2.38it/s, est. speed input: 1631.47 toks/s, output: 146.46 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.80it/s, est. speed input: 1764.98 toks/s, output: 191.03 toks/s]Processed prompts: 100%|██████████| 12/12 [00:05<00:00,  2.22it/s, est. speed input: 1764.98 toks/s, output: 191.03 toks/s]
[2025-01-06 15:37:08,317][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:37:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 1115.04 toks/s, output: 38.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 1115.04 toks/s, output: 38.96 toks/s]
[2025-01-06 15:37:13,904][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 15:37:21,144][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:37:21,145][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.02 GB
[2025-01-06 15:37:21,481][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.66 GB
[2025-01-06 15:37:33,236][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:37:33,237][root][INFO] - Before destroying HF.: GPU memory allocated: 15.66 GB
[2025-01-06 15:37:33,647][root][INFO] - After destroying HF.: GPU memory allocated: 0.06 GB
[2025-01-06 15:37:33,795][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 15:37:41,046][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:37:56,188][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:37:56,190][root][INFO] - Iteration 2 took 1m 58s. Generation: 64.28%, Training: 35.72%. Estimated time remaining: 4h 9m 15s. Estimated total time for complete run: 4h 13m 25s.
[2025-01-06 15:37:56,551][root][INFO] - Loading VLLM model.
WARNING 01-06 15:37:56 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:37:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:37:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:37:57 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]

INFO 01-06 15:38:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:38:15 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:38:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:38:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:38:37 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:38:37,442][root][INFO] - Before destroying HF.: GPU memory allocated: 71.04 GB
[2025-01-06 15:38:37,674][root][INFO] - After destroying HF.: GPU memory allocated: 55.44 GB
[2025-01-06 15:38:37,675][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:38:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:47,  3.46s/it, est. speed input: 145.92 toks/s, output: 5.78 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:07,  2.23s/it, est. speed input: 208.81 toks/s, output: 13.02 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:43,  1.51s/it, est. speed input: 276.55 toks/s, output: 21.36 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:29,  1.07s/it, est. speed input: 343.64 toks/s, output: 30.28 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:21,  1.27it/s, est. speed input: 410.14 toks/s, output: 39.63 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:15,  1.69it/s, est. speed input: 475.52 toks/s, output: 49.28 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:07,  3.07it/s, est. speed input: 623.80 toks/s, output: 70.56 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:04,  5.23it/s, est. speed input: 832.97 toks/s, output: 101.82 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  5.80it/s, est. speed input: 946.39 toks/s, output: 120.95 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  6.64it/s, est. speed input: 1060.25 toks/s, output: 141.09 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  6.12it/s, est. speed input: 1140.48 toks/s, output: 158.62 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  7.48it/s, est. speed input: 1295.60 toks/s, output: 191.52 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00,  9.07it/s, est. speed input: 1451.14 toks/s, output: 227.01 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00,  9.42it/s, est. speed input: 1541.05 toks/s, output: 250.11 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00,  9.87it/s, est. speed input: 1629.21 toks/s, output: 274.70 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00,  7.53it/s, est. speed input: 1664.51 toks/s, output: 292.33 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  6.26it/s, est. speed input: 1665.85 toks/s, output: 300.51 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  5.29it/s, est. speed input: 1664.09 toks/s, output: 309.43 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  4.40it/s, est. speed input: 1653.49 toks/s, output: 318.32 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s, est. speed input: 1653.49 toks/s, output: 318.32 toks/s]
[2025-01-06 15:38:47,896][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:38:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:08,  1.34s/it, est. speed input: 421.39 toks/s, output: 20.17 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  5.15it/s, est. speed input: 2599.57 toks/s, output: 129.18 toks/s]Processed prompts: 100%|██████████| 7/7 [00:01<00:00,  4.11it/s, est. speed input: 2599.57 toks/s, output: 129.18 toks/s]
[2025-01-06 15:38:49,984][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:38:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:22,  3.46s/it, est. speed input: 202.24 toks/s, output: 8.39 toks/s]Processed prompts:   8%|▊         | 2/25 [00:05<00:53,  2.34s/it, est. speed input: 279.13 toks/s, output: 17.77 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:05<00:31,  1.45s/it, est. speed input: 388.58 toks/s, output: 29.09 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:05<00:10,  1.89it/s, est. speed input: 751.46 toks/s, output: 66.12 toks/s]Processed prompts:  40%|████      | 10/25 [00:05<00:03,  3.98it/s, est. speed input: 1226.00 toks/s, output: 116.29 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:06<00:03,  4.24it/s, est. speed input: 1375.70 toks/s, output: 135.96 toks/s]Processed prompts:  56%|█████▌    | 14/25 [00:06<00:02,  5.09it/s, est. speed input: 1554.68 toks/s, output: 159.98 toks/s]Processed prompts:  64%|██████▍   | 16/25 [00:06<00:01,  6.04it/s, est. speed input: 1726.59 toks/s, output: 185.26 toks/s]Processed prompts:  72%|███████▏  | 18/25 [00:06<00:01,  5.94it/s, est. speed input: 1843.19 toks/s, output: 206.41 toks/s]Processed prompts:  76%|███████▌  | 19/25 [00:07<00:01,  5.86it/s, est. speed input: 1895.01 toks/s, output: 217.88 toks/s]Processed prompts:  80%|████████  | 20/25 [00:07<00:00,  6.04it/s, est. speed input: 1954.18 toks/s, output: 230.78 toks/s]Processed prompts:  84%|████████▍ | 21/25 [00:07<00:00,  5.58it/s, est. speed input: 1988.89 toks/s, output: 241.85 toks/s]Processed prompts:  88%|████████▊ | 22/25 [00:07<00:00,  5.79it/s, est. speed input: 2041.61 toks/s, output: 255.70 toks/s]Processed prompts:  92%|█████████▏| 23/25 [00:07<00:00,  5.74it/s, est. speed input: 2084.73 toks/s, output: 269.20 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:08<00:00,  4.85it/s, est. speed input: 2095.46 toks/s, output: 280.04 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  4.22it/s, est. speed input: 2099.59 toks/s, output: 291.60 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  3.00it/s, est. speed input: 2099.59 toks/s, output: 291.60 toks/s]
[2025-01-06 15:38:58,858][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:38:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:10,  1.54s/it, est. speed input: 543.73 toks/s, output: 18.79 toks/s]Processed prompts:  25%|██▌       | 2/8 [00:02<00:06,  1.06s/it, est. speed input: 678.87 toks/s, output: 37.96 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:02<00:04,  1.17it/s, est. speed input: 779.96 toks/s, output: 58.58 toks/s]Processed prompts:  50%|█████     | 4/8 [00:03<00:02,  1.76it/s, est. speed input: 977.30 toks/s, output: 85.21 toks/s]Processed prompts:  88%|████████▊ | 7/8 [00:04<00:00,  2.35it/s, est. speed input: 1251.28 toks/s, output: 144.20 toks/s]Processed prompts: 100%|██████████| 8/8 [00:04<00:00,  2.83it/s, est. speed input: 1384.74 toks/s, output: 176.35 toks/s]Processed prompts: 100%|██████████| 8/8 [00:04<00:00,  1.93it/s, est. speed input: 1384.74 toks/s, output: 176.35 toks/s]
[2025-01-06 15:39:03,417][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:39:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it, est. speed input: 517.28 toks/s, output: 50.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it, est. speed input: 517.28 toks/s, output: 50.98 toks/s]
[2025-01-06 15:39:10,059][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 15:39:17,284][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:39:17,285][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.03 GB
[2025-01-06 15:39:17,631][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.66 GB
[2025-01-06 15:39:29,395][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:39:29,396][root][INFO] - Before destroying HF.: GPU memory allocated: 15.66 GB
[2025-01-06 15:39:29,739][root][INFO] - After destroying HF.: GPU memory allocated: 0.07 GB
[2025-01-06 15:39:29,878][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 15:39:37,302][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:39:51,984][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:39:51,985][root][INFO] - Iteration 3 took 1m 55s. Generation: 63.67%, Training: 36.33%. Estimated time remaining: 4h 0m 56s. Estimated total time for complete run: 4h 7m 1s.
[2025-01-06 15:39:52,293][root][INFO] - Loading VLLM model.
WARNING 01-06 15:39:52 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:39:52 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:39:53 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:39:53 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:39:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:40:11 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:40:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:40:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:40:33 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:40:33,159][root][INFO] - Before destroying HF.: GPU memory allocated: 71.04 GB
[2025-01-06 15:40:33,393][root][INFO] - After destroying HF.: GPU memory allocated: 55.45 GB
[2025-01-06 15:40:33,394][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:40:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:44,  3.38s/it, est. speed input: 149.37 toks/s, output: 5.92 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:06,  2.20s/it, est. speed input: 212.34 toks/s, output: 13.24 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.46s/it, est. speed input: 283.75 toks/s, output: 21.73 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:20,  1.31it/s, est. speed input: 436.39 toks/s, output: 39.92 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.61it/s, est. speed input: 500.39 toks/s, output: 49.05 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:07,  2.92it/s, est. speed input: 709.46 toks/s, output: 78.52 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:06<00:04,  4.72it/s, est. speed input: 925.63 toks/s, output: 111.04 toks/s]Processed prompts:  50%|█████     | 16/32 [00:06<00:02,  7.73it/s, est. speed input: 1211.14 toks/s, output: 155.29 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:06<00:01,  7.73it/s, est. speed input: 1311.67 toks/s, output: 173.73 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  7.59it/s, est. speed input: 1401.19 toks/s, output: 192.84 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:07<00:01,  7.62it/s, est. speed input: 1487.90 toks/s, output: 213.34 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:07<00:00,  9.04it/s, est. speed input: 1598.46 toks/s, output: 237.79 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:01,  5.56it/s, est. speed input: 1584.02 toks/s, output: 248.28 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00,  5.75it/s, est. speed input: 1616.82 toks/s, output: 260.52 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:08<00:00,  5.58it/s, est. speed input: 1637.42 toks/s, output: 271.55 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  4.99it/s, est. speed input: 1662.74 toks/s, output: 292.60 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  5.38it/s, est. speed input: 1694.11 toks/s, output: 307.87 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  4.49it/s, est. speed input: 1685.08 toks/s, output: 317.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.34it/s, est. speed input: 1685.08 toks/s, output: 317.52 toks/s]
[2025-01-06 15:40:43,393][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:40:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:09,  1.38s/it, est. speed input: 446.00 toks/s, output: 18.79 toks/s]Processed prompts:  88%|████████▊ | 7/8 [00:01<00:00,  5.55it/s, est. speed input: 2761.19 toks/s, output: 126.99 toks/s]Processed prompts: 100%|██████████| 8/8 [00:01<00:00,  4.39it/s, est. speed input: 2785.78 toks/s, output: 140.49 toks/s]
[2025-01-06 15:40:45,612][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:40:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:17,  3.35s/it, est. speed input: 208.51 toks/s, output: 8.65 toks/s]Processed prompts:  17%|█▋        | 4/24 [00:03<00:15,  1.32it/s, est. speed input: 733.62 toks/s, output: 33.06 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:04<00:07,  2.43it/s, est. speed input: 1179.26 toks/s, output: 61.22 toks/s]Processed prompts:  33%|███▎      | 8/24 [00:04<00:07,  2.13it/s, est. speed input: 1155.15 toks/s, output: 65.69 toks/s]Processed prompts:  38%|███▊      | 9/24 [00:05<00:06,  2.32it/s, est. speed input: 1222.78 toks/s, output: 75.80 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:05<00:05,  2.76it/s, est. speed input: 1321.09 toks/s, output: 88.07 toks/s]Processed prompts:  46%|████▌     | 11/24 [00:05<00:04,  3.18it/s, est. speed input: 1406.00 toks/s, output: 100.02 toks/s]Processed prompts:  50%|█████     | 12/24 [00:05<00:03,  3.75it/s, est. speed input: 1496.69 toks/s, output: 112.77 toks/s]Processed prompts:  58%|█████▊    | 14/24 [00:05<00:02,  4.72it/s, est. speed input: 1662.74 toks/s, output: 137.80 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:06<00:01,  4.60it/s, est. speed input: 1764.24 toks/s, output: 160.27 toks/s]Processed prompts:  75%|███████▌  | 18/24 [00:06<00:00,  6.28it/s, est. speed input: 1952.09 toks/s, output: 192.70 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:06<00:01,  4.76it/s, est. speed input: 1936.90 toks/s, output: 200.24 toks/s]Processed prompts:  83%|████████▎ | 20/24 [00:07<00:01,  3.28it/s, est. speed input: 1866.51 toks/s, output: 204.54 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:07<00:00,  3.81it/s, est. speed input: 1926.42 toks/s, output: 222.71 toks/s]Processed prompts:  92%|█████████▏| 22/24 [00:07<00:00,  4.24it/s, est. speed input: 1976.84 toks/s, output: 240.39 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  5.56it/s, est. speed input: 2100.58 toks/s, output: 279.35 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  3.01it/s, est. speed input: 2100.58 toks/s, output: 279.35 toks/s]
[2025-01-06 15:40:54,171][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:40:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.73s/it, est. speed input: 294.78 toks/s, output: 16.76 toks/s]Processed prompts:  30%|███       | 3/10 [00:02<00:04,  1.52it/s, est. speed input: 942.03 toks/s, output: 47.14 toks/s]Processed prompts:  40%|████      | 4/10 [00:02<00:03,  1.83it/s, est. speed input: 1087.85 toks/s, output: 65.11 toks/s]Processed prompts:  60%|██████    | 6/10 [00:02<00:01,  2.79it/s, est. speed input: 1370.64 toks/s, output: 105.85 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:00,  3.06it/s, est. speed input: 1487.95 toks/s, output: 125.84 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  4.13it/s, est. speed input: 1726.02 toks/s, output: 172.20 toks/s]Processed prompts: 100%|██████████| 10/10 [00:03<00:00,  2.83it/s, est. speed input: 1837.79 toks/s, output: 199.25 toks/s]
[2025-01-06 15:40:58,167][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:40:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 815.97 toks/s, output: 41.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 815.97 toks/s, output: 41.20 toks/s]
[2025-01-06 15:41:03,962][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:41:11,498][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:41:11,498][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.04 GB
[2025-01-06 15:41:11,900][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.67 GB
[2025-01-06 15:41:23,687][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:41:23,688][root][INFO] - Before destroying HF.: GPU memory allocated: 15.67 GB
[2025-01-06 15:41:24,079][root][INFO] - After destroying HF.: GPU memory allocated: 0.08 GB
[2025-01-06 15:41:24,222][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 15:41:31,459][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:41:45,697][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:41:45,698][root][INFO] - Iteration 4 took 1m 53s. Generation: 63.17%, Training: 36.83%. Estimated time remaining: 3h 54m 35s. Estimated total time for complete run: 4h 2m 35s.
[2025-01-06 15:41:46,018][root][INFO] - Loading VLLM model.
WARNING 01-06 15:41:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:41:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:41:46 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:41:46 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:41:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:42:05 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:42:05 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:42:05 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:42:26 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:42:26,847][root][INFO] - Before destroying HF.: GPU memory allocated: 71.05 GB
[2025-01-06 15:42:27,077][root][INFO] - After destroying HF.: GPU memory allocated: 55.45 GB
[2025-01-06 15:42:27,079][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:42:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:53,  3.66s/it, est. speed input: 137.82 toks/s, output: 7.10 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:03,  2.11s/it, est. speed input: 215.71 toks/s, output: 14.74 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:44,  1.52s/it, est. speed input: 275.56 toks/s, output: 22.92 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:19,  1.42it/s, est. speed input: 445.71 toks/s, output: 42.89 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:15,  1.68it/s, est. speed input: 505.98 toks/s, output: 51.60 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.68it/s, est. speed input: 652.13 toks/s, output: 71.99 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:07,  3.26it/s, est. speed input: 721.94 toks/s, output: 82.28 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  6.23it/s, est. speed input: 1007.21 toks/s, output: 124.58 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:06<00:02,  6.39it/s, est. speed input: 1112.10 toks/s, output: 143.14 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  7.21it/s, est. speed input: 1225.80 toks/s, output: 164.06 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  8.33it/s, est. speed input: 1388.23 toks/s, output: 196.00 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:07<00:01,  8.77it/s, est. speed input: 1487.16 toks/s, output: 218.46 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:07<00:01,  7.20it/s, est. speed input: 1538.55 toks/s, output: 236.62 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:07<00:00, 10.05it/s, est. speed input: 1709.13 toks/s, output: 278.40 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00,  6.64it/s, est. speed input: 1709.49 toks/s, output: 292.05 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  5.45it/s, est. speed input: 1718.62 toks/s, output: 311.12 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  4.69it/s, est. speed input: 1705.61 toks/s, output: 320.12 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.38it/s, est. speed input: 1705.61 toks/s, output: 320.12 toks/s]
[2025-01-06 15:42:36,979][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:42:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:08,  1.37s/it, est. speed input: 416.43 toks/s, output: 21.19 toks/s]Processed prompts: 100%|██████████| 7/7 [00:02<00:00,  3.16it/s, est. speed input: 1704.34 toks/s, output: 107.85 toks/s]Processed prompts: 100%|██████████| 7/7 [00:02<00:00,  2.77it/s, est. speed input: 1704.34 toks/s, output: 107.85 toks/s]
[2025-01-06 15:42:39,893][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:42:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:22,  3.46s/it, est. speed input: 202.27 toks/s, output: 8.39 toks/s]Processed prompts:   8%|▊         | 2/25 [00:04<00:40,  1.78s/it, est. speed input: 344.60 toks/s, output: 17.25 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:05<00:30,  1.41s/it, est. speed input: 417.10 toks/s, output: 26.06 toks/s]Processed prompts:  20%|██        | 5/25 [00:05<00:13,  1.54it/s, est. speed input: 676.35 toks/s, output: 49.73 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:05<00:09,  1.94it/s, est. speed input: 784.65 toks/s, output: 60.80 toks/s]Processed prompts:  44%|████▍     | 11/25 [00:05<00:02,  4.67it/s, est. speed input: 1352.57 toks/s, output: 119.62 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:05<00:02,  4.80it/s, est. speed input: 1430.81 toks/s, output: 129.98 toks/s]Processed prompts:  56%|█████▌    | 14/25 [00:05<00:01,  6.12it/s, est. speed input: 1632.25 toks/s, output: 155.29 toks/s]Processed prompts:  64%|██████▍   | 16/25 [00:06<00:01,  6.85it/s, est. speed input: 1801.10 toks/s, output: 179.40 toks/s]Processed prompts:  72%|███████▏  | 18/25 [00:06<00:00,  8.50it/s, est. speed input: 1991.76 toks/s, output: 206.43 toks/s]Processed prompts:  80%|████████  | 20/25 [00:06<00:00,  7.05it/s, est. speed input: 2082.98 toks/s, output: 226.18 toks/s]Processed prompts:  88%|████████▊ | 22/25 [00:07<00:00,  6.90it/s, est. speed input: 2192.09 toks/s, output: 250.74 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:07<00:00,  4.05it/s, est. speed input: 2102.93 toks/s, output: 259.48 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  3.77it/s, est. speed input: 2098.26 toks/s, output: 272.56 toks/s]Processed prompts: 100%|██████████| 25/25 [00:08<00:00,  3.00it/s, est. speed input: 2098.26 toks/s, output: 272.56 toks/s]
[2025-01-06 15:42:48,768][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:42:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:16,  1.79s/it, est. speed input: 475.60 toks/s, output: 16.23 toks/s]Processed prompts:  40%|████      | 4/10 [00:01<00:02,  2.54it/s, est. speed input: 1554.95 toks/s, output: 62.16 toks/s]Processed prompts:  60%|██████    | 6/10 [00:02<00:01,  2.30it/s, est. speed input: 1557.47 toks/s, output: 82.22 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:01,  2.80it/s, est. speed input: 1729.80 toks/s, output: 106.95 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  2.91it/s, est. speed input: 1803.46 toks/s, output: 144.41 toks/s]Processed prompts: 100%|██████████| 10/10 [00:05<00:00,  1.77it/s, est. speed input: 1465.20 toks/s, output: 145.77 toks/s]Processed prompts: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s, est. speed input: 1465.20 toks/s, output: 145.77 toks/s]
[2025-01-06 15:42:54,297][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:42:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:01<00:02,  1.19s/it, est. speed input: 638.49 toks/s, output: 32.72 toks/s]Processed prompts:  67%|██████▋   | 2/3 [00:01<00:00,  1.76it/s, est. speed input: 1184.83 toks/s, output: 64.35 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, est. speed input: 1352.72 toks/s, output: 88.01 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, est. speed input: 1352.72 toks/s, output: 88.01 toks/s]
[2025-01-06 15:43:01,290][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]
[2025-01-06 15:43:08,388][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:43:08,388][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.05 GB
[2025-01-06 15:43:08,756][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.68 GB
[2025-01-06 15:43:20,505][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:43:20,506][root][INFO] - Before destroying HF.: GPU memory allocated: 15.68 GB
[2025-01-06 15:43:20,851][root][INFO] - After destroying HF.: GPU memory allocated: 0.09 GB
[2025-01-06 15:43:20,999][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:43:28,309][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:43:42,897][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:43:42,898][root][INFO] - Iteration 5 took 1m 57s. Generation: 64.38%, Training: 35.62%. Estimated time remaining: 4h 0m 5s. Estimated total time for complete run: 4h 10m 1s.
[2025-01-06 15:43:43,186][root][INFO] - Loading VLLM model.
WARNING 01-06 15:43:43 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:43:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:43:43 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:43:43 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:43:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:44:02 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:44:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:44:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:44:24 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 15:44:24,790][root][INFO] - Before destroying HF.: GPU memory allocated: 71.06 GB
[2025-01-06 15:44:25,063][root][INFO] - After destroying HF.: GPU memory allocated: 55.46 GB
[2025-01-06 15:44:25,064][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:44:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:53,  5.60s/it, est. speed input: 90.20 toks/s, output: 9.47 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:17,  2.59s/it, est. speed input: 166.18 toks/s, output: 18.76 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:44,  1.54s/it, est. speed input: 237.86 toks/s, output: 28.26 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:27,  1.00it/s, est. speed input: 308.82 toks/s, output: 38.07 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:19,  1.39it/s, est. speed input: 373.32 toks/s, output: 47.61 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:13,  1.89it/s, est. speed input: 437.52 toks/s, output: 57.47 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:05,  4.24it/s, est. speed input: 646.70 toks/s, output: 89.50 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:04,  4.73it/s, est. speed input: 754.39 toks/s, output: 108.24 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:03,  6.15it/s, est. speed input: 875.24 toks/s, output: 129.72 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.62it/s, est. speed input: 1021.93 toks/s, output: 157.97 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  6.92it/s, est. speed input: 1113.66 toks/s, output: 178.01 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:00, 10.25it/s, est. speed input: 1334.77 toks/s, output: 226.10 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00,  8.64it/s, est. speed input: 1398.19 toks/s, output: 244.68 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  6.15it/s, est. speed input: 1417.34 toks/s, output: 258.21 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  6.94it/s, est. speed input: 1496.22 toks/s, output: 285.07 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  9.42it/s, est. speed input: 1635.49 toks/s, output: 330.65 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.16it/s, est. speed input: 1594.28 toks/s, output: 331.29 toks/s]
[2025-01-06 15:44:35,610][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:44:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:06,  1.34s/it, est. speed input: 493.93 toks/s, output: 21.67 toks/s]Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  5.49it/s, est. speed input: 2736.65 toks/s, output: 125.65 toks/s]Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.16it/s, est. speed input: 2736.65 toks/s, output: 125.65 toks/s]
[2025-01-06 15:44:37,450][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:44:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:29,  3.57s/it, est. speed input: 195.69 toks/s, output: 8.12 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:04<00:25,  1.11s/it, est. speed input: 514.93 toks/s, output: 23.82 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:04<00:10,  1.98it/s, est. speed input: 943.26 toks/s, output: 50.60 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:05<00:10,  1.88it/s, est. speed input: 965.77 toks/s, output: 56.45 toks/s]Processed prompts:  31%|███       | 8/26 [00:05<00:07,  2.31it/s, est. speed input: 1076.10 toks/s, output: 67.35 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:05<00:06,  2.74it/s, est. speed input: 1172.91 toks/s, output: 77.93 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:05<00:05,  3.01it/s, est. speed input: 1246.46 toks/s, output: 87.73 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:06<00:05,  2.81it/s, est. speed input: 1275.95 toks/s, output: 95.75 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:06<00:04,  3.49it/s, est. speed input: 1367.03 toks/s, output: 108.38 toks/s]Processed prompts:  50%|█████     | 13/26 [00:06<00:03,  4.27it/s, est. speed input: 1455.58 toks/s, output: 121.10 toks/s]Processed prompts:  58%|█████▊    | 15/26 [00:06<00:02,  4.93it/s, est. speed input: 1595.22 toks/s, output: 144.38 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:06<00:02,  3.96it/s, est. speed input: 1602.61 toks/s, output: 152.32 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:07<00:02,  4.44it/s, est. speed input: 1667.42 toks/s, output: 165.86 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:07<00:01,  5.12it/s, est. speed input: 1737.86 toks/s, output: 180.25 toks/s]Processed prompts:  73%|███████▎  | 19/26 [00:07<00:01,  5.61it/s, est. speed input: 1801.01 toks/s, output: 194.33 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:07<00:00,  5.66it/s, est. speed input: 1946.39 toks/s, output: 233.39 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:08<00:00,  5.71it/s, est. speed input: 2034.81 toks/s, output: 262.72 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:08<00:00,  3.91it/s, est. speed input: 1978.60 toks/s, output: 267.89 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  2.94it/s, est. speed input: 2057.70 toks/s, output: 290.53 toks/s]
[2025-01-06 15:44:46,855][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:44:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:19,  1.97s/it, est. speed input: 425.43 toks/s, output: 14.72 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:02<00:03,  2.24it/s, est. speed input: 1544.43 toks/s, output: 56.69 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:02<00:01,  2.80it/s, est. speed input: 1781.34 toks/s, output: 83.60 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:02<00:01,  2.97it/s, est. speed input: 1854.03 toks/s, output: 99.76 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:03<00:00,  3.55it/s, est. speed input: 2118.22 toks/s, output: 149.71 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  1.91it/s, est. speed input: 1628.76 toks/s, output: 143.99 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  2.11it/s, est. speed input: 1628.76 toks/s, output: 143.99 toks/s]
[2025-01-06 15:44:52,538][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:44:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 1266.05 toks/s, output: 38.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 1266.05 toks/s, output: 38.48 toks/s]
[2025-01-06 15:44:58,499][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 15:45:05,727][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:45:05,727][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.05 GB
[2025-01-06 15:45:06,079][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.69 GB
[2025-01-06 15:45:18,686][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:45:18,688][root][INFO] - Before destroying HF.: GPU memory allocated: 15.69 GB
[2025-01-06 15:45:19,017][root][INFO] - After destroying HF.: GPU memory allocated: 0.09 GB
[2025-01-06 15:45:19,179][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 15:45:26,831][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:45:41,680][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:45:41,681][root][INFO] - Iteration 6 took 1m 58s. Generation: 63.51%, Training: 36.49%. Estimated time remaining: 4h 1m 28s. Estimated total time for complete run: 4h 13m 24s.
[2025-01-06 15:45:41,981][root][INFO] - Loading VLLM model.
WARNING 01-06 15:45:42 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:45:42 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:45:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:45:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:45:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:46:01 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:46:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:46:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:46:23 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:46:23,103][root][INFO] - Before destroying HF.: GPU memory allocated: 71.07 GB
[2025-01-06 15:46:23,341][root][INFO] - After destroying HF.: GPU memory allocated: 55.47 GB
[2025-01-06 15:46:23,343][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:46:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:51,  3.59s/it, est. speed input: 140.83 toks/s, output: 5.58 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:08,  2.29s/it, est. speed input: 203.57 toks/s, output: 12.70 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:43,  1.51s/it, est. speed input: 273.26 toks/s, output: 20.92 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:21,  1.28it/s, est. speed input: 421.46 toks/s, output: 38.56 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.58it/s, est. speed input: 484.00 toks/s, output: 47.44 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:07,  2.95it/s, est. speed input: 692.66 toks/s, output: 76.50 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:05,  4.05it/s, est. speed input: 828.53 toks/s, output: 96.95 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.40it/s, est. speed input: 1008.56 toks/s, output: 126.25 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.20it/s, est. speed input: 1119.98 toks/s, output: 146.37 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:01,  7.66it/s, est. speed input: 1240.82 toks/s, output: 168.44 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  9.30it/s, est. speed input: 1359.31 toks/s, output: 190.98 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:07<00:01,  9.65it/s, est. speed input: 1458.12 toks/s, output: 211.70 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:01,  7.02it/s, est. speed input: 1498.65 toks/s, output: 227.89 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00,  6.49it/s, est. speed input: 1553.83 toks/s, output: 247.33 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  3.63it/s, est. speed input: 1487.05 toks/s, output: 258.62 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  3.98it/s, est. speed input: 1520.00 toks/s, output: 275.61 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s, est. speed input: 1621.28 toks/s, output: 315.73 toks/s]
[2025-01-06 15:46:33,721][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:46:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:14,  1.66s/it, est. speed input: 339.71 toks/s, output: 16.26 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.39it/s, est. speed input: 1416.16 toks/s, output: 100.29 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s, est. speed input: 1416.16 toks/s, output: 100.29 toks/s]
[2025-01-06 15:46:38,702][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:46:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:08,  3.27s/it, est. speed input: 213.71 toks/s, output: 9.78 toks/s]Processed prompts:   9%|▉         | 2/22 [00:04<00:40,  2.05s/it, est. speed input: 313.21 toks/s, output: 20.16 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:23,  1.23s/it, est. speed input: 443.32 toks/s, output: 32.56 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:05<00:10,  1.61it/s, est. speed input: 695.40 toks/s, output: 57.90 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:05<00:05,  2.71it/s, est. speed input: 951.33 toks/s, output: 84.96 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:05<00:04,  3.27it/s, est. speed input: 1064.50 toks/s, output: 97.85 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:03,  3.43it/s, est. speed input: 1143.50 toks/s, output: 108.70 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:05<00:02,  4.01it/s, est. speed input: 1239.97 toks/s, output: 121.69 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:02,  4.92it/s, est. speed input: 1415.99 toks/s, output: 147.54 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:01,  6.54it/s, est. speed input: 1612.94 toks/s, output: 177.02 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:06<00:01,  6.23it/s, est. speed input: 1676.22 toks/s, output: 189.12 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:01,  5.10it/s, est. speed input: 1703.03 toks/s, output: 198.56 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  7.13it/s, est. speed input: 1883.29 toks/s, output: 232.16 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:06<00:00,  5.83it/s, est. speed input: 1907.68 toks/s, output: 242.75 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:07<00:00,  3.65it/s, est. speed input: 1849.72 toks/s, output: 245.97 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:08<00:00,  2.86it/s, est. speed input: 1806.04 toks/s, output: 253.33 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.71it/s, est. speed input: 1892.01 toks/s, output: 277.93 toks/s]
[2025-01-06 15:46:47,366][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:46:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:26,  2.18s/it, est. speed input: 438.16 toks/s, output: 13.32 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:02<00:07,  1.42it/s, est. speed input: 1064.76 toks/s, output: 38.80 toks/s]Processed prompts:  31%|███       | 4/13 [00:03<00:05,  1.58it/s, est. speed input: 1119.28 toks/s, output: 51.44 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:03<00:04,  1.93it/s, est. speed input: 1234.18 toks/s, output: 67.48 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:03<00:01,  3.10it/s, est. speed input: 1544.26 toks/s, output: 104.20 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:01,  3.25it/s, est. speed input: 1669.55 toks/s, output: 133.59 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:04<00:00,  4.13it/s, est. speed input: 1887.55 toks/s, output: 175.37 toks/s]Processed prompts: 100%|██████████| 13/13 [00:04<00:00,  5.46it/s, est. speed input: 2136.71 toks/s, output: 223.20 toks/s]Processed prompts: 100%|██████████| 13/13 [00:04<00:00,  2.86it/s, est. speed input: 2136.71 toks/s, output: 223.20 toks/s]
[2025-01-06 15:46:52,381][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:46:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.14it/s, est. speed input: 933.43 toks/s, output: 32.97 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s, est. speed input: 1905.07 toks/s, output: 65.93 toks/s]
[2025-01-06 15:46:58,552][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 15:47:05,740][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:47:05,740][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.06 GB
[2025-01-06 15:47:06,154][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.70 GB
[2025-01-06 15:47:18,141][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:47:18,142][root][INFO] - Before destroying HF.: GPU memory allocated: 15.70 GB
[2025-01-06 15:47:18,460][root][INFO] - After destroying HF.: GPU memory allocated: 0.10 GB
[2025-01-06 15:47:18,616][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 15:47:25,994][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:47:40,998][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:47:40,999][root][INFO] - Iteration 7 took 1m 59s. Generation: 64.31%, Training: 35.69%. Estimated time remaining: 4h 0m 38s. Estimated total time for complete run: 4h 14m 32s.
[2025-01-06 15:47:41,390][root][INFO] - Loading VLLM model.
WARNING 01-06 15:47:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:47:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:47:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:47:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:47:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:48:00 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:48:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:48:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:48:22 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:48:22,961][root][INFO] - Before destroying HF.: GPU memory allocated: 71.08 GB
[2025-01-06 15:48:23,203][root][INFO] - After destroying HF.: GPU memory allocated: 55.48 GB
[2025-01-06 15:48:23,205][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:48:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:52,  5.56s/it, est. speed input: 90.85 toks/s, output: 9.54 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:20,  2.68s/it, est. speed input: 162.46 toks/s, output: 18.82 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:43,  1.51s/it, est. speed input: 239.17 toks/s, output: 28.89 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:29,  1.05s/it, est. speed input: 302.50 toks/s, output: 38.19 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:14,  1.82it/s, est. speed input: 439.40 toks/s, output: 58.59 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:09,  2.64it/s, est. speed input: 560.91 toks/s, output: 78.03 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:05,  4.11it/s, est. speed input: 741.66 toks/s, output: 108.95 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.95it/s, est. speed input: 922.34 toks/s, output: 141.68 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.19it/s, est. speed input: 1015.77 toks/s, output: 161.29 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  6.16it/s, est. speed input: 1097.46 toks/s, output: 180.01 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:02,  5.86it/s, est. speed input: 1129.60 toks/s, output: 188.95 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00, 10.34it/s, est. speed input: 1397.51 toks/s, output: 252.17 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00, 10.55it/s, est. speed input: 1524.48 toks/s, output: 286.56 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00, 10.36it/s, est. speed input: 1638.68 toks/s, output: 321.68 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  5.50it/s, est. speed input: 1587.40 toks/s, output: 328.19 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.14it/s, est. speed input: 1587.40 toks/s, output: 328.19 toks/s]
[2025-01-06 15:48:33,802][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:48:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it, est. speed input: 686.03 toks/s, output: 26.70 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.68it/s, est. speed input: 2421.25 toks/s, output: 106.79 toks/s]
[2025-01-06 15:48:35,293][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:48:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:04<02:06,  4.70s/it, est. speed input: 148.72 toks/s, output: 9.57 toks/s]Processed prompts:   7%|▋         | 2/28 [00:05<01:01,  2.35s/it, est. speed input: 258.63 toks/s, output: 19.05 toks/s]Processed prompts:  11%|█         | 3/28 [00:05<00:34,  1.37s/it, est. speed input: 373.30 toks/s, output: 29.37 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:05<00:20,  1.15it/s, est. speed input: 488.76 toks/s, output: 40.03 toks/s]Processed prompts:  18%|█▊        | 5/28 [00:05<00:13,  1.68it/s, est. speed input: 600.38 toks/s, output: 50.68 toks/s]Processed prompts:  21%|██▏       | 6/28 [00:06<00:11,  1.97it/s, est. speed input: 680.65 toks/s, output: 59.72 toks/s]Processed prompts:  25%|██▌       | 7/28 [00:06<00:08,  2.57it/s, est. speed input: 776.09 toks/s, output: 70.42 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:06<00:06,  3.09it/s, est. speed input: 861.72 toks/s, output: 80.75 toks/s]Processed prompts:  36%|███▌      | 10/28 [00:06<00:03,  4.51it/s, est. speed input: 1041.87 toks/s, output: 102.99 toks/s]Processed prompts:  39%|███▉      | 11/28 [00:06<00:03,  4.57it/s, est. speed input: 1111.37 toks/s, output: 112.89 toks/s]Processed prompts:  46%|████▋     | 13/28 [00:07<00:02,  6.60it/s, est. speed input: 1291.47 toks/s, output: 137.15 toks/s]Processed prompts:  54%|█████▎    | 15/28 [00:07<00:01,  8.21it/s, est. speed input: 1460.13 toks/s, output: 161.12 toks/s]Processed prompts:  61%|██████    | 17/28 [00:07<00:01,  6.44it/s, est. speed input: 1560.17 toks/s, output: 180.14 toks/s]Processed prompts:  64%|██████▍   | 18/28 [00:08<00:02,  3.82it/s, est. speed input: 1515.14 toks/s, output: 181.11 toks/s]Processed prompts:  68%|██████▊   | 19/28 [00:08<00:02,  4.23it/s, est. speed input: 1571.26 toks/s, output: 194.14 toks/s]Processed prompts:  75%|███████▌  | 21/28 [00:08<00:01,  4.10it/s, est. speed input: 1637.30 toks/s, output: 215.94 toks/s]Processed prompts:  82%|████████▏ | 23/28 [00:09<00:01,  4.41it/s, est. speed input: 1718.23 toks/s, output: 242.18 toks/s]Processed prompts:  86%|████████▌ | 24/28 [00:09<00:00,  4.80it/s, est. speed input: 1767.16 toks/s, output: 257.45 toks/s]Processed prompts:  89%|████████▉ | 25/28 [00:09<00:00,  5.38it/s, est. speed input: 1820.02 toks/s, output: 273.60 toks/s]Processed prompts:  96%|█████████▋| 27/28 [00:09<00:00,  5.85it/s, est. speed input: 1907.15 toks/s, output: 304.06 toks/s]Processed prompts: 100%|██████████| 28/28 [00:09<00:00,  2.82it/s, est. speed input: 1974.35 toks/s, output: 323.71 toks/s]
[2025-01-06 15:48:45,812][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:48:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:06,  1.37s/it, est. speed input: 673.21 toks/s, output: 21.11 toks/s]Processed prompts:  50%|█████     | 3/6 [00:02<00:02,  1.21it/s, est. speed input: 975.38 toks/s, output: 55.28 toks/s]Processed prompts:  83%|████████▎ | 5/6 [00:02<00:00,  2.07it/s, est. speed input: 1342.37 toks/s, output: 115.86 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  2.30it/s, est. speed input: 1432.70 toks/s, output: 142.87 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, est. speed input: 1432.70 toks/s, output: 142.87 toks/s]
[2025-01-06 15:48:54,602][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 15:49:01,829][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:49:01,830][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.07 GB
[2025-01-06 15:49:02,199][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.70 GB
[2025-01-06 15:49:13,930][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:49:13,931][root][INFO] - Before destroying HF.: GPU memory allocated: 15.70 GB
[2025-01-06 15:49:14,264][root][INFO] - After destroying HF.: GPU memory allocated: 0.11 GB
[2025-01-06 15:49:14,413][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:49:21,726][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:49:36,713][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:49:36,714][root][INFO] - Iteration 8 took 1m 55s. Generation: 63.48%, Training: 36.52%. Estimated time remaining: 3h 51m 1s. Estimated total time for complete run: 4h 6m 51s.
[2025-01-06 15:49:37,024][root][INFO] - Loading VLLM model.
WARNING 01-06 15:49:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:49:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:49:37 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:49:37 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:49:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:49:56 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:49:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:49:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:50:18 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:50:18,665][root][INFO] - Before destroying HF.: GPU memory allocated: 71.08 GB
[2025-01-06 15:50:18,915][root][INFO] - After destroying HF.: GPU memory allocated: 55.48 GB
[2025-01-06 15:50:18,916][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:50:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:42,  3.31s/it, est. speed input: 152.46 toks/s, output: 6.04 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:05,  2.17s/it, est. speed input: 215.43 toks/s, output: 13.44 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.47s/it, est. speed input: 284.26 toks/s, output: 21.95 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:33,  1.19s/it, est. speed input: 332.63 toks/s, output: 30.30 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.19it/s, est. speed input: 401.08 toks/s, output: 40.50 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:15,  1.69it/s, est. speed input: 473.16 toks/s, output: 51.22 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.97it/s, est. speed input: 616.10 toks/s, output: 72.74 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:04,  4.70it/s, est. speed input: 811.76 toks/s, output: 104.19 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.43it/s, est. speed input: 971.42 toks/s, output: 133.55 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.46it/s, est. speed input: 1085.84 toks/s, output: 156.02 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:02,  6.35it/s, est. speed input: 1170.02 toks/s, output: 175.44 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:02,  5.41it/s, est. speed input: 1186.70 toks/s, output: 182.67 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  7.12it/s, est. speed input: 1295.56 toks/s, output: 208.90 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  8.37it/s, est. speed input: 1393.38 toks/s, output: 234.05 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00, 10.04it/s, est. speed input: 1494.36 toks/s, output: 260.76 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00,  9.37it/s, est. speed input: 1568.39 toks/s, output: 283.89 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00,  9.33it/s, est. speed input: 1643.69 toks/s, output: 308.76 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  7.77it/s, est. speed input: 1689.51 toks/s, output: 330.99 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  7.38it/s, est. speed input: 1713.23 toks/s, output: 343.70 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.39it/s, est. speed input: 1713.23 toks/s, output: 343.70 toks/s]
[2025-01-06 15:50:28,809][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:50:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:14,  1.66s/it, est. speed input: 338.82 toks/s, output: 16.22 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:02<00:00,  4.06it/s, est. speed input: 2212.12 toks/s, output: 117.70 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  4.01it/s, est. speed input: 2219.23 toks/s, output: 139.58 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  3.44it/s, est. speed input: 2219.23 toks/s, output: 139.58 toks/s]
[2025-01-06 15:50:32,108][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:50:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:04<01:34,  4.50s/it, est. speed input: 155.27 toks/s, output: 12.88 toks/s]Processed prompts:   9%|▉         | 2/22 [00:04<00:40,  2.04s/it, est. speed input: 289.83 toks/s, output: 25.50 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:22,  1.17s/it, est. speed input: 423.06 toks/s, output: 38.53 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:05<00:14,  1.23it/s, est. speed input: 535.90 toks/s, output: 50.79 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:05<00:04,  3.12it/s, est. speed input: 993.37 toks/s, output: 102.50 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:03,  3.51it/s, est. speed input: 1090.00 toks/s, output: 115.39 toks/s]Processed prompts:  50%|█████     | 11/22 [00:05<00:02,  4.85it/s, est. speed input: 1302.19 toks/s, output: 143.78 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:06<00:01,  5.68it/s, est. speed input: 1479.66 toks/s, output: 169.83 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:01,  5.90it/s, est. speed input: 1557.56 toks/s, output: 182.88 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:06<00:01,  6.41it/s, est. speed input: 1640.74 toks/s, output: 197.01 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:01,  5.54it/s, est. speed input: 1681.74 toks/s, output: 207.36 toks/s]Processed prompts:  77%|███████▋  | 17/22 [00:06<00:00,  5.82it/s, est. speed input: 1748.63 toks/s, output: 221.47 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  5.55it/s, est. speed input: 1797.50 toks/s, output: 234.29 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:07<00:00,  4.55it/s, est. speed input: 1813.42 toks/s, output: 244.41 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:07<00:00,  4.89it/s, est. speed input: 1907.72 toks/s, output: 274.48 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  3.63it/s, est. speed input: 1876.32 toks/s, output: 282.09 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.68it/s, est. speed input: 1876.32 toks/s, output: 282.09 toks/s]
[2025-01-06 15:50:40,840][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:50:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:25,  2.15s/it, est. speed input: 405.13 toks/s, output: 13.46 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:02<00:06,  1.46it/s, est. speed input: 1011.39 toks/s, output: 39.24 toks/s]Processed prompts:  31%|███       | 4/13 [00:02<00:04,  2.00it/s, est. speed input: 1290.75 toks/s, output: 54.06 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:02<00:02,  2.69it/s, est. speed input: 1422.75 toks/s, output: 69.61 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:03<00:02,  2.75it/s, est. speed input: 1489.21 toks/s, output: 81.82 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:03<00:02,  2.53it/s, est. speed input: 1490.19 toks/s, output: 93.52 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:04<00:02,  2.41it/s, est. speed input: 1493.65 toks/s, output: 107.42 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:01,  3.10it/s, est. speed input: 1621.40 toks/s, output: 129.57 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:04<00:00,  3.79it/s, est. speed input: 1735.69 toks/s, output: 151.37 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:04<00:00,  3.16it/s, est. speed input: 1722.90 toks/s, output: 165.33 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:04<00:00,  3.55it/s, est. speed input: 1794.27 toks/s, output: 187.63 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.92it/s, est. speed input: 1762.90 toks/s, output: 202.67 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.41it/s, est. speed input: 1762.90 toks/s, output: 202.67 toks/s]
[2025-01-06 15:50:46,746][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:50:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it, est. speed input: 746.82 toks/s, output: 25.42 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.55it/s, est. speed input: 1804.34 toks/s, output: 86.37 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.23it/s, est. speed input: 1804.34 toks/s, output: 86.37 toks/s]
[2025-01-06 15:50:54,135][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:51:01,433][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:51:01,433][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.08 GB
[2025-01-06 15:51:01,779][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.71 GB
[2025-01-06 15:51:13,754][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:51:13,755][root][INFO] - Before destroying HF.: GPU memory allocated: 15.71 GB
[2025-01-06 15:51:14,127][root][INFO] - After destroying HF.: GPU memory allocated: 0.12 GB
[2025-01-06 15:51:14,293][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:51:21,597][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:51:36,730][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:51:36,731][root][INFO] - Iteration 9 took 2m 0s. Generation: 64.39%, Training: 35.61%. Estimated time remaining: 3h 58m 11s. Estimated total time for complete run: 4h 16m 2s.
[2025-01-06 15:51:37,020][root][INFO] - Loading VLLM model.
WARNING 01-06 15:51:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:51:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:51:37 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:51:37 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:51:42 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:51:56 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:51:56 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:51:56 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:52:18 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:52:18,163][root][INFO] - Before destroying HF.: GPU memory allocated: 71.09 GB
[2025-01-06 15:52:18,436][root][INFO] - After destroying HF.: GPU memory allocated: 55.49 GB
[2025-01-06 15:52:18,437][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:52:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:42,  3.29s/it, est. speed input: 153.36 toks/s, output: 6.07 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:04,  2.17s/it, est. speed input: 216.31 toks/s, output: 13.49 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.47s/it, est. speed input: 285.30 toks/s, output: 22.03 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:33,  1.18s/it, est. speed input: 333.71 toks/s, output: 30.40 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:21,  1.26it/s, est. speed input: 409.59 toks/s, output: 41.04 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:10,  2.39it/s, est. speed input: 563.76 toks/s, output: 62.68 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.99it/s, est. speed input: 633.93 toks/s, output: 73.12 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:03,  5.48it/s, est. speed input: 852.00 toks/s, output: 105.98 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  6.04it/s, est. speed input: 967.65 toks/s, output: 125.14 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  6.04it/s, est. speed input: 1064.51 toks/s, output: 143.48 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:01,  7.79it/s, est. speed input: 1238.37 toks/s, output: 177.10 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:07<00:01, 10.69it/s, est. speed input: 1424.94 toks/s, output: 214.18 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:07<00:00, 11.75it/s, est. speed input: 1535.37 toks/s, output: 237.81 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:07<00:00, 12.40it/s, est. speed input: 1639.17 toks/s, output: 261.23 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00,  5.95it/s, est. speed input: 1606.15 toks/s, output: 269.16 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00,  5.22it/s, est. speed input: 1628.89 toks/s, output: 288.40 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  5.92it/s, est. speed input: 1698.59 toks/s, output: 318.13 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.44it/s, est. speed input: 1737.62 toks/s, output: 334.51 toks/s]
[2025-01-06 15:52:28,156][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:52:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.55s/it, est. speed input: 416.38 toks/s, output: 17.40 toks/s]Processed prompts: 100%|██████████| 9/9 [00:01<00:00,  6.32it/s, est. speed input: 3112.79 toks/s, output: 149.29 toks/s]Processed prompts: 100%|██████████| 9/9 [00:01<00:00,  4.89it/s, est. speed input: 3112.79 toks/s, output: 149.29 toks/s]
[2025-01-06 15:52:30,410][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:52:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:04<01:32,  4.20s/it, est. speed input: 166.51 toks/s, output: 11.67 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:04<00:24,  1.24s/it, est. speed input: 454.38 toks/s, output: 34.24 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:04<00:12,  1.49it/s, est. speed input: 716.57 toks/s, output: 57.82 toks/s]Processed prompts:  30%|███       | 7/23 [00:05<00:06,  2.42it/s, est. speed input: 978.18 toks/s, output: 82.96 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:05<00:05,  2.77it/s, est. speed input: 1076.92 toks/s, output: 93.79 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:05<00:03,  4.18it/s, est. speed input: 1318.67 toks/s, output: 119.79 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:05<00:02,  4.46it/s, est. speed input: 1471.96 toks/s, output: 140.39 toks/s]Processed prompts:  57%|█████▋    | 13/23 [00:05<00:02,  4.90it/s, est. speed input: 1560.29 toks/s, output: 152.99 toks/s]Processed prompts:  61%|██████    | 14/23 [00:05<00:01,  5.42it/s, est. speed input: 1646.72 toks/s, output: 165.92 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:06<00:00,  8.36it/s, est. speed input: 1947.65 toks/s, output: 209.63 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:06<00:00,  4.87it/s, est. speed input: 1931.35 toks/s, output: 220.75 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:07<00:00,  4.82it/s, est. speed input: 1970.93 toks/s, output: 234.45 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:08<00:00,  3.27it/s, est. speed input: 1904.52 toks/s, output: 249.06 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.84it/s, est. speed input: 1982.76 toks/s, output: 272.68 toks/s]
[2025-01-06 15:52:39,082][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:52:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:02<00:19,  2.47s/it, est. speed input: 282.51 toks/s, output: 23.44 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:02<00:07,  1.13s/it, est. speed input: 525.32 toks/s, output: 46.22 toks/s]Processed prompts:  44%|████▍     | 4/9 [00:02<00:02,  2.14it/s, est. speed input: 995.02 toks/s, output: 92.88 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:03<00:01,  2.95it/s, est. speed input: 1309.58 toks/s, output: 132.39 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:03<00:00,  4.29it/s, est. speed input: 1667.52 toks/s, output: 182.50 toks/s]Processed prompts: 100%|██████████| 9/9 [00:04<00:00,  2.17it/s, est. speed input: 1365.78 toks/s, output: 170.21 toks/s]Processed prompts: 100%|██████████| 9/9 [00:04<00:00,  1.95it/s, est. speed input: 1365.78 toks/s, output: 170.21 toks/s]
[2025-01-06 15:52:49,449][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 15:52:56,650][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:52:56,650][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.09 GB
[2025-01-06 15:52:57,025][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.72 GB
[2025-01-06 15:53:08,882][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:53:08,883][root][INFO] - Before destroying HF.: GPU memory allocated: 15.72 GB
[2025-01-06 15:53:09,219][root][INFO] - After destroying HF.: GPU memory allocated: 0.13 GB
[2025-01-06 15:53:09,366][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:53:16,801][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:53:31,245][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:53:31,246][root][INFO] - Iteration 10 took 1m 54s. Generation: 63.35%, Training: 36.65%. Estimated time remaining: 3h 44m 33s. Estimated total time for complete run: 4h 4m 17s.
[2025-01-06 15:53:31,656][root][INFO] - Loading VLLM model.
WARNING 01-06 15:53:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:53:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:53:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:53:32 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:53:37 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:53:51 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:53:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:53:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:54:12 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:54:12,797][root][INFO] - Before destroying HF.: GPU memory allocated: 71.10 GB
[2025-01-06 15:54:13,049][root][INFO] - After destroying HF.: GPU memory allocated: 55.50 GB
[2025-01-06 15:54:13,050][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:54:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:59,  3.86s/it, est. speed input: 130.67 toks/s, output: 5.18 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:12,  2.40s/it, est. speed input: 192.61 toks/s, output: 12.01 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:46,  1.60s/it, est. speed input: 257.44 toks/s, output: 19.88 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:28,  1.04s/it, est. speed input: 333.52 toks/s, output: 28.73 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:20,  1.35it/s, est. speed input: 402.11 toks/s, output: 37.42 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:14,  1.78it/s, est. speed input: 466.52 toks/s, output: 46.19 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:07,  3.21it/s, est. speed input: 612.18 toks/s, output: 65.61 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:04,  4.99it/s, est. speed input: 806.93 toks/s, output: 93.11 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  8.17it/s, est. speed input: 1073.59 toks/s, output: 133.37 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:01,  8.68it/s, est. speed input: 1184.97 toks/s, output: 151.83 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:07<00:01,  7.27it/s, est. speed input: 1255.96 toks/s, output: 167.94 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:07<00:01,  7.39it/s, est. speed input: 1342.48 toks/s, output: 186.97 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:01,  6.31it/s, est. speed input: 1427.28 toks/s, output: 212.33 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:01,  4.18it/s, est. speed input: 1393.71 toks/s, output: 221.21 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:01,  4.54it/s, est. speed input: 1429.01 toks/s, output: 235.50 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  5.28it/s, est. speed input: 1497.04 toks/s, output: 264.55 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  4.03it/s, est. speed input: 1471.56 toks/s, output: 270.80 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.11it/s, est. speed input: 1569.63 toks/s, output: 309.65 toks/s]
[2025-01-06 15:54:23,783][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:54:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:17,  1.79s/it, est. speed input: 314.63 toks/s, output: 15.06 toks/s]Processed prompts: 100%|██████████| 11/11 [00:02<00:00,  5.54it/s, est. speed input: 2883.80 toks/s, output: 144.09 toks/s]Processed prompts: 100%|██████████| 11/11 [00:02<00:00,  4.45it/s, est. speed input: 2883.80 toks/s, output: 144.09 toks/s]
[2025-01-06 15:54:26,651][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:54:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:03<01:12,  3.62s/it, est. speed input: 193.22 toks/s, output: 11.61 toks/s]Processed prompts:  10%|▉         | 2/21 [00:04<00:36,  1.91s/it, est. speed input: 323.13 toks/s, output: 23.11 toks/s]Processed prompts:  19%|█▉        | 4/21 [00:04<00:14,  1.19it/s, est. speed input: 589.34 toks/s, output: 47.64 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:04<00:07,  2.08it/s, est. speed input: 855.50 toks/s, output: 75.06 toks/s]Processed prompts:  33%|███▎      | 7/21 [00:05<00:05,  2.41it/s, est. speed input: 955.35 toks/s, output: 87.08 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:05<00:03,  3.43it/s, est. speed input: 1173.16 toks/s, output: 113.94 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:05<00:03,  3.33it/s, est. speed input: 1228.22 toks/s, output: 124.05 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:05<00:02,  3.47it/s, est. speed input: 1293.94 toks/s, output: 136.14 toks/s]Processed prompts:  62%|██████▏   | 13/21 [00:06<00:01,  5.04it/s, est. speed input: 1493.15 toks/s, output: 167.77 toks/s]Processed prompts:  71%|███████▏  | 15/21 [00:06<00:00,  6.70it/s, est. speed input: 1686.41 toks/s, output: 199.92 toks/s]Processed prompts:  81%|████████  | 17/21 [00:06<00:00,  5.81it/s, est. speed input: 1787.55 toks/s, output: 223.84 toks/s]Processed prompts:  86%|████████▌ | 18/21 [00:06<00:00,  6.11it/s, est. speed input: 1856.40 toks/s, output: 239.91 toks/s]Processed prompts:  90%|█████████ | 19/21 [00:07<00:00,  5.55it/s, est. speed input: 1892.83 toks/s, output: 253.12 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:07<00:00,  4.81it/s, est. speed input: 1912.18 toks/s, output: 265.63 toks/s]Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  3.59it/s, est. speed input: 1883.09 toks/s, output: 274.14 toks/s]Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  2.69it/s, est. speed input: 1883.09 toks/s, output: 274.14 toks/s]
[2025-01-06 15:54:34,985][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:54:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:26,  2.19s/it, est. speed input: 412.27 toks/s, output: 13.24 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:02<00:13,  1.23s/it, est. speed input: 583.01 toks/s, output: 27.29 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:03<00:08,  1.13it/s, est. speed input: 715.63 toks/s, output: 42.30 toks/s]Processed prompts:  31%|███       | 4/13 [00:03<00:05,  1.72it/s, est. speed input: 899.96 toks/s, output: 60.30 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:03<00:04,  1.68it/s, est. speed input: 934.51 toks/s, output: 72.76 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:04<00:03,  2.23it/s, est. speed input: 1127.85 toks/s, output: 92.47 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:01,  3.84it/s, est. speed input: 1494.60 toks/s, output: 150.92 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:04<00:00,  4.70it/s, est. speed input: 1706.50 toks/s, output: 191.57 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:04<00:00,  5.01it/s, est. speed input: 1797.46 toks/s, output: 212.46 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  3.53it/s, est. speed input: 1738.80 toks/s, output: 220.54 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.37it/s, est. speed input: 1738.80 toks/s, output: 220.54 toks/s]
[2025-01-06 15:54:40,983][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:54:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1189.50 toks/s, output: 38.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1189.50 toks/s, output: 38.87 toks/s]
[2025-01-06 15:54:47,224][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:54:54,490][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:54:54,490][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.09 GB
[2025-01-06 15:54:54,846][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.73 GB
[2025-01-06 15:55:06,938][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:55:06,939][root][INFO] - Before destroying HF.: GPU memory allocated: 15.73 GB
[2025-01-06 15:55:07,343][root][INFO] - After destroying HF.: GPU memory allocated: 0.13 GB
[2025-01-06 15:55:07,517][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 15:55:14,860][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:55:29,776][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:55:29,777][root][INFO] - Iteration 11 took 1m 58s. Generation: 63.96%, Training: 36.04%. Estimated time remaining: 3h 51m 8s. Estimated total time for complete run: 4h 12m 51s.
[2025-01-06 15:55:30,150][root][INFO] - Loading VLLM model.
WARNING 01-06 15:55:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:55:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:55:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:55:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 15:55:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:55:50 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:55:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:55:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:56:12 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 15:56:12,516][root][INFO] - Before destroying HF.: GPU memory allocated: 71.11 GB
[2025-01-06 15:56:12,760][root][INFO] - After destroying HF.: GPU memory allocated: 55.51 GB
[2025-01-06 15:56:12,761][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:56:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:43,  3.34s/it, est. speed input: 151.37 toks/s, output: 5.99 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:05,  2.18s/it, est. speed input: 214.33 toks/s, output: 13.37 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.48s/it, est. speed input: 283.00 toks/s, output: 21.86 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:28,  1.03s/it, est. speed input: 354.60 toks/s, output: 31.07 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:21,  1.28it/s, est. speed input: 418.71 toks/s, output: 40.30 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.60it/s, est. speed input: 476.89 toks/s, output: 49.58 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:06,  3.52it/s, est. speed input: 698.43 toks/s, output: 82.37 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:04,  4.82it/s, est. speed input: 835.27 toks/s, output: 103.75 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:06<00:03,  5.18it/s, est. speed input: 892.79 toks/s, output: 113.59 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  5.57it/s, est. speed input: 948.69 toks/s, output: 123.55 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  7.46it/s, est. speed input: 1074.77 toks/s, output: 145.86 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  7.44it/s, est. speed input: 1173.05 toks/s, output: 165.47 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01, 10.98it/s, est. speed input: 1360.73 toks/s, output: 201.82 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:07<00:00, 12.01it/s, est. speed input: 1471.23 toks/s, output: 224.99 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:07<00:00,  9.49it/s, est. speed input: 1540.51 toks/s, output: 243.79 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:07<00:00, 10.68it/s, est. speed input: 1641.43 toks/s, output: 268.65 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:08<00:00, 11.60it/s, est. speed input: 1737.80 toks/s, output: 293.85 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:08<00:00,  7.59it/s, est. speed input: 1758.59 toks/s, output: 310.51 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  4.26it/s, est. speed input: 1688.87 toks/s, output: 319.07 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.34it/s, est. speed input: 1688.87 toks/s, output: 319.07 toks/s]
[2025-01-06 15:56:22,741][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:56:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:19,  1.78s/it, est. speed input: 387.21 toks/s, output: 14.03 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:01<00:02,  2.71it/s, est. speed input: 1336.03 toks/s, output: 56.81 toks/s]Processed prompts: 100%|██████████| 12/12 [00:02<00:00,  6.36it/s, est. speed input: 2901.42 toks/s, output: 148.63 toks/s]Processed prompts: 100%|██████████| 12/12 [00:02<00:00,  4.60it/s, est. speed input: 2901.42 toks/s, output: 148.63 toks/s]
[2025-01-06 15:56:25,759][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:56:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:04<01:23,  4.39s/it, est. speed input: 159.38 toks/s, output: 14.14 toks/s]Processed prompts:  20%|██        | 4/20 [00:04<00:14,  1.10it/s, est. speed input: 598.07 toks/s, output: 55.19 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:04<00:05,  2.24it/s, est. speed input: 1015.42 toks/s, output: 97.54 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:05<00:03,  2.94it/s, est. speed input: 1237.09 toks/s, output: 123.30 toks/s]Processed prompts:  55%|█████▌    | 11/20 [00:06<00:03,  2.47it/s, est. speed input: 1248.25 toks/s, output: 134.91 toks/s]Processed prompts:  75%|███████▌  | 15/20 [00:06<00:01,  3.39it/s, est. speed input: 1525.82 toks/s, output: 194.27 toks/s]Processed prompts:  80%|████████  | 16/20 [00:07<00:01,  3.63it/s, est. speed input: 1590.74 toks/s, output: 211.64 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:07<00:00,  4.00it/s, est. speed input: 1659.48 toks/s, output: 230.01 toks/s]Processed prompts:  90%|█████████ | 18/20 [00:07<00:00,  2.76it/s, est. speed input: 1577.39 toks/s, output: 231.56 toks/s]Processed prompts: 100%|██████████| 20/20 [00:07<00:00,  2.51it/s, est. speed input: 1752.62 toks/s, output: 281.70 toks/s]
[2025-01-06 15:56:34,253][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:56:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:02<00:47,  2.79s/it, est. speed input: 341.56 toks/s, output: 10.38 toks/s]Processed prompts:  17%|█▋        | 3/18 [00:03<00:15,  1.03s/it, est. speed input: 752.15 toks/s, output: 30.02 toks/s]Processed prompts:  22%|██▏       | 4/18 [00:04<00:11,  1.21it/s, est. speed input: 796.42 toks/s, output: 42.27 toks/s]Processed prompts:  39%|███▉      | 7/18 [00:04<00:03,  2.83it/s, est. speed input: 1352.66 toks/s, output: 87.75 toks/s]Processed prompts:  50%|█████     | 9/18 [00:04<00:02,  3.88it/s, est. speed input: 1657.96 toks/s, output: 116.08 toks/s]Processed prompts:  72%|███████▏  | 13/18 [00:04<00:00,  6.17it/s, est. speed input: 2162.13 toks/s, output: 175.62 toks/s]Processed prompts:  83%|████████▎ | 15/18 [00:05<00:00,  5.11it/s, est. speed input: 2191.85 toks/s, output: 195.99 toks/s]Processed prompts:  94%|█████████▍| 17/18 [00:05<00:00,  6.23it/s, est. speed input: 2362.32 toks/s, output: 234.06 toks/s]Processed prompts: 100%|██████████| 18/18 [00:05<00:00,  3.09it/s, est. speed input: 2285.78 toks/s, output: 239.48 toks/s]
[2025-01-06 15:56:40,585][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:56:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:01<00:02,  1.01s/it, est. speed input: 659.22 toks/s, output: 28.75 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.97it/s, est. speed input: 2323.10 toks/s, output: 86.22 toks/s]
[2025-01-06 15:56:47,161][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 15:56:54,366][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:56:54,366][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.10 GB
[2025-01-06 15:56:54,716][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.74 GB
[2025-01-06 15:57:06,901][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:57:06,902][root][INFO] - Before destroying HF.: GPU memory allocated: 15.74 GB
[2025-01-06 15:57:07,337][root][INFO] - After destroying HF.: GPU memory allocated: 0.14 GB
[2025-01-06 15:57:07,534][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:57:14,867][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:57:29,892][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:57:29,893][root][INFO] - Iteration 12 took 2m 0s. Generation: 64.26%, Training: 35.74%. Estimated time remaining: 3h 52m 31s. Estimated total time for complete run: 4h 16m 14s.
[2025-01-06 15:57:30,215][root][INFO] - Loading VLLM model.
WARNING 01-06 15:57:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:57:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:57:30 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:57:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 15:57:35 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:57:49 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:57:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:57:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 15:58:11 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 15:58:11,478][root][INFO] - Before destroying HF.: GPU memory allocated: 71.12 GB
[2025-01-06 15:58:11,720][root][INFO] - After destroying HF.: GPU memory allocated: 55.52 GB
[2025-01-06 15:58:11,722][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:58:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:49,  3.53s/it, est. speed input: 143.05 toks/s, output: 6.52 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:04,  2.16s/it, est. speed input: 213.68 toks/s, output: 13.96 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:51,  1.78s/it, est. speed input: 249.75 toks/s, output: 21.76 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:33,  1.19s/it, est. speed input: 318.01 toks/s, output: 31.96 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:21,  1.25it/s, est. speed input: 390.64 toks/s, output: 42.70 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.89it/s, est. speed input: 610.52 toks/s, output: 75.26 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:05,  3.67it/s, est. speed input: 731.13 toks/s, output: 94.97 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:03,  5.20it/s, est. speed input: 914.74 toks/s, output: 126.93 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  6.24it/s, est. speed input: 1031.33 toks/s, output: 148.68 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  6.75it/s, est. speed input: 1132.13 toks/s, output: 169.59 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  4.75it/s, est. speed input: 1123.72 toks/s, output: 172.82 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  8.21it/s, est. speed input: 1351.47 toks/s, output: 225.65 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00,  8.59it/s, est. speed input: 1438.85 toks/s, output: 249.19 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00, 10.19it/s, est. speed input: 1581.64 toks/s, output: 287.56 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  6.65it/s, est. speed input: 1587.39 toks/s, output: 300.68 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  8.93it/s, est. speed input: 1728.04 toks/s, output: 348.92 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.42it/s, est. speed input: 1728.04 toks/s, output: 348.92 toks/s]
[2025-01-06 15:58:21,501][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 15:58:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.72s/it, est. speed input: 329.54 toks/s, output: 16.85 toks/s]Processed prompts:  80%|████████  | 8/10 [00:01<00:00,  5.65it/s, est. speed input: 2761.10 toks/s, output: 127.73 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  3.38it/s, est. speed input: 2191.02 toks/s, output: 131.13 toks/s]
[2025-01-06 15:58:24,855][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:58:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:04<01:24,  4.03s/it, est. speed input: 173.26 toks/s, output: 11.90 toks/s]Processed prompts:   9%|▉         | 2/22 [00:04<00:34,  1.74s/it, est. speed input: 335.05 toks/s, output: 23.73 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:22,  1.21s/it, est. speed input: 441.62 toks/s, output: 34.33 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:04<00:09,  1.78it/s, est. speed input: 716.59 toks/s, output: 60.69 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:05<00:05,  2.77it/s, est. speed input: 958.38 toks/s, output: 85.40 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:05<00:04,  3.33it/s, est. speed input: 1072.12 toks/s, output: 98.16 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:05<00:01,  7.11it/s, est. speed input: 1577.75 toks/s, output: 154.80 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:05<00:01,  5.55it/s, est. speed input: 1668.08 toks/s, output: 172.16 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:05<00:00,  6.82it/s, est. speed input: 1864.14 toks/s, output: 202.35 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  7.62it/s, est. speed input: 2032.99 toks/s, output: 231.54 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:07<00:00,  4.55it/s, est. speed input: 1984.02 toks/s, output: 241.26 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:07<00:00,  3.60it/s, est. speed input: 1931.82 toks/s, output: 247.94 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  3.62it/s, est. speed input: 1955.04 toks/s, output: 264.94 toks/s]Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  2.80it/s, est. speed input: 1955.04 toks/s, output: 264.94 toks/s]
[2025-01-06 15:58:33,274][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:58:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:33,  2.37s/it, est. speed input: 355.55 toks/s, output: 12.25 toks/s]Processed prompts:  27%|██▋       | 4/15 [00:03<00:07,  1.46it/s, est. speed input: 1083.83 toks/s, output: 43.99 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:03<00:05,  1.83it/s, est. speed input: 1266.37 toks/s, output: 59.64 toks/s]Processed prompts:  47%|████▋     | 7/15 [00:03<00:02,  2.91it/s, est. speed input: 1592.70 toks/s, output: 93.61 toks/s]Processed prompts:  53%|█████▎    | 8/15 [00:03<00:02,  3.08it/s, est. speed input: 1662.90 toks/s, output: 107.25 toks/s]Processed prompts:  60%|██████    | 9/15 [00:04<00:01,  3.55it/s, est. speed input: 1771.86 toks/s, output: 123.92 toks/s]Processed prompts:  67%|██████▋   | 10/15 [00:04<00:01,  2.58it/s, est. speed input: 1625.52 toks/s, output: 129.89 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:05<00:01,  2.76it/s, est. speed input: 1669.58 toks/s, output: 147.22 toks/s]Processed prompts:  80%|████████  | 12/15 [00:05<00:00,  3.12it/s, est. speed input: 1698.43 toks/s, output: 167.01 toks/s]Processed prompts:  93%|█████████▎| 14/15 [00:05<00:00,  4.11it/s, est. speed input: 1826.35 toks/s, output: 210.06 toks/s]Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  4.61it/s, est. speed input: 1906.81 toks/s, output: 233.22 toks/s]Processed prompts: 100%|██████████| 15/15 [00:05<00:00,  2.65it/s, est. speed input: 1906.81 toks/s, output: 233.22 toks/s]
[2025-01-06 15:58:39,436][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 15:58:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 937.15 toks/s, output: 39.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 937.15 toks/s, output: 39.73 toks/s]
[2025-01-06 15:58:45,752][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 15:58:53,030][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 15:58:53,031][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.11 GB
[2025-01-06 15:58:53,391][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.74 GB
[2025-01-06 15:59:05,377][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 15:59:05,378][root][INFO] - Before destroying HF.: GPU memory allocated: 15.74 GB
[2025-01-06 15:59:05,837][root][INFO] - After destroying HF.: GPU memory allocated: 0.15 GB
[2025-01-06 15:59:05,984][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 15:59:13,195][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 15:59:27,873][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 15:59:27,874][root][INFO] - Iteration 13 took 1m 57s. Generation: 64.17%, Training: 35.83%. Estimated time remaining: 3h 46m 0s. Estimated total time for complete run: 4h 11m 41s.
[2025-01-06 15:59:28,199][root][INFO] - Loading VLLM model.
WARNING 01-06 15:59:28 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 15:59:28 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 15:59:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 15:59:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 15:59:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 15:59:47 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 15:59:48 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 15:59:48 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:00:09 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:00:09,668][root][INFO] - Before destroying HF.: GPU memory allocated: 71.12 GB
[2025-01-06 16:00:09,919][root][INFO] - After destroying HF.: GPU memory allocated: 55.52 GB
[2025-01-06 16:00:09,920][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:00:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:45,  3.42s/it, est. speed input: 147.78 toks/s, output: 5.85 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:06,  2.22s/it, est. speed input: 210.71 toks/s, output: 13.14 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:43,  1.50s/it, est. speed input: 278.79 toks/s, output: 21.53 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:32,  1.18s/it, est. speed input: 330.07 toks/s, output: 29.90 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:23,  1.17it/s, est. speed input: 394.64 toks/s, output: 39.70 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.81it/s, est. speed input: 621.23 toks/s, output: 72.27 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:05,  3.96it/s, est. speed input: 759.52 toks/s, output: 93.10 toks/s]Processed prompts:  41%|████      | 13/32 [00:06<00:03,  6.20it/s, est. speed input: 967.59 toks/s, output: 125.57 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  5.70it/s, est. speed input: 1051.52 toks/s, output: 142.01 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:01,  8.19it/s, est. speed input: 1242.41 toks/s, output: 177.00 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  8.58it/s, est. speed input: 1343.14 toks/s, output: 197.61 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:07<00:00, 10.80it/s, est. speed input: 1513.71 toks/s, output: 233.15 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00,  8.70it/s, est. speed input: 1572.09 toks/s, output: 251.04 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00,  9.35it/s, est. speed input: 1662.42 toks/s, output: 275.18 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00,  7.90it/s, est. speed input: 1711.52 toks/s, output: 295.79 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:08<00:00,  7.14it/s, est. speed input: 1758.08 toks/s, output: 318.82 toks/s]Processed prompts: 100%|██████████| 32/32 [00:08<00:00,  3.57it/s, est. speed input: 1804.59 toks/s, output: 334.90 toks/s]
[2025-01-06 16:00:19,325][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:00:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.61s/it, est. speed input: 349.91 toks/s, output: 17.99 toks/s]Processed prompts: 100%|██████████| 9/9 [00:02<00:00,  4.02it/s, est. speed input: 2193.67 toks/s, output: 122.84 toks/s]Processed prompts: 100%|██████████| 9/9 [00:02<00:00,  3.40it/s, est. speed input: 2193.67 toks/s, output: 122.84 toks/s]
[2025-01-06 16:00:22,382][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:00:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:04<01:32,  4.19s/it, est. speed input: 166.74 toks/s, output: 11.69 toks/s]Processed prompts:   9%|▊         | 2/23 [00:04<00:42,  2.03s/it, est. speed input: 296.58 toks/s, output: 23.12 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:04<00:15,  1.25it/s, est. speed input: 576.84 toks/s, output: 47.86 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:05<00:08,  2.12it/s, est. speed input: 829.22 toks/s, output: 71.97 toks/s]Processed prompts:  30%|███       | 7/23 [00:05<00:06,  2.47it/s, est. speed input: 929.94 toks/s, output: 83.05 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:05<00:05,  2.80it/s, est. speed input: 1018.51 toks/s, output: 93.98 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:05<00:02,  4.41it/s, est. speed input: 1248.53 toks/s, output: 121.10 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:06<00:02,  4.36it/s, est. speed input: 1382.83 toks/s, output: 142.27 toks/s]Processed prompts:  61%|██████    | 14/23 [00:06<00:01,  5.24it/s, est. speed input: 1552.20 toks/s, output: 168.76 toks/s]Processed prompts:  65%|██████▌   | 15/23 [00:06<00:01,  4.65it/s, est. speed input: 1584.80 toks/s, output: 178.20 toks/s]Processed prompts:  70%|██████▉   | 16/23 [00:06<00:01,  4.41it/s, est. speed input: 1624.46 toks/s, output: 189.40 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:07<00:00,  6.62it/s, est. speed input: 1870.74 toks/s, output: 238.61 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:07<00:00,  5.69it/s, est. speed input: 1894.12 toks/s, output: 249.43 toks/s]Processed prompts:  91%|█████████▏| 21/23 [00:07<00:00,  5.29it/s, est. speed input: 1926.42 toks/s, output: 262.47 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  4.05it/s, est. speed input: 1936.07 toks/s, output: 284.08 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.77it/s, est. speed input: 1936.07 toks/s, output: 284.08 toks/s]
[2025-01-06 16:00:31,246][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:00:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.76s/it, est. speed input: 487.05 toks/s, output: 16.48 toks/s]Processed prompts:  20%|██        | 2/10 [00:02<00:08,  1.01s/it, est. speed input: 695.10 toks/s, output: 33.50 toks/s]Processed prompts:  40%|████      | 4/10 [00:02<00:02,  2.01it/s, est. speed input: 1124.99 toks/s, output: 69.31 toks/s]Processed prompts:  80%|████████  | 8/10 [00:03<00:00,  3.44it/s, est. speed input: 1728.14 toks/s, output: 140.65 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  3.21it/s, est. speed input: 1728.02 toks/s, output: 157.02 toks/s]Processed prompts: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s, est. speed input: 1366.34 toks/s, output: 154.32 toks/s]Processed prompts: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s, est. speed input: 1366.34 toks/s, output: 154.32 toks/s]
[2025-01-06 16:00:36,797][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:00:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s, est. speed input: 938.31 toks/s, output: 34.06 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  2.05it/s, est. speed input: 1414.51 toks/s, output: 66.35 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s, est. speed input: 1414.51 toks/s, output: 66.35 toks/s]
[2025-01-06 16:00:43,653][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:00:50,848][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:00:50,848][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.12 GB
[2025-01-06 16:00:51,197][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.75 GB
[2025-01-06 16:01:03,044][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:01:03,045][root][INFO] - Before destroying HF.: GPU memory allocated: 15.75 GB
[2025-01-06 16:01:03,377][root][INFO] - After destroying HF.: GPU memory allocated: 0.16 GB
[2025-01-06 16:01:03,602][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:01:10,988][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:01:25,579][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:01:25,580][root][INFO] - Iteration 14 took 1m 57s. Generation: 64.26%, Training: 35.74%. Estimated time remaining: 3h 43m 27s. Estimated total time for complete run: 4h 11m 6s.
[2025-01-06 16:01:25,893][root][INFO] - Loading VLLM model.
WARNING 01-06 16:01:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:01:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:01:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:01:26 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:01:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:01:45 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:01:45 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:01:45 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:02:07 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:02:07,122][root][INFO] - Before destroying HF.: GPU memory allocated: 71.13 GB
[2025-01-06 16:02:07,380][root][INFO] - After destroying HF.: GPU memory allocated: 55.53 GB
[2025-01-06 16:02:07,382][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:02:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:55,  3.71s/it, est. speed input: 135.98 toks/s, output: 5.39 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:10,  2.34s/it, est. speed input: 198.42 toks/s, output: 12.38 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:44,  1.54s/it, est. speed input: 267.05 toks/s, output: 20.45 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:19,  1.37it/s, est. speed input: 428.12 toks/s, output: 38.49 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.55it/s, est. speed input: 478.76 toks/s, output: 46.14 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:12,  2.05it/s, est. speed input: 549.38 toks/s, output: 55.79 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:09,  2.47it/s, est. speed input: 608.51 toks/s, output: 64.77 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:08,  2.79it/s, est. speed input: 659.72 toks/s, output: 73.45 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:06,  3.19it/s, est. speed input: 749.65 toks/s, output: 90.55 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  5.82it/s, est. speed input: 988.29 toks/s, output: 134.12 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  5.65it/s, est. speed input: 1027.06 toks/s, output: 143.13 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  5.81it/s, est. speed input: 1070.64 toks/s, output: 153.14 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  6.74it/s, est. speed input: 1165.92 toks/s, output: 174.74 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  8.68it/s, est. speed input: 1273.15 toks/s, output: 199.17 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00,  9.28it/s, est. speed input: 1364.18 toks/s, output: 222.21 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:01,  6.98it/s, est. speed input: 1410.05 toks/s, output: 240.24 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  9.30it/s, est. speed input: 1550.75 toks/s, output: 280.87 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  5.94it/s, est. speed input: 1550.65 toks/s, output: 295.29 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s, est. speed input: 1638.93 toks/s, output: 328.29 toks/s]
[2025-01-06 16:02:17,662][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:02:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:10,  1.52s/it, est. speed input: 458.74 toks/s, output: 19.11 toks/s]Processed prompts:  88%|████████▊ | 7/8 [00:01<00:00,  5.30it/s, est. speed input: 2577.51 toks/s, output: 124.43 toks/s]Processed prompts: 100%|██████████| 8/8 [00:02<00:00,  3.83it/s, est. speed input: 2469.55 toks/s, output: 132.24 toks/s]
[2025-01-06 16:02:20,142][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:02:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:17,  3.39s/it, est. speed input: 206.47 toks/s, output: 8.57 toks/s]Processed prompts:   8%|▊         | 2/24 [00:03<00:32,  1.48s/it, est. speed input: 395.88 toks/s, output: 17.27 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:03<00:19,  1.07it/s, est. speed input: 549.50 toks/s, output: 25.94 toks/s]Processed prompts:  17%|█▋        | 4/24 [00:04<00:13,  1.48it/s, est. speed input: 683.34 toks/s, output: 34.95 toks/s]Processed prompts:  21%|██        | 5/24 [00:04<00:12,  1.53it/s, est. speed input: 741.78 toks/s, output: 42.66 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:05<00:09,  1.82it/s, est. speed input: 829.21 toks/s, output: 52.79 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:05<00:06,  2.43it/s, est. speed input: 944.02 toks/s, output: 64.83 toks/s]Processed prompts:  33%|███▎      | 8/24 [00:05<00:05,  3.13it/s, est. speed input: 1053.98 toks/s, output: 76.90 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:05<00:02,  5.15it/s, est. speed input: 1289.93 toks/s, output: 102.79 toks/s]Processed prompts:  46%|████▌     | 11/24 [00:05<00:02,  5.82it/s, est. speed input: 1391.52 toks/s, output: 114.92 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:05<00:01,  7.83it/s, est. speed input: 1605.62 toks/s, output: 140.82 toks/s]Processed prompts:  62%|██████▎   | 15/24 [00:05<00:00,  9.76it/s, est. speed input: 1813.98 toks/s, output: 167.12 toks/s]Processed prompts:  71%|███████   | 17/24 [00:06<00:00,  7.93it/s, est. speed input: 1941.58 toks/s, output: 189.86 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:06<00:00,  6.06it/s, est. speed input: 2010.07 toks/s, output: 210.68 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:06<00:00,  7.71it/s, est. speed input: 2185.08 toks/s, output: 243.08 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:07<00:00,  6.12it/s, est. speed input: 2235.66 toks/s, output: 265.74 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  4.10it/s, est. speed input: 2152.98 toks/s, output: 268.74 toks/s]Processed prompts: 100%|██████████| 24/24 [00:07<00:00,  3.08it/s, est. speed input: 2152.98 toks/s, output: 268.74 toks/s]
[2025-01-06 16:02:28,519][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:02:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:16,  1.79s/it, est. speed input: 464.30 toks/s, output: 17.28 toks/s]Processed prompts:  30%|███       | 3/10 [00:01<00:03,  1.87it/s, est. speed input: 1169.43 toks/s, output: 50.47 toks/s]Processed prompts:  40%|████      | 4/10 [00:02<00:03,  1.62it/s, est. speed input: 1025.81 toks/s, output: 60.96 toks/s]Processed prompts:  50%|█████     | 5/10 [00:02<00:02,  2.21it/s, est. speed input: 1225.52 toks/s, output: 83.76 toks/s]Processed prompts:  70%|███████   | 7/10 [00:03<00:00,  3.15it/s, est. speed input: 1526.40 toks/s, output: 125.86 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:03<00:00,  2.96it/s, est. speed input: 1548.49 toks/s, output: 157.55 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  3.34it/s, est. speed input: 1655.09 toks/s, output: 184.84 toks/s]Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.42it/s, est. speed input: 1655.09 toks/s, output: 184.84 toks/s]
[2025-01-06 16:02:33,103][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:02:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s, est. speed input: 964.83 toks/s, output: 33.75 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s, est. speed input: 1884.98 toks/s, output: 67.49 toks/s]
[2025-01-06 16:02:39,773][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:02:46,970][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:02:46,970][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.13 GB
[2025-01-06 16:02:47,362][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.76 GB
[2025-01-06 16:02:59,207][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:02:59,209][root][INFO] - Before destroying HF.: GPU memory allocated: 15.76 GB
[2025-01-06 16:02:59,574][root][INFO] - After destroying HF.: GPU memory allocated: 0.17 GB
[2025-01-06 16:02:59,723][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:03:07,021][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:03:21,645][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:03:21,646][root][INFO] - Iteration 15 took 1m 56s. Generation: 63.79%, Training: 36.21%. Estimated time remaining: 3h 38m 1s. Estimated total time for complete run: 4h 7m 36s.
[2025-01-06 16:03:22,022][root][INFO] - Loading VLLM model.
WARNING 01-06 16:03:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:03:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:03:22 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:03:22 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:03:27 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:03:41 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:03:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:03:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:04:03 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:04:03,621][root][INFO] - Before destroying HF.: GPU memory allocated: 71.14 GB
[2025-01-06 16:04:03,859][root][INFO] - After destroying HF.: GPU memory allocated: 55.54 GB
[2025-01-06 16:04:03,861][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:04:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:06,  4.09s/it, est. speed input: 123.52 toks/s, output: 5.63 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:11,  2.39s/it, est. speed input: 191.08 toks/s, output: 12.49 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:51,  1.78s/it, est. speed input: 239.18 toks/s, output: 20.05 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:33,  1.19s/it, est. speed input: 305.13 toks/s, output: 29.15 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:15,  1.66it/s, est. speed input: 446.63 toks/s, output: 48.50 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:11,  2.15it/s, est. speed input: 513.08 toks/s, output: 58.06 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:06,  3.53it/s, est. speed input: 650.05 toks/s, output: 77.95 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:04,  4.96it/s, est. speed input: 778.56 toks/s, output: 97.41 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:04,  4.41it/s, est. speed input: 854.85 toks/s, output: 113.15 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  4.64it/s, est. speed input: 900.41 toks/s, output: 122.39 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  7.46it/s, est. speed input: 1076.95 toks/s, output: 155.80 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  8.72it/s, est. speed input: 1182.55 toks/s, output: 177.10 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  9.53it/s, est. speed input: 1280.98 toks/s, output: 198.34 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00,  9.50it/s, est. speed input: 1367.96 toks/s, output: 219.18 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00, 10.69it/s, est. speed input: 1463.91 toks/s, output: 242.11 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:08<00:00, 10.46it/s, est. speed input: 1585.16 toks/s, output: 274.88 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00, 10.35it/s, est. speed input: 1661.36 toks/s, output: 298.39 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  8.07it/s, est. speed input: 1699.80 toks/s, output: 318.19 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.37it/s, est. speed input: 1699.80 toks/s, output: 318.19 toks/s]
[2025-01-06 16:04:13,827][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:04:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:15,  1.71s/it, est. speed input: 393.93 toks/s, output: 16.95 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:01<00:00,  6.47it/s, est. speed input: 3095.68 toks/s, output: 144.72 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  3.39it/s, est. speed input: 2155.65 toks/s, output: 125.99 toks/s]
[2025-01-06 16:04:17,181][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:04:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:20,  3.85s/it, est. speed input: 181.77 toks/s, output: 11.44 toks/s]Processed prompts:   9%|▉         | 2/22 [00:04<00:41,  2.07s/it, est. speed input: 299.25 toks/s, output: 22.69 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:22,  1.19s/it, est. speed input: 436.42 toks/s, output: 35.59 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:04<00:13,  1.30it/s, est. speed input: 566.40 toks/s, output: 48.42 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:05<00:09,  1.76it/s, est. speed input: 679.23 toks/s, output: 60.63 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:02,  4.38it/s, est. speed input: 1164.78 toks/s, output: 113.87 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:05<00:01,  6.36it/s, est. speed input: 1584.93 toks/s, output: 165.70 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:05<00:01,  6.91it/s, est. speed input: 1761.80 toks/s, output: 191.89 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:01,  4.40it/s, est. speed input: 1687.87 toks/s, output: 191.21 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:06<00:00,  4.83it/s, est. speed input: 1810.77 toks/s, output: 220.77 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:07<00:00,  3.81it/s, est. speed input: 1809.39 toks/s, output: 239.83 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  4.17it/s, est. speed input: 1898.05 toks/s, output: 275.98 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.72it/s, est. speed input: 1898.05 toks/s, output: 275.98 toks/s]
[2025-01-06 16:04:25,827][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:04:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:24,  2.40s/it, est. speed input: 212.43 toks/s, output: 19.58 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:02<00:11,  1.23s/it, est. speed input: 429.82 toks/s, output: 38.40 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:03<00:06,  1.27it/s, est. speed input: 621.99 toks/s, output: 58.03 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:03<00:03,  1.93it/s, est. speed input: 821.03 toks/s, output: 79.36 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:03<00:02,  2.52it/s, est. speed input: 984.68 toks/s, output: 99.18 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:03<00:02,  2.49it/s, est. speed input: 1012.58 toks/s, output: 114.37 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:04<00:02,  1.92it/s, est. speed input: 994.81 toks/s, output: 124.05 toks/s] Processed prompts:  73%|███████▎  | 8/11 [00:05<00:01,  1.82it/s, est. speed input: 1013.40 toks/s, output: 140.52 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:05<00:01,  1.99it/s, est. speed input: 1066.84 toks/s, output: 162.92 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:05<00:00,  2.17it/s, est. speed input: 1134.39 toks/s, output: 186.61 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  1.86it/s, est. speed input: 1252.62 toks/s, output: 220.44 toks/s]
[2025-01-06 16:04:32,195][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:04:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:01<00:02,  1.03s/it, est. speed input: 811.22 toks/s, output: 28.21 toks/s]Processed prompts:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, est. speed input: 1463.20 toks/s, output: 56.12 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  2.11it/s, est. speed input: 1675.75 toks/s, output: 80.65 toks/s]Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, est. speed input: 1675.75 toks/s, output: 80.65 toks/s]
[2025-01-06 16:04:39,794][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:04:47,030][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:04:47,030][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.13 GB
[2025-01-06 16:04:47,443][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.77 GB
[2025-01-06 16:04:59,380][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:04:59,382][root][INFO] - Before destroying HF.: GPU memory allocated: 15.77 GB
[2025-01-06 16:04:59,777][root][INFO] - After destroying HF.: GPU memory allocated: 0.17 GB
[2025-01-06 16:04:59,957][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:05:07,237][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:05:22,114][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:05:22,116][root][INFO] - Iteration 16 took 2m 0s. Generation: 64.71%, Training: 35.29%. Estimated time remaining: 3h 45m 24s. Estimated total time for complete run: 4h 17m 0s.
[2025-01-06 16:05:22,433][root][INFO] - Loading VLLM model.
WARNING 01-06 16:05:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:05:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:05:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:05:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:05:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:05:42 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:05:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:05:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:06:04 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:06:04,737][root][INFO] - Before destroying HF.: GPU memory allocated: 71.15 GB
[2025-01-06 16:06:04,983][root][INFO] - After destroying HF.: GPU memory allocated: 55.55 GB
[2025-01-06 16:06:04,984][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:06:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:08,  4.14s/it, est. speed input: 121.90 toks/s, output: 5.55 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:12,  2.41s/it, est. speed input: 189.14 toks/s, output: 12.36 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:55,  1.92s/it, est. speed input: 226.78 toks/s, output: 19.76 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.23s/it, est. speed input: 294.79 toks/s, output: 29.33 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.21it/s, est. speed input: 362.57 toks/s, output: 39.06 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:08,  2.89it/s, est. speed input: 571.45 toks/s, output: 69.17 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:05,  4.06it/s, est. speed input: 699.66 toks/s, output: 88.53 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:04,  4.30it/s, est. speed input: 794.18 toks/s, output: 104.97 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  6.10it/s, est. speed input: 966.10 toks/s, output: 135.57 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  6.43it/s, est. speed input: 1058.40 toks/s, output: 154.35 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.89it/s, est. speed input: 1209.33 toks/s, output: 186.07 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  7.12it/s, est. speed input: 1275.85 toks/s, output: 204.76 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  7.31it/s, est. speed input: 1316.09 toks/s, output: 215.51 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:01,  7.27it/s, est. speed input: 1351.77 toks/s, output: 225.96 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  9.23it/s, est. speed input: 1447.20 toks/s, output: 251.41 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00, 10.04it/s, est. speed input: 1568.65 toks/s, output: 287.27 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  7.72it/s, est. speed input: 1606.82 toks/s, output: 307.71 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  7.07it/s, est. speed input: 1625.35 toks/s, output: 318.93 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s, est. speed input: 1625.35 toks/s, output: 318.93 toks/s]
[2025-01-06 16:06:15,341][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:06:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:12,  1.61s/it, est. speed input: 353.11 toks/s, output: 18.06 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  3.00it/s, est. speed input: 1712.65 toks/s, output: 109.33 toks/s]Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.66it/s, est. speed input: 1712.65 toks/s, output: 109.33 toks/s]
[2025-01-06 16:06:19,119][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:06:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:04<01:37,  4.41s/it, est. speed input: 158.45 toks/s, output: 12.01 toks/s]Processed prompts:   9%|▊         | 2/23 [00:05<00:51,  2.43s/it, est. speed input: 256.23 toks/s, output: 23.46 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:05<00:27,  1.38s/it, est. speed input: 374.82 toks/s, output: 36.82 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:05<00:11,  1.54it/s, est. speed input: 605.70 toks/s, output: 63.60 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:05<00:08,  2.00it/s, est. speed input: 711.34 toks/s, output: 76.66 toks/s]Processed prompts:  30%|███       | 7/23 [00:06<00:06,  2.42it/s, est. speed input: 802.13 toks/s, output: 88.85 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:06<00:03,  3.74it/s, est. speed input: 1000.98 toks/s, output: 115.83 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:06<00:02,  5.46it/s, est. speed input: 1203.17 toks/s, output: 144.43 toks/s]Processed prompts:  57%|█████▋    | 13/23 [00:06<00:01,  5.22it/s, est. speed input: 1335.30 toks/s, output: 167.08 toks/s]Processed prompts:  61%|██████    | 14/23 [00:06<00:01,  5.47it/s, est. speed input: 1407.33 toks/s, output: 180.19 toks/s]Processed prompts:  65%|██████▌   | 15/23 [00:07<00:01,  4.33it/s, est. speed input: 1426.51 toks/s, output: 188.16 toks/s]Processed prompts:  70%|██████▉   | 16/23 [00:07<00:01,  4.82it/s, est. speed input: 1494.30 toks/s, output: 202.82 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:07<00:00,  7.50it/s, est. speed input: 1731.25 toks/s, output: 252.11 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:07<00:00,  6.69it/s, est. speed input: 1772.43 toks/s, output: 264.60 toks/s]Processed prompts:  96%|█████████▌| 22/23 [00:08<00:00,  4.71it/s, est. speed input: 1801.41 toks/s, output: 284.89 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  4.76it/s, est. speed input: 1840.08 toks/s, output: 301.24 toks/s]Processed prompts: 100%|██████████| 23/23 [00:08<00:00,  2.63it/s, est. speed input: 1840.08 toks/s, output: 301.24 toks/s]
[2025-01-06 16:06:28,429][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:06:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:02<00:22,  2.07s/it, est. speed input: 411.61 toks/s, output: 14.04 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:02<00:05,  1.53it/s, est. speed input: 1047.83 toks/s, output: 41.03 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:03<00:05,  1.35it/s, est. speed input: 915.09 toks/s, output: 51.63 toks/s] Processed prompts:  42%|████▏     | 5/12 [00:03<00:04,  1.74it/s, est. speed input: 1081.79 toks/s, output: 71.02 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:03<00:01,  3.02it/s, est. speed input: 1418.98 toks/s, output: 113.88 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:03<00:01,  3.63it/s, est. speed input: 1560.80 toks/s, output: 134.72 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:04<00:00,  3.47it/s, est. speed input: 1607.87 toks/s, output: 150.12 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:04<00:00,  3.64it/s, est. speed input: 1680.21 toks/s, output: 169.21 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  3.40it/s, est. speed input: 1745.12 toks/s, output: 203.42 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  2.40it/s, est. speed input: 1745.12 toks/s, output: 203.42 toks/s]
[2025-01-06 16:06:33,894][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:06:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 1065.53 toks/s, output: 38.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 1065.53 toks/s, output: 38.97 toks/s]
[2025-01-06 16:06:40,584][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:06:47,804][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:06:47,805][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.14 GB
[2025-01-06 16:06:48,167][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.78 GB
[2025-01-06 16:07:00,117][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:07:00,118][root][INFO] - Before destroying HF.: GPU memory allocated: 15.78 GB
[2025-01-06 16:07:00,466][root][INFO] - After destroying HF.: GPU memory allocated: 0.18 GB
[2025-01-06 16:07:00,665][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:07:08,047][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:07:23,215][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:07:23,217][root][INFO] - Iteration 17 took 2m 1s. Generation: 64.69%, Training: 35.31%. Estimated time remaining: 3h 44m 44s. Estimated total time for complete run: 4h 18m 20s.
[2025-01-06 16:07:23,531][root][INFO] - Loading VLLM model.
WARNING 01-06 16:07:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:07:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:07:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:07:24 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:07:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:07:42 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:07:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:07:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:08:04 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:08:04,648][root][INFO] - Before destroying HF.: GPU memory allocated: 71.15 GB
[2025-01-06 16:08:04,902][root][INFO] - After destroying HF.: GPU memory allocated: 55.56 GB
[2025-01-06 16:08:04,903][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:08:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:54,  3.68s/it, est. speed input: 137.14 toks/s, output: 6.25 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:06,  2.22s/it, est. speed input: 207.00 toks/s, output: 13.53 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.47s/it, est. speed input: 277.37 toks/s, output: 21.79 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:33,  1.21s/it, est. speed input: 322.58 toks/s, output: 29.70 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:23,  1.17it/s, est. speed input: 389.38 toks/s, output: 39.63 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:08,  2.81it/s, est. speed input: 613.02 toks/s, output: 71.92 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:05,  4.10it/s, est. speed input: 754.78 toks/s, output: 93.26 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:04,  4.04it/s, est. speed input: 841.63 toks/s, output: 109.72 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  4.94it/s, est. speed input: 952.83 toks/s, output: 130.86 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.35it/s, est. speed input: 1071.07 toks/s, output: 153.50 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:02,  6.97it/s, est. speed input: 1169.83 toks/s, output: 174.77 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:07<00:00, 11.00it/s, est. speed input: 1406.50 toks/s, output: 223.44 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00, 10.38it/s, est. speed input: 1491.90 toks/s, output: 243.85 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00,  8.95it/s, est. speed input: 1557.09 toks/s, output: 262.91 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00, 11.08it/s, est. speed input: 1704.92 toks/s, output: 302.92 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  5.82it/s, est. speed input: 1664.22 toks/s, output: 311.26 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.38it/s, est. speed input: 1705.81 toks/s, output: 327.86 toks/s]
[2025-01-06 16:08:14,837][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:08:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:18,  1.85s/it, est. speed input: 390.33 toks/s, output: 15.68 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:02<00:00,  6.56it/s, est. speed input: 3184.93 toks/s, output: 147.49 toks/s]Processed prompts: 100%|██████████| 11/11 [00:02<00:00,  4.29it/s, est. speed input: 2764.79 toks/s, output: 144.67 toks/s]
[2025-01-06 16:08:17,797][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:08:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:04<01:26,  4.30s/it, est. speed input: 162.46 toks/s, output: 13.25 toks/s]Processed prompts:  10%|▉         | 2/21 [00:04<00:37,  1.98s/it, est. speed input: 300.17 toks/s, output: 26.20 toks/s]Processed prompts:  19%|█▉        | 4/21 [00:04<00:14,  1.19it/s, est. speed input: 560.17 toks/s, output: 52.09 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:05<00:08,  1.81it/s, est. speed input: 769.41 toks/s, output: 76.87 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:05<00:04,  2.84it/s, est. speed input: 1005.93 toks/s, output: 106.67 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:05<00:03,  3.29it/s, est. speed input: 1104.71 toks/s, output: 120.29 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:05<00:02,  4.37it/s, est. speed input: 1299.12 toks/s, output: 148.51 toks/s]Processed prompts:  62%|██████▏   | 13/21 [00:06<00:01,  5.62it/s, est. speed input: 1492.12 toks/s, output: 178.00 toks/s]Processed prompts:  71%|███████▏  | 15/21 [00:06<00:00,  7.16it/s, est. speed input: 1685.63 toks/s, output: 208.83 toks/s]Processed prompts:  81%|████████  | 17/21 [00:06<00:00,  7.14it/s, est. speed input: 1827.66 toks/s, output: 236.40 toks/s]Processed prompts:  90%|█████████ | 19/21 [00:07<00:00,  3.22it/s, est. speed input: 1690.50 toks/s, output: 237.01 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:07<00:00,  3.61it/s, est. speed input: 1750.77 toks/s, output: 257.73 toks/s]Processed prompts: 100%|██████████| 21/21 [00:08<00:00,  2.61it/s, est. speed input: 1822.98 toks/s, output: 280.42 toks/s]
[2025-01-06 16:08:26,371][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:08:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/14 [00:02<00:29,  2.25s/it, est. speed input: 359.74 toks/s, output: 12.88 toks/s]Processed prompts:  21%|██▏       | 3/14 [00:02<00:07,  1.44it/s, est. speed input: 964.83 toks/s, output: 37.70 toks/s]Processed prompts:  29%|██▊       | 4/14 [00:03<00:06,  1.47it/s, est. speed input: 1031.51 toks/s, output: 48.42 toks/s]Processed prompts:  36%|███▌      | 5/14 [00:03<00:04,  1.95it/s, est. speed input: 1184.40 toks/s, output: 65.13 toks/s]Processed prompts:  43%|████▎     | 6/14 [00:03<00:03,  2.60it/s, est. speed input: 1346.17 toks/s, output: 82.77 toks/s]Processed prompts:  57%|█████▋    | 8/14 [00:03<00:01,  3.19it/s, est. speed input: 1491.70 toks/s, output: 113.10 toks/s]Processed prompts:  71%|███████▏  | 10/14 [00:04<00:00,  4.22it/s, est. speed input: 1734.68 toks/s, output: 150.51 toks/s]Processed prompts:  93%|█████████▎| 13/14 [00:04<00:00,  5.24it/s, est. speed input: 2032.79 toks/s, output: 206.65 toks/s]Processed prompts: 100%|██████████| 14/14 [00:05<00:00,  3.39it/s, est. speed input: 1873.73 toks/s, output: 208.09 toks/s]Processed prompts: 100%|██████████| 14/14 [00:05<00:00,  2.60it/s, est. speed input: 1873.73 toks/s, output: 208.09 toks/s]
[2025-01-06 16:08:32,292][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:08:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 872.45 toks/s, output: 39.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 872.45 toks/s, output: 39.78 toks/s]
[2025-01-06 16:08:39,067][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 16:08:46,232][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:08:46,232][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.15 GB
[2025-01-06 16:08:46,615][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.78 GB
[2025-01-06 16:08:58,544][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:08:58,545][root][INFO] - Before destroying HF.: GPU memory allocated: 15.78 GB
[2025-01-06 16:08:58,881][root][INFO] - After destroying HF.: GPU memory allocated: 0.19 GB
[2025-01-06 16:08:59,073][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:09:06,444][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:09:21,267][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:09:21,268][root][INFO] - Iteration 18 took 1m 58s. Generation: 64.13%, Training: 35.87%. Estimated time remaining: 3h 36m 15s. Estimated total time for complete run: 4h 11m 50s.
[2025-01-06 16:09:21,565][root][INFO] - Loading VLLM model.
WARNING 01-06 16:09:21 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:09:21 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:09:22 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:09:22 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:09:27 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:09:40 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:09:41 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:09:41 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:10:03 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:10:03,150][root][INFO] - Before destroying HF.: GPU memory allocated: 71.16 GB
[2025-01-06 16:10:03,394][root][INFO] - After destroying HF.: GPU memory allocated: 55.56 GB
[2025-01-06 16:10:03,395][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:10:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:49,  3.53s/it, est. speed input: 143.03 toks/s, output: 6.51 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:13,  2.44s/it, est. speed input: 194.01 toks/s, output: 14.21 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:39,  1.38s/it, est. speed input: 284.62 toks/s, output: 23.86 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:27,  1.01it/s, est. speed input: 352.94 toks/s, output: 32.67 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:18,  1.48it/s, est. speed input: 432.74 toks/s, output: 42.67 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:14,  1.74it/s, est. speed input: 487.77 toks/s, output: 51.19 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:10,  2.38it/s, est. speed input: 559.53 toks/s, output: 61.57 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:06,  3.59it/s, est. speed input: 691.84 toks/s, output: 81.74 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:06<00:04,  4.69it/s, est. speed input: 816.11 toks/s, output: 102.10 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:06<00:02,  8.23it/s, est. speed input: 1085.50 toks/s, output: 147.17 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  7.04it/s, est. speed input: 1164.89 toks/s, output: 164.45 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:07<00:01,  7.57it/s, est. speed input: 1265.26 toks/s, output: 185.93 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  6.44it/s, est. speed input: 1286.14 toks/s, output: 193.43 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  5.69it/s, est. speed input: 1307.14 toks/s, output: 201.65 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  5.84it/s, est. speed input: 1343.73 toks/s, output: 212.63 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  5.18it/s, est. speed input: 1361.41 toks/s, output: 221.41 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00,  7.87it/s, est. speed input: 1506.04 toks/s, output: 263.01 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00, 10.44it/s, est. speed input: 1650.15 toks/s, output: 306.25 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  7.16it/s, est. speed input: 1669.88 toks/s, output: 323.95 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.41it/s, est. speed input: 1720.61 toks/s, output: 341.78 toks/s]
[2025-01-06 16:10:13,212][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:10:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:14,  1.65s/it, est. speed input: 371.33 toks/s, output: 16.36 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  5.22it/s, est. speed input: 2723.68 toks/s, output: 138.54 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  4.25it/s, est. speed input: 2723.68 toks/s, output: 138.54 toks/s]
[2025-01-06 16:10:15,972][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:10:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:06,  3.15s/it, est. speed input: 221.86 toks/s, output: 9.20 toks/s]Processed prompts:   9%|▉         | 2/22 [00:03<00:27,  1.38s/it, est. speed input: 425.08 toks/s, output: 18.55 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:24,  1.27s/it, est. speed input: 472.25 toks/s, output: 26.80 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:05<00:13,  1.30it/s, est. speed input: 672.79 toks/s, output: 49.09 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:05<00:09,  1.69it/s, est. speed input: 782.71 toks/s, output: 62.52 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:05<00:06,  2.20it/s, est. speed input: 894.07 toks/s, output: 76.38 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:03,  3.57it/s, est. speed input: 1120.15 toks/s, output: 104.87 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:06<00:03,  3.21it/s, est. speed input: 1160.39 toks/s, output: 114.21 toks/s]Processed prompts:  50%|█████     | 11/22 [00:06<00:04,  2.73it/s, est. speed input: 1174.05 toks/s, output: 122.61 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:06<00:03,  2.98it/s, est. speed input: 1233.64 toks/s, output: 136.19 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:01,  4.53it/s, est. speed input: 1409.25 toks/s, output: 169.78 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:07<00:01,  5.67it/s, est. speed input: 1563.43 toks/s, output: 201.86 toks/s]Processed prompts:  77%|███████▋  | 17/22 [00:07<00:01,  4.04it/s, est. speed input: 1550.84 toks/s, output: 208.94 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:08<00:00,  4.46it/s, est. speed input: 1653.20 toks/s, output: 240.86 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:08<00:00,  3.83it/s, est. speed input: 1658.09 toks/s, output: 252.51 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:08<00:00,  4.42it/s, est. speed input: 1718.43 toks/s, output: 272.65 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.58it/s, est. speed input: 1800.23 toks/s, output: 296.06 toks/s]
[2025-01-06 16:10:25,097][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:10:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:02<00:22,  2.02s/it, est. speed input: 472.56 toks/s, output: 14.36 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:02<00:07,  1.21it/s, est. speed input: 902.86 toks/s, output: 40.46 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:03<00:06,  1.24it/s, est. speed input: 853.39 toks/s, output: 55.21 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:04<00:04,  1.43it/s, est. speed input: 923.50 toks/s, output: 73.64 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:04<00:02,  2.29it/s, est. speed input: 1173.96 toks/s, output: 118.01 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:04<00:00,  4.14it/s, est. speed input: 1590.74 toks/s, output: 191.68 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  4.37it/s, est. speed input: 1704.78 toks/s, output: 230.81 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  2.41it/s, est. speed input: 1704.78 toks/s, output: 230.81 toks/s]
[2025-01-06 16:10:30,537][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:10:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 784.07 toks/s, output: 45.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 784.07 toks/s, output: 45.65 toks/s]
[2025-01-06 16:10:37,833][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:10:45,135][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:10:45,136][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.16 GB
[2025-01-06 16:10:45,493][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.79 GB
[2025-01-06 16:10:57,466][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:10:57,467][root][INFO] - Before destroying HF.: GPU memory allocated: 15.79 GB
[2025-01-06 16:10:57,841][root][INFO] - After destroying HF.: GPU memory allocated: 0.20 GB
[2025-01-06 16:10:57,991][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:11:05,287][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:11:20,132][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:11:20,133][root][INFO] - Iteration 19 took 1m 58s. Generation: 64.28%, Training: 35.72%. Estimated time remaining: 3h 36m 1s. Estimated total time for complete run: 4h 13m 34s.
[2025-01-06 16:11:20,516][root][INFO] - Loading VLLM model.
WARNING 01-06 16:11:20 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:11:20 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:11:21 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:11:21 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:11:26 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:11:40 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:11:40 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:11:40 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:12:02 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:12:02,417][root][INFO] - Before destroying HF.: GPU memory allocated: 71.17 GB
[2025-01-06 16:12:02,663][root][INFO] - After destroying HF.: GPU memory allocated: 55.57 GB
[2025-01-06 16:12:02,664][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:12:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:00,  3.87s/it, est. speed input: 130.34 toks/s, output: 5.94 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:17,  2.58s/it, est. speed input: 181.99 toks/s, output: 13.33 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:42,  1.46s/it, est. speed input: 267.35 toks/s, output: 22.41 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:30,  1.08s/it, est. speed input: 326.80 toks/s, output: 30.58 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.20it/s, est. speed input: 384.28 toks/s, output: 39.27 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:15,  1.65it/s, est. speed input: 450.06 toks/s, output: 49.02 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:13,  1.83it/s, est. speed input: 494.17 toks/s, output: 57.31 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:07,  3.27it/s, est. speed input: 626.58 toks/s, output: 78.86 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:02,  6.72it/s, est. speed input: 888.05 toks/s, output: 122.69 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  7.00it/s, est. speed input: 990.11 toks/s, output: 142.34 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:01,  8.09it/s, est. speed input: 1099.91 toks/s, output: 163.48 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  8.44it/s, est. speed input: 1196.61 toks/s, output: 183.83 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  8.89it/s, est. speed input: 1290.91 toks/s, output: 204.86 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00,  9.89it/s, est. speed input: 1388.72 toks/s, output: 227.53 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:08<00:00, 14.97it/s, est. speed input: 1609.80 toks/s, output: 278.75 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:08<00:00, 12.94it/s, est. speed input: 1685.65 toks/s, output: 300.07 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:08<00:00, 10.73it/s, est. speed input: 1746.34 toks/s, output: 320.60 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.30it/s, est. speed input: 1664.43 toks/s, output: 315.17 toks/s]
[2025-01-06 16:12:12,802][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:12:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:01<00:14,  1.63s/it, est. speed input: 376.57 toks/s, output: 16.59 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:01<00:00,  6.47it/s, est. speed input: 3112.24 toks/s, output: 145.73 toks/s]Processed prompts: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s, est. speed input: 2690.42 toks/s, output: 142.81 toks/s]
[2025-01-06 16:12:15,544][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:12:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:05<01:45,  5.02s/it, est. speed input: 139.19 toks/s, output: 13.74 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:05<00:27,  1.44s/it, est. speed input: 389.68 toks/s, output: 40.32 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:05<00:17,  1.02it/s, est. speed input: 507.28 toks/s, output: 53.89 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:05<00:09,  1.68it/s, est. speed input: 708.40 toks/s, output: 79.05 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:06<00:07,  2.13it/s, est. speed input: 810.80 toks/s, output: 92.96 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:06<00:07,  1.96it/s, est. speed input: 840.23 toks/s, output: 100.82 toks/s]Processed prompts:  41%|████      | 9/22 [00:06<00:05,  2.39it/s, est. speed input: 920.68 toks/s, output: 115.03 toks/s]Processed prompts:  50%|█████     | 11/22 [00:06<00:02,  3.93it/s, est. speed input: 1108.98 toks/s, output: 147.11 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:07<00:03,  3.27it/s, est. speed input: 1133.19 toks/s, output: 155.77 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:07<00:02,  3.66it/s, est. speed input: 1198.80 toks/s, output: 170.45 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:07<00:01,  4.52it/s, est. speed input: 1330.59 toks/s, output: 201.02 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:07<00:01,  5.15it/s, est. speed input: 1400.76 toks/s, output: 217.68 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:08<00:00,  6.75it/s, est. speed input: 1601.92 toks/s, output: 267.41 toks/s]Processed prompts:  95%|█████████▌| 21/22 [00:08<00:00,  5.53it/s, est. speed input: 1670.01 toks/s, output: 293.64 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, est. speed input: 1733.01 toks/s, output: 313.40 toks/s]
[2025-01-06 16:12:24,983][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:12:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:25,  2.13s/it, est. speed input: 447.32 toks/s, output: 13.61 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:03<00:08,  1.13it/s, est. speed input: 888.78 toks/s, output: 38.56 toks/s]Processed prompts:  31%|███       | 4/13 [00:03<00:06,  1.38it/s, est. speed input: 930.44 toks/s, output: 54.83 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:03<00:04,  1.82it/s, est. speed input: 1071.56 toks/s, output: 73.52 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:03<00:02,  2.35it/s, est. speed input: 1209.82 toks/s, output: 92.74 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:03<00:01,  4.06it/s, est. speed input: 1487.29 toks/s, output: 135.58 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:00,  4.14it/s, est. speed input: 1574.72 toks/s, output: 152.08 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:04<00:00,  4.48it/s, est. speed input: 1673.77 toks/s, output: 170.81 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:04<00:00,  4.85it/s, est. speed input: 1840.91 toks/s, output: 207.40 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.74it/s, est. speed input: 1674.74 toks/s, output: 206.89 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.34it/s, est. speed input: 1674.74 toks/s, output: 206.89 toks/s]
[2025-01-06 16:12:31,035][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:12:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it, est. speed input: 347.86 toks/s, output: 53.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it, est. speed input: 347.86 toks/s, output: 53.76 toks/s]
[2025-01-06 16:12:39,698][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:12:46,902][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:12:46,902][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.17 GB
[2025-01-06 16:12:47,255][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.80 GB
[2025-01-06 16:12:59,151][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:12:59,153][root][INFO] - Before destroying HF.: GPU memory allocated: 15.80 GB
[2025-01-06 16:12:59,548][root][INFO] - After destroying HF.: GPU memory allocated: 0.21 GB
[2025-01-06 16:12:59,712][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:13:07,052][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:13:22,167][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:13:22,168][root][INFO] - Iteration 20 took 2m 2s. Generation: 65.08%, Training: 34.92%. Estimated time remaining: 3h 40m 44s. Estimated total time for complete run: 4h 20m 20s.
[2025-01-06 16:13:22,506][root][INFO] - Loading VLLM model.
WARNING 01-06 16:13:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:13:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:13:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:13:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:13:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:13:41 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:13:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:13:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:14:03 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:14:03,687][root][INFO] - Before destroying HF.: GPU memory allocated: 71.18 GB
[2025-01-06 16:14:03,929][root][INFO] - After destroying HF.: GPU memory allocated: 55.58 GB
[2025-01-06 16:14:03,930][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:14:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:06,  4.09s/it, est. speed input: 123.57 toks/s, output: 5.63 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:04,  2.14s/it, est. speed input: 207.61 toks/s, output: 12.13 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:56,  1.96s/it, est. speed input: 229.12 toks/s, output: 18.90 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:35,  1.25s/it, est. speed input: 297.74 toks/s, output: 28.60 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.19it/s, est. speed input: 366.15 toks/s, output: 38.43 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:10,  2.30it/s, est. speed input: 545.30 toks/s, output: 66.00 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:05,  3.52it/s, est. speed input: 717.51 toks/s, output: 95.71 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:05,  3.88it/s, est. speed input: 769.09 toks/s, output: 105.59 toks/s]Processed prompts:  41%|████      | 13/32 [00:08<00:04,  4.15it/s, est. speed input: 814.91 toks/s, output: 115.07 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:08<00:04,  4.43it/s, est. speed input: 859.22 toks/s, output: 124.69 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01, 10.74it/s, est. speed input: 1205.37 toks/s, output: 194.89 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  9.95it/s, est. speed input: 1287.83 toks/s, output: 214.10 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00, 10.91it/s, est. speed input: 1468.39 toks/s, output: 258.11 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00, 11.89it/s, est. speed input: 1601.93 toks/s, output: 294.46 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  4.78it/s, est. speed input: 1501.52 toks/s, output: 292.53 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.07it/s, est. speed input: 1549.92 toks/s, output: 311.71 toks/s]
[2025-01-06 16:14:14,816][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:14:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:28,  2.05s/it, est. speed input: 282.34 toks/s, output: 11.68 toks/s]Processed prompts:  13%|█▎        | 2/15 [00:02<00:12,  1.08it/s, est. speed input: 543.23 toks/s, output: 23.68 toks/s]Processed prompts: 100%|██████████| 15/15 [00:03<00:00,  4.89it/s, est. speed input: 2433.98 toks/s, output: 134.44 toks/s]Processed prompts: 100%|██████████| 15/15 [00:03<00:00,  3.76it/s, est. speed input: 2433.98 toks/s, output: 134.44 toks/s]
[2025-01-06 16:14:19,213][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:14:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:03<01:02,  3.90s/it, est. speed input: 179.10 toks/s, output: 15.89 toks/s]Processed prompts:  18%|█▊        | 3/17 [00:04<00:14,  1.05s/it, est. speed input: 522.38 toks/s, output: 47.33 toks/s]Processed prompts:  29%|██▉       | 5/17 [00:04<00:07,  1.60it/s, est. speed input: 783.74 toks/s, output: 75.35 toks/s]Processed prompts:  35%|███▌      | 6/17 [00:04<00:05,  1.99it/s, est. speed input: 907.03 toks/s, output: 90.62 toks/s]Processed prompts:  41%|████      | 7/17 [00:04<00:04,  2.31it/s, est. speed input: 1003.95 toks/s, output: 104.64 toks/s]Processed prompts:  53%|█████▎    | 9/17 [00:05<00:02,  2.77it/s, est. speed input: 1161.79 toks/s, output: 131.86 toks/s]Processed prompts:  59%|█████▉    | 10/17 [00:05<00:02,  3.33it/s, est. speed input: 1265.73 toks/s, output: 149.93 toks/s]Processed prompts:  65%|██████▍   | 11/17 [00:05<00:01,  4.00it/s, est. speed input: 1366.50 toks/s, output: 168.12 toks/s]Processed prompts:  82%|████████▏ | 14/17 [00:06<00:00,  5.23it/s, est. speed input: 1620.71 toks/s, output: 218.61 toks/s]Processed prompts:  88%|████████▊ | 15/17 [00:07<00:00,  2.49it/s, est. speed input: 1438.36 toks/s, output: 208.52 toks/s]Processed prompts: 100%|██████████| 17/17 [00:07<00:00,  2.33it/s, est. speed input: 1630.10 toks/s, output: 263.38 toks/s]
[2025-01-06 16:14:26,985][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:14:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:03<01:01,  3.10s/it, est. speed input: 308.23 toks/s, output: 9.37 toks/s]Processed prompts:  24%|██▍       | 5/21 [00:03<00:10,  1.56it/s, est. speed input: 1091.72 toks/s, output: 42.06 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:04<00:08,  1.80it/s, est. speed input: 1229.42 toks/s, output: 52.93 toks/s]Processed prompts:  33%|███▎      | 7/21 [00:04<00:07,  1.94it/s, est. speed input: 1233.08 toks/s, output: 63.06 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:04<00:03,  3.10it/s, est. speed input: 1462.12 toks/s, output: 91.34 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:05<00:02,  3.67it/s, est. speed input: 1628.32 toks/s, output: 115.00 toks/s]Processed prompts:  57%|█████▋    | 12/21 [00:05<00:02,  3.96it/s, est. speed input: 1705.98 toks/s, output: 128.00 toks/s]Processed prompts:  71%|███████▏  | 15/21 [00:05<00:01,  4.62it/s, est. speed input: 1950.79 toks/s, output: 166.83 toks/s]Processed prompts:  76%|███████▌  | 16/21 [00:05<00:00,  5.03it/s, est. speed input: 2028.85 toks/s, output: 182.66 toks/s]Processed prompts:  81%|████████  | 17/21 [00:06<00:00,  5.52it/s, est. speed input: 2106.62 toks/s, output: 198.95 toks/s]Processed prompts:  86%|████████▌ | 18/21 [00:06<00:00,  4.79it/s, est. speed input: 2116.41 toks/s, output: 210.44 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:06<00:00,  4.10it/s, est. speed input: 2109.62 toks/s, output: 235.84 toks/s]Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  3.14it/s, est. speed input: 2038.56 toks/s, output: 244.11 toks/s]Processed prompts: 100%|██████████| 21/21 [00:07<00:00,  2.79it/s, est. speed input: 2038.56 toks/s, output: 244.11 toks/s]
[2025-01-06 16:14:35,080][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:14:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1141.19 toks/s, output: 39.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1141.19 toks/s, output: 39.16 toks/s]
[2025-01-06 16:14:42,034][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:14:49,338][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:14:49,338][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.17 GB
[2025-01-06 16:14:49,700][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.81 GB
[2025-01-06 16:15:02,095][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:15:02,097][root][INFO] - Before destroying HF.: GPU memory allocated: 15.81 GB
[2025-01-06 16:15:02,456][root][INFO] - After destroying HF.: GPU memory allocated: 0.21 GB
[2025-01-06 16:15:02,612][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:15:10,003][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:15:24,888][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:15:24,888][root][INFO] - Iteration 21 took 2m 2s. Generation: 64.96%, Training: 35.04%. Estimated time remaining: 3h 40m 9s. Estimated total time for complete run: 4h 21m 48s.
[2025-01-06 16:15:25,177][root][INFO] - Loading VLLM model.
WARNING 01-06 16:15:25 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:15:25 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:15:25 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:15:26 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:15:30 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:15:44 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:15:45 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:15:45 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:16:06 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:16:06,445][root][INFO] - Before destroying HF.: GPU memory allocated: 71.19 GB
[2025-01-06 16:16:06,688][root][INFO] - After destroying HF.: GPU memory allocated: 55.59 GB
[2025-01-06 16:16:06,689][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:16:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:47,  3.45s/it, est. speed input: 146.20 toks/s, output: 6.66 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:12,  2.41s/it, est. speed input: 196.82 toks/s, output: 14.42 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:39,  1.36s/it, est. speed input: 288.64 toks/s, output: 24.20 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:31,  1.12s/it, est. speed input: 337.10 toks/s, output: 32.21 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:25,  1.05it/s, est. speed input: 379.15 toks/s, output: 40.69 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:17,  1.50it/s, est. speed input: 447.71 toks/s, output: 51.86 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:09,  2.65it/s, est. speed input: 583.52 toks/s, output: 74.38 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:04,  4.62it/s, est. speed input: 780.65 toks/s, output: 108.21 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:03,  5.71it/s, est. speed input: 899.46 toks/s, output: 130.43 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  7.96it/s, est. speed input: 1082.44 toks/s, output: 164.91 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:01,  7.56it/s, est. speed input: 1170.99 toks/s, output: 184.73 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.21it/s, est. speed input: 1251.36 toks/s, output: 204.80 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00, 10.14it/s, est. speed input: 1465.34 toks/s, output: 255.10 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:01,  4.84it/s, est. speed input: 1403.48 toks/s, output: 255.79 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:01,  4.57it/s, est. speed input: 1413.81 toks/s, output: 266.07 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00,  4.11it/s, est. speed input: 1413.36 toks/s, output: 275.38 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  5.02it/s, est. speed input: 1481.49 toks/s, output: 307.64 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.13it/s, est. speed input: 1580.23 toks/s, output: 346.75 toks/s]
[2025-01-06 16:16:17,335][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:16:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:21,  1.94s/it, est. speed input: 364.20 toks/s, output: 14.46 toks/s]Processed prompts:  92%|█████████▏| 11/12 [00:02<00:00,  7.32it/s, est. speed input: 3652.57 toks/s, output: 157.68 toks/s]Processed prompts: 100%|██████████| 12/12 [00:03<00:00,  4.00it/s, est. speed input: 2696.02 toks/s, output: 137.53 toks/s]
[2025-01-06 16:16:20,747][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:16:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:04<01:17,  4.09s/it, est. speed input: 170.93 toks/s, output: 13.69 toks/s]Processed prompts:  10%|█         | 2/20 [00:04<00:31,  1.76s/it, est. speed input: 331.31 toks/s, output: 27.25 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:04<00:17,  1.03s/it, est. speed input: 478.04 toks/s, output: 40.58 toks/s]Processed prompts:  20%|██        | 4/20 [00:04<00:11,  1.42it/s, est. speed input: 609.12 toks/s, output: 53.59 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:04<00:08,  1.87it/s, est. speed input: 725.27 toks/s, output: 66.40 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:05<00:04,  2.69it/s, est. speed input: 932.68 toks/s, output: 91.69 toks/s]Processed prompts:  40%|████      | 8/20 [00:05<00:04,  2.83it/s, est. speed input: 1007.38 toks/s, output: 103.76 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:05<00:03,  3.04it/s, est. speed input: 1082.05 toks/s, output: 116.79 toks/s]Processed prompts:  50%|█████     | 10/20 [00:06<00:03,  3.09it/s, est. speed input: 1140.99 toks/s, output: 129.28 toks/s]Processed prompts:  60%|██████    | 12/20 [00:06<00:01,  4.60it/s, est. speed input: 1331.91 toks/s, output: 162.76 toks/s]Processed prompts:  65%|██████▌   | 13/20 [00:06<00:01,  5.06it/s, est. speed input: 1412.77 toks/s, output: 178.64 toks/s]Processed prompts:  70%|███████   | 14/20 [00:06<00:01,  5.35it/s, est. speed input: 1485.35 toks/s, output: 194.13 toks/s]Processed prompts:  80%|████████  | 16/20 [00:06<00:00,  5.14it/s, est. speed input: 1598.12 toks/s, output: 222.63 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:07<00:00,  5.01it/s, est. speed input: 1647.11 toks/s, output: 237.86 toks/s]Processed prompts:  90%|█████████ | 18/20 [00:07<00:00,  4.67it/s, est. speed input: 1683.60 toks/s, output: 252.50 toks/s]Processed prompts:  95%|█████████▌| 19/20 [00:08<00:00,  3.36it/s, est. speed input: 1658.92 toks/s, output: 260.68 toks/s]Processed prompts: 100%|██████████| 20/20 [00:08<00:00,  2.50it/s, est. speed input: 1746.20 toks/s, output: 285.66 toks/s]
[2025-01-06 16:16:29,301][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:16:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:27,  2.30s/it, est. speed input: 389.08 toks/s, output: 15.22 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:03<00:16,  1.49s/it, est. speed input: 495.83 toks/s, output: 30.48 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:03<00:10,  1.06s/it, est. speed input: 607.33 toks/s, output: 47.41 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:03<00:04,  1.93it/s, est. speed input: 880.67 toks/s, output: 88.02 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:04<00:02,  2.52it/s, est. speed input: 1053.92 toks/s, output: 121.92 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:01,  3.66it/s, est. speed input: 1319.74 toks/s, output: 165.48 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:05<00:00,  3.20it/s, est. speed input: 1339.42 toks/s, output: 177.26 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:05<00:00,  3.85it/s, est. speed input: 1510.06 toks/s, output: 219.17 toks/s]Processed prompts: 100%|██████████| 13/13 [00:06<00:00,  2.71it/s, est. speed input: 1436.10 toks/s, output: 224.32 toks/s]Processed prompts: 100%|██████████| 13/13 [00:06<00:00,  2.10it/s, est. speed input: 1436.10 toks/s, output: 224.32 toks/s]
[2025-01-06 16:16:35,987][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:16:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1198.86 toks/s, output: 39.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1198.86 toks/s, output: 39.11 toks/s]
[2025-01-06 16:16:43,018][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:16:50,203][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:16:50,204][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.18 GB
[2025-01-06 16:16:50,602][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.82 GB
[2025-01-06 16:17:02,896][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:17:02,898][root][INFO] - Before destroying HF.: GPU memory allocated: 15.82 GB
[2025-01-06 16:17:03,244][root][INFO] - After destroying HF.: GPU memory allocated: 0.22 GB
[2025-01-06 16:17:03,396][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:17:10,840][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:17:25,591][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:17:25,592][root][INFO] - Iteration 22 took 2m 0s. Generation: 64.60%, Training: 35.40%. Estimated time remaining: 3h 33m 50s. Estimated total time for complete run: 4h 17m 30s.
[2025-01-06 16:17:25,927][root][INFO] - Loading VLLM model.
WARNING 01-06 16:17:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:17:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:17:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:17:26 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:17:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:17:45 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:17:45 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:17:45 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:18:07 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:18:07,590][root][INFO] - Before destroying HF.: GPU memory allocated: 71.19 GB
[2025-01-06 16:18:07,865][root][INFO] - After destroying HF.: GPU memory allocated: 55.60 GB
[2025-01-06 16:18:07,866][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:18:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:59,  3.86s/it, est. speed input: 130.78 toks/s, output: 5.96 toks/s]Processed prompts:   6%|▋         | 2/32 [00:03<00:49,  1.66s/it, est. speed input: 253.66 toks/s, output: 12.05 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:41,  1.43s/it, est. speed input: 294.37 toks/s, output: 18.07 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:28,  1.00s/it, est. speed input: 367.95 toks/s, output: 26.23 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.19it/s, est. speed input: 417.63 toks/s, output: 33.91 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:20,  1.29it/s, est. speed input: 452.77 toks/s, output: 41.54 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:11,  2.13it/s, est. speed input: 577.08 toks/s, output: 61.57 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:05,  3.95it/s, est. speed input: 777.39 toks/s, output: 94.18 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:05,  3.41it/s, est. speed input: 797.10 toks/s, output: 100.62 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:04,  3.88it/s, est. speed input: 884.60 toks/s, output: 120.12 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:03,  4.58it/s, est. speed input: 976.42 toks/s, output: 141.27 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  6.83it/s, est. speed input: 1139.76 toks/s, output: 177.94 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  7.78it/s, est. speed input: 1235.21 toks/s, output: 201.62 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  8.89it/s, est. speed input: 1330.15 toks/s, output: 225.60 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00, 10.38it/s, est. speed input: 1468.13 toks/s, output: 262.54 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  9.33it/s, est. speed input: 1534.07 toks/s, output: 284.57 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  6.15it/s, est. speed input: 1540.77 toks/s, output: 299.00 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  4.59it/s, est. speed input: 1515.62 toks/s, output: 303.99 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.10it/s, est. speed input: 1564.48 toks/s, output: 323.35 toks/s]
[2025-01-06 16:18:18,613][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:18:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:01<00:23,  1.95s/it, est. speed input: 381.61 toks/s, output: 13.83 toks/s]Processed prompts: 100%|██████████| 13/13 [00:02<00:00,  6.44it/s, est. speed input: 4162.49 toks/s, output: 185.80 toks/s]
[2025-01-06 16:18:21,026][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:18:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:03<01:08,  3.78s/it, est. speed input: 184.98 toks/s, output: 13.76 toks/s]Processed prompts:  11%|█         | 2/19 [00:04<00:30,  1.78s/it, est. speed input: 336.55 toks/s, output: 27.20 toks/s]Processed prompts:  21%|██        | 4/19 [00:04<00:10,  1.38it/s, est. speed input: 643.65 toks/s, output: 55.25 toks/s]Processed prompts:  26%|██▋       | 5/19 [00:04<00:07,  1.80it/s, est. speed input: 772.16 toks/s, output: 68.71 toks/s]Processed prompts:  37%|███▋      | 7/19 [00:04<00:03,  3.13it/s, est. speed input: 1056.41 toks/s, output: 98.88 toks/s]Processed prompts:  47%|████▋     | 9/19 [00:05<00:03,  2.77it/s, est. speed input: 1148.37 toks/s, output: 119.20 toks/s]Processed prompts:  53%|█████▎    | 10/19 [00:05<00:03,  2.94it/s, est. speed input: 1216.77 toks/s, output: 132.64 toks/s]Processed prompts:  63%|██████▎   | 12/19 [00:06<00:02,  2.74it/s, est. speed input: 1280.00 toks/s, output: 154.58 toks/s]Processed prompts:  68%|██████▊   | 13/19 [00:06<00:01,  3.02it/s, est. speed input: 1344.00 toks/s, output: 171.57 toks/s]Processed prompts:  74%|███████▎  | 14/19 [00:06<00:01,  3.29it/s, est. speed input: 1402.23 toks/s, output: 188.57 toks/s]Processed prompts:  79%|███████▉  | 15/19 [00:07<00:01,  3.91it/s, est. speed input: 1478.32 toks/s, output: 208.25 toks/s]Processed prompts:  84%|████████▍ | 16/19 [00:07<00:00,  4.61it/s, est. speed input: 1553.16 toks/s, output: 228.17 toks/s]Processed prompts:  89%|████████▉ | 17/19 [00:07<00:00,  4.85it/s, est. speed input: 1610.18 toks/s, output: 246.34 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:07<00:00,  4.87it/s, est. speed input: 1659.46 toks/s, output: 264.31 toks/s]Processed prompts: 100%|██████████| 19/19 [00:07<00:00,  5.41it/s, est. speed input: 1721.21 toks/s, output: 284.86 toks/s]Processed prompts: 100%|██████████| 19/19 [00:07<00:00,  2.46it/s, est. speed input: 1721.21 toks/s, output: 284.86 toks/s]
[2025-01-06 16:18:29,303][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:18:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:37,  2.53s/it, est. speed input: 326.99 toks/s, output: 11.47 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:03<00:13,  1.05s/it, est. speed input: 648.69 toks/s, output: 32.56 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:03<00:09,  1.21it/s, est. speed input: 757.78 toks/s, output: 47.02 toks/s]Processed prompts:  44%|████▍     | 7/16 [00:04<00:03,  2.58it/s, est. speed input: 1247.41 toks/s, output: 96.71 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:04<00:02,  3.17it/s, est. speed input: 1446.61 toks/s, output: 126.70 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:05<00:01,  3.08it/s, est. speed input: 1523.41 toks/s, output: 151.53 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:05<00:01,  3.48it/s, est. speed input: 1613.28 toks/s, output: 171.17 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:05<00:00,  4.33it/s, est. speed input: 1784.48 toks/s, output: 210.40 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:06<00:00,  3.98it/s, est. speed input: 1802.36 toks/s, output: 225.15 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  2.87it/s, est. speed input: 1723.30 toks/s, output: 231.99 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  2.37it/s, est. speed input: 1723.30 toks/s, output: 231.99 toks/s]
[2025-01-06 16:18:36,540][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:18:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.13it/s, est. speed input: 1073.63 toks/s, output: 32.64 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.25it/s, est. speed input: 2026.33 toks/s, output: 65.26 toks/s]
[2025-01-06 16:18:43,869][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:18:51,122][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:18:51,123][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.19 GB
[2025-01-06 16:18:51,481][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.82 GB
[2025-01-06 16:19:03,640][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:19:03,642][root][INFO] - Before destroying HF.: GPU memory allocated: 15.82 GB
[2025-01-06 16:19:04,013][root][INFO] - After destroying HF.: GPU memory allocated: 0.23 GB
[2025-01-06 16:19:04,194][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:19:11,560][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:19:26,934][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:19:26,935][root][INFO] - Iteration 23 took 2m 1s. Generation: 64.38%, Training: 35.62%. Estimated time remaining: 3h 33m 11s. Estimated total time for complete run: 4h 18m 51s.
[2025-01-06 16:19:27,250][root][INFO] - Loading VLLM model.
WARNING 01-06 16:19:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:19:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:19:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:19:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:19:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:19:46 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:19:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:19:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:20:08 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:20:08,930][root][INFO] - Before destroying HF.: GPU memory allocated: 71.20 GB
[2025-01-06 16:20:09,210][root][INFO] - After destroying HF.: GPU memory allocated: 55.60 GB
[2025-01-06 16:20:09,212][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:20:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:46,  3.45s/it, est. speed input: 146.41 toks/s, output: 6.67 toks/s]Processed prompts:   6%|▋         | 2/32 [00:03<00:44,  1.49s/it, est. speed input: 282.95 toks/s, output: 13.45 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:52,  1.79s/it, est. speed input: 264.67 toks/s, output: 19.22 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:32,  1.15s/it, est. speed input: 342.61 toks/s, output: 29.68 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:23,  1.14it/s, est. speed input: 401.74 toks/s, output: 39.30 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:11,  2.11it/s, est. speed input: 548.49 toks/s, output: 61.29 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:09,  2.58it/s, est. speed input: 612.23 toks/s, output: 71.68 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:06<00:03,  5.48it/s, est. speed input: 892.90 toks/s, output: 116.70 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.99it/s, est. speed input: 1002.90 toks/s, output: 137.03 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  7.40it/s, est. speed input: 1126.55 toks/s, output: 159.78 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:07<00:01,  8.97it/s, est. speed input: 1247.78 toks/s, output: 182.98 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:07<00:01,  6.45it/s, est. speed input: 1295.35 toks/s, output: 197.76 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00,  9.34it/s, est. speed input: 1514.62 toks/s, output: 248.69 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:08<00:00, 10.71it/s, est. speed input: 1619.85 toks/s, output: 274.62 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:08<00:00,  9.61it/s, est. speed input: 1688.53 toks/s, output: 295.91 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:08<00:00,  7.30it/s, est. speed input: 1716.96 toks/s, output: 315.51 toks/s]Processed prompts: 100%|██████████| 32/32 [00:08<00:00,  8.31it/s, est. speed input: 1799.66 toks/s, output: 345.45 toks/s]Processed prompts: 100%|██████████| 32/32 [00:08<00:00,  3.56it/s, est. speed input: 1799.66 toks/s, output: 345.45 toks/s]
[2025-01-06 16:20:18,648][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:20:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:01<00:17,  1.76s/it, est. speed input: 350.86 toks/s, output: 15.30 toks/s]Processed prompts: 100%|██████████| 11/11 [00:02<00:00,  4.95it/s, est. speed input: 2599.89 toks/s, output: 137.95 toks/s]Processed prompts: 100%|██████████| 11/11 [00:02<00:00,  4.09it/s, est. speed input: 2599.89 toks/s, output: 137.95 toks/s]
[2025-01-06 16:20:21,734][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:20:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:04<01:28,  4.42s/it, est. speed input: 158.27 toks/s, output: 13.36 toks/s]Processed prompts:  10%|▉         | 2/21 [00:04<00:38,  2.00s/it, est. speed input: 295.76 toks/s, output: 26.44 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:04<00:21,  1.21s/it, est. speed input: 420.49 toks/s, output: 39.50 toks/s]Processed prompts:  24%|██▍       | 5/21 [00:05<00:10,  1.56it/s, est. speed input: 647.71 toks/s, output: 65.23 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:05<00:04,  3.11it/s, est. speed input: 1002.56 toks/s, output: 108.65 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:05<00:03,  3.21it/s, est. speed input: 1075.63 toks/s, output: 119.86 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:06<00:03,  2.96it/s, est. speed input: 1113.83 toks/s, output: 128.91 toks/s]Processed prompts:  67%|██████▋   | 14/21 [00:06<00:01,  4.89it/s, est. speed input: 1463.68 toks/s, output: 188.61 toks/s]Processed prompts:  71%|███████▏  | 15/21 [00:07<00:01,  4.40it/s, est. speed input: 1492.80 toks/s, output: 198.90 toks/s]Processed prompts:  76%|███████▌  | 16/21 [00:07<00:01,  3.44it/s, est. speed input: 1475.43 toks/s, output: 205.27 toks/s]Processed prompts:  86%|████████▌ | 18/21 [00:07<00:00,  4.27it/s, est. speed input: 1604.00 toks/s, output: 240.94 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:08<00:00,  4.06it/s, est. speed input: 1667.23 toks/s, output: 270.24 toks/s]Processed prompts: 100%|██████████| 21/21 [00:08<00:00,  2.50it/s, est. speed input: 1750.56 toks/s, output: 294.08 toks/s]
[2025-01-06 16:20:30,636][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:20:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:34,  2.33s/it, est. speed input: 300.10 toks/s, output: 9.87 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:02<00:15,  1.09s/it, est. speed input: 612.69 toks/s, output: 20.40 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:02<00:09,  1.36it/s, est. speed input: 834.25 toks/s, output: 31.36 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:03<00:10,  1.14it/s, est. speed input: 781.96 toks/s, output: 40.45 toks/s]Processed prompts:  31%|███▏      | 5/16 [00:04<00:07,  1.47it/s, est. speed input: 884.73 toks/s, output: 56.00 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:04<00:04,  2.04it/s, est. speed input: 1075.65 toks/s, output: 73.43 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:04<00:01,  4.49it/s, est. speed input: 1550.33 toks/s, output: 129.19 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:04<00:01,  4.97it/s, est. speed input: 1734.20 toks/s, output: 160.48 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:05<00:01,  3.96it/s, est. speed input: 1717.80 toks/s, output: 169.57 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:05<00:00,  5.59it/s, est. speed input: 2051.50 toks/s, output: 229.31 toks/s]Processed prompts: 100%|██████████| 16/16 [00:05<00:00,  5.57it/s, est. speed input: 2107.42 toks/s, output: 247.49 toks/s]Processed prompts: 100%|██████████| 16/16 [00:05<00:00,  2.77it/s, est. speed input: 2107.42 toks/s, output: 247.49 toks/s]
[2025-01-06 16:20:36,940][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:20:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1045.10 toks/s, output: 39.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1045.10 toks/s, output: 39.05 toks/s]
[2025-01-06 16:20:44,168][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:20:51,493][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:20:51,493][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.20 GB
[2025-01-06 16:20:51,855][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.83 GB
[2025-01-06 16:21:04,108][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:21:04,109][root][INFO] - Before destroying HF.: GPU memory allocated: 15.83 GB
[2025-01-06 16:21:04,480][root][INFO] - After destroying HF.: GPU memory allocated: 0.24 GB
[2025-01-06 16:21:04,669][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:21:12,136][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:21:27,367][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:21:27,368][root][INFO] - Iteration 24 took 2m 0s. Generation: 64.01%, Training: 35.99%. Estimated time remaining: 3h 29m 14s. Estimated total time for complete run: 4h 16m 55s.
[2025-01-06 16:21:27,700][root][INFO] - Loading VLLM model.
WARNING 01-06 16:21:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:21:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:21:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:21:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:21:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:21:46 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:21:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:21:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:22:08 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:22:08,893][root][INFO] - Before destroying HF.: GPU memory allocated: 71.21 GB
[2025-01-06 16:22:09,170][root][INFO] - After destroying HF.: GPU memory allocated: 55.61 GB
[2025-01-06 16:22:09,171][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:22:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:00,  3.89s/it, est. speed input: 129.88 toks/s, output: 5.14 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:53,  1.78s/it, est. speed input: 241.18 toks/s, output: 10.75 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:50,  1.74s/it, est. speed input: 257.81 toks/s, output: 16.85 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.23s/it, est. speed input: 318.92 toks/s, output: 25.42 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:26,  1.01it/s, est. speed input: 366.48 toks/s, output: 33.82 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:13,  1.90it/s, est. speed input: 501.44 toks/s, output: 54.04 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:10,  2.27it/s, est. speed input: 556.91 toks/s, output: 63.41 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:05,  4.15it/s, est. speed input: 745.80 toks/s, output: 94.52 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:04,  4.55it/s, est. speed input: 798.89 toks/s, output: 104.15 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:03,  5.00it/s, est. speed input: 850.61 toks/s, output: 113.89 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:08<00:03,  4.73it/s, est. speed input: 926.56 toks/s, output: 130.76 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  6.38it/s, est. speed input: 1035.27 toks/s, output: 153.51 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  8.20it/s, est. speed input: 1142.17 toks/s, output: 176.65 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  8.33it/s, est. speed input: 1228.45 toks/s, output: 197.73 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00,  9.87it/s, est. speed input: 1326.61 toks/s, output: 221.58 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:00,  8.07it/s, est. speed input: 1386.93 toks/s, output: 240.36 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  5.83it/s, est. speed input: 1411.63 toks/s, output: 256.03 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  6.71it/s, est. speed input: 1485.98 toks/s, output: 283.09 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  5.28it/s, est. speed input: 1480.97 toks/s, output: 290.43 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  4.83it/s, est. speed input: 1490.05 toks/s, output: 301.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.03it/s, est. speed input: 1528.38 toks/s, output: 318.44 toks/s]
[2025-01-06 16:22:20,166][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:22:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:20,  1.89s/it, est. speed input: 299.17 toks/s, output: 14.32 toks/s]Processed prompts: 100%|██████████| 12/12 [00:02<00:00,  8.10it/s, est. speed input: 3841.29 toks/s, output: 172.59 toks/s]Processed prompts: 100%|██████████| 12/12 [00:02<00:00,  5.97it/s, est. speed input: 3841.29 toks/s, output: 172.59 toks/s]
[2025-01-06 16:22:22,612][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:22:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:03<01:07,  3.57s/it, est. speed input: 195.82 toks/s, output: 12.33 toks/s]Processed prompts:  10%|█         | 2/20 [00:04<00:39,  2.18s/it, est. speed input: 292.39 toks/s, output: 24.26 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:04<00:21,  1.24s/it, est. speed input: 427.34 toks/s, output: 38.92 toks/s]Processed prompts:  20%|██        | 4/20 [00:05<00:13,  1.20it/s, est. speed input: 547.03 toks/s, output: 53.02 toks/s]Processed prompts:  30%|███       | 6/20 [00:05<00:06,  2.32it/s, est. speed input: 797.71 toks/s, output: 82.74 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:05<00:04,  2.63it/s, est. speed input: 888.65 toks/s, output: 95.53 toks/s]Processed prompts:  40%|████      | 8/20 [00:05<00:03,  3.12it/s, est. speed input: 985.25 toks/s, output: 109.59 toks/s]Processed prompts:  60%|██████    | 12/20 [00:05<00:01,  5.80it/s, est. speed input: 1398.68 toks/s, output: 170.58 toks/s]Processed prompts:  65%|██████▌   | 13/20 [00:06<00:02,  3.37it/s, est. speed input: 1325.56 toks/s, output: 169.51 toks/s]Processed prompts:  80%|████████  | 16/20 [00:06<00:00,  5.41it/s, est. speed input: 1602.80 toks/s, output: 227.72 toks/s]Processed prompts:  90%|█████████ | 18/20 [00:07<00:00,  3.74it/s, est. speed input: 1591.03 toks/s, output: 247.09 toks/s]Processed prompts:  95%|█████████▌| 19/20 [00:08<00:00,  3.96it/s, est. speed input: 1641.20 toks/s, output: 265.93 toks/s]Processed prompts: 100%|██████████| 20/20 [00:08<00:00,  2.46it/s, est. speed input: 1720.38 toks/s, output: 289.44 toks/s]
[2025-01-06 16:22:31,263][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:22:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:37,  2.50s/it, est. speed input: 371.31 toks/s, output: 11.58 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:03<00:09,  1.29it/s, est. speed input: 880.53 toks/s, output: 41.67 toks/s]Processed prompts:  31%|███▏      | 5/16 [00:03<00:06,  1.68it/s, est. speed input: 1035.72 toks/s, output: 57.78 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:03<00:04,  2.12it/s, est. speed input: 1173.06 toks/s, output: 73.62 toks/s]Processed prompts:  44%|████▍     | 7/16 [00:04<00:04,  2.01it/s, est. speed input: 1181.94 toks/s, output: 84.49 toks/s]Processed prompts:  50%|█████     | 8/16 [00:04<00:03,  2.59it/s, est. speed input: 1337.72 toks/s, output: 102.88 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:04<00:02,  2.98it/s, est. speed input: 1423.41 toks/s, output: 119.52 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:04<00:01,  3.64it/s, est. speed input: 1527.55 toks/s, output: 138.06 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:05<00:01,  3.39it/s, est. speed input: 1603.71 toks/s, output: 166.06 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:05<00:00,  4.27it/s, est. speed input: 1764.76 toks/s, output: 207.04 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:05<00:00,  4.74it/s, est. speed input: 1843.54 toks/s, output: 228.64 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  4.30it/s, est. speed input: 1866.72 toks/s, output: 245.41 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  2.55it/s, est. speed input: 1866.72 toks/s, output: 245.41 toks/s]
[2025-01-06 16:22:44,290][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:22:51,682][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:22:51,683][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.21 GB
[2025-01-06 16:22:52,094][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.84 GB
[2025-01-06 16:23:04,307][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:23:04,309][root][INFO] - Before destroying HF.: GPU memory allocated: 15.84 GB
[2025-01-06 16:23:04,640][root][INFO] - After destroying HF.: GPU memory allocated: 0.25 GB
[2025-01-06 16:23:04,799][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:23:12,541][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:23:27,709][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:23:27,710][root][INFO] - Iteration 25 took 2m 0s. Generation: 63.80%, Training: 36.20%. Estimated time remaining: 3h 27m 2s. Estimated total time for complete run: 4h 16m 43s.
[2025-01-06 16:23:28,045][root][INFO] - Loading VLLM model.
WARNING 01-06 16:23:28 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:23:28 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:23:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:23:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:23:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:23:47 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:23:48 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:23:48 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:24:09 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:24:09,493][root][INFO] - Before destroying HF.: GPU memory allocated: 71.22 GB
[2025-01-06 16:24:09,765][root][INFO] - After destroying HF.: GPU memory allocated: 55.62 GB
[2025-01-06 16:24:09,766][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:24:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:50,  3.56s/it, est. speed input: 141.73 toks/s, output: 6.45 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:59,  2.00s/it, est. speed input: 226.43 toks/s, output: 13.68 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:49,  1.72s/it, est. speed input: 258.60 toks/s, output: 20.99 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:32,  1.16s/it, est. speed input: 328.74 toks/s, output: 30.92 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.19it/s, est. speed input: 393.13 toks/s, output: 40.79 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:11,  2.20it/s, est. speed input: 537.02 toks/s, output: 62.28 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:07,  3.28it/s, est. speed input: 670.04 toks/s, output: 83.15 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:05,  3.69it/s, est. speed input: 770.46 toks/s, output: 101.66 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:05,  3.44it/s, est. speed input: 799.96 toks/s, output: 109.43 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  4.67it/s, est. speed input: 912.35 toks/s, output: 132.40 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  5.72it/s, est. speed input: 1016.01 toks/s, output: 154.92 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  6.15it/s, est. speed input: 1064.22 toks/s, output: 166.11 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  7.04it/s, est. speed input: 1158.91 toks/s, output: 188.78 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  7.42it/s, est. speed input: 1283.54 toks/s, output: 221.93 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:08<00:00,  8.30it/s, est. speed input: 1372.88 toks/s, output: 247.05 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  8.50it/s, est. speed input: 1487.38 toks/s, output: 282.75 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  8.31it/s, est. speed input: 1520.05 toks/s, output: 294.98 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  6.37it/s, est. speed input: 1521.65 toks/s, output: 302.46 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:09<00:00,  5.77it/s, est. speed input: 1536.33 toks/s, output: 313.35 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  4.50it/s, est. speed input: 1527.60 toks/s, output: 321.03 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.12it/s, est. speed input: 1576.86 toks/s, output: 340.55 toks/s]
[2025-01-06 16:24:20,440][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:24:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:25,  2.09s/it, est. speed input: 340.07 toks/s, output: 13.87 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:02<00:00,  6.08it/s, est. speed input: 3021.17 toks/s, output: 139.93 toks/s]Processed prompts: 100%|██████████| 13/13 [00:02<00:00,  4.69it/s, est. speed input: 3102.05 toks/s, output: 161.20 toks/s]
[2025-01-06 16:24:23,645][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:24:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:02<00:45,  2.54s/it, est. speed input: 275.39 toks/s, output: 9.06 toks/s]Processed prompts:  11%|█         | 2/19 [00:05<00:43,  2.56s/it, est. speed input: 272.95 toks/s, output: 21.09 toks/s]Processed prompts:  21%|██        | 4/19 [00:05<00:15,  1.03s/it, est. speed input: 522.04 toks/s, output: 53.40 toks/s]Processed prompts:  32%|███▏      | 6/19 [00:05<00:07,  1.68it/s, est. speed input: 753.07 toks/s, output: 85.11 toks/s]Processed prompts:  37%|███▋      | 7/19 [00:05<00:06,  1.86it/s, est. speed input: 823.52 toks/s, output: 97.95 toks/s]Processed prompts:  53%|█████▎    | 10/19 [00:06<00:02,  3.11it/s, est. speed input: 1108.57 toks/s, output: 146.38 toks/s]Processed prompts:  68%|██████▊   | 13/19 [00:06<00:01,  4.23it/s, est. speed input: 1362.31 toks/s, output: 194.59 toks/s]Processed prompts:  79%|███████▉  | 15/19 [00:07<00:00,  4.54it/s, est. speed input: 1490.82 toks/s, output: 224.94 toks/s]Processed prompts:  89%|████████▉ | 17/19 [00:07<00:00,  3.56it/s, est. speed input: 1506.60 toks/s, output: 244.06 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:08<00:00,  3.93it/s, est. speed input: 1569.55 toks/s, output: 265.08 toks/s]Processed prompts: 100%|██████████| 19/19 [00:08<00:00,  2.37it/s, est. speed input: 1656.71 toks/s, output: 290.03 toks/s]
[2025-01-06 16:24:32,167][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:24:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:02<00:47,  2.80s/it, est. speed input: 340.34 toks/s, output: 10.35 toks/s]Processed prompts:  17%|█▋        | 3/18 [00:03<00:12,  1.17it/s, est. speed input: 841.34 toks/s, output: 30.51 toks/s]Processed prompts:  22%|██▏       | 4/18 [00:04<00:14,  1.03s/it, est. speed input: 785.85 toks/s, output: 38.02 toks/s]Processed prompts:  28%|██▊       | 5/18 [00:04<00:10,  1.28it/s, est. speed input: 885.13 toks/s, output: 53.11 toks/s]Processed prompts:  33%|███▎      | 6/18 [00:05<00:07,  1.56it/s, est. speed input: 1012.38 toks/s, output: 67.74 toks/s]Processed prompts:  39%|███▉      | 7/18 [00:05<00:06,  1.75it/s, est. speed input: 1060.46 toks/s, output: 81.63 toks/s]Processed prompts:  50%|█████     | 9/18 [00:06<00:03,  2.33it/s, est. speed input: 1196.40 toks/s, output: 112.69 toks/s]Processed prompts:  61%|██████    | 11/18 [00:06<00:01,  3.53it/s, est. speed input: 1394.83 toks/s, output: 151.04 toks/s]Processed prompts:  72%|███████▏  | 13/18 [00:06<00:01,  3.99it/s, est. speed input: 1524.60 toks/s, output: 183.64 toks/s]Processed prompts:  83%|████████▎ | 15/18 [00:07<00:00,  3.02it/s, est. speed input: 1513.46 toks/s, output: 204.53 toks/s]Processed prompts:  89%|████████▉ | 16/18 [00:07<00:00,  3.27it/s, est. speed input: 1564.57 toks/s, output: 225.00 toks/s]Processed prompts: 100%|██████████| 18/18 [00:07<00:00,  2.31it/s, est. speed input: 1744.17 toks/s, output: 276.39 toks/s]
[2025-01-06 16:24:40,455][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:24:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 885.66 toks/s, output: 26.92 toks/s]Processed prompts:  67%|██████▋   | 2/3 [00:01<00:00,  1.06it/s, est. speed input: 930.86 toks/s, output: 54.08 toks/s]Processed prompts: 100%|██████████| 3/3 [00:03<00:00,  1.37s/it, est. speed input: 707.88 toks/s, output: 76.58 toks/s]Processed prompts: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it, est. speed input: 707.88 toks/s, output: 76.58 toks/s]
[2025-01-06 16:24:51,002][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-06 16:24:58,223][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:24:58,224][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.21 GB
[2025-01-06 16:24:58,614][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.85 GB
[2025-01-06 16:25:10,919][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:25:10,920][root][INFO] - Before destroying HF.: GPU memory allocated: 15.85 GB
[2025-01-06 16:25:11,322][root][INFO] - After destroying HF.: GPU memory allocated: 0.25 GB
[2025-01-06 16:25:11,478][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:25:18,929][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:25:34,637][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:25:34,638][root][INFO] - Iteration 26 took 2m 6s. Generation: 65.50%, Training: 34.50%. Estimated time remaining: 3h 38m 58s. Estimated total time for complete run: 4h 30m 46s.
[2025-01-06 16:25:35,012][root][INFO] - Loading VLLM model.
WARNING 01-06 16:25:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:25:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:25:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:25:35 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:25:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:25:54 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:25:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:25:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:26:16 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:26:16,789][root][INFO] - Before destroying HF.: GPU memory allocated: 71.23 GB
[2025-01-06 16:26:17,068][root][INFO] - After destroying HF.: GPU memory allocated: 55.63 GB
[2025-01-06 16:26:17,069][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:26:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:04,  4.03s/it, est. speed input: 125.38 toks/s, output: 5.71 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:59,  1.98s/it, est. speed input: 221.16 toks/s, output: 12.04 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:47,  1.63s/it, est. speed input: 261.66 toks/s, output: 18.65 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.24s/it, est. speed input: 314.72 toks/s, output: 26.80 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:18,  1.44it/s, est. speed input: 442.38 toks/s, output: 44.97 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:10,  2.33it/s, est. speed input: 576.75 toks/s, output: 65.10 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:08,  2.84it/s, est. speed input: 639.64 toks/s, output: 75.01 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:04,  4.17it/s, est. speed input: 849.18 toks/s, output: 111.37 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:03,  4.98it/s, est. speed input: 953.80 toks/s, output: 132.21 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:01,  7.25it/s, est. speed input: 1128.73 toks/s, output: 166.64 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.03it/s, est. speed input: 1207.90 toks/s, output: 185.73 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  5.54it/s, est. speed input: 1245.14 toks/s, output: 200.61 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:00,  7.87it/s, est. speed input: 1397.67 toks/s, output: 240.67 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  9.14it/s, est. speed input: 1489.71 toks/s, output: 266.48 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  9.16it/s, est. speed input: 1562.97 toks/s, output: 290.39 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  7.38it/s, est. speed input: 1601.36 toks/s, output: 311.58 toks/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s, est. speed input: 1625.37 toks/s, output: 324.07 toks/s]
[2025-01-06 16:26:27,445][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:26:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:20,  1.85s/it, est. speed input: 310.68 toks/s, output: 14.56 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:01<00:00,  6.93it/s, est. speed input: 3215.69 toks/s, output: 147.82 toks/s]Processed prompts: 100%|██████████| 12/12 [00:03<00:00,  3.63it/s, est. speed input: 2290.83 toks/s, output: 149.90 toks/s]
[2025-01-06 16:26:31,232][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:26:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:04<01:18,  4.11s/it, est. speed input: 170.16 toks/s, output: 13.63 toks/s]Processed prompts:  10%|█         | 2/20 [00:04<00:35,  1.97s/it, est. speed input: 304.93 toks/s, output: 26.83 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:05<00:22,  1.32s/it, est. speed input: 408.92 toks/s, output: 39.59 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:05<00:11,  1.34it/s, est. speed input: 608.88 toks/s, output: 66.20 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:05<00:05,  2.29it/s, est. speed input: 836.51 toks/s, output: 98.64 toks/s]Processed prompts:  40%|████      | 8/20 [00:05<00:04,  2.75it/s, est. speed input: 934.34 toks/s, output: 113.62 toks/s]Processed prompts:  50%|█████     | 10/20 [00:06<00:02,  3.66it/s, est. speed input: 1115.13 toks/s, output: 142.94 toks/s]Processed prompts:  55%|█████▌    | 11/20 [00:06<00:02,  3.78it/s, est. speed input: 1181.98 toks/s, output: 156.18 toks/s]Processed prompts:  60%|██████    | 12/20 [00:06<00:02,  3.53it/s, est. speed input: 1225.44 toks/s, output: 167.72 toks/s]Processed prompts:  70%|███████   | 14/20 [00:07<00:01,  4.64it/s, est. speed input: 1381.85 toks/s, output: 201.08 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:08<00:00,  3.18it/s, est. speed input: 1421.50 toks/s, output: 228.48 toks/s]Processed prompts: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s, est. speed input: 1672.31 toks/s, output: 300.25 toks/s]
[2025-01-06 16:26:40,106][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:26:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:02<00:48,  2.83s/it, est. speed input: 300.10 toks/s, output: 10.25 toks/s]Processed prompts:  22%|██▏       | 4/18 [00:04<00:13,  1.05it/s, est. speed input: 831.83 toks/s, output: 36.25 toks/s]Processed prompts:  28%|██▊       | 5/18 [00:04<00:09,  1.40it/s, est. speed input: 968.44 toks/s, output: 51.95 toks/s]Processed prompts:  33%|███▎      | 6/18 [00:05<00:08,  1.48it/s, est. speed input: 1046.90 toks/s, output: 64.05 toks/s]Processed prompts:  39%|███▉      | 7/18 [00:05<00:06,  1.63it/s, est. speed input: 1086.30 toks/s, output: 77.77 toks/s]Processed prompts:  44%|████▍     | 8/18 [00:05<00:05,  1.87it/s, est. speed input: 1141.86 toks/s, output: 93.03 toks/s]Processed prompts:  61%|██████    | 11/18 [00:05<00:01,  3.87it/s, est. speed input: 1482.75 toks/s, output: 150.96 toks/s]Processed prompts:  78%|███████▊  | 14/18 [00:06<00:00,  5.11it/s, est. speed input: 1732.77 toks/s, output: 202.95 toks/s]Processed prompts:  83%|████████▎ | 15/18 [00:06<00:00,  5.43it/s, est. speed input: 1806.39 toks/s, output: 220.75 toks/s]Processed prompts:  89%|████████▉ | 16/18 [00:06<00:00,  5.13it/s, est. speed input: 1846.76 toks/s, output: 235.80 toks/s]Processed prompts:  94%|█████████▍| 17/18 [00:06<00:00,  5.19it/s, est. speed input: 1899.00 toks/s, output: 253.22 toks/s]Processed prompts: 100%|██████████| 18/18 [00:07<00:00,  4.76it/s, est. speed input: 1899.76 toks/s, output: 268.89 toks/s]Processed prompts: 100%|██████████| 18/18 [00:07<00:00,  2.52it/s, est. speed input: 1899.76 toks/s, output: 268.89 toks/s]
[2025-01-06 16:26:47,770][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:26:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.35s/it, est. speed input: 303.38 toks/s, output: 53.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.35s/it, est. speed input: 303.38 toks/s, output: 53.54 toks/s]
[2025-01-06 16:26:56,941][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:27:04,174][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:27:04,175][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.22 GB
[2025-01-06 16:27:04,532][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.85 GB
[2025-01-06 16:27:16,633][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:27:16,634][root][INFO] - Before destroying HF.: GPU memory allocated: 15.85 GB
[2025-01-06 16:27:16,958][root][INFO] - After destroying HF.: GPU memory allocated: 0.26 GB
[2025-01-06 16:27:17,107][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:27:24,572][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:27:39,901][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:27:39,902][root][INFO] - Iteration 27 took 2m 5s. Generation: 65.59%, Training: 34.41%. Estimated time remaining: 3h 33m 20s. Estimated total time for complete run: 4h 27m 13s.
[2025-01-06 16:27:40,208][root][INFO] - Loading VLLM model.
WARNING 01-06 16:27:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:27:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:27:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:27:41 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:27:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:27:59 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:28:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:28:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:28:21 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:28:21,646][root][INFO] - Before destroying HF.: GPU memory allocated: 71.23 GB
[2025-01-06 16:28:21,918][root][INFO] - After destroying HF.: GPU memory allocated: 55.64 GB
[2025-01-06 16:28:21,919][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:28:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:50,  3.57s/it, est. speed input: 141.29 toks/s, output: 6.43 toks/s]Processed prompts:   6%|▋         | 2/32 [00:03<00:46,  1.54s/it, est. speed input: 273.38 toks/s, output: 12.99 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:45,  1.58s/it, est. speed input: 284.49 toks/s, output: 18.97 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.23s/it, est. speed input: 336.03 toks/s, output: 27.61 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.58it/s, est. speed input: 486.58 toks/s, output: 48.18 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:12,  1.99it/s, est. speed input: 553.62 toks/s, output: 58.26 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:09,  2.47it/s, est. speed input: 617.85 toks/s, output: 68.36 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:07,  2.89it/s, est. speed input: 674.44 toks/s, output: 78.05 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:06<00:06,  3.17it/s, est. speed input: 723.46 toks/s, output: 87.39 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:04,  4.44it/s, est. speed input: 840.25 toks/s, output: 108.84 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  4.70it/s, est. speed input: 930.18 toks/s, output: 128.02 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:03,  5.17it/s, est. speed input: 980.55 toks/s, output: 139.02 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:03,  4.87it/s, est. speed input: 1014.03 toks/s, output: 148.21 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  6.00it/s, est. speed input: 1110.18 toks/s, output: 171.23 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.00it/s, est. speed input: 1203.66 toks/s, output: 194.85 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:08<00:01,  6.57it/s, est. speed input: 1272.42 toks/s, output: 215.77 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:01,  5.74it/s, est. speed input: 1291.20 toks/s, output: 224.89 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:01,  6.22it/s, est. speed input: 1330.63 toks/s, output: 237.80 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:01,  6.47it/s, est. speed input: 1365.94 toks/s, output: 250.36 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  7.06it/s, est. speed input: 1436.93 toks/s, output: 276.43 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:09<00:00,  4.98it/s, est. speed input: 1429.24 toks/s, output: 282.92 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  7.15it/s, est. speed input: 1546.44 toks/s, output: 330.33 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  6.62it/s, est. speed input: 1565.49 toks/s, output: 343.32 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.10it/s, est. speed input: 1565.49 toks/s, output: 343.32 toks/s]
[2025-01-06 16:28:32,688][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:28:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:01<00:21,  1.96s/it, est. speed input: 316.15 toks/s, output: 13.79 toks/s]Processed prompts: 100%|██████████| 12/12 [00:02<00:00,  5.84it/s, est. speed input: 3789.16 toks/s, output: 169.40 toks/s]
[2025-01-06 16:28:35,150][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:28:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:51,  2.73s/it, est. speed input: 256.15 toks/s, output: 9.16 toks/s]Processed prompts:  10%|█         | 2/20 [00:03<00:28,  1.56s/it, est. speed input: 403.60 toks/s, output: 19.34 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:04<00:24,  1.42s/it, est. speed input: 444.71 toks/s, output: 29.48 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:04<00:10,  1.47it/s, est. speed input: 706.47 toks/s, output: 58.62 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:05<00:06,  2.08it/s, est. speed input: 904.48 toks/s, output: 85.03 toks/s]Processed prompts:  40%|████      | 8/20 [00:05<00:04,  2.52it/s, est. speed input: 1008.42 toks/s, output: 100.08 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:06<00:05,  2.20it/s, est. speed input: 1019.72 toks/s, output: 108.44 toks/s]Processed prompts:  55%|█████▌    | 11/20 [00:06<00:02,  3.13it/s, est. speed input: 1194.37 toks/s, output: 140.89 toks/s]Processed prompts:  60%|██████    | 12/20 [00:06<00:02,  2.80it/s, est. speed input: 1212.34 toks/s, output: 151.33 toks/s]Processed prompts:  65%|██████▌   | 13/20 [00:07<00:02,  3.39it/s, est. speed input: 1293.25 toks/s, output: 169.50 toks/s]Processed prompts:  75%|███████▌  | 15/20 [00:07<00:01,  4.80it/s, est. speed input: 1456.23 toks/s, output: 206.66 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:07<00:00,  5.69it/s, est. speed input: 1596.81 toks/s, output: 242.42 toks/s]Processed prompts:  95%|█████████▌| 19/20 [00:07<00:00,  7.28it/s, est. speed input: 1753.80 toks/s, output: 282.06 toks/s]Processed prompts: 100%|██████████| 20/20 [00:07<00:00,  2.64it/s, est. speed input: 1846.07 toks/s, output: 304.38 toks/s]
[2025-01-06 16:28:43,238][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:28:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:37,  2.50s/it, est. speed input: 311.04 toks/s, output: 11.59 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:03<00:22,  1.63s/it, est. speed input: 456.75 toks/s, output: 24.41 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:03<00:12,  1.03it/s, est. speed input: 649.67 toks/s, output: 40.00 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:03<00:07,  1.50it/s, est. speed input: 840.24 toks/s, output: 55.33 toks/s]Processed prompts:  31%|███▏      | 5/16 [00:04<00:05,  2.11it/s, est. speed input: 986.07 toks/s, output: 71.37 toks/s]Processed prompts:  44%|████▍     | 7/16 [00:04<00:02,  3.23it/s, est. speed input: 1239.85 toks/s, output: 102.61 toks/s]Processed prompts:  50%|█████     | 8/16 [00:04<00:02,  3.48it/s, est. speed input: 1331.40 toks/s, output: 117.23 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:04<00:02,  3.29it/s, est. speed input: 1379.22 toks/s, output: 129.88 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:05<00:01,  3.81it/s, est. speed input: 1474.67 toks/s, output: 147.39 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:05<00:01,  3.82it/s, est. speed input: 1586.82 toks/s, output: 176.89 toks/s]Processed prompts:  81%|████████▏ | 13/16 [00:05<00:00,  3.99it/s, est. speed input: 1648.11 toks/s, output: 194.76 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:06<00:00,  4.12it/s, est. speed input: 1704.24 toks/s, output: 213.07 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:06<00:00,  3.56it/s, est. speed input: 1710.53 toks/s, output: 227.36 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  3.09it/s, est. speed input: 1704.36 toks/s, output: 242.19 toks/s]Processed prompts: 100%|██████████| 16/16 [00:06<00:00,  2.34it/s, est. speed input: 1704.36 toks/s, output: 242.19 toks/s]
[2025-01-06 16:28:50,580][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:28:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.86s/it, est. speed input: 498.51 toks/s, output: 43.56 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s, est. speed input: 928.83 toks/s, output: 85.26 toks/s]Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s, est. speed input: 928.83 toks/s, output: 85.26 toks/s]
[2025-01-06 16:28:59,500][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:29:06,753][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:29:06,753][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.23 GB
[2025-01-06 16:29:07,108][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.86 GB
[2025-01-06 16:29:19,418][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:29:19,419][root][INFO] - Before destroying HF.: GPU memory allocated: 15.86 GB
[2025-01-06 16:29:19,755][root][INFO] - After destroying HF.: GPU memory allocated: 0.27 GB
[2025-01-06 16:29:19,959][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:29:27,357][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:29:42,813][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:29:42,814][root][INFO] - Iteration 28 took 2m 2s. Generation: 64.62%, Training: 35.38%. Estimated time remaining: 3h 26m 16s. Estimated total time for complete run: 4h 22m 12s.
[2025-01-06 16:29:43,228][root][INFO] - Loading VLLM model.
WARNING 01-06 16:29:43 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:29:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:29:44 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:29:44 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:29:49 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:30:02 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:30:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:30:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:30:25 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:30:25,040][root][INFO] - Before destroying HF.: GPU memory allocated: 71.24 GB
[2025-01-06 16:30:25,288][root][INFO] - After destroying HF.: GPU memory allocated: 55.64 GB
[2025-01-06 16:30:25,290][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:30:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:06,  4.08s/it, est. speed input: 123.82 toks/s, output: 5.64 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:52,  1.75s/it, est. speed input: 240.55 toks/s, output: 11.43 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:49,  1.70s/it, est. speed input: 259.87 toks/s, output: 17.32 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:36,  1.30s/it, est. speed input: 309.98 toks/s, output: 25.47 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.53it/s, est. speed input: 453.70 toks/s, output: 44.77 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:09,  2.40it/s, est. speed input: 586.77 toks/s, output: 63.91 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:07,  3.11it/s, est. speed input: 699.08 toks/s, output: 81.81 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:06,  3.09it/s, est. speed input: 735.39 toks/s, output: 89.62 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:04,  4.46it/s, est. speed input: 894.65 toks/s, output: 119.71 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:03,  5.19it/s, est. speed input: 991.67 toks/s, output: 140.16 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  5.03it/s, est. speed input: 1025.05 toks/s, output: 149.01 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  6.71it/s, est. speed input: 1131.16 toks/s, output: 172.59 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  6.48it/s, est. speed input: 1203.34 toks/s, output: 192.56 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:01,  7.45it/s, est. speed input: 1327.55 toks/s, output: 226.74 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  8.19it/s, est. speed input: 1410.04 toks/s, output: 251.08 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  5.88it/s, est. speed input: 1402.73 toks/s, output: 256.37 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  7.51it/s, est. speed input: 1489.48 toks/s, output: 285.59 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  4.69it/s, est. speed input: 1476.17 toks/s, output: 299.95 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s, est. speed input: 1523.77 toks/s, output: 318.80 toks/s]
[2025-01-06 16:30:36,318][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:30:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:01<00:22,  1.87s/it, est. speed input: 399.11 toks/s, output: 12.86 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:02<00:05,  1.84it/s, est. speed input: 1018.94 toks/s, output: 39.46 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:02<00:00,  8.05it/s, est. speed input: 3172.26 toks/s, output: 145.38 toks/s]Processed prompts: 100%|██████████| 13/13 [00:03<00:00,  4.28it/s, est. speed input: 2777.26 toks/s, output: 149.57 toks/s]
[2025-01-06 16:30:39,762][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:30:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:04<01:26,  4.79s/it, est. speed input: 145.97 toks/s, output: 15.66 toks/s]Processed prompts:  11%|█         | 2/19 [00:05<00:36,  2.12s/it, est. speed input: 277.37 toks/s, output: 30.95 toks/s]Processed prompts:  21%|██        | 4/19 [00:05<00:14,  1.02it/s, est. speed input: 494.43 toks/s, output: 59.42 toks/s]Processed prompts:  26%|██▋       | 5/19 [00:05<00:10,  1.36it/s, est. speed input: 598.65 toks/s, output: 75.02 toks/s]Processed prompts:  32%|███▏      | 6/19 [00:06<00:07,  1.74it/s, est. speed input: 693.06 toks/s, output: 90.23 toks/s]Processed prompts:  37%|███▋      | 7/19 [00:06<00:05,  2.21it/s, est. speed input: 786.51 toks/s, output: 105.93 toks/s]Processed prompts:  47%|████▋     | 9/19 [00:06<00:03,  2.67it/s, est. speed input: 926.65 toks/s, output: 133.45 toks/s]Processed prompts:  58%|█████▊    | 11/19 [00:07<00:02,  3.55it/s, est. speed input: 1087.09 toks/s, output: 166.55 toks/s]Processed prompts:  63%|██████▎   | 12/19 [00:07<00:01,  4.12it/s, est. speed input: 1168.10 toks/s, output: 184.24 toks/s]Processed prompts:  74%|███████▎  | 14/19 [00:07<00:00,  5.48it/s, est. speed input: 1330.58 toks/s, output: 220.54 toks/s]Processed prompts:  79%|███████▉  | 15/19 [00:07<00:00,  4.15it/s, est. speed input: 1342.82 toks/s, output: 229.76 toks/s]Processed prompts:  89%|████████▉ | 17/19 [00:08<00:00,  4.91it/s, est. speed input: 1468.14 toks/s, output: 266.37 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:08<00:00,  5.33it/s, est. speed input: 1530.10 toks/s, output: 285.66 toks/s]Processed prompts: 100%|██████████| 19/19 [00:08<00:00,  5.85it/s, est. speed input: 1592.42 toks/s, output: 305.63 toks/s]Processed prompts: 100%|██████████| 19/19 [00:08<00:00,  2.28it/s, est. speed input: 1592.42 toks/s, output: 305.63 toks/s]
[2025-01-06 16:30:48,665][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:30:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:04<00:59,  4.25s/it, est. speed input: 217.73 toks/s, output: 19.30 toks/s]Processed prompts:  13%|█▎        | 2/15 [00:04<00:24,  1.92s/it, est. speed input: 316.60 toks/s, output: 37.95 toks/s]Processed prompts:  20%|██        | 3/15 [00:04<00:14,  1.23s/it, est. speed input: 432.04 toks/s, output: 55.47 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:05<00:05,  1.71it/s, est. speed input: 651.73 toks/s, output: 94.55 toks/s]Processed prompts:  47%|████▋     | 7/15 [00:05<00:02,  2.86it/s, est. speed input: 936.16 toks/s, output: 134.77 toks/s]Processed prompts:  60%|██████    | 9/15 [00:05<00:01,  3.85it/s, est. speed input: 1150.67 toks/s, output: 171.99 toks/s]Processed prompts:  67%|██████▋   | 10/15 [00:05<00:01,  3.19it/s, est. speed input: 1169.72 toks/s, output: 181.10 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:06<00:01,  3.07it/s, est. speed input: 1212.97 toks/s, output: 195.63 toks/s]Processed prompts:  80%|████████  | 12/15 [00:06<00:01,  2.98it/s, est. speed input: 1250.68 toks/s, output: 210.98 toks/s]Processed prompts:  87%|████████▋ | 13/15 [00:07<00:00,  2.64it/s, est. speed input: 1261.25 toks/s, output: 224.13 toks/s]Processed prompts: 100%|██████████| 15/15 [00:07<00:00,  2.08it/s, est. speed input: 1428.69 toks/s, output: 279.53 toks/s]
[2025-01-06 16:30:56,383][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:30:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.57s/it, est. speed input: 607.69 toks/s, output: 41.40 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it, est. speed input: 497.44 toks/s, output: 69.09 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it, est. speed input: 497.44 toks/s, output: 69.09 toks/s]
[2025-01-06 16:31:07,091][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:31:14,493][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:31:14,494][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.24 GB
[2025-01-06 16:31:14,868][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.87 GB
[2025-01-06 16:31:27,207][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:31:27,209][root][INFO] - Before destroying HF.: GPU memory allocated: 15.87 GB
[2025-01-06 16:31:27,535][root][INFO] - After destroying HF.: GPU memory allocated: 0.28 GB
[2025-01-06 16:31:27,782][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:31:35,124][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:31:50,449][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:31:50,450][root][INFO] - Iteration 29 took 2m 7s. Generation: 65.89%, Training: 34.11%. Estimated time remaining: 3h 34m 13s. Estimated total time for complete run: 4h 32m 17s.
[2025-01-06 16:31:50,838][root][INFO] - Loading VLLM model.
WARNING 01-06 16:31:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:31:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:31:51 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:31:51 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 16:31:56 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:32:10 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:32:10 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:32:10 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:32:32 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:32:32,516][root][INFO] - Before destroying HF.: GPU memory allocated: 71.25 GB
[2025-01-06 16:32:32,759][root][INFO] - After destroying HF.: GPU memory allocated: 55.65 GB
[2025-01-06 16:32:32,760][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:32:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:50,  3.56s/it, est. speed input: 142.03 toks/s, output: 6.47 toks/s]Processed prompts:   6%|▋         | 2/32 [00:03<00:46,  1.53s/it, est. speed input: 274.77 toks/s, output: 13.06 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:45,  1.58s/it, est. speed input: 285.38 toks/s, output: 19.03 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:34,  1.23s/it, est. speed input: 336.92 toks/s, output: 27.69 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:26,  1.04it/s, est. speed input: 388.67 toks/s, output: 36.94 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:13,  1.89it/s, est. speed input: 526.91 toks/s, output: 58.58 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:10,  2.33it/s, est. speed input: 588.69 toks/s, output: 69.07 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:08,  2.84it/s, est. speed input: 648.05 toks/s, output: 79.56 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:06,  3.25it/s, est. speed input: 700.75 toks/s, output: 89.64 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:04,  4.72it/s, est. speed input: 819.71 toks/s, output: 112.00 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:04,  4.07it/s, est. speed input: 847.60 toks/s, output: 119.81 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:04,  4.41it/s, est. speed input: 892.94 toks/s, output: 130.34 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:02,  5.47it/s, est. speed input: 989.88 toks/s, output: 152.89 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  5.99it/s, est. speed input: 1037.23 toks/s, output: 164.43 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  8.20it/s, est. speed input: 1144.31 toks/s, output: 189.86 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:01,  9.33it/s, est. speed input: 1240.66 toks/s, output: 213.97 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:08<00:00, 10.96it/s, est. speed input: 1339.84 toks/s, output: 239.59 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:08<00:00, 12.54it/s, est. speed input: 1437.71 toks/s, output: 265.79 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  7.59it/s, est. speed input: 1471.34 toks/s, output: 283.58 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:09<00:00,  7.08it/s, est. speed input: 1527.04 toks/s, output: 306.56 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:09<00:00,  7.56it/s, est. speed input: 1595.01 toks/s, output: 334.18 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  5.45it/s, est. speed input: 1579.69 toks/s, output: 340.18 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.13it/s, est. speed input: 1579.69 toks/s, output: 340.18 toks/s]
[2025-01-06 16:32:43,412][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:32:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:01<00:22,  1.86s/it, est. speed input: 361.05 toks/s, output: 12.89 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:02<00:09,  1.16it/s, est. speed input: 612.82 toks/s, output: 26.17 toks/s]Processed prompts: 100%|██████████| 13/13 [00:02<00:00,  6.31it/s, est. speed input: 4029.51 toks/s, output: 181.63 toks/s]
[2025-01-06 16:32:45,900][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:32:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:04<01:26,  4.78s/it, est. speed input: 146.15 toks/s, output: 15.68 toks/s]Processed prompts:  11%|█         | 2/19 [00:05<00:35,  2.12s/it, est. speed input: 277.71 toks/s, output: 30.99 toks/s]Processed prompts:  16%|█▌        | 3/19 [00:05<00:20,  1.30s/it, est. speed input: 391.24 toks/s, output: 45.71 toks/s]Processed prompts:  21%|██        | 4/19 [00:05<00:13,  1.09it/s, est. speed input: 490.33 toks/s, output: 60.15 toks/s]Processed prompts:  32%|███▏      | 6/19 [00:06<00:06,  1.93it/s, est. speed input: 696.46 toks/s, output: 91.00 toks/s]Processed prompts:  37%|███▋      | 7/19 [00:06<00:06,  1.94it/s, est. speed input: 749.34 toks/s, output: 102.61 toks/s]Processed prompts:  47%|████▋     | 9/19 [00:06<00:03,  3.15it/s, est. speed input: 944.55 toks/s, output: 138.28 toks/s]Processed prompts:  58%|█████▊    | 11/19 [00:06<00:01,  4.18it/s, est. speed input: 1115.98 toks/s, output: 171.70 toks/s]Processed prompts:  68%|██████▊   | 13/19 [00:06<00:01,  5.77it/s, est. speed input: 1298.70 toks/s, output: 208.37 toks/s]Processed prompts:  79%|███████▉  | 15/19 [00:07<00:00,  6.49it/s, est. speed input: 1450.17 toks/s, output: 241.76 toks/s]Processed prompts:  89%|████████▉ | 17/19 [00:07<00:00,  4.47it/s, est. speed input: 1491.80 toks/s, output: 262.63 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:08<00:00,  4.22it/s, est. speed input: 1523.27 toks/s, output: 277.49 toks/s]Processed prompts: 100%|██████████| 19/19 [00:08<00:00,  2.30it/s, est. speed input: 1607.87 toks/s, output: 301.70 toks/s]
[2025-01-06 16:32:54,706][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:32:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:01<00:29,  1.95s/it, est. speed input: 489.66 toks/s, output: 7.70 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:04<00:28,  2.05s/it, est. speed input: 406.81 toks/s, output: 21.66 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:04<00:16,  1.31s/it, est. speed input: 523.77 toks/s, output: 38.53 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:04<00:09,  1.20it/s, est. speed input: 664.30 toks/s, output: 56.83 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:04<00:04,  2.07it/s, est. speed input: 862.42 toks/s, output: 90.90 toks/s]Processed prompts:  50%|█████     | 8/16 [00:05<00:02,  2.68it/s, est. speed input: 1013.14 toks/s, output: 123.19 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:05<00:02,  3.22it/s, est. speed input: 1120.15 toks/s, output: 142.38 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:05<00:01,  3.50it/s, est. speed input: 1201.41 toks/s, output: 159.38 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:06<00:01,  3.26it/s, est. speed input: 1244.16 toks/s, output: 173.16 toks/s]Processed prompts:  81%|████████▏ | 13/16 [00:06<00:00,  3.79it/s, est. speed input: 1379.98 toks/s, output: 208.90 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:06<00:00,  3.80it/s, est. speed input: 1430.22 toks/s, output: 226.60 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:07<00:00,  3.15it/s, est. speed input: 1467.43 toks/s, output: 239.25 toks/s]Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.21it/s, est. speed input: 1590.41 toks/s, output: 266.89 toks/s]
[2025-01-06 16:33:02,444][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:33:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s, est. speed input: 1018.86 toks/s, output: 32.71 toks/s]Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s, est. speed input: 1735.55 toks/s, output: 65.18 toks/s]
[2025-01-06 16:33:10,211][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:33:17,615][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:33:17,616][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.24 GB
[2025-01-06 16:33:17,987][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.88 GB
[2025-01-06 16:33:30,304][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:33:30,306][root][INFO] - Before destroying HF.: GPU memory allocated: 15.88 GB
[2025-01-06 16:33:30,720][root][INFO] - After destroying HF.: GPU memory allocated: 0.29 GB
[2025-01-06 16:33:30,867][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:33:38,424][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:33:53,740][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:33:53,741][root][INFO] - Iteration 30 took 2m 3s. Generation: 64.57%, Training: 35.43%. Estimated time remaining: 3h 22m 53s. Estimated total time for complete run: 4h 23m 1s.
[2025-01-06 16:33:54,147][root][INFO] - Loading VLLM model.
WARNING 01-06 16:33:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:33:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:33:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:33:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:33:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:34:13 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:34:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:34:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:34:36 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:34:36,047][root][INFO] - Before destroying HF.: GPU memory allocated: 71.26 GB
[2025-01-06 16:34:36,323][root][INFO] - After destroying HF.: GPU memory allocated: 55.66 GB
[2025-01-06 16:34:36,324][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:34:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:54,  3.68s/it, est. speed input: 137.22 toks/s, output: 6.25 toks/s]Processed prompts:   6%|▋         | 2/32 [00:03<00:49,  1.66s/it, est. speed input: 257.65 toks/s, output: 12.75 toks/s]Processed prompts:   9%|▉         | 3/32 [00:04<00:29,  1.03s/it, est. speed input: 359.71 toks/s, output: 19.47 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:21,  1.28it/s, est. speed input: 469.18 toks/s, output: 31.22 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:18,  1.44it/s, est. speed input: 516.51 toks/s, output: 39.21 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:13,  1.86it/s, est. speed input: 586.78 toks/s, output: 48.97 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:11,  2.06it/s, est. speed input: 633.01 toks/s, output: 57.50 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:10,  2.25it/s, est. speed input: 675.21 toks/s, output: 66.26 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:09,  2.42it/s, est. speed input: 714.40 toks/s, output: 75.26 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:07,  2.89it/s, est. speed input: 765.41 toks/s, output: 85.70 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  4.78it/s, est. speed input: 934.78 toks/s, output: 119.26 toks/s]Processed prompts:  50%|█████     | 16/32 [00:07<00:02,  6.06it/s, est. speed input: 1045.60 toks/s, output: 142.61 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:07<00:02,  6.46it/s, est. speed input: 1094.80 toks/s, output: 153.79 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  6.22it/s, est. speed input: 1133.03 toks/s, output: 163.91 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:02,  5.52it/s, est. speed input: 1160.39 toks/s, output: 173.06 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:02,  4.72it/s, est. speed input: 1178.43 toks/s, output: 181.55 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:08<00:02,  4.45it/s, est. speed input: 1200.88 toks/s, output: 191.26 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:09<00:02,  4.17it/s, est. speed input: 1219.47 toks/s, output: 200.98 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:09<00:02,  4.19it/s, est. speed input: 1242.79 toks/s, output: 211.96 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:01,  6.16it/s, est. speed input: 1331.45 toks/s, output: 241.19 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:01,  5.57it/s, est. speed input: 1351.57 toks/s, output: 252.30 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:09<00:00,  6.09it/s, est. speed input: 1386.33 toks/s, output: 266.39 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  5.39it/s, est. speed input: 1449.87 toks/s, output: 302.32 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.06it/s, est. speed input: 1546.50 toks/s, output: 340.59 toks/s]
[2025-01-06 16:34:47,208][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:34:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:02<00:38,  2.44s/it, est. speed input: 277.93 toks/s, output: 11.08 toks/s]Processed prompts: 100%|██████████| 17/17 [00:02<00:00,  8.89it/s, est. speed input: 4195.02 toks/s, output: 189.67 toks/s]Processed prompts: 100%|██████████| 17/17 [00:02<00:00,  6.51it/s, est. speed input: 4195.02 toks/s, output: 189.67 toks/s]
[2025-01-06 16:34:50,236][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:34:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:03<00:51,  3.66s/it, est. speed input: 190.91 toks/s, output: 18.03 toks/s]Processed prompts:  13%|█▎        | 2/15 [00:04<00:23,  1.80s/it, est. speed input: 336.23 toks/s, output: 35.11 toks/s]Processed prompts:  20%|██        | 3/15 [00:04<00:13,  1.09s/it, est. speed input: 477.13 toks/s, output: 53.01 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:04<00:05,  1.87it/s, est. speed input: 756.64 toks/s, output: 90.28 toks/s]Processed prompts:  40%|████      | 6/15 [00:04<00:03,  2.32it/s, est. speed input: 874.34 toks/s, output: 107.78 toks/s]Processed prompts:  47%|████▋     | 7/15 [00:05<00:03,  2.28it/s, est. speed input: 931.98 toks/s, output: 120.57 toks/s]Processed prompts:  53%|█████▎    | 8/15 [00:05<00:02,  2.58it/s, est. speed input: 1013.36 toks/s, output: 137.54 toks/s]Processed prompts:  60%|██████    | 9/15 [00:05<00:02,  2.62it/s, est. speed input: 1069.52 toks/s, output: 152.84 toks/s]Processed prompts:  67%|██████▋   | 10/15 [00:06<00:01,  3.07it/s, est. speed input: 1150.44 toks/s, output: 172.32 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:07<00:02,  1.90it/s, est. speed input: 1087.26 toks/s, output: 175.20 toks/s]Processed prompts:  80%|████████  | 12/15 [00:07<00:01,  2.38it/s, est. speed input: 1157.68 toks/s, output: 198.60 toks/s]Processed prompts: 100%|██████████| 15/15 [00:07<00:00,  2.07it/s, est. speed input: 1447.05 toks/s, output: 281.40 toks/s]
[2025-01-06 16:34:57,984][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:34:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:08,  3.27s/it, est. speed input: 291.26 toks/s, output: 8.86 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:04<00:16,  1.07it/s, est. speed input: 805.58 toks/s, output: 32.18 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:04<00:14,  1.21it/s, est. speed input: 857.85 toks/s, output: 42.50 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:05<00:10,  1.51it/s, est. speed input: 951.23 toks/s, output: 54.84 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:05<00:08,  1.69it/s, est. speed input: 1004.29 toks/s, output: 65.99 toks/s]Processed prompts:  41%|████      | 9/22 [00:05<00:04,  2.68it/s, est. speed input: 1205.51 toks/s, output: 94.23 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:06<00:03,  3.02it/s, est. speed input: 1280.09 toks/s, output: 107.20 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:06<00:01,  4.90it/s, est. speed input: 1601.21 toks/s, output: 151.61 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:06<00:01,  5.22it/s, est. speed input: 1674.26 toks/s, output: 165.46 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:06<00:00,  6.82it/s, est. speed input: 1852.75 toks/s, output: 196.88 toks/s]Processed prompts:  77%|███████▋  | 17/22 [00:06<00:00,  6.40it/s, est. speed input: 1902.79 toks/s, output: 209.52 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:08<00:01,  2.28it/s, est. speed input: 1662.09 toks/s, output: 196.33 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:08<00:01,  2.76it/s, est. speed input: 1720.13 toks/s, output: 216.34 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:08<00:00,  3.22it/s, est. speed input: 1770.07 toks/s, output: 235.84 toks/s]Processed prompts: 100%|██████████| 22/22 [00:08<00:00,  2.59it/s, est. speed input: 1934.88 toks/s, output: 283.00 toks/s]
[2025-01-06 16:35:07,011][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:35:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it, est. speed input: 801.66 toks/s, output: 24.37 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.99it/s, est. speed input: 2257.21 toks/s, output: 88.43 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.51it/s, est. speed input: 2257.21 toks/s, output: 88.43 toks/s]
[2025-01-06 16:35:15,622][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:35:22,927][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:35:22,928][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.25 GB
[2025-01-06 16:35:23,333][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:35:35,998][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:35:36,000][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:35:36,344][root][INFO] - After destroying HF.: GPU memory allocated: 0.29 GB
[2025-01-06 16:35:36,507][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 16:35:43,989][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:35:59,775][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:35:59,776][root][INFO] - Iteration 31 took 2m 6s. Generation: 64.85%, Training: 35.15%. Estimated time remaining: 3h 26m 38s. Estimated total time for complete run: 4h 28m 52s.
[2025-01-06 16:36:00,189][root][INFO] - Loading VLLM model.
WARNING 01-06 16:36:00 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:36:00 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:36:00 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:36:00 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:36:05 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:36:19 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:36:20 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:36:20 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:36:41 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:36:41,591][root][INFO] - Before destroying HF.: GPU memory allocated: 71.26 GB
[2025-01-06 16:36:41,844][root][INFO] - After destroying HF.: GPU memory allocated: 55.66 GB
[2025-01-06 16:36:41,845][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:36:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 120.83 toks/s, output: 5.50 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:53,  1.79s/it, est. speed input: 234.90 toks/s, output: 11.16 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:49,  1.72s/it, est. speed input: 255.45 toks/s, output: 17.03 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:36,  1.31s/it, est. speed input: 305.26 toks/s, output: 25.09 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:23,  1.14it/s, est. speed input: 375.25 toks/s, output: 34.63 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.58it/s, est. speed input: 439.72 toks/s, output: 43.97 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:09,  2.59it/s, est. speed input: 565.16 toks/s, output: 62.81 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:07,  3.21it/s, est. speed input: 626.99 toks/s, output: 72.56 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:06,  3.27it/s, est. speed input: 669.90 toks/s, output: 80.79 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:06,  3.50it/s, est. speed input: 714.53 toks/s, output: 89.65 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:08<00:02,  5.93it/s, est. speed input: 929.01 toks/s, output: 130.25 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  6.59it/s, est. speed input: 1024.40 toks/s, output: 150.59 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.32it/s, est. speed input: 1158.14 toks/s, output: 181.63 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:01,  5.66it/s, est. speed input: 1164.00 toks/s, output: 187.58 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:09<00:01,  5.99it/s, est. speed input: 1203.04 toks/s, output: 199.03 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:01,  6.72it/s, est. speed input: 1280.70 toks/s, output: 222.64 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  6.82it/s, est. speed input: 1346.93 toks/s, output: 245.38 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:10<00:01,  4.20it/s, est. speed input: 1314.50 toks/s, output: 247.38 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00,  5.10it/s, est. speed input: 1380.09 toks/s, output: 275.83 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  5.42it/s, est. speed input: 1409.30 toks/s, output: 290.14 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  5.68it/s, est. speed input: 1436.57 toks/s, output: 304.56 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  2.94it/s, est. speed input: 1482.89 toks/s, output: 322.91 toks/s]
[2025-01-06 16:36:53,201][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:36:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:48,  2.57s/it, est. speed input: 246.45 toks/s, output: 8.95 toks/s]Processed prompts:  10%|█         | 2/20 [00:02<00:20,  1.16s/it, est. speed input: 476.33 toks/s, output: 18.24 toks/s]Processed prompts:  90%|█████████ | 18/20 [00:03<00:00, 10.48it/s, est. speed input: 3906.75 toks/s, output: 174.17 toks/s]Processed prompts: 100%|██████████| 20/20 [00:03<00:00,  5.48it/s, est. speed input: 3616.88 toks/s, output: 176.33 toks/s]
[2025-01-06 16:36:57,261][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:36:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:03<00:40,  3.71s/it, est. speed input: 188.56 toks/s, output: 22.12 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:04<00:20,  2.00s/it, est. speed input: 309.36 toks/s, output: 42.04 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:04<00:06,  1.21it/s, est. speed input: 585.36 toks/s, output: 87.30 toks/s]Processed prompts:  50%|█████     | 6/12 [00:05<00:03,  1.94it/s, est. speed input: 820.03 toks/s, output: 129.83 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:05<00:02,  2.09it/s, est. speed input: 893.27 toks/s, output: 147.69 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:05<00:01,  2.62it/s, est. speed input: 1000.14 toks/s, output: 171.52 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:05<00:00,  3.03it/s, est. speed input: 1087.30 toks/s, output: 193.23 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:06<00:00,  2.14it/s, est. speed input: 1058.85 toks/s, output: 199.65 toks/s]Processed prompts: 100%|██████████| 12/12 [00:06<00:00,  1.82it/s, est. speed input: 1270.59 toks/s, output: 260.24 toks/s]
[2025-01-06 16:37:04,327][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:37:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:18,  3.42s/it, est. speed input: 279.08 toks/s, output: 8.48 toks/s]Processed prompts:   8%|▊         | 2/24 [00:05<00:53,  2.41s/it, est. speed input: 285.83 toks/s, output: 18.16 toks/s]Processed prompts:  21%|██        | 5/24 [00:06<00:17,  1.09it/s, est. speed input: 552.06 toks/s, output: 50.75 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:06<00:13,  1.35it/s, est. speed input: 643.52 toks/s, output: 63.39 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:06<00:10,  1.67it/s, est. speed input: 729.73 toks/s, output: 76.03 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:06<00:04,  3.19it/s, est. speed input: 1048.49 toks/s, output: 117.77 toks/s]Processed prompts:  46%|████▌     | 11/24 [00:06<00:03,  3.41it/s, est. speed input: 1116.87 toks/s, output: 129.56 toks/s]Processed prompts:  50%|█████     | 12/24 [00:07<00:03,  3.87it/s, est. speed input: 1194.18 toks/s, output: 142.75 toks/s]Processed prompts:  62%|██████▎   | 15/24 [00:07<00:01,  5.55it/s, est. speed input: 1486.82 toks/s, output: 183.63 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:07<00:01,  5.78it/s, est. speed input: 1551.81 toks/s, output: 196.94 toks/s]Processed prompts:  71%|███████   | 17/24 [00:07<00:01,  6.07it/s, est. speed input: 1615.95 toks/s, output: 210.61 toks/s]Processed prompts:  83%|████████▎ | 20/24 [00:07<00:00,  9.21it/s, est. speed input: 1855.85 toks/s, output: 258.39 toks/s]Processed prompts:  92%|█████████▏| 22/24 [00:08<00:00,  8.65it/s, est. speed input: 1968.99 toks/s, output: 285.86 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  4.78it/s, est. speed input: 1940.85 toks/s, output: 298.54 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  2.70it/s, est. speed input: 1940.85 toks/s, output: 298.54 toks/s]
[2025-01-06 16:37:13,801][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:37:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:01<00:03,  1.11s/it, est. speed input: 551.13 toks/s, output: 26.03 toks/s]Processed prompts:  75%|███████▌  | 3/4 [00:01<00:00,  1.70it/s, est. speed input: 1070.43 toks/s, output: 68.14 toks/s]Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s, est. speed input: 1199.80 toks/s, output: 95.20 toks/s]Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s, est. speed input: 1199.80 toks/s, output: 95.20 toks/s]
[2025-01-06 16:37:23,343][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:37:30,681][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:37:30,682][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.25 GB
[2025-01-06 16:37:31,044][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:37:43,858][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:37:43,859][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:37:44,226][root][INFO] - After destroying HF.: GPU memory allocated: 0.29 GB
[2025-01-06 16:37:44,374][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:37:51,723][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:38:07,204][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:38:07,205][root][INFO] - Iteration 32 took 2m 7s. Generation: 65.46%, Training: 34.54%. Estimated time remaining: 3h 27m 30s. Estimated total time for complete run: 4h 31m 51s.
[2025-01-06 16:38:07,531][root][INFO] - Loading VLLM model.
WARNING 01-06 16:38:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:38:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:38:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:38:08 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:38:13 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:38:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:38:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:38:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:38:49 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:38:49,182][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:38:49,429][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:38:49,430][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:38:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:50,  3.57s/it, est. speed input: 141.31 toks/s, output: 7.00 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:52,  1.75s/it, est. speed input: 249.21 toks/s, output: 14.31 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:43,  1.48s/it, est. speed input: 290.33 toks/s, output: 21.27 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:30,  1.10s/it, est. speed input: 352.36 toks/s, output: 30.18 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:25,  1.06it/s, est. speed input: 394.53 toks/s, output: 38.59 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:12,  1.93it/s, est. speed input: 534.60 toks/s, output: 60.49 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:10,  2.37it/s, est. speed input: 597.11 toks/s, output: 71.09 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:06,  3.09it/s, est. speed input: 743.03 toks/s, output: 99.11 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:07,  2.85it/s, est. speed input: 763.93 toks/s, output: 106.77 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:08<00:05,  3.32it/s, est. speed input: 845.27 toks/s, output: 127.93 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:03,  4.18it/s, est. speed input: 938.80 toks/s, output: 151.97 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:03,  4.36it/s, est. speed input: 976.01 toks/s, output: 163.14 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:08<00:01,  7.11it/s, est. speed input: 1134.50 toks/s, output: 204.77 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:09<00:01,  7.55it/s, est. speed input: 1216.95 toks/s, output: 229.59 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:01,  6.04it/s, est. speed input: 1260.63 toks/s, output: 248.80 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  6.62it/s, est. speed input: 1333.12 toks/s, output: 275.25 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00,  8.96it/s, est. speed input: 1464.35 toks/s, output: 320.77 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  9.54it/s, est. speed input: 1538.76 toks/s, output: 349.92 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.14it/s, est. speed input: 1585.73 toks/s, output: 366.99 toks/s]
[2025-01-06 16:39:00,080][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:39:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:02<00:37,  2.36s/it, est. speed input: 271.18 toks/s, output: 10.59 toks/s]Processed prompts:  12%|█▏        | 2/17 [00:02<00:15,  1.04s/it, est. speed input: 551.34 toks/s, output: 21.42 toks/s]Processed prompts: 100%|██████████| 17/17 [00:02<00:00,  6.77it/s, est. speed input: 4400.37 toks/s, output: 194.35 toks/s]
[2025-01-06 16:39:03,000][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:39:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:03<00:54,  3.87s/it, est. speed input: 180.68 toks/s, output: 18.61 toks/s]Processed prompts:  20%|██        | 3/15 [00:03<00:12,  1.04s/it, est. speed input: 528.10 toks/s, output: 55.15 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:05<00:08,  1.19it/s, est. speed input: 663.19 toks/s, output: 80.08 toks/s]Processed prompts:  40%|████      | 6/15 [00:05<00:06,  1.46it/s, est. speed input: 757.57 toks/s, output: 98.80 toks/s]Processed prompts:  60%|██████    | 9/15 [00:05<00:02,  2.63it/s, est. speed input: 1070.11 toks/s, output: 159.72 toks/s]Processed prompts:  67%|██████▋   | 10/15 [00:06<00:01,  2.56it/s, est. speed input: 1107.02 toks/s, output: 173.42 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:07<00:02,  1.90it/s, est. speed input: 1051.83 toks/s, output: 177.15 toks/s]Processed prompts: 100%|██████████| 15/15 [00:07<00:00,  2.05it/s, est. speed input: 1434.27 toks/s, output: 286.58 toks/s]
[2025-01-06 16:39:10,771][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:39:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:20,  3.50s/it, est. speed input: 272.83 toks/s, output: 8.29 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:04<00:23,  1.10s/it, est. speed input: 696.13 toks/s, output: 24.62 toks/s]Processed prompts:  17%|█▋        | 4/24 [00:04<00:16,  1.20it/s, est. speed input: 761.76 toks/s, output: 33.61 toks/s]Processed prompts:  21%|██        | 5/24 [00:05<00:16,  1.16it/s, est. speed input: 793.22 toks/s, output: 40.54 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:05<00:13,  1.30it/s, est. speed input: 857.57 toks/s, output: 50.48 toks/s]Processed prompts:  33%|███▎      | 8/24 [00:06<00:07,  2.13it/s, est. speed input: 1087.34 toks/s, output: 75.70 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:06<00:04,  2.95it/s, est. speed input: 1226.01 toks/s, output: 100.90 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:06<00:02,  4.13it/s, est. speed input: 1499.81 toks/s, output: 139.19 toks/s]Processed prompts:  58%|█████▊    | 14/24 [00:07<00:02,  4.30it/s, est. speed input: 1559.40 toks/s, output: 151.73 toks/s]Processed prompts:  62%|██████▎   | 15/24 [00:07<00:02,  4.18it/s, est. speed input: 1598.44 toks/s, output: 163.09 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:07<00:01,  4.47it/s, est. speed input: 1655.88 toks/s, output: 176.70 toks/s]Processed prompts:  75%|███████▌  | 18/24 [00:07<00:01,  5.67it/s, est. speed input: 1793.60 toks/s, output: 206.67 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:07<00:00,  5.71it/s, est. speed input: 1844.06 toks/s, output: 220.60 toks/s]Processed prompts:  83%|████████▎ | 20/24 [00:08<00:01,  3.72it/s, est. speed input: 1802.84 toks/s, output: 225.85 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:08<00:00,  4.39it/s, est. speed input: 1862.12 toks/s, output: 243.45 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:08<00:00,  4.28it/s, est. speed input: 1917.70 toks/s, output: 272.24 toks/s]Processed prompts: 100%|██████████| 24/24 [00:08<00:00,  2.67it/s, est. speed input: 1995.53 toks/s, output: 294.51 toks/s]
[2025-01-06 16:39:20,327][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:39:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:01<00:05,  1.28s/it, est. speed input: 698.93 toks/s, output: 22.60 toks/s]Processed prompts:  60%|██████    | 3/5 [00:02<00:01,  1.18it/s, est. speed input: 917.22 toks/s, output: 58.47 toks/s]Processed prompts:  80%|████████  | 4/5 [00:02<00:00,  1.56it/s, est. speed input: 1137.93 toks/s, output: 91.61 toks/s]Processed prompts: 100%|██████████| 5/5 [00:04<00:00,  1.09it/s, est. speed input: 972.52 toks/s, output: 106.27 toks/s]Processed prompts: 100%|██████████| 5/5 [00:04<00:00,  1.14it/s, est. speed input: 972.52 toks/s, output: 106.27 toks/s]
[2025-01-06 16:39:31,809][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:39:39,032][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:39:39,032][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:39:39,400][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:39:52,119][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:39:52,120][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:39:52,502][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:39:52,650][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:40:00,101][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:40:16,174][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:40:16,175][root][INFO] - Iteration 33 took 2m 8s. Generation: 65.48%, Training: 34.52%. Estimated time remaining: 3h 28m 38s. Estimated total time for complete run: 4h 35m 8s.
[2025-01-06 16:40:16,508][root][INFO] - Loading VLLM model.
WARNING 01-06 16:40:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:40:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:40:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:40:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 16:40:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:40:36 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:40:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:40:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:40:58 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:40:58,332][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:40:58,610][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:40:58,611][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:40:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:59,  3.87s/it, est. speed input: 130.66 toks/s, output: 6.47 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:56,  1.87s/it, est. speed input: 232.50 toks/s, output: 13.35 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:44,  1.55s/it, est. speed input: 275.02 toks/s, output: 20.15 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:31,  1.14s/it, est. speed input: 335.37 toks/s, output: 28.72 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:20,  1.30it/s, est. speed input: 411.58 toks/s, output: 38.63 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.62it/s, est. speed input: 469.16 toks/s, output: 47.54 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:11,  2.22it/s, est. speed input: 538.55 toks/s, output: 57.74 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:06<00:06,  3.71it/s, est. speed input: 676.90 toks/s, output: 78.34 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:08,  2.68it/s, est. speed input: 683.43 toks/s, output: 83.23 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:07<00:06,  3.21it/s, est. speed input: 737.62 toks/s, output: 93.88 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:05,  3.47it/s, est. speed input: 780.99 toks/s, output: 103.62 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.03it/s, est. speed input: 891.36 toks/s, output: 126.33 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:04,  3.59it/s, est. speed input: 924.42 toks/s, output: 140.15 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:03,  4.12it/s, est. speed input: 969.54 toks/s, output: 152.35 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:08<00:02,  4.73it/s, est. speed input: 1014.10 toks/s, output: 164.67 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:09<00:02,  5.15it/s, est. speed input: 1085.89 toks/s, output: 187.18 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:09<00:01,  6.32it/s, est. speed input: 1170.38 toks/s, output: 213.22 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:01,  5.82it/s, est. speed input: 1225.72 toks/s, output: 235.13 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:01,  6.29it/s, est. speed input: 1263.08 toks/s, output: 248.71 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00,  9.24it/s, est. speed input: 1394.56 toks/s, output: 293.90 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  5.67it/s, est. speed input: 1402.20 toks/s, output: 310.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  2.96it/s, est. speed input: 1495.65 toks/s, output: 347.53 toks/s]
[2025-01-06 16:41:09,861][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:41:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:06,  3.04s/it, est. speed input: 207.74 toks/s, output: 8.55 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:03<00:16,  1.19it/s, est. speed input: 590.79 toks/s, output: 26.07 toks/s]Processed prompts: 100%|██████████| 23/23 [00:03<00:00,  7.14it/s, est. speed input: 4718.22 toks/s, output: 207.01 toks/s]
[2025-01-06 16:41:13,507][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:41:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:02<00:20,  2.62s/it, est. speed input: 266.48 toks/s, output: 24.40 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:03<00:11,  1.66s/it, est. speed input: 386.80 toks/s, output: 45.65 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:04<00:08,  1.40s/it, est. speed input: 445.71 toks/s, output: 65.46 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:05<00:03,  1.27it/s, est. speed input: 653.80 toks/s, output: 117.10 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:05<00:02,  1.35it/s, est. speed input: 702.22 toks/s, output: 138.30 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  1.51it/s, est. speed input: 1053.29 toks/s, output: 238.75 toks/s]
[2025-01-06 16:41:19,913][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:41:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:03<01:46,  3.95s/it, est. speed input: 241.76 toks/s, output: 7.35 toks/s]Processed prompts:   7%|▋         | 2/28 [00:05<01:07,  2.61s/it, est. speed input: 309.52 toks/s, output: 15.81 toks/s]Processed prompts:  11%|█         | 3/28 [00:05<00:39,  1.57s/it, est. speed input: 410.46 toks/s, output: 26.06 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:06<00:27,  1.15s/it, est. speed input: 525.26 toks/s, output: 35.74 toks/s]Processed prompts:  21%|██▏       | 6/28 [00:06<00:14,  1.55it/s, est. speed input: 773.48 toks/s, output: 57.18 toks/s]Processed prompts:  25%|██▌       | 7/28 [00:07<00:10,  1.92it/s, est. speed input: 851.79 toks/s, output: 68.12 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:07<00:09,  2.15it/s, est. speed input: 909.36 toks/s, output: 78.03 toks/s]Processed prompts:  32%|███▏      | 9/28 [00:08<00:09,  1.91it/s, est. speed input: 920.93 toks/s, output: 85.25 toks/s]Processed prompts:  36%|███▌      | 10/28 [00:08<00:09,  1.92it/s, est. speed input: 946.54 toks/s, output: 94.31 toks/s]Processed prompts:  43%|████▎     | 12/28 [00:08<00:05,  3.01it/s, est. speed input: 1083.78 toks/s, output: 120.74 toks/s]Processed prompts:  54%|█████▎    | 15/28 [00:08<00:02,  5.04it/s, est. speed input: 1275.03 toks/s, output: 161.77 toks/s]Processed prompts:  61%|██████    | 17/28 [00:09<00:02,  4.36it/s, est. speed input: 1342.40 toks/s, output: 181.39 toks/s]Processed prompts:  68%|██████▊   | 19/28 [00:10<00:02,  4.31it/s, est. speed input: 1418.32 toks/s, output: 204.44 toks/s]Processed prompts:  71%|███████▏  | 20/28 [00:10<00:02,  3.54it/s, est. speed input: 1416.03 toks/s, output: 212.02 toks/s]Processed prompts:  75%|███████▌  | 21/28 [00:10<00:01,  3.65it/s, est. speed input: 1449.21 toks/s, output: 225.20 toks/s]Processed prompts:  79%|███████▊  | 22/28 [00:10<00:01,  4.04it/s, est. speed input: 1492.45 toks/s, output: 240.20 toks/s]Processed prompts: 100%|██████████| 28/28 [00:10<00:00,  2.56it/s, est. speed input: 1872.25 toks/s, output: 349.29 toks/s]
[2025-01-06 16:41:31,446][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:41:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:07,  1.45s/it, est. speed input: 656.64 toks/s, output: 19.96 toks/s]Processed prompts:  50%|█████     | 3/6 [00:02<00:02,  1.41it/s, est. speed input: 1219.20 toks/s, output: 55.40 toks/s]Processed prompts:  83%|████████▎ | 5/6 [00:03<00:00,  1.86it/s, est. speed input: 1492.10 toks/s, output: 101.10 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  2.08it/s, est. speed input: 1631.60 toks/s, output: 129.64 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  1.75it/s, est. speed input: 1631.60 toks/s, output: 129.64 toks/s]
[2025-01-06 16:41:42,012][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:41:50,361][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:41:50,361][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:41:50,718][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:42:03,792][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:42:03,793][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:42:04,148][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:42:04,296][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:42:11,871][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:42:27,998][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:42:27,999][root][INFO] - Iteration 34 took 2m 11s. Generation: 65.01%, Training: 34.99%. Estimated time remaining: 3h 32m 31s. Estimated total time for complete run: 4h 41m 13s.
[2025-01-06 16:42:28,317][root][INFO] - Loading VLLM model.
WARNING 01-06 16:42:28 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:42:28 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:42:29 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:42:29 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:42:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:42:47 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:42:48 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:42:48 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:43:09 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:43:09,799][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:43:10,089][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:43:10,090][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:43:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:10,  4.19s/it, est. speed input: 120.40 toks/s, output: 5.96 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:19,  2.64s/it, est. speed input: 175.64 toks/s, output: 13.22 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:43,  1.49s/it, est. speed input: 258.21 toks/s, output: 21.99 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:30,  1.10s/it, est. speed input: 316.51 toks/s, output: 29.93 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:26,  1.03it/s, est. speed input: 355.36 toks/s, output: 37.44 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:19,  1.30it/s, est. speed input: 404.93 toks/s, output: 46.51 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:13,  1.82it/s, est. speed input: 465.84 toks/s, output: 56.93 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:07,  3.11it/s, est. speed input: 587.16 toks/s, output: 78.03 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:06,  3.17it/s, est. speed input: 664.79 toks/s, output: 94.90 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:05,  3.63it/s, est. speed input: 713.50 toks/s, output: 105.49 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:08<00:03,  4.77it/s, est. speed input: 811.83 toks/s, output: 127.11 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  7.27it/s, est. speed input: 968.54 toks/s, output: 162.01 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:01,  8.14it/s, est. speed input: 1061.12 toks/s, output: 184.36 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:09<00:01,  9.28it/s, est. speed input: 1195.19 toks/s, output: 218.27 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:09<00:00,  8.53it/s, est. speed input: 1265.06 toks/s, output: 239.23 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:09<00:00,  7.58it/s, est. speed input: 1323.60 toks/s, output: 259.88 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00,  8.82it/s, est. speed input: 1440.38 toks/s, output: 299.09 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  7.28it/s, est. speed input: 1480.24 toks/s, output: 319.97 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  6.45it/s, est. speed input: 1492.81 toks/s, output: 330.71 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  2.96it/s, est. speed input: 1492.81 toks/s, output: 330.71 toks/s]
[2025-01-06 16:43:21,351][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:43:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:02<00:47,  2.63s/it, est. speed input: 258.41 toks/s, output: 10.28 toks/s]Processed prompts:  84%|████████▍ | 16/19 [00:02<00:00,  8.08it/s, est. speed input: 3778.24 toks/s, output: 168.78 toks/s]Processed prompts: 100%|██████████| 19/19 [00:03<00:00,  4.90it/s, est. speed input: 3161.13 toks/s, output: 160.38 toks/s]
[2025-01-06 16:43:25,682][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:43:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:03<00:40,  3.40s/it, est. speed input: 205.52 toks/s, output: 19.99 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:04<00:21,  1.98s/it, est. speed input: 318.78 toks/s, output: 37.85 toks/s]Processed prompts:  31%|███       | 4/13 [00:05<00:09,  1.02s/it, est. speed input: 530.08 toks/s, output: 74.51 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:05<00:06,  1.29it/s, est. speed input: 635.29 toks/s, output: 96.16 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:05<00:01,  2.67it/s, est. speed input: 976.79 toks/s, output: 165.77 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:06<00:01,  2.65it/s, est. speed input: 1029.58 toks/s, output: 181.82 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:06<00:01,  2.82it/s, est. speed input: 1093.64 toks/s, output: 201.20 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:06<00:00,  3.02it/s, est. speed input: 1156.21 toks/s, output: 221.65 toks/s]Processed prompts:  92%|█████████▏| 12/13 [00:06<00:00,  3.31it/s, est. speed input: 1220.78 toks/s, output: 243.63 toks/s]Processed prompts: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s, est. speed input: 1322.49 toks/s, output: 272.73 toks/s]
[2025-01-06 16:43:33,031][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:43:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:12,  3.46s/it, est. speed input: 201.76 toks/s, output: 9.81 toks/s]Processed prompts:   9%|▉         | 2/22 [00:03<00:31,  1.56s/it, est. speed input: 429.74 toks/s, output: 19.75 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:25,  1.32s/it, est. speed input: 484.70 toks/s, output: 28.61 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:05<00:16,  1.07it/s, est. speed input: 639.74 toks/s, output: 40.46 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:05<00:11,  1.49it/s, est. speed input: 795.20 toks/s, output: 53.08 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:06<00:11,  1.44it/s, est. speed input: 814.48 toks/s, output: 62.08 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:06<00:07,  1.94it/s, est. speed input: 907.75 toks/s, output: 76.28 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:06<00:06,  2.31it/s, est. speed input: 980.47 toks/s, output: 89.45 toks/s]Processed prompts:  41%|████      | 9/22 [00:06<00:05,  2.38it/s, est. speed input: 1026.88 toks/s, output: 101.20 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:07<00:04,  2.60it/s, est. speed input: 1081.09 toks/s, output: 114.29 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:07<00:02,  4.27it/s, est. speed input: 1255.17 toks/s, output: 147.23 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:07<00:02,  3.68it/s, est. speed input: 1283.55 toks/s, output: 158.30 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:07<00:01,  5.34it/s, est. speed input: 1440.87 toks/s, output: 192.72 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:08<00:01,  4.76it/s, est. speed input: 1476.47 toks/s, output: 205.37 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:08<00:00,  4.17it/s, est. speed input: 1540.82 toks/s, output: 231.44 toks/s]Processed prompts:  86%|████████▋ | 19/22 [00:08<00:00,  4.09it/s, est. speed input: 1574.43 toks/s, output: 246.51 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:09<00:00,  4.66it/s, est. speed input: 1631.08 toks/s, output: 265.44 toks/s]Processed prompts: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, est. speed input: 1786.38 toks/s, output: 309.88 toks/s]
[2025-01-06 16:43:42,568][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:43:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:09,  1.58s/it, est. speed input: 536.95 toks/s, output: 18.32 toks/s]Processed prompts:  43%|████▎     | 3/7 [00:02<00:02,  1.75it/s, est. speed input: 1233.77 toks/s, output: 52.61 toks/s]Processed prompts:  57%|█████▋    | 4/7 [00:02<00:01,  1.71it/s, est. speed input: 1310.64 toks/s, output: 69.34 toks/s]Processed prompts:  71%|███████▏  | 5/7 [00:02<00:00,  2.13it/s, est. speed input: 1533.26 toks/s, output: 94.21 toks/s]Processed prompts:  86%|████████▌ | 6/7 [00:03<00:00,  2.06it/s, est. speed input: 1540.89 toks/s, output: 114.18 toks/s]Processed prompts: 100%|██████████| 7/7 [00:03<00:00,  2.43it/s, est. speed input: 1684.71 toks/s, output: 142.34 toks/s]Processed prompts: 100%|██████████| 7/7 [00:03<00:00,  1.93it/s, est. speed input: 1684.71 toks/s, output: 142.34 toks/s]
[2025-01-06 16:43:53,731][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:44:00,954][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:44:00,955][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:44:01,327][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:44:14,029][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:44:14,030][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:44:14,371][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:44:14,535][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:44:21,942][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:44:37,904][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:44:37,905][root][INFO] - Iteration 35 took 2m 9s. Generation: 65.89%, Training: 34.11%. Estimated time remaining: 3h 26m 16s. Estimated total time for complete run: 4h 37m 7s.
[2025-01-06 16:44:38,229][root][INFO] - Loading VLLM model.
WARNING 01-06 16:44:38 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:44:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:44:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:44:39 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:44:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:44:57 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:44:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:44:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:45:19 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:45:19,522][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:45:19,796][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:45:19,797][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:45:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:57,  3.78s/it, est. speed input: 133.65 toks/s, output: 6.62 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:01,  2.05s/it, est. speed input: 218.76 toks/s, output: 13.86 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:47,  1.65s/it, est. speed input: 262.02 toks/s, output: 21.27 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:32,  1.18s/it, est. speed input: 323.75 toks/s, output: 30.45 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:21,  1.26it/s, est. speed input: 397.58 toks/s, output: 40.78 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:20,  1.25it/s, est. speed input: 423.28 toks/s, output: 47.92 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:13,  1.76it/s, est. speed input: 516.34 toks/s, output: 67.10 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:10,  2.24it/s, est. speed input: 573.55 toks/s, output: 78.74 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:07,  2.97it/s, est. speed input: 668.93 toks/s, output: 100.19 toks/s]Processed prompts:  41%|████      | 13/32 [00:08<00:04,  4.11it/s, est. speed input: 773.78 toks/s, output: 123.99 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:08<00:04,  4.38it/s, est. speed input: 816.71 toks/s, output: 134.81 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:08<00:04,  4.12it/s, est. speed input: 846.66 toks/s, output: 144.07 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:03,  4.85it/s, est. speed input: 928.18 toks/s, output: 166.61 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:02,  5.11it/s, est. speed input: 999.18 toks/s, output: 188.69 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:09<00:02,  5.26it/s, est. speed input: 1033.63 toks/s, output: 200.28 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:02,  5.23it/s, est. speed input: 1063.96 toks/s, output: 211.49 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  5.55it/s, est. speed input: 1128.45 toks/s, output: 235.31 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:01,  5.63it/s, est. speed input: 1158.48 toks/s, output: 247.56 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:10<00:00,  7.74it/s, est. speed input: 1242.64 toks/s, output: 277.30 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00,  7.86it/s, est. speed input: 1307.60 toks/s, output: 303.78 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:11<00:00,  7.04it/s, est. speed input: 1357.95 toks/s, output: 329.13 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.87it/s, est. speed input: 1448.45 toks/s, output: 364.98 toks/s]
[2025-01-06 16:45:31,388][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:45:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:02<00:49,  2.76s/it, est. speed input: 206.49 toks/s, output: 10.52 toks/s]Processed prompts: 100%|██████████| 19/19 [00:04<00:00,  4.67it/s, est. speed input: 2640.52 toks/s, output: 141.34 toks/s]Processed prompts: 100%|██████████| 19/19 [00:04<00:00,  3.93it/s, est. speed input: 2640.52 toks/s, output: 141.34 toks/s]
[2025-01-06 16:45:36,629][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:45:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:04<00:56,  4.75s/it, est. speed input: 147.17 toks/s, output: 22.74 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:04<00:23,  2.09s/it, est. speed input: 280.74 toks/s, output: 44.78 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:05<00:12,  1.29s/it, est. speed input: 393.90 toks/s, output: 65.56 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:05<00:03,  2.16it/s, est. speed input: 771.49 toks/s, output: 135.02 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:05<00:01,  2.82it/s, est. speed input: 963.31 toks/s, output: 174.33 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:06<00:01,  2.97it/s, est. speed input: 1035.14 toks/s, output: 192.35 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:06<00:01,  2.95it/s, est. speed input: 1088.24 toks/s, output: 208.93 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:06<00:00,  2.57it/s, est. speed input: 1104.71 toks/s, output: 221.55 toks/s]Processed prompts: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s, est. speed input: 1305.53 toks/s, output: 279.01 toks/s]
[2025-01-06 16:45:44,072][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:45:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:13,  3.33s/it, est. speed input: 286.67 toks/s, output: 8.71 toks/s]Processed prompts:   9%|▊         | 2/23 [00:04<00:43,  2.07s/it, est. speed input: 412.73 toks/s, output: 18.38 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:05<00:27,  1.38s/it, est. speed input: 505.63 toks/s, output: 29.39 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:05<00:11,  1.57it/s, est. speed input: 841.62 toks/s, output: 54.98 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:05<00:10,  1.58it/s, est. speed input: 871.09 toks/s, output: 63.48 toks/s]Processed prompts:  30%|███       | 7/23 [00:06<00:08,  1.88it/s, est. speed input: 944.79 toks/s, output: 75.41 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:06<00:08,  1.77it/s, est. speed input: 957.86 toks/s, output: 84.17 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:06<00:03,  3.54it/s, est. speed input: 1208.77 toks/s, output: 130.06 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:07<00:03,  2.94it/s, est. speed input: 1212.08 toks/s, output: 137.73 toks/s]Processed prompts:  61%|██████    | 14/23 [00:07<00:02,  3.57it/s, est. speed input: 1334.77 toks/s, output: 166.34 toks/s]Processed prompts:  65%|██████▌   | 15/23 [00:08<00:02,  3.49it/s, est. speed input: 1369.49 toks/s, output: 178.74 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:08<00:01,  4.92it/s, est. speed input: 1516.54 toks/s, output: 213.43 toks/s]Processed prompts:  78%|███████▊  | 18/23 [00:08<00:00,  5.23it/s, est. speed input: 1573.22 toks/s, output: 229.18 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:08<00:00,  4.65it/s, est. speed input: 1600.20 toks/s, output: 241.72 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:08<00:00,  4.42it/s, est. speed input: 1631.78 toks/s, output: 255.76 toks/s]Processed prompts:  91%|█████████▏| 21/23 [00:09<00:00,  4.46it/s, est. speed input: 1668.83 toks/s, output: 271.39 toks/s]Processed prompts: 100%|██████████| 23/23 [00:09<00:00,  2.50it/s, est. speed input: 1820.55 toks/s, output: 314.81 toks/s]
[2025-01-06 16:45:53,816][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:45:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:01<00:14,  1.84s/it, est. speed input: 360.44 toks/s, output: 15.79 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:02<00:01,  2.09it/s, est. speed input: 1511.14 toks/s, output: 66.91 toks/s]Processed prompts:  67%|██████▋   | 6/9 [00:03<00:01,  2.21it/s, est. speed input: 1634.35 toks/s, output: 86.82 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:03<00:00,  2.63it/s, est. speed input: 1813.39 toks/s, output: 111.42 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:04<00:00,  1.66it/s, est. speed input: 1533.06 toks/s, output: 117.08 toks/s]Processed prompts: 100%|██████████| 9/9 [00:04<00:00,  1.97it/s, est. speed input: 1741.95 toks/s, output: 152.82 toks/s]
[2025-01-06 16:46:05,839][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:46:13,229][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:46:13,230][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:46:13,653][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:46:26,658][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:46:26,659][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:46:27,048][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:46:27,201][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:46:34,683][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:46:50,980][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:46:50,981][root][INFO] - Iteration 36 took 2m 13s. Generation: 65.97%, Training: 34.03%. Estimated time remaining: 3h 30m 49s. Estimated total time for complete run: 4h 43m 53s.
[2025-01-06 16:46:51,419][root][INFO] - Loading VLLM model.
WARNING 01-06 16:46:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:46:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:46:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:46:52 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:46:56 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:47:10 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:47:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:47:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:47:32 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:47:32,690][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:47:32,962][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:47:32,964][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:47:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:09,  4.17s/it, est. speed input: 121.18 toks/s, output: 6.00 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:30,  3.02s/it, est. speed input: 158.28 toks/s, output: 13.63 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:55,  1.91s/it, est. speed input: 217.56 toks/s, output: 22.83 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:36,  1.31s/it, est. speed input: 274.30 toks/s, output: 32.32 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:23,  1.14it/s, est. speed input: 337.76 toks/s, output: 42.67 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:16,  1.53it/s, est. speed input: 393.89 toks/s, output: 52.52 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:12,  1.97it/s, est. speed input: 480.47 toks/s, output: 69.93 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:08,  2.72it/s, est. speed input: 577.34 toks/s, output: 90.66 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:05,  3.65it/s, est. speed input: 675.02 toks/s, output: 112.61 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:04,  3.94it/s, est. speed input: 717.16 toks/s, output: 123.00 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:02,  6.03it/s, est. speed input: 863.42 toks/s, output: 157.83 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:03,  4.72it/s, est. speed input: 878.40 toks/s, output: 164.53 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:01,  6.31it/s, est. speed input: 1005.36 toks/s, output: 200.18 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:01,  6.94it/s, est. speed input: 1082.22 toks/s, output: 223.75 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  6.68it/s, est. speed input: 1112.27 toks/s, output: 234.52 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:00,  7.84it/s, est. speed input: 1190.12 toks/s, output: 260.37 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00, 10.25it/s, est. speed input: 1312.36 toks/s, output: 301.45 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:11<00:00,  8.21it/s, est. speed input: 1359.88 toks/s, output: 323.32 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:11<00:00,  6.49it/s, est. speed input: 1366.80 toks/s, output: 331.94 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.79it/s, est. speed input: 1410.86 toks/s, output: 349.40 toks/s]
[2025-01-06 16:47:44,863][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:47:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:02<01:02,  2.95s/it, est. speed input: 246.15 toks/s, output: 8.80 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:03<00:15,  1.22it/s, est. speed input: 673.68 toks/s, output: 26.86 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:03<00:00, 10.16it/s, est. speed input: 3744.16 toks/s, output: 163.90 toks/s]Processed prompts: 100%|██████████| 22/22 [00:03<00:00,  5.87it/s, est. speed input: 3941.74 toks/s, output: 188.83 toks/s]
[2025-01-06 16:47:49,031][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:47:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:03<00:30,  3.41s/it, est. speed input: 205.23 toks/s, output: 24.96 toks/s]Processed prompts:  20%|██        | 2/10 [00:03<00:12,  1.62s/it, est. speed input: 370.45 toks/s, output: 48.49 toks/s]Processed prompts:  30%|███       | 3/10 [00:04<00:06,  1.01it/s, est. speed input: 522.25 toks/s, output: 72.22 toks/s]Processed prompts:  40%|████      | 4/10 [00:04<00:04,  1.33it/s, est. speed input: 634.72 toks/s, output: 93.53 toks/s]Processed prompts:  50%|█████     | 5/10 [00:04<00:03,  1.61it/s, est. speed input: 729.22 toks/s, output: 114.75 toks/s]Processed prompts:  60%|██████    | 6/10 [00:05<00:02,  1.77it/s, est. speed input: 799.49 toks/s, output: 134.96 toks/s]Processed prompts:  70%|███████   | 7/10 [00:05<00:01,  2.19it/s, est. speed input: 892.24 toks/s, output: 159.92 toks/s]Processed prompts:  80%|████████  | 8/10 [00:05<00:00,  2.62it/s, est. speed input: 980.56 toks/s, output: 185.34 toks/s]Processed prompts:  90%|█████████ | 9/10 [00:06<00:00,  2.69it/s, est. speed input: 1039.42 toks/s, output: 207.52 toks/s]Processed prompts: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s, est. speed input: 1151.63 toks/s, output: 239.88 toks/s]
[2025-01-06 16:47:55,563][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:47:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:05<02:04,  5.66s/it, est. speed input: 90.03 toks/s, output: 13.95 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:06<00:32,  1.61s/it, est. speed input: 358.24 toks/s, output: 40.91 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:07<00:31,  1.65s/it, est. speed input: 368.35 toks/s, output: 48.01 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:07<00:20,  1.15s/it, est. speed input: 450.75 toks/s, output: 63.54 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:08<00:15,  1.11it/s, est. speed input: 514.71 toks/s, output: 77.33 toks/s]Processed prompts:  30%|███       | 7/23 [00:08<00:11,  1.44it/s, est. speed input: 581.97 toks/s, output: 92.01 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:08<00:06,  2.21it/s, est. speed input: 715.22 toks/s, output: 121.85 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:09<00:03,  3.35it/s, est. speed input: 859.04 toks/s, output: 154.75 toks/s]Processed prompts:  57%|█████▋    | 13/23 [00:09<00:02,  3.64it/s, est. speed input: 963.88 toks/s, output: 181.97 toks/s]Processed prompts:  61%|██████    | 14/23 [00:10<00:03,  2.61it/s, est. speed input: 957.08 toks/s, output: 187.30 toks/s]Processed prompts: 100%|██████████| 23/23 [00:10<00:00,  2.23it/s, est. speed input: 1531.46 toks/s, output: 362.15 toks/s]
[2025-01-06 16:48:06,390][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:48:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:02<00:25,  2.28s/it, est. speed input: 418.51 toks/s, output: 12.72 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:02<00:10,  1.07s/it, est. speed input: 681.68 toks/s, output: 26.00 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:02<00:06,  1.42it/s, est. speed input: 937.72 toks/s, output: 39.75 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:03<00:02,  2.47it/s, est. speed input: 1404.20 toks/s, output: 68.88 toks/s]Processed prompts:  50%|█████     | 6/12 [00:03<00:02,  2.69it/s, est. speed input: 1550.92 toks/s, output: 83.31 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:03<00:01,  2.99it/s, est. speed input: 1752.48 toks/s, output: 112.82 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:04<00:00,  3.38it/s, est. speed input: 1897.23 toks/s, output: 132.55 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:04<00:00,  3.68it/s, est. speed input: 2027.71 toks/s, output: 152.05 toks/s]Processed prompts:  92%|█████████▏| 11/12 [00:04<00:00,  4.03it/s, est. speed input: 2155.17 toks/s, output: 172.69 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  3.67it/s, est. speed input: 2201.91 toks/s, output: 189.85 toks/s]Processed prompts: 100%|██████████| 12/12 [00:04<00:00,  2.45it/s, est. speed input: 2201.91 toks/s, output: 189.85 toks/s]
[2025-01-06 16:48:18,826][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:48:26,164][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:48:26,164][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:48:26,573][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:48:39,660][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:48:39,661][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:48:40,009][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:48:40,151][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:48:47,552][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:49:03,885][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:49:03,886][root][INFO] - Iteration 37 took 2m 12s. Generation: 65.97%, Training: 34.03%. Estimated time remaining: 3h 28m 14s. Estimated total time for complete run: 4h 43m 31s.
[2025-01-06 16:49:04,261][root][INFO] - Loading VLLM model.
WARNING 01-06 16:49:04 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:49:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:49:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:49:05 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:49:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:49:23 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:49:24 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:49:24 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:49:45 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:49:45,608][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:49:45,862][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:49:45,863][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:49:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:09,  4.19s/it, est. speed input: 120.50 toks/s, output: 5.97 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:53,  1.80s/it, est. speed input: 234.28 toks/s, output: 12.06 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:56,  1.96s/it, est. speed input: 234.34 toks/s, output: 17.94 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.23s/it, est. speed input: 307.01 toks/s, output: 27.66 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:25,  1.08it/s, est. speed input: 362.32 toks/s, output: 36.59 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:20,  1.26it/s, est. speed input: 403.59 toks/s, output: 45.02 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:17,  1.45it/s, est. speed input: 442.90 toks/s, output: 53.88 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:08,  2.62it/s, est. speed input: 562.26 toks/s, output: 76.33 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:09,  2.31it/s, est. speed input: 583.00 toks/s, output: 83.47 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:05,  3.36it/s, est. speed input: 681.48 toks/s, output: 105.82 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:03,  4.57it/s, est. speed input: 779.74 toks/s, output: 128.93 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:03,  5.03it/s, est. speed input: 860.34 toks/s, output: 149.92 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:01,  6.70it/s, est. speed input: 995.23 toks/s, output: 185.25 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:01,  8.29it/s, est. speed input: 1088.67 toks/s, output: 210.55 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  6.19it/s, est. speed input: 1131.81 toks/s, output: 228.70 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:00,  7.68it/s, est. speed input: 1217.09 toks/s, output: 255.76 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00,  7.06it/s, est. speed input: 1302.98 toks/s, output: 289.35 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:11<00:00,  5.84it/s, est. speed input: 1334.13 toks/s, output: 310.06 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.82it/s, est. speed input: 1423.05 toks/s, output: 345.28 toks/s]
[2025-01-06 16:49:57,674][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:49:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:02<00:42,  2.13s/it, est. speed input: 350.39 toks/s, output: 5.17 toks/s]Processed prompts:  10%|▉         | 2/21 [00:02<00:19,  1.03s/it, est. speed input: 590.36 toks/s, output: 11.71 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:02<00:14,  1.25it/s, est. speed input: 680.79 toks/s, output: 19.59 toks/s]Processed prompts: 100%|██████████| 21/21 [00:03<00:00, 14.00it/s, est. speed input: 4482.90 toks/s, output: 192.33 toks/s]Processed prompts: 100%|██████████| 21/21 [00:03<00:00,  6.81it/s, est. speed input: 4482.90 toks/s, output: 192.33 toks/s]
[2025-01-06 16:50:01,172][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:50:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:26,  2.69s/it, est. speed input: 259.84 toks/s, output: 20.44 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:04<00:16,  1.89s/it, est. speed input: 348.02 toks/s, output: 38.59 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:04<00:09,  1.17s/it, est. speed input: 484.48 toks/s, output: 61.46 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:04<00:06,  1.03it/s, est. speed input: 559.36 toks/s, output: 80.42 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:05<00:04,  1.46it/s, est. speed input: 674.64 toks/s, output: 105.20 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:05<00:02,  1.71it/s, est. speed input: 753.26 toks/s, output: 126.44 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:06<00:02,  1.43it/s, est. speed input: 753.18 toks/s, output: 139.15 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  1.69it/s, est. speed input: 1183.53 toks/s, output: 262.29 toks/s]
[2025-01-06 16:50:08,110][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:50:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:02<01:07,  2.60s/it, est. speed input: 196.17 toks/s, output: 2.69 toks/s]Processed prompts:   7%|▋         | 2/27 [00:03<00:42,  1.69s/it, est. speed input: 399.87 toks/s, output: 9.29 toks/s]Processed prompts:  11%|█         | 3/27 [00:03<00:23,  1.03it/s, est. speed input: 603.49 toks/s, output: 16.73 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:05<00:21,  1.00it/s, est. speed input: 675.61 toks/s, output: 28.06 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:05<00:11,  1.71it/s, est. speed input: 980.03 toks/s, output: 52.08 toks/s]Processed prompts:  30%|██▉       | 8/27 [00:06<00:11,  1.68it/s, est. speed input: 993.33 toks/s, output: 60.52 toks/s]Processed prompts:  33%|███▎      | 9/27 [00:07<00:10,  1.67it/s, est. speed input: 1006.34 toks/s, output: 69.59 toks/s]Processed prompts:  37%|███▋      | 10/27 [00:07<00:10,  1.62it/s, est. speed input: 1009.57 toks/s, output: 78.68 toks/s]Processed prompts:  41%|████      | 11/27 [00:08<00:09,  1.75it/s, est. speed input: 1039.06 toks/s, output: 89.95 toks/s]Processed prompts:  44%|████▍     | 12/27 [00:08<00:07,  2.11it/s, est. speed input: 1093.08 toks/s, output: 103.35 toks/s]Processed prompts:  48%|████▊     | 13/27 [00:09<00:06,  2.06it/s, est. speed input: 1108.39 toks/s, output: 113.97 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:09<00:04,  2.63it/s, est. speed input: 1222.30 toks/s, output: 140.65 toks/s]Processed prompts:  67%|██████▋   | 18/27 [00:09<00:02,  4.20it/s, est. speed input: 1402.00 toks/s, output: 188.06 toks/s]Processed prompts:  70%|███████   | 19/27 [00:09<00:01,  4.55it/s, est. speed input: 1452.20 toks/s, output: 203.24 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:10<00:02,  3.35it/s, est. speed input: 1437.28 toks/s, output: 210.83 toks/s]Processed prompts: 100%|██████████| 27/27 [00:10<00:00,  2.56it/s, est. speed input: 1864.85 toks/s, output: 343.42 toks/s]
[2025-01-06 16:50:19,264][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:50:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:09,  1.55s/it, est. speed input: 484.82 toks/s, output: 18.75 toks/s]Processed prompts:  71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s, est. speed input: 1910.57 toks/s, output: 85.42 toks/s]Processed prompts:  86%|████████▌ | 6/7 [00:02<00:00,  2.31it/s, est. speed input: 1736.33 toks/s, output: 96.02 toks/s]Processed prompts: 100%|██████████| 7/7 [00:03<00:00,  1.97it/s, est. speed input: 1633.81 toks/s, output: 115.10 toks/s]Processed prompts: 100%|██████████| 7/7 [00:03<00:00,  1.98it/s, est. speed input: 1633.81 toks/s, output: 115.10 toks/s]
[2025-01-06 16:50:31,204][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:50:38,531][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:50:38,531][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:50:38,897][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:50:51,968][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:50:51,969][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:50:52,331][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:50:52,506][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 16:51:00,071][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:51:16,257][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:51:16,258][root][INFO] - Iteration 38 took 2m 12s. Generation: 65.19%, Training: 34.81%. Estimated time remaining: 3h 24m 53s. Estimated total time for complete run: 4h 42m 23s.
[2025-01-06 16:51:16,573][root][INFO] - Loading VLLM model.
WARNING 01-06 16:51:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:51:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:51:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:51:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:51:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:51:35 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:51:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:51:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:51:57 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:51:57,858][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:51:58,133][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:51:58,134][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:51:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:09,  4.17s/it, est. speed input: 121.03 toks/s, output: 5.99 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:53,  1.79s/it, est. speed input: 235.28 toks/s, output: 12.11 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<01:01,  2.12s/it, est. speed input: 222.86 toks/s, output: 17.95 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:37,  1.33s/it, est. speed input: 292.21 toks/s, output: 28.06 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:10,  2.15it/s, est. speed input: 589.24 toks/s, output: 74.16 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:09,  2.26it/s, est. speed input: 627.22 toks/s, output: 82.84 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:08,  2.55it/s, est. speed input: 674.11 toks/s, output: 92.96 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:06,  2.98it/s, est. speed input: 723.34 toks/s, output: 103.61 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:05,  3.33it/s, est. speed input: 826.22 toks/s, output: 130.34 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:02,  4.93it/s, est. speed input: 971.81 toks/s, output: 167.31 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:02,  5.35it/s, est. speed input: 1014.32 toks/s, output: 179.08 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:02,  5.13it/s, est. speed input: 1072.87 toks/s, output: 198.89 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  5.42it/s, est. speed input: 1137.82 toks/s, output: 221.49 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:01,  6.54it/s, est. speed input: 1217.31 toks/s, output: 248.19 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:10<00:01,  5.98it/s, est. speed input: 1238.25 toks/s, output: 258.59 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:01,  4.61it/s, est. speed input: 1238.18 toks/s, output: 265.80 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:11<00:00,  6.12it/s, est. speed input: 1338.36 toks/s, output: 309.55 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.83it/s, est. speed input: 1427.56 toks/s, output: 344.88 toks/s]
[2025-01-06 16:52:09,876][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:52:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:50,  2.68s/it, est. speed input: 241.32 toks/s, output: 9.34 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:02<00:12,  1.35it/s, est. speed input: 695.05 toks/s, output: 28.17 toks/s]Processed prompts: 100%|██████████| 20/20 [00:02<00:00,  6.99it/s, est. speed input: 4653.31 toks/s, output: 200.02 toks/s]
[2025-01-06 16:52:13,154][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:52:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:03<00:40,  3.65s/it, est. speed input: 191.25 toks/s, output: 21.89 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:04<00:20,  2.09s/it, est. speed input: 300.38 toks/s, output: 41.25 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:04<00:10,  1.20s/it, est. speed input: 436.67 toks/s, output: 64.34 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:05<00:04,  1.46it/s, est. speed input: 651.18 toks/s, output: 105.27 toks/s]Processed prompts:  50%|█████     | 6/12 [00:05<00:03,  1.54it/s, est. speed input: 706.25 toks/s, output: 122.09 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:06<00:02,  1.68it/s, est. speed input: 764.77 toks/s, output: 141.29 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:06<00:02,  1.78it/s, est. speed input: 813.29 toks/s, output: 160.56 toks/s]Processed prompts: 100%|██████████| 12/12 [00:06<00:00,  1.75it/s, est. speed input: 1219.90 toks/s, output: 276.91 toks/s]
[2025-01-06 16:52:20,522][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:52:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:25,  3.55s/it, est. speed input: 234.79 toks/s, output: 8.17 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:05<00:37,  1.69s/it, est. speed input: 486.04 toks/s, output: 23.05 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:07<00:36,  1.75s/it, est. speed input: 458.99 toks/s, output: 32.16 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:07<00:18,  1.03it/s, est. speed input: 615.90 toks/s, output: 60.21 toks/s]Processed prompts:  28%|██▊       | 7/25 [00:08<00:13,  1.31it/s, est. speed input: 717.75 toks/s, output: 74.23 toks/s]Processed prompts:  36%|███▌      | 9/25 [00:08<00:07,  2.09it/s, est. speed input: 846.60 toks/s, output: 102.97 toks/s]Processed prompts:  40%|████      | 10/25 [00:08<00:06,  2.48it/s, est. speed input: 914.47 toks/s, output: 116.83 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:09<00:05,  2.23it/s, est. speed input: 962.36 toks/s, output: 135.19 toks/s]Processed prompts:  52%|█████▏    | 13/25 [00:09<00:05,  2.23it/s, est. speed input: 990.05 toks/s, output: 146.76 toks/s]Processed prompts:  56%|█████▌    | 14/25 [00:09<00:04,  2.69it/s, est. speed input: 1046.87 toks/s, output: 162.63 toks/s]Processed prompts:  64%|██████▍   | 16/25 [00:10<00:02,  3.49it/s, est. speed input: 1149.04 toks/s, output: 193.10 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:10<00:02,  3.66it/s, est. speed input: 1172.59 toks/s, output: 207.61 toks/s]Processed prompts: 100%|██████████| 25/25 [00:10<00:00,  2.35it/s, est. speed input: 1695.56 toks/s, output: 356.45 toks/s]
[2025-01-06 16:52:31,684][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:52:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:11,  1.70s/it, est. speed input: 438.01 toks/s, output: 17.03 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:02<00:04,  1.14it/s, est. speed input: 909.85 toks/s, output: 46.99 toks/s]Processed prompts:  62%|██████▎   | 5/8 [00:03<00:01,  1.63it/s, est. speed input: 1235.09 toks/s, output: 90.30 toks/s]Processed prompts: 100%|██████████| 8/8 [00:03<00:00,  3.16it/s, est. speed input: 1897.41 toks/s, output: 178.71 toks/s]Processed prompts: 100%|██████████| 8/8 [00:03<00:00,  2.14it/s, est. speed input: 1897.41 toks/s, output: 178.71 toks/s]
[2025-01-06 16:52:42,884][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:52:50,117][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:52:50,117][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:52:50,482][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:53:03,416][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:53:03,417][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:53:03,809][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:53:03,953][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:53:11,358][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:53:27,748][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:53:27,749][root][INFO] - Iteration 39 took 2m 11s. Generation: 65.77%, Training: 34.23%. Estimated time remaining: 3h 20m 49s. Estimated total time for complete run: 4h 40m 30s.
[2025-01-06 16:53:28,084][root][INFO] - Loading VLLM model.
WARNING 01-06 16:53:28 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:53:28 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:53:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:53:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:53:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:53:47 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:53:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:53:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:54:09 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:54:09,289][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:54:09,560][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:54:09,561][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:54:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:57,  3.78s/it, est. speed input: 133.61 toks/s, output: 6.61 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:25,  2.86s/it, est. speed input: 168.51 toks/s, output: 14.52 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:46,  1.61s/it, est. speed input: 247.91 toks/s, output: 24.71 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:28,  1.02s/it, est. speed input: 324.46 toks/s, output: 34.85 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:27,  1.03s/it, est. speed input: 346.72 toks/s, output: 41.47 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:18,  1.39it/s, est. speed input: 409.97 toks/s, output: 52.63 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:14,  1.76it/s, est. speed input: 461.83 toks/s, output: 62.84 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:17,  1.37it/s, est. speed input: 462.92 toks/s, output: 68.06 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:09,  2.40it/s, est. speed input: 569.12 toks/s, output: 92.75 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:07,  2.70it/s, est. speed input: 609.81 toks/s, output: 103.63 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:06,  3.12it/s, est. speed input: 652.15 toks/s, output: 115.04 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:04,  4.03it/s, est. speed input: 736.75 toks/s, output: 138.39 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:02,  5.80it/s, est. speed input: 869.92 toks/s, output: 175.71 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:01,  8.25it/s, est. speed input: 1009.10 toks/s, output: 215.61 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:01,  8.14it/s, est. speed input: 1082.53 toks/s, output: 239.21 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:00, 10.97it/s, est. speed input: 1216.59 toks/s, output: 281.00 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00, 13.14it/s, est. speed input: 1381.91 toks/s, output: 334.98 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:10<00:00,  9.97it/s, est. speed input: 1428.21 toks/s, output: 356.62 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.85it/s, est. speed input: 1439.42 toks/s, output: 366.00 toks/s]
[2025-01-06 16:54:21,213][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:54:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:30,  2.21s/it, est. speed input: 304.21 toks/s, output: 12.24 toks/s]Processed prompts:  87%|████████▋ | 13/15 [00:02<00:00,  7.67it/s, est. speed input: 3672.06 toks/s, output: 162.73 toks/s]Processed prompts: 100%|██████████| 15/15 [00:02<00:00,  6.47it/s, est. speed input: 4279.48 toks/s, output: 189.48 toks/s]
[2025-01-06 16:54:23,942][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:54:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:05<01:33,  5.83s/it, est. speed input: 119.84 toks/s, output: 18.69 toks/s]Processed prompts:  12%|█▏        | 2/17 [00:05<00:37,  2.49s/it, est. speed input: 233.57 toks/s, output: 37.09 toks/s]Processed prompts:  18%|█▊        | 3/17 [00:06<00:21,  1.54s/it, est. speed input: 328.33 toks/s, output: 54.17 toks/s]Processed prompts:  24%|██▎       | 4/17 [00:07<00:19,  1.53s/it, est. speed input: 353.36 toks/s, output: 64.83 toks/s]Processed prompts:  29%|██▉       | 5/17 [00:08<00:12,  1.05s/it, est. speed input: 430.62 toks/s, output: 84.52 toks/s]Processed prompts:  47%|████▋     | 8/17 [00:08<00:04,  2.25it/s, est. speed input: 676.35 toks/s, output: 146.59 toks/s]Processed prompts:  53%|█████▎    | 9/17 [00:08<00:03,  2.45it/s, est. speed input: 735.70 toks/s, output: 163.72 toks/s]Processed prompts:  71%|███████   | 12/17 [00:08<00:01,  3.90it/s, est. speed input: 948.07 toks/s, output: 223.68 toks/s]Processed prompts: 100%|██████████| 17/17 [00:08<00:00,  1.92it/s, est. speed input: 1343.06 toks/s, output: 336.70 toks/s]
[2025-01-06 16:54:33,300][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:54:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:10,  3.34s/it, est. speed input: 282.78 toks/s, output: 8.69 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:04<00:27,  1.44s/it, est. speed input: 579.95 toks/s, output: 24.92 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:05<00:18,  1.00s/it, est. speed input: 748.21 toks/s, output: 37.48 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:06<00:15,  1.05it/s, est. speed input: 755.93 toks/s, output: 54.17 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:07<00:13,  1.10it/s, est. speed input: 801.17 toks/s, output: 65.85 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:07<00:10,  1.34it/s, est. speed input: 859.44 toks/s, output: 81.13 toks/s]Processed prompts:  41%|████      | 9/22 [00:08<00:08,  1.55it/s, est. speed input: 902.78 toks/s, output: 95.52 toks/s]Processed prompts:  45%|████▌     | 10/22 [00:08<00:06,  1.73it/s, est. speed input: 940.50 toks/s, output: 109.79 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:08<00:03,  2.89it/s, est. speed input: 1084.24 toks/s, output: 145.67 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:09<00:02,  3.43it/s, est. speed input: 1147.52 toks/s, output: 162.84 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:09<00:02,  3.54it/s, est. speed input: 1215.70 toks/s, output: 177.87 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:09<00:02,  3.41it/s, est. speed input: 1247.52 toks/s, output: 191.98 toks/s]Processed prompts:  77%|███████▋  | 17/22 [00:09<00:01,  4.91it/s, est. speed input: 1394.51 toks/s, output: 229.00 toks/s]Processed prompts: 100%|██████████| 22/22 [00:09<00:00,  2.25it/s, est. speed input: 1752.42 toks/s, output: 331.41 toks/s]
[2025-01-06 16:54:43,595][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:54:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:01<00:09,  1.62s/it, est. speed input: 589.15 toks/s, output: 17.91 toks/s]Processed prompts:  29%|██▊       | 2/7 [00:02<00:06,  1.23s/it, est. speed input: 742.61 toks/s, output: 37.75 toks/s]Processed prompts:  43%|████▎     | 3/7 [00:02<00:02,  1.33it/s, est. speed input: 1039.59 toks/s, output: 62.84 toks/s]Processed prompts:  57%|█████▋    | 4/7 [00:03<00:02,  1.25it/s, est. speed input: 1051.65 toks/s, output: 79.65 toks/s]Processed prompts:  71%|███████▏  | 5/7 [00:03<00:01,  1.83it/s, est. speed input: 1265.84 toks/s, output: 109.93 toks/s]Processed prompts:  86%|████████▌ | 6/7 [00:04<00:00,  1.35it/s, est. speed input: 1157.50 toks/s, output: 122.04 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.73it/s, est. speed input: 1291.49 toks/s, output: 154.92 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.38it/s, est. speed input: 1291.49 toks/s, output: 154.92 toks/s]
[2025-01-06 16:54:56,265][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:55:03,597][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:55:03,598][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:55:03,971][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:55:16,716][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:55:16,717][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:55:17,065][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:55:17,255][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:55:24,868][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:55:41,877][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:55:41,878][root][INFO] - Iteration 40 took 2m 14s. Generation: 65.87%, Training: 34.13%. Estimated time remaining: 3h 24m 12s. Estimated total time for complete run: 4h 46m 8s.
[2025-01-06 16:55:42,208][root][INFO] - Loading VLLM model.
WARNING 01-06 16:55:42 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:55:42 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:55:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:55:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:55:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:56:01 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:56:02 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:56:02 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:56:23 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 16:56:23,707][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:56:23,957][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:56:23,959][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:56:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:13,  4.30s/it, est. speed input: 117.53 toks/s, output: 6.28 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:01,  2.05s/it, est. speed input: 211.46 toks/s, output: 12.98 toks/s]Processed prompts:   9%|▉         | 3/32 [00:04<00:33,  1.17s/it, est. speed input: 309.60 toks/s, output: 20.23 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.25s/it, est. speed input: 322.43 toks/s, output: 25.54 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:17,  1.52it/s, est. speed input: 463.68 toks/s, output: 43.92 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:14,  1.73it/s, est. speed input: 512.06 toks/s, output: 52.15 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:11,  2.11it/s, est. speed input: 568.36 toks/s, output: 61.48 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:09,  2.44it/s, est. speed input: 617.67 toks/s, output: 70.53 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:08,  2.53it/s, est. speed input: 684.66 toks/s, output: 86.28 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:06,  2.91it/s, est. speed input: 730.44 toks/s, output: 96.67 toks/s]Processed prompts:  41%|████      | 13/32 [00:08<00:05,  3.46it/s, est. speed input: 778.86 toks/s, output: 107.61 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:02,  5.34it/s, est. speed input: 927.15 toks/s, output: 141.25 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:02,  5.79it/s, est. speed input: 972.34 toks/s, output: 152.45 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:08<00:01,  7.73it/s, est. speed input: 1073.70 toks/s, output: 176.80 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:01,  7.09it/s, est. speed input: 1144.88 toks/s, output: 197.24 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:09<00:01,  8.75it/s, est. speed input: 1237.97 toks/s, output: 222.55 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:00,  9.29it/s, est. speed input: 1319.28 toks/s, output: 246.51 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:10<00:00,  6.88it/s, est. speed input: 1360.02 toks/s, output: 266.12 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00,  6.50it/s, est. speed input: 1412.13 toks/s, output: 289.17 toks/s]Processed prompts:  94%|█████████▍| 30/32 [00:10<00:00,  4.50it/s, est. speed input: 1389.51 toks/s, output: 293.40 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  2.93it/s, est. speed input: 1482.12 toks/s, output: 330.08 toks/s]
[2025-01-06 16:56:35,310][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:56:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:02<00:56,  2.85s/it, est. speed input: 236.36 toks/s, output: 9.48 toks/s]Processed prompts:  95%|█████████▌| 20/21 [00:02<00:00,  9.39it/s, est. speed input: 4382.48 toks/s, output: 194.84 toks/s]Processed prompts: 100%|██████████| 21/21 [00:03<00:00,  6.77it/s, est. speed input: 4401.56 toks/s, output: 197.99 toks/s]
[2025-01-06 16:56:38,865][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:56:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:04<00:47,  4.73s/it, est. speed input: 147.81 toks/s, output: 25.38 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:04<00:18,  2.05s/it, est. speed input: 284.91 toks/s, output: 50.13 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:05<00:11,  1.49s/it, est. speed input: 366.15 toks/s, output: 70.02 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:06<00:08,  1.22s/it, est. speed input: 428.05 toks/s, output: 89.71 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:06<00:05,  1.09it/s, est. speed input: 504.91 toks/s, output: 113.55 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  1.59it/s, est. speed input: 1110.76 toks/s, output: 286.90 toks/s]
[2025-01-06 16:56:46,280][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:56:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:33,  3.74s/it, est. speed input: 254.91 toks/s, output: 7.75 toks/s]Processed prompts:   8%|▊         | 2/26 [00:06<01:20,  3.36s/it, est. speed input: 279.05 toks/s, output: 17.26 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:07<00:46,  2.03s/it, est. speed input: 357.53 toks/s, output: 29.62 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:07<00:29,  1.33s/it, est. speed input: 466.41 toks/s, output: 42.33 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:08<00:25,  1.23s/it, est. speed input: 490.83 toks/s, output: 51.71 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:08<00:17,  1.15it/s, est. speed input: 560.15 toks/s, output: 65.33 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:08<00:06,  2.62it/s, est. speed input: 787.53 toks/s, output: 108.41 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:09<00:05,  3.00it/s, est. speed input: 878.47 toks/s, output: 121.45 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:09<00:04,  3.34it/s, est. speed input: 935.82 toks/s, output: 134.17 toks/s]Processed prompts:  50%|█████     | 13/26 [00:09<00:02,  4.51it/s, est. speed input: 1062.03 toks/s, output: 161.72 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:09<00:02,  4.94it/s, est. speed input: 1119.71 toks/s, output: 175.14 toks/s]Processed prompts:  58%|█████▊    | 15/26 [00:09<00:02,  5.17it/s, est. speed input: 1172.30 toks/s, output: 188.14 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:09<00:01,  5.21it/s, est. speed input: 1220.30 toks/s, output: 200.84 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:10<00:01,  4.71it/s, est. speed input: 1256.80 toks/s, output: 212.32 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:10<00:00,  8.51it/s, est. speed input: 1471.34 toks/s, output: 260.25 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:10<00:00,  5.62it/s, est. speed input: 1519.38 toks/s, output: 281.99 toks/s]Processed prompts: 100%|██████████| 26/26 [00:10<00:00,  2.38it/s, est. speed input: 1775.17 toks/s, output: 355.18 toks/s]
[2025-01-06 16:56:57,772][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:56:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:01<00:07,  1.47s/it, est. speed input: 649.37 toks/s, output: 19.72 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:02<00:05,  1.37s/it, est. speed input: 688.62 toks/s, output: 41.48 toks/s]Processed prompts:  50%|█████     | 3/6 [00:02<00:02,  1.21it/s, est. speed input: 971.29 toks/s, output: 70.90 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  2.58it/s, est. speed input: 1634.81 toks/s, output: 152.03 toks/s]Processed prompts: 100%|██████████| 6/6 [00:03<00:00,  1.74it/s, est. speed input: 1634.81 toks/s, output: 152.03 toks/s]
[2025-01-06 16:57:08,956][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 16:57:16,178][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:57:16,178][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:57:16,557][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:57:29,374][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:57:29,375][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:57:29,701][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:57:29,877][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 16:57:37,254][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 16:57:53,809][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 16:57:53,810][root][INFO] - Iteration 41 took 2m 11s. Generation: 65.90%, Training: 34.10%. Estimated time remaining: 3h 17m 19s. Estimated total time for complete run: 4h 41m 27s.
[2025-01-06 16:57:54,155][root][INFO] - Loading VLLM model.
WARNING 01-06 16:57:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 16:57:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 16:57:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 16:57:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 16:57:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 16:58:13 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 16:58:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 16:58:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 16:58:35 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 16:58:35,836][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 16:58:36,107][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 16:58:36,108][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:58:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:04,  4.02s/it, est. speed input: 125.52 toks/s, output: 6.71 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:58,  1.94s/it, est. speed input: 224.32 toks/s, output: 13.77 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:50,  1.74s/it, est. speed input: 251.80 toks/s, output: 20.44 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:32,  1.17s/it, est. speed input: 320.49 toks/s, output: 29.99 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:23,  1.13it/s, est. speed input: 377.28 toks/s, output: 39.15 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:17,  1.48it/s, est. speed input: 435.20 toks/s, output: 48.83 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:09,  2.46it/s, est. speed input: 559.71 toks/s, output: 69.41 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:06,  3.59it/s, est. speed input: 681.31 toks/s, output: 90.53 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:03,  5.69it/s, est. speed input: 864.88 toks/s, output: 123.44 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:03,  5.97it/s, est. speed input: 915.78 toks/s, output: 133.55 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:02,  5.98it/s, est. speed input: 960.52 toks/s, output: 143.16 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:02,  6.38it/s, est. speed input: 1009.02 toks/s, output: 153.60 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:08<00:03,  3.75it/s, est. speed input: 996.93 toks/s, output: 156.42 toks/s] Processed prompts:  56%|█████▋    | 18/32 [00:08<00:03,  4.43it/s, est. speed input: 1042.40 toks/s, output: 168.23 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:03,  4.15it/s, est. speed input: 1065.92 toks/s, output: 177.19 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:09<00:02,  4.54it/s, est. speed input: 1101.46 toks/s, output: 188.45 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:09<00:02,  3.71it/s, est. speed input: 1109.16 toks/s, output: 195.89 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:09<00:01,  5.13it/s, est. speed input: 1188.91 toks/s, output: 222.22 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:09<00:01,  6.61it/s, est. speed input: 1270.70 toks/s, output: 250.01 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:10<00:00,  7.31it/s, est. speed input: 1342.35 toks/s, output: 276.84 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:10<00:00,  6.27it/s, est. speed input: 1358.96 toks/s, output: 287.36 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:10<00:00,  4.35it/s, est. speed input: 1346.38 toks/s, output: 293.27 toks/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  2.94it/s, est. speed input: 1485.63 toks/s, output: 348.42 toks/s]
[2025-01-06 16:58:47,409][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 16:58:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:53,  2.81s/it, est. speed input: 224.54 toks/s, output: 9.96 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:02<00:00,  7.79it/s, est. speed input: 3678.35 toks/s, output: 167.46 toks/s]Processed prompts: 100%|██████████| 20/20 [00:03<00:00,  6.42it/s, est. speed input: 4184.16 toks/s, output: 199.00 toks/s]
[2025-01-06 16:58:50,942][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:58:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:04<00:54,  4.94s/it, est. speed input: 141.55 toks/s, output: 24.10 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:05<00:25,  2.53s/it, est. speed input: 241.80 toks/s, output: 45.83 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:05<00:13,  1.46s/it, est. speed input: 351.87 toks/s, output: 69.97 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:06<00:08,  1.10s/it, est. speed input: 428.47 toks/s, output: 90.26 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:07<00:06,  1.10it/s, est. speed input: 493.04 toks/s, output: 110.32 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:07<00:02,  2.03it/s, est. speed input: 673.56 toks/s, output: 162.16 toks/s]Processed prompts: 100%|██████████| 12/12 [00:07<00:00,  1.65it/s, est. speed input: 1154.64 toks/s, output: 299.81 toks/s]
[2025-01-06 16:58:58,698][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:58:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:04<01:52,  4.68s/it, est. speed input: 108.93 toks/s, output: 10.68 toks/s]Processed prompts:   8%|▊         | 2/25 [00:06<01:15,  3.29s/it, est. speed input: 209.22 toks/s, output: 20.86 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:07<00:42,  1.94s/it, est. speed input: 329.43 toks/s, output: 33.92 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:07<00:26,  1.27s/it, est. speed input: 411.27 toks/s, output: 47.10 toks/s]Processed prompts:  20%|██        | 5/25 [00:07<00:18,  1.10it/s, est. speed input: 517.54 toks/s, output: 59.95 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:08<00:15,  1.20it/s, est. speed input: 588.80 toks/s, output: 70.37 toks/s]Processed prompts:  36%|███▌      | 9/25 [00:08<00:06,  2.59it/s, est. speed input: 840.30 toks/s, output: 113.42 toks/s]Processed prompts:  40%|████      | 10/25 [00:09<00:06,  2.39it/s, est. speed input: 847.01 toks/s, output: 122.94 toks/s]Processed prompts:  48%|████▊     | 12/25 [00:09<00:03,  3.60it/s, est. speed input: 965.70 toks/s, output: 153.60 toks/s]Processed prompts:  52%|█████▏    | 13/25 [00:09<00:02,  4.17it/s, est. speed input: 1028.80 toks/s, output: 168.24 toks/s]Processed prompts:  60%|██████    | 15/25 [00:09<00:01,  5.78it/s, est. speed input: 1159.98 toks/s, output: 198.76 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:10<00:01,  4.21it/s, est. speed input: 1215.33 toks/s, output: 218.52 toks/s]Processed prompts:  72%|███████▏  | 18/25 [00:10<00:01,  3.62it/s, est. speed input: 1231.74 toks/s, output: 228.36 toks/s]Processed prompts: 100%|██████████| 25/25 [00:10<00:00,  2.32it/s, est. speed input: 1685.36 toks/s, output: 358.15 toks/s]
[2025-01-06 16:59:10,020][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 16:59:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:01<00:12,  1.72s/it, est. speed input: 544.70 toks/s, output: 16.86 toks/s]Processed prompts:  25%|██▌       | 2/8 [00:01<00:04,  1.23it/s, est. speed input: 992.87 toks/s, output: 34.15 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:02<00:02,  1.96it/s, est. speed input: 1214.11 toks/s, output: 52.19 toks/s]Processed prompts:  62%|██████▎   | 5/8 [00:02<00:00,  3.18it/s, est. speed input: 1849.03 toks/s, output: 87.05 toks/s]Processed prompts:  75%|███████▌  | 6/8 [00:03<00:00,  2.35it/s, est. speed input: 1727.25 toks/s, output: 97.04 toks/s]Processed prompts:  88%|████████▊ | 7/8 [00:03<00:00,  2.69it/s, est. speed input: 1889.78 toks/s, output: 121.65 toks/s]Processed prompts: 100%|██████████| 8/8 [00:03<00:00,  2.37it/s, est. speed input: 2144.30 toks/s, output: 151.85 toks/s]
[2025-01-06 16:59:21,211][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 16:59:28,512][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 16:59:28,512][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 16:59:28,867][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 16:59:41,784][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 16:59:41,785][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 16:59:42,151][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 16:59:42,303][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 16:59:49,728][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:00:06,260][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:00:06,261][root][INFO] - Iteration 42 took 2m 12s. Generation: 65.78%, Training: 34.22%. Estimated time remaining: 3h 16m 13s. Estimated total time for complete run: 4h 42m 33s.
[2025-01-06 17:00:06,622][root][INFO] - Loading VLLM model.
WARNING 01-06 17:00:06 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:00:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:00:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:00:07 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:00:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:00:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:00:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:00:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:00:48 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:00:48,290][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:00:48,537][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:00:48,539][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:00:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:14,  4.33s/it, est. speed input: 116.73 toks/s, output: 6.24 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:01,  2.06s/it, est. speed input: 210.19 toks/s, output: 12.90 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:47,  1.65s/it, est. speed input: 253.77 toks/s, output: 19.60 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.25s/it, est. speed input: 306.11 toks/s, output: 27.73 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:16,  1.55it/s, est. speed input: 444.49 toks/s, output: 46.94 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:12,  1.96it/s, est. speed input: 506.81 toks/s, output: 56.34 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:14,  1.63it/s, est. speed input: 515.10 toks/s, output: 61.58 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:10,  2.15it/s, est. speed input: 572.14 toks/s, output: 72.38 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:10,  2.19it/s, est. speed input: 602.76 toks/s, output: 80.69 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:09,  2.24it/s, est. speed input: 631.08 toks/s, output: 89.29 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:05,  3.18it/s, est. speed input: 720.26 toks/s, output: 111.36 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:04,  3.78it/s, est. speed input: 798.08 toks/s, output: 132.75 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:03,  4.45it/s, est. speed input: 876.02 toks/s, output: 155.31 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:02,  5.99it/s, est. speed input: 968.37 toks/s, output: 181.16 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:01,  5.67it/s, est. speed input: 1029.60 toks/s, output: 202.04 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  5.51it/s, est. speed input: 1086.94 toks/s, output: 224.03 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:01,  5.92it/s, est. speed input: 1122.34 toks/s, output: 237.06 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:01,  5.78it/s, est. speed input: 1149.10 toks/s, output: 248.75 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:00,  6.88it/s, est. speed input: 1219.04 toks/s, output: 276.26 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:11<00:00,  7.99it/s, est. speed input: 1289.15 toks/s, output: 304.75 toks/s]Processed prompts:  97%|█████████▋| 31/32 [00:11<00:00,  8.47it/s, est. speed input: 1353.33 toks/s, output: 333.17 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.77it/s, est. speed input: 1396.96 toks/s, output: 350.45 toks/s]
[2025-01-06 17:01:00,527][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:01:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:18,  3.27s/it, est. speed input: 205.13 toks/s, output: 7.96 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:03<00:13,  1.52it/s, est. speed input: 816.31 toks/s, output: 31.95 toks/s]Processed prompts: 100%|██████████| 25/25 [00:03<00:00,  7.21it/s, est. speed input: 4841.22 toks/s, output: 208.57 toks/s]
[2025-01-06 17:01:04,430][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:01:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:03<00:21,  3.66s/it, est. speed input: 190.86 toks/s, output: 31.40 toks/s]Processed prompts:  29%|██▊       | 2/7 [00:04<00:08,  1.71s/it, est. speed input: 349.38 toks/s, output: 60.98 toks/s]Processed prompts:  43%|████▎     | 3/7 [00:04<00:05,  1.33s/it, est. speed input: 429.27 toks/s, output: 84.34 toks/s]Processed prompts:  57%|█████▋    | 4/7 [00:05<00:02,  1.10it/s, est. speed input: 543.50 toks/s, output: 115.07 toks/s]Processed prompts:  71%|███████▏  | 5/7 [00:05<00:01,  1.38it/s, est. speed input: 630.55 toks/s, output: 142.89 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s, est. speed input: 882.74 toks/s, output: 215.05 toks/s]
[2025-01-06 17:01:10,440][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:01:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:07<02:56,  7.07s/it, est. speed input: 134.87 toks/s, output: 13.15 toks/s]Processed prompts:   8%|▊         | 2/26 [00:07<01:12,  3.03s/it, est. speed input: 227.06 toks/s, output: 26.10 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:08<00:51,  2.22s/it, est. speed input: 275.53 toks/s, output: 36.55 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:08<00:22,  1.06s/it, est. speed input: 422.71 toks/s, output: 63.58 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:09<00:16,  1.24it/s, est. speed input: 491.28 toks/s, output: 76.97 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:09<00:13,  1.43it/s, est. speed input: 541.85 toks/s, output: 88.41 toks/s]Processed prompts:  31%|███       | 8/26 [00:09<00:09,  1.83it/s, est. speed input: 604.35 toks/s, output: 102.02 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:10<00:08,  1.99it/s, est. speed input: 685.22 toks/s, output: 123.33 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:11<00:07,  2.00it/s, est. speed input: 717.71 toks/s, output: 134.25 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:11<00:03,  3.18it/s, est. speed input: 858.10 toks/s, output: 178.38 toks/s]Processed prompts:  58%|█████▊    | 15/26 [00:11<00:03,  3.43it/s, est. speed input: 903.37 toks/s, output: 192.47 toks/s]Processed prompts: 100%|██████████| 26/26 [00:11<00:00,  2.23it/s, est. speed input: 1561.65 toks/s, output: 380.82 toks/s]
[2025-01-06 17:01:22,645][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:01:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:24,  2.50s/it, est. speed input: 370.10 toks/s, output: 15.60 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:02<00:11,  1.26s/it, est. speed input: 651.12 toks/s, output: 31.53 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:03<00:02,  2.07it/s, est. speed input: 1388.88 toks/s, output: 79.39 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:03<00:02,  2.04it/s, est. speed input: 1449.71 toks/s, output: 92.66 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:04<00:01,  2.56it/s, est. speed input: 1644.33 toks/s, output: 114.25 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:05<00:00,  2.77it/s, est. speed input: 1882.90 toks/s, output: 161.94 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  2.22it/s, est. speed input: 1784.01 toks/s, output: 173.82 toks/s]Processed prompts: 100%|██████████| 11/11 [00:05<00:00,  1.88it/s, est. speed input: 1784.01 toks/s, output: 173.82 toks/s]
[2025-01-06 17:01:36,355][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:01:43,733][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:01:43,733][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:01:44,100][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:01:57,344][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:01:57,345][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:01:57,720][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:01:57,871][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 17:02:05,396][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:02:22,092][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:02:22,093][root][INFO] - Iteration 43 took 2m 15s. Generation: 66.22%, Training: 33.78%. Estimated time remaining: 3h 21m 10s. Estimated total time for complete run: 4h 49m 46s.
[2025-01-06 17:02:22,427][root][INFO] - Loading VLLM model.
WARNING 01-06 17:02:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:02:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:02:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:02:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:02:27 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:02:41 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:02:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:02:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:03:03 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:03:03,625][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:03:03,898][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:03:03,899][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:03:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:03,  3.99s/it, est. speed input: 126.71 toks/s, output: 6.77 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:25,  2.84s/it, est. speed input: 167.77 toks/s, output: 14.62 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:50,  1.73s/it, est. speed input: 235.67 toks/s, output: 24.27 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:31,  1.14s/it, est. speed input: 303.41 toks/s, output: 34.25 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:19,  1.36it/s, est. speed input: 408.69 toks/s, output: 52.20 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:16,  1.48it/s, est. speed input: 445.20 toks/s, output: 60.83 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:15,  1.52it/s, est. speed input: 472.33 toks/s, output: 69.10 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:12,  1.85it/s, est. speed input: 516.28 toks/s, output: 79.97 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:09,  2.27it/s, est. speed input: 561.32 toks/s, output: 91.26 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:07,  2.83it/s, est. speed input: 607.88 toks/s, output: 102.97 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:06,  3.16it/s, est. speed input: 646.96 toks/s, output: 113.80 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:05,  3.64it/s, est. speed input: 687.89 toks/s, output: 125.21 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:02,  5.73it/s, est. speed input: 821.88 toks/s, output: 161.93 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:02,  6.06it/s, est. speed input: 897.71 toks/s, output: 185.07 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:01,  6.95it/s, est. speed input: 977.91 toks/s, output: 209.91 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:01,  7.01it/s, est. speed input: 1047.34 toks/s, output: 233.51 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  5.67it/s, est. speed input: 1062.55 toks/s, output: 242.15 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:11<00:01,  5.53it/s, est. speed input: 1089.10 toks/s, output: 253.76 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:00,  7.53it/s, est. speed input: 1168.68 toks/s, output: 283.40 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:01,  4.97it/s, est. speed input: 1166.26 toks/s, output: 289.45 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.74it/s, est. speed input: 1382.20 toks/s, output: 374.97 toks/s]
[2025-01-06 17:03:16,008][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:03:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:11,  3.12s/it, est. speed input: 238.49 toks/s, output: 8.00 toks/s]Processed prompts:   8%|▊         | 2/24 [00:03<00:30,  1.40s/it, est. speed input: 423.74 toks/s, output: 16.27 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:03<00:00, 10.55it/s, est. speed input: 3931.39 toks/s, output: 174.43 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.84it/s, est. speed input: 2585.69 toks/s, output: 146.10 toks/s]
[2025-01-06 17:03:22,719][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:03:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:02<00:16,  2.41s/it, est. speed input: 289.85 toks/s, output: 25.71 toks/s]Processed prompts:  25%|██▌       | 2/8 [00:04<00:14,  2.49s/it, est. speed input: 282.29 toks/s, output: 44.83 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:05<00:07,  1.42s/it, est. speed input: 411.35 toks/s, output: 76.11 toks/s]Processed prompts:  50%|█████     | 4/8 [00:05<00:03,  1.11it/s, est. speed input: 536.51 toks/s, output: 107.26 toks/s]Processed prompts:  62%|██████▎   | 5/8 [00:05<00:01,  1.51it/s, est. speed input: 641.40 toks/s, output: 135.99 toks/s]Processed prompts:  75%|███████▌  | 6/8 [00:05<00:00,  2.06it/s, est. speed input: 750.47 toks/s, output: 166.41 toks/s]Processed prompts: 100%|██████████| 8/8 [00:05<00:00,  3.43it/s, est. speed input: 968.73 toks/s, output: 228.50 toks/s]Processed prompts: 100%|██████████| 8/8 [00:05<00:00,  1.39it/s, est. speed input: 968.73 toks/s, output: 228.50 toks/s]
[2025-01-06 17:03:28,942][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:03:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:30,  3.62s/it, est. speed input: 263.77 toks/s, output: 8.02 toks/s]Processed prompts:   8%|▊         | 2/26 [00:04<00:54,  2.27s/it, est. speed input: 357.23 toks/s, output: 16.96 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:07<00:55,  2.40s/it, est. speed input: 303.46 toks/s, output: 25.30 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:08<00:42,  1.92s/it, est. speed input: 343.19 toks/s, output: 36.88 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:09<00:31,  1.52s/it, est. speed input: 387.64 toks/s, output: 49.23 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:10<00:24,  1.24s/it, est. speed input: 430.02 toks/s, output: 61.81 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:10<00:17,  1.07it/s, est. speed input: 483.89 toks/s, output: 76.09 toks/s]Processed prompts:  31%|███       | 8/26 [00:10<00:13,  1.36it/s, est. speed input: 535.03 toks/s, output: 90.25 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:11<00:09,  1.72it/s, est. speed input: 586.15 toks/s, output: 104.68 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:11<00:07,  2.15it/s, est. speed input: 637.65 toks/s, output: 119.40 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:11<00:06,  2.47it/s, est. speed input: 683.53 toks/s, output: 133.48 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:11<00:04,  2.87it/s, est. speed input: 730.29 toks/s, output: 148.02 toks/s]Processed prompts: 100%|██████████| 26/26 [00:11<00:00,  2.22it/s, est. speed input: 1548.04 toks/s, output: 386.60 toks/s]
[2025-01-06 17:03:41,194][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:03:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:42,  2.84s/it, est. speed input: 263.67 toks/s, output: 10.20 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:03<00:09,  1.24it/s, est. speed input: 942.46 toks/s, output: 37.84 toks/s]Processed prompts:  31%|███▏      | 5/16 [00:04<00:07,  1.53it/s, est. speed input: 1041.49 toks/s, output: 51.51 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:04<00:06,  1.54it/s, est. speed input: 1104.42 toks/s, output: 62.84 toks/s]Processed prompts:  50%|█████     | 8/16 [00:04<00:03,  2.61it/s, est. speed input: 1471.50 toks/s, output: 98.06 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:05<00:01,  3.45it/s, est. speed input: 1757.52 toks/s, output: 129.74 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:05<00:00,  4.54it/s, est. speed input: 2053.00 toks/s, output: 165.29 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:06<00:00,  2.71it/s, est. speed input: 1929.12 toks/s, output: 175.55 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:07<00:00,  2.66it/s, est. speed input: 1953.68 toks/s, output: 193.28 toks/s]Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.25it/s, est. speed input: 2069.22 toks/s, output: 219.56 toks/s]
[2025-01-06 17:03:56,267][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:04:03,576][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:04:03,576][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:04:03,948][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:04:17,710][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:04:17,712][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:04:18,079][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:04:18,249][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:04:25,563][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:04:42,795][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:04:42,796][root][INFO] - Iteration 44 took 2m 20s. Generation: 66.82%, Training: 33.18%. Estimated time remaining: 3h 29m 13s. Estimated total time for complete run: 5h 0m 9s.
[2025-01-06 17:04:43,094][root][INFO] - Loading VLLM model.
WARNING 01-06 17:04:43 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:04:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:04:43 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:04:43 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:04:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:05:02 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:05:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:05:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:05:25 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:05:25,442][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:05:25,693][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:05:25,694][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:05:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:15,  4.36s/it, est. speed input: 115.87 toks/s, output: 6.20 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:06,  2.22s/it, est. speed input: 198.91 toks/s, output: 13.00 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:51,  1.79s/it, est. speed input: 238.21 toks/s, output: 19.97 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:36,  1.29s/it, est. speed input: 293.83 toks/s, output: 28.66 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:23,  1.13it/s, est. speed input: 358.56 toks/s, output: 38.34 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:17,  1.52it/s, est. speed input: 417.48 toks/s, output: 47.81 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:14,  1.78it/s, est. speed input: 463.53 toks/s, output: 56.52 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:10,  2.18it/s, est. speed input: 545.90 toks/s, output: 73.75 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:12,  1.81it/s, est. speed input: 552.18 toks/s, output: 79.71 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:06,  2.87it/s, est. speed input: 652.69 toks/s, output: 103.72 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:03,  4.78it/s, est. speed input: 801.22 toks/s, output: 139.94 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:02,  5.54it/s, est. speed input: 886.21 toks/s, output: 162.58 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:02,  5.34it/s, est. speed input: 917.77 toks/s, output: 172.55 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  5.63it/s, est. speed input: 955.13 toks/s, output: 183.86 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:02,  4.40it/s, est. speed input: 994.09 toks/s, output: 200.97 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:02,  4.71it/s, est. speed input: 1026.40 toks/s, output: 213.04 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:11<00:02,  3.68it/s, est. speed input: 1028.39 toks/s, output: 219.93 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:11<00:01,  4.30it/s, est. speed input: 1062.48 toks/s, output: 233.71 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:01,  4.68it/s, est. speed input: 1091.34 toks/s, output: 246.71 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:01,  4.17it/s, est. speed input: 1105.39 toks/s, output: 257.11 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.69it/s, est. speed input: 1360.46 toks/s, output: 358.13 toks/s]
[2025-01-06 17:05:38,006][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:05:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:07,  3.08s/it, est. speed input: 241.67 toks/s, output: 8.43 toks/s]Processed prompts:   9%|▊         | 2/23 [00:03<00:28,  1.35s/it, est. speed input: 407.97 toks/s, output: 17.05 toks/s]Processed prompts:  83%|████████▎ | 19/23 [00:03<00:00,  9.83it/s, est. speed input: 3660.68 toks/s, output: 163.63 toks/s]Processed prompts: 100%|██████████| 23/23 [00:06<00:00,  4.08it/s, est. speed input: 2477.06 toks/s, output: 153.14 toks/s]Processed prompts: 100%|██████████| 23/23 [00:06<00:00,  3.67it/s, est. speed input: 2477.06 toks/s, output: 153.14 toks/s]
[2025-01-06 17:05:44,723][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:05:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:04<00:33,  4.23s/it, est. speed input: 165.18 toks/s, output: 28.36 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:05<00:16,  2.34s/it, est. speed input: 266.27 toks/s, output: 52.95 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:05<00:08,  1.50s/it, est. speed input: 365.09 toks/s, output: 79.22 toks/s]Processed prompts:  44%|████▍     | 4/9 [00:06<00:05,  1.13s/it, est. speed input: 443.73 toks/s, output: 103.95 toks/s]Processed prompts: 100%|██████████| 9/9 [00:06<00:00,  1.43it/s, est. speed input: 998.36 toks/s, output: 262.64 toks/s]
[2025-01-06 17:05:51,449][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:05:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:07<03:35,  7.70s/it, est. speed input: 123.88 toks/s, output: 11.95 toks/s]Processed prompts:   7%|▋         | 2/29 [00:07<01:28,  3.27s/it, est. speed input: 242.44 toks/s, output: 23.76 toks/s]Processed prompts:  10%|█         | 3/29 [00:08<00:50,  1.93s/it, est. speed input: 349.15 toks/s, output: 35.13 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:08<00:30,  1.23s/it, est. speed input: 456.60 toks/s, output: 46.90 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:10<00:36,  1.53s/it, est. speed input: 457.57 toks/s, output: 51.43 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:10<00:18,  1.21it/s, est. speed input: 574.02 toks/s, output: 77.54 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:11<00:15,  1.38it/s, est. speed input: 614.70 toks/s, output: 88.83 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:11<00:08,  2.13it/s, est. speed input: 722.60 toks/s, output: 115.28 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:11<00:07,  2.51it/s, est. speed input: 772.07 toks/s, output: 128.13 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:11<00:04,  3.57it/s, est. speed input: 877.07 toks/s, output: 155.14 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:04,  3.40it/s, est. speed input: 909.88 toks/s, output: 165.83 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:12<00:02,  4.81it/s, est. speed input: 1012.97 toks/s, output: 194.16 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:12<00:03,  3.87it/s, est. speed input: 1032.83 toks/s, output: 203.16 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.28it/s, est. speed input: 1662.29 toks/s, output: 391.76 toks/s]
[2025-01-06 17:06:04,745][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:06:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:03<00:37,  3.10s/it, est. speed input: 296.11 toks/s, output: 15.81 toks/s]Processed prompts:  15%|█▌        | 2/13 [00:03<00:17,  1.59s/it, est. speed input: 459.74 toks/s, output: 31.42 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:03<00:09,  1.07it/s, est. speed input: 681.57 toks/s, output: 48.61 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:04<00:02,  2.47it/s, est. speed input: 1294.36 toks/s, output: 97.57 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:04<00:02,  2.89it/s, est. speed input: 1466.68 toks/s, output: 114.94 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:04<00:01,  2.89it/s, est. speed input: 1562.73 toks/s, output: 128.91 toks/s]Processed prompts:  69%|██████▉   | 9/13 [00:04<00:01,  3.25it/s, est. speed input: 1692.00 toks/s, output: 146.78 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:05<00:00,  3.69it/s, est. speed input: 1870.08 toks/s, output: 181.73 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  4.13it/s, est. speed input: 2075.25 toks/s, output: 221.30 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.27it/s, est. speed input: 2075.25 toks/s, output: 221.30 toks/s]
[2025-01-06 17:06:18,617][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 17:06:26,131][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:06:26,132][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:06:26,505][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:06:40,161][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:06:40,162][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:06:40,611][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:06:40,764][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:06:48,084][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:07:05,430][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:07:05,432][root][INFO] - Iteration 45 took 2m 22s. Generation: 67.07%, Training: 32.93%. Estimated time remaining: 3h 30m 58s. Estimated total time for complete run: 5h 4m 17s.
[2025-01-06 17:07:05,748][root][INFO] - Loading VLLM model.
WARNING 01-06 17:07:05 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:07:05 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:07:06 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:07:06 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:07:11 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:07:25 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:07:25 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:07:25 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:07:47 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:07:47,290][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:07:47,540][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:07:47,541][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:07:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:01,  3.92s/it, est. speed input: 128.75 toks/s, output: 6.88 toks/s]Processed prompts:   9%|▉         | 3/32 [00:04<00:38,  1.34s/it, est. speed input: 315.83 toks/s, output: 20.01 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:41,  1.49s/it, est. speed input: 307.57 toks/s, output: 25.73 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:31,  1.16s/it, est. speed input: 357.22 toks/s, output: 35.51 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:25,  1.04it/s, est. speed input: 398.31 toks/s, output: 45.09 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:18,  1.35it/s, est. speed input: 449.14 toks/s, output: 55.90 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:16,  1.46it/s, est. speed input: 479.09 toks/s, output: 64.99 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:12,  1.92it/s, est. speed input: 529.53 toks/s, output: 76.78 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:07,  2.90it/s, est. speed input: 626.44 toks/s, output: 100.14 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:06,  3.17it/s, est. speed input: 666.19 toks/s, output: 111.03 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:03,  4.54it/s, est. speed input: 762.63 toks/s, output: 135.81 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:01,  7.62it/s, est. speed input: 957.21 toks/s, output: 186.70 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:01,  6.00it/s, est. speed input: 1008.87 toks/s, output: 205.57 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:01,  5.83it/s, est. speed input: 1038.95 toks/s, output: 216.61 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  6.87it/s, est. speed input: 1117.96 toks/s, output: 243.13 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:10<00:00,  9.60it/s, est. speed input: 1247.85 toks/s, output: 286.06 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:11<00:00,  5.12it/s, est. speed input: 1242.44 toks/s, output: 299.27 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.81it/s, est. speed input: 1419.90 toks/s, output: 369.56 toks/s]
[2025-01-06 17:07:59,391][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:07:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:17,  3.35s/it, est. speed input: 200.25 toks/s, output: 8.65 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:03<00:00,  8.56it/s, est. speed input: 4262.07 toks/s, output: 188.46 toks/s]Processed prompts: 100%|██████████| 24/24 [00:04<00:00,  4.84it/s, est. speed input: 3287.04 toks/s, output: 163.90 toks/s]
[2025-01-06 17:08:04,767][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:08:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:05<00:39,  5.61s/it, est. speed input: 124.64 toks/s, output: 32.27 toks/s]Processed prompts:  25%|██▌       | 2/8 [00:06<00:15,  2.60s/it, est. speed input: 229.07 toks/s, output: 62.43 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 916.26 toks/s, output: 259.05 toks/s]
[2025-01-06 17:08:11,283][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:08:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:19,  4.51s/it, est. speed input: 211.35 toks/s, output: 6.42 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:08,  2.29s/it, est. speed input: 364.13 toks/s, output: 13.36 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:52,  1.81s/it, est. speed input: 437.37 toks/s, output: 20.38 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:43,  1.55s/it, est. speed input: 495.94 toks/s, output: 28.03 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:32,  1.22s/it, est. speed input: 571.74 toks/s, output: 37.18 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:25,  1.03it/s, est. speed input: 619.51 toks/s, output: 46.75 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:15,  1.44it/s, est. speed input: 757.11 toks/s, output: 72.77 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:13,  1.62it/s, est. speed input: 798.56 toks/s, output: 83.52 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:10,  1.98it/s, est. speed input: 876.77 toks/s, output: 104.65 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:11<00:06,  2.87it/s, est. speed input: 1011.59 toks/s, output: 130.73 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:11<00:05,  3.04it/s, est. speed input: 1049.69 toks/s, output: 141.81 toks/s]Processed prompts:  50%|█████     | 16/32 [00:12<00:07,  2.06it/s, est. speed input: 1040.08 toks/s, output: 145.10 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:13<00:05,  2.72it/s, est. speed input: 1117.96 toks/s, output: 170.54 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:13<00:04,  3.22it/s, est. speed input: 1162.00 toks/s, output: 184.36 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.43it/s, est. speed input: 1853.35 toks/s, output: 382.17 toks/s]
[2025-01-06 17:08:25,029][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:08:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:02<00:29,  2.45s/it, est. speed input: 348.79 toks/s, output: 11.83 toks/s]Processed prompts:  31%|███       | 4/13 [00:03<00:07,  1.22it/s, est. speed input: 978.00 toks/s, output: 42.56 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:03<00:05,  1.59it/s, est. speed input: 1186.65 toks/s, output: 60.98 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:04<00:03,  2.01it/s, est. speed input: 1373.60 toks/s, output: 79.19 toks/s]Processed prompts:  54%|█████▍    | 7/13 [00:04<00:03,  1.98it/s, est. speed input: 1402.57 toks/s, output: 92.76 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:04<00:02,  2.33it/s, est. speed input: 1528.58 toks/s, output: 111.62 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:05<00:00,  3.34it/s, est. speed input: 1814.91 toks/s, output: 152.75 toks/s]Processed prompts:  85%|████████▍ | 11/13 [00:05<00:00,  3.22it/s, est. speed input: 1875.21 toks/s, output: 169.48 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  4.31it/s, est. speed input: 2130.30 toks/s, output: 215.74 toks/s]Processed prompts: 100%|██████████| 13/13 [00:05<00:00,  2.28it/s, est. speed input: 2130.30 toks/s, output: 215.74 toks/s]
[2025-01-06 17:08:38,865][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-06 17:08:46,062][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:08:46,063][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:08:46,425][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:08:59,976][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:08:59,977][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:09:00,364][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:09:00,574][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:09:07,958][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:09:25,642][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:09:25,644][root][INFO] - Iteration 46 took 2m 20s. Generation: 66.53%, Training: 33.47%. Estimated time remaining: 3h 23m 27s. Estimated total time for complete run: 4h 59m 7s.
[2025-01-06 17:09:26,064][root][INFO] - Loading VLLM model.
WARNING 01-06 17:09:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:09:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:09:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:09:26 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:09:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:09:45 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:09:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:09:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:10:07 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:10:07,513][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:10:07,776][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:10:07,777][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:10:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:13,  4.31s/it, est. speed input: 117.25 toks/s, output: 6.27 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:40,  3.36s/it, est. speed input: 144.29 toks/s, output: 14.14 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:40,  1.43s/it, est. speed input: 266.73 toks/s, output: 33.54 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:28,  1.04s/it, est. speed input: 326.20 toks/s, output: 43.80 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:16,  1.48it/s, est. speed input: 427.45 toks/s, output: 62.88 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:14,  1.61it/s, est. speed input: 462.76 toks/s, output: 71.48 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:11,  2.06it/s, est. speed input: 514.69 toks/s, output: 82.67 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:06,  3.06it/s, est. speed input: 612.68 toks/s, output: 104.45 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:05,  3.41it/s, est. speed input: 655.16 toks/s, output: 114.82 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:03,  4.75it/s, est. speed input: 750.26 toks/s, output: 137.64 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:03,  4.77it/s, est. speed input: 786.53 toks/s, output: 147.55 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:04,  3.50it/s, est. speed input: 795.57 toks/s, output: 153.40 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:03,  3.82it/s, est. speed input: 829.73 toks/s, output: 164.30 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:03,  4.00it/s, est. speed input: 860.39 toks/s, output: 174.92 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  4.76it/s, est. speed input: 899.14 toks/s, output: 187.32 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:02,  5.30it/s, est. speed input: 934.63 toks/s, output: 199.33 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:11<00:02,  5.24it/s, est. speed input: 963.87 toks/s, output: 210.41 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:11<00:01,  5.83it/s, est. speed input: 998.47 toks/s, output: 222.88 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:11<00:01,  6.08it/s, est. speed input: 1030.23 toks/s, output: 235.05 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:00, 10.79it/s, est. speed input: 1153.64 toks/s, output: 278.17 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:11<00:00,  6.10it/s, est. speed input: 1180.48 toks/s, output: 296.12 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s, est. speed input: 1349.09 toks/s, output: 362.90 toks/s]
[2025-01-06 17:10:20,183][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:10:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:29,  3.58s/it, est. speed input: 191.69 toks/s, output: 8.10 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:03<00:00,  9.04it/s, est. speed input: 4428.78 toks/s, output: 192.43 toks/s]Processed prompts: 100%|██████████| 26/26 [00:06<00:00,  4.01it/s, est. speed input: 2746.24 toks/s, output: 147.91 toks/s]
[2025-01-06 17:10:27,104][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:10:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:03<00:18,  3.66s/it, est. speed input: 190.94 toks/s, output: 34.42 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:04<00:09,  2.25s/it, est. speed input: 283.59 toks/s, output: 62.48 toks/s]Processed prompts:  50%|█████     | 3/6 [00:05<00:04,  1.35s/it, est. speed input: 402.44 toks/s, output: 96.53 toks/s]Processed prompts:  67%|██████▋   | 4/6 [00:05<00:01,  1.16it/s, est. speed input: 526.46 toks/s, output: 132.37 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.13it/s, est. speed input: 789.67 toks/s, output: 207.68 toks/s]
[2025-01-06 17:10:32,872][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:10:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:08<03:59,  8.54s/it, est. speed input: 81.82 toks/s, output: 12.64 toks/s]Processed prompts:   7%|▋         | 2/29 [00:08<01:36,  3.58s/it, est. speed input: 190.86 toks/s, output: 25.19 toks/s]Processed prompts:  10%|█         | 3/29 [00:09<01:04,  2.49s/it, est. speed input: 238.60 toks/s, output: 35.52 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:10<00:44,  1.76s/it, est. speed input: 290.69 toks/s, output: 47.08 toks/s]Processed prompts:  21%|██        | 6/29 [00:10<00:21,  1.08it/s, est. speed input: 410.05 toks/s, output: 72.83 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:11<00:16,  1.31it/s, est. speed input: 443.07 toks/s, output: 84.72 toks/s]Processed prompts:  31%|███       | 9/29 [00:11<00:10,  1.89it/s, est. speed input: 529.14 toks/s, output: 109.40 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:11<00:06,  2.72it/s, est. speed input: 658.54 toks/s, output: 136.17 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:12<00:06,  2.58it/s, est. speed input: 690.51 toks/s, output: 145.98 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:12<00:05,  2.82it/s, est. speed input: 732.55 toks/s, output: 158.22 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:04,  3.40it/s, est. speed input: 780.96 toks/s, output: 171.99 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:12<00:03,  3.51it/s, est. speed input: 819.40 toks/s, output: 184.01 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.24it/s, est. speed input: 1579.79 toks/s, output: 400.12 toks/s]
[2025-01-06 17:10:46,369][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:10:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:21,  2.14s/it, est. speed input: 331.13 toks/s, output: 13.58 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:03<00:09,  1.14s/it, est. speed input: 635.69 toks/s, output: 38.37 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:03<00:03,  1.68it/s, est. speed input: 1082.53 toks/s, output: 82.58 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:04<00:02,  2.02it/s, est. speed input: 1256.52 toks/s, output: 102.48 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:04<00:01,  2.33it/s, est. speed input: 1402.40 toks/s, output: 121.99 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:04<00:01,  2.19it/s, est. speed input: 1440.50 toks/s, output: 136.46 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:05<00:01,  1.81it/s, est. speed input: 1408.69 toks/s, output: 148.18 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:05<00:00,  2.25it/s, est. speed input: 1527.14 toks/s, output: 174.91 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  2.54it/s, est. speed input: 1615.72 toks/s, output: 199.86 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  1.79it/s, est. speed input: 1615.72 toks/s, output: 199.86 toks/s]
[2025-01-06 17:11:00,672][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:11:07,986][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:11:07,986][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:11:08,348][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:11:21,978][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:11:21,979][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:11:22,327][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:11:22,478][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 17:11:29,973][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:11:46,929][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:11:46,930][root][INFO] - Iteration 47 took 2m 21s. Generation: 67.16%, Training: 32.84%. Estimated time remaining: 3h 23m 24s. Estimated total time for complete run: 5h 1m 24s.
[2025-01-06 17:11:47,268][root][INFO] - Loading VLLM model.
WARNING 01-06 17:11:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:11:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:11:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:11:48 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:11:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:12:06 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:12:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:12:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:12:28 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:12:28,676][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:12:28,963][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:12:28,964][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:12:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 120.87 toks/s, output: 6.46 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:47,  1.62s/it, est. speed input: 268.82 toks/s, output: 18.81 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:40,  1.46s/it, est. speed input: 297.93 toks/s, output: 26.25 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:31,  1.16s/it, est. speed input: 344.14 toks/s, output: 35.44 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:22,  1.17it/s, est. speed input: 401.15 toks/s, output: 45.81 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:21,  1.17it/s, est. speed input: 421.04 toks/s, output: 53.36 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:16,  1.46it/s, est. speed input: 464.20 toks/s, output: 63.88 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:13,  1.77it/s, est. speed input: 504.81 toks/s, output: 74.42 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:12,  1.71it/s, est. speed input: 524.31 toks/s, output: 82.75 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:08,  2.45it/s, est. speed input: 603.25 toks/s, output: 105.62 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:05,  3.46it/s, est. speed input: 688.78 toks/s, output: 130.55 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:01,  7.52it/s, est. speed input: 924.24 toks/s, output: 197.85 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:01,  9.41it/s, est. speed input: 1053.96 toks/s, output: 236.50 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:01,  7.89it/s, est. speed input: 1109.59 toks/s, output: 257.81 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:00,  7.27it/s, est. speed input: 1165.97 toks/s, output: 280.08 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:11<00:00,  5.64it/s, est. speed input: 1195.07 toks/s, output: 298.68 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.70it/s, est. speed input: 1365.77 toks/s, output: 366.29 toks/s]
[2025-01-06 17:12:41,219][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:12:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:50,  2.67s/it, est. speed input: 251.84 toks/s, output: 8.99 toks/s]Processed prompts:  10%|█         | 2/20 [00:02<00:21,  1.18s/it, est. speed input: 492.31 toks/s, output: 18.22 toks/s]Processed prompts:  80%|████████  | 16/20 [00:02<00:00,  9.98it/s, est. speed input: 3654.96 toks/s, output: 155.68 toks/s]Processed prompts: 100%|██████████| 20/20 [00:04<00:00,  4.31it/s, est. speed input: 2896.81 toks/s, output: 158.03 toks/s]
[2025-01-06 17:12:46,310][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:12:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:04<00:53,  4.88s/it, est. speed input: 143.16 toks/s, output: 23.35 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:06<00:26,  2.69s/it, est. speed input: 231.46 toks/s, output: 43.87 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:06<00:16,  1.87s/it, est. speed input: 302.67 toks/s, output: 64.37 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:07<00:06,  1.11it/s, est. speed input: 481.70 toks/s, output: 113.43 toks/s]Processed prompts:  50%|█████     | 6/12 [00:07<00:04,  1.44it/s, est. speed input: 563.84 toks/s, output: 137.53 toks/s]Processed prompts: 100%|██████████| 12/12 [00:07<00:00,  1.61it/s, est. speed input: 1127.63 toks/s, output: 298.85 toks/s]
[2025-01-06 17:12:54,202][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:12:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:05<02:23,  5.75s/it, est. speed input: 157.31 toks/s, output: 11.83 toks/s]Processed prompts:   8%|▊         | 2/26 [00:06<01:02,  2.61s/it, est. speed input: 301.59 toks/s, output: 23.37 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:07<00:41,  1.81s/it, est. speed input: 388.16 toks/s, output: 33.76 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:08<00:35,  1.62s/it, est. speed input: 410.53 toks/s, output: 42.80 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:09<00:29,  1.41s/it, est. speed input: 438.92 toks/s, output: 53.12 toks/s]Processed prompts:  31%|███       | 8/26 [00:10<00:12,  1.43it/s, est. speed input: 622.62 toks/s, output: 93.50 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:11<00:10,  1.59it/s, est. speed input: 673.89 toks/s, output: 115.63 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:11<00:06,  2.12it/s, est. speed input: 776.22 toks/s, output: 145.15 toks/s]Processed prompts:  50%|█████     | 13/26 [00:11<00:05,  2.42it/s, est. speed input: 824.35 toks/s, output: 159.85 toks/s]Processed prompts:  58%|█████▊    | 15/26 [00:11<00:03,  3.53it/s, est. speed input: 941.88 toks/s, output: 192.45 toks/s]Processed prompts: 100%|██████████| 26/26 [00:11<00:00,  2.21it/s, est. speed input: 1602.49 toks/s, output: 379.86 toks/s]
[2025-01-06 17:13:06,489][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:13:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:22,  2.29s/it, est. speed input: 302.56 toks/s, output: 15.28 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:02<00:10,  1.21s/it, est. speed input: 591.25 toks/s, output: 31.06 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:03<00:06,  1.25it/s, est. speed input: 843.28 toks/s, output: 47.87 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:03<00:03,  1.90it/s, est. speed input: 1051.83 toks/s, output: 66.81 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:03<00:03,  1.64it/s, est. speed input: 1090.75 toks/s, output: 77.80 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:04<00:02,  2.13it/s, est. speed input: 1260.43 toks/s, output: 98.88 toks/s]Processed prompts:  64%|██████▎   | 7/11 [00:05<00:02,  1.65it/s, est. speed input: 1226.54 toks/s, output: 109.43 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:05<00:02,  1.43it/s, est. speed input: 1163.54 toks/s, output: 123.56 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:06<00:01,  1.68it/s, est. speed input: 1248.04 toks/s, output: 148.27 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  1.75it/s, est. speed input: 1547.57 toks/s, output: 212.04 toks/s]
[2025-01-06 17:13:20,955][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:13:28,239][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:13:28,239][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:13:28,601][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:13:41,882][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:13:41,883][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:13:42,239][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:13:42,386][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 17:13:49,825][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:14:07,202][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:14:07,203][root][INFO] - Iteration 48 took 2m 20s. Generation: 66.92%, Training: 33.08%. Estimated time remaining: 3h 18m 54s. Estimated total time for complete run: 4h 59m 15s.
[2025-01-06 17:14:07,508][root][INFO] - Loading VLLM model.
WARNING 01-06 17:14:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:14:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:14:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:14:08 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:14:13 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:14:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:14:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:14:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:14:49 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:14:49,063][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:14:49,348][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:14:49,350][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:14:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:13,  4.32s/it, est. speed input: 116.85 toks/s, output: 6.25 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:29,  2.98s/it, est. speed input: 158.90 toks/s, output: 13.84 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.33s/it, est. speed input: 286.78 toks/s, output: 31.66 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:28,  1.07s/it, est. speed input: 334.69 toks/s, output: 40.43 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:21,  1.24it/s, est. speed input: 390.46 toks/s, output: 50.39 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:18,  1.35it/s, est. speed input: 423.92 toks/s, output: 58.52 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:13,  1.83it/s, est. speed input: 478.59 toks/s, output: 69.54 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:10,  2.12it/s, est. speed input: 547.93 toks/s, output: 87.02 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:09,  2.23it/s, est. speed input: 579.02 toks/s, output: 96.42 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:05,  3.34it/s, est. speed input: 671.68 toks/s, output: 120.42 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:04,  3.70it/s, est. speed input: 710.81 toks/s, output: 131.50 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:03,  5.32it/s, est. speed input: 802.43 toks/s, output: 156.31 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:02,  5.31it/s, est. speed input: 836.80 toks/s, output: 166.97 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:01,  7.00it/s, est. speed input: 922.53 toks/s, output: 191.81 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  7.89it/s, est. speed input: 1070.61 toks/s, output: 238.36 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:11<00:01,  7.03it/s, est. speed input: 1094.39 toks/s, output: 248.49 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:01,  4.78it/s, est. speed input: 1090.13 toks/s, output: 253.60 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:01,  4.06it/s, est. speed input: 1097.06 toks/s, output: 262.11 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s, est. speed input: 1350.20 toks/s, output: 362.36 toks/s]
[2025-01-06 17:15:01,777][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:15:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:06,  3.03s/it, est. speed input: 245.52 toks/s, output: 8.24 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:03<00:17,  1.17it/s, est. speed input: 626.33 toks/s, output: 25.14 toks/s]Processed prompts:  87%|████████▋ | 20/23 [00:03<00:00, 10.92it/s, est. speed input: 4006.11 toks/s, output: 174.27 toks/s]Processed prompts: 100%|██████████| 23/23 [00:04<00:00,  4.80it/s, est. speed input: 3246.91 toks/s, output: 170.89 toks/s]
[2025-01-06 17:15:07,007][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:15:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:05<00:41,  5.19s/it, est. speed input: 134.63 toks/s, output: 29.66 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:06<00:20,  2.86s/it, est. speed input: 217.53 toks/s, output: 55.08 toks/s]Processed prompts: 100%|██████████| 9/9 [00:06<00:00,  1.40it/s, est. speed input: 978.86 toks/s, output: 272.92 toks/s]
[2025-01-06 17:15:13,870][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:15:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:05<02:39,  5.70s/it, est. speed input: 167.32 toks/s, output: 10.01 toks/s]Processed prompts:   7%|▋         | 2/29 [00:06<01:08,  2.54s/it, est. speed input: 316.08 toks/s, output: 19.89 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:06<00:26,  1.05s/it, est. speed input: 600.45 toks/s, output: 39.82 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:06<00:19,  1.24it/s, est. speed input: 682.66 toks/s, output: 49.45 toks/s]Processed prompts:  21%|██        | 6/29 [00:08<00:24,  1.08s/it, est. speed input: 603.34 toks/s, output: 52.24 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:08<00:17,  1.23it/s, est. speed input: 701.41 toks/s, output: 64.18 toks/s]Processed prompts:  31%|███       | 9/29 [00:08<00:10,  1.92it/s, est. speed input: 829.27 toks/s, output: 87.69 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:09<00:10,  1.83it/s, est. speed input: 848.25 toks/s, output: 96.01 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:11<00:14,  1.24it/s, est. speed input: 794.43 toks/s, output: 98.06 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:11<00:10,  1.59it/s, est. speed input: 844.84 toks/s, output: 112.02 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:08,  1.72it/s, est. speed input: 887.66 toks/s, output: 133.01 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.37it/s, est. speed input: 1749.13 toks/s, output: 377.96 toks/s]
[2025-01-06 17:15:26,686][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:15:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:42,  2.83s/it, est. speed input: 264.97 toks/s, output: 10.25 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:03<00:10,  1.19it/s, est. speed input: 757.64 toks/s, output: 30.49 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:04<00:12,  1.06s/it, est. speed input: 723.40 toks/s, output: 38.19 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:04<00:06,  1.66it/s, est. speed input: 1071.98 toks/s, output: 71.22 toks/s]Processed prompts:  50%|█████     | 8/16 [00:05<00:03,  2.61it/s, est. speed input: 1421.40 toks/s, output: 105.84 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:05<00:02,  2.75it/s, est. speed input: 1521.00 toks/s, output: 119.65 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:05<00:01,  4.08it/s, est. speed input: 1835.33 toks/s, output: 156.86 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:06<00:01,  2.79it/s, est. speed input: 1764.69 toks/s, output: 160.73 toks/s]Processed prompts:  81%|████████▏ | 13/16 [00:06<00:00,  3.35it/s, est. speed input: 1864.86 toks/s, output: 181.37 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:06<00:00,  2.61it/s, est. speed input: 1833.79 toks/s, output: 190.77 toks/s]Processed prompts:  94%|█████████▍| 15/16 [00:07<00:00,  2.61it/s, est. speed input: 1867.20 toks/s, output: 208.00 toks/s]Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.18it/s, est. speed input: 1997.37 toks/s, output: 235.29 toks/s]
[2025-01-06 17:15:42,225][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:15:49,616][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:15:49,617][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:15:49,990][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:16:03,509][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:16:03,510][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:16:03,869][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:16:04,102][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:16:11,464][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:16:29,128][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:16:29,130][root][INFO] - Iteration 49 took 2m 21s. Generation: 66.85%, Training: 33.15%. Estimated time remaining: 3h 20m 3s. Estimated total time for complete run: 5h 2m 46s.
[2025-01-06 17:16:29,449][root][INFO] - Loading VLLM model.
WARNING 01-06 17:16:29 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:16:29 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:16:30 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:16:30 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:16:35 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:16:49 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:16:49 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:16:49 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:17:11 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:17:11,301][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:17:11,576][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:17:11,577][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:17:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:39,  7.09s/it, est. speed input: 71.21 toks/s, output: 10.29 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:00,  2.08s/it, est. speed input: 195.91 toks/s, output: 29.74 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:42,  1.52s/it, est. speed input: 246.62 toks/s, output: 39.31 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:29,  1.11s/it, est. speed input: 298.13 toks/s, output: 49.47 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:18,  1.38it/s, est. speed input: 388.41 toks/s, output: 68.67 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.14it/s, est. speed input: 488.53 toks/s, output: 91.15 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:05,  3.69it/s, est. speed input: 641.69 toks/s, output: 126.22 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:03,  4.70it/s, est. speed input: 771.23 toks/s, output: 158.01 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:03,  4.88it/s, est. speed input: 809.30 toks/s, output: 168.37 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:03,  4.63it/s, est. speed input: 837.66 toks/s, output: 177.29 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  6.03it/s, est. speed input: 923.33 toks/s, output: 201.51 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:01,  7.53it/s, est. speed input: 1007.70 toks/s, output: 226.05 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  6.96it/s, est. speed input: 1069.75 toks/s, output: 247.20 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:01,  6.98it/s, est. speed input: 1101.94 toks/s, output: 258.57 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:01,  5.22it/s, est. speed input: 1110.18 toks/s, output: 265.39 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:01,  5.46it/s, est. speed input: 1139.03 toks/s, output: 277.43 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:11<00:00,  7.51it/s, est. speed input: 1214.55 toks/s, output: 306.13 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:11<00:00,  5.71it/s, est. speed input: 1224.01 toks/s, output: 314.59 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s, est. speed input: 1350.61 toks/s, output: 364.73 toks/s]
[2025-01-06 17:17:24,007][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:17:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:27,  3.51s/it, est. speed input: 212.25 toks/s, output: 7.98 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:03<00:00,  9.00it/s, est. speed input: 4440.06 toks/s, output: 191.22 toks/s]Processed prompts: 100%|██████████| 26/26 [00:04<00:00,  5.83it/s, est. speed input: 3972.81 toks/s, output: 185.39 toks/s]
[2025-01-06 17:17:28,895][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:17:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:25,  5.19s/it, est. speed input: 134.67 toks/s, output: 36.41 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:05<00:09,  2.28s/it, est. speed input: 256.97 toks/s, output: 71.50 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 770.88 toks/s, output: 218.54 toks/s]
[2025-01-06 17:17:34,764][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:17:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:08<03:57,  8.47s/it, est. speed input: 112.60 toks/s, output: 12.63 toks/s]Processed prompts:   7%|▋         | 2/29 [00:08<01:40,  3.72s/it, est. speed input: 215.20 toks/s, output: 24.93 toks/s]Processed prompts:  10%|█         | 3/29 [00:10<01:13,  2.82s/it, est. speed input: 245.79 toks/s, output: 34.60 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:11<00:53,  2.15s/it, est. speed input: 281.98 toks/s, output: 45.55 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:11<00:33,  1.41s/it, est. speed input: 338.60 toks/s, output: 59.44 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:12<00:16,  1.34it/s, est. speed input: 449.35 toks/s, output: 86.91 toks/s]Processed prompts:  31%|███       | 9/29 [00:12<00:09,  2.00it/s, est. speed input: 571.38 toks/s, output: 113.29 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:12<00:09,  2.05it/s, est. speed input: 606.14 toks/s, output: 124.20 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:13<00:06,  2.65it/s, est. speed input: 692.62 toks/s, output: 149.83 toks/s]Processed prompts: 100%|██████████| 29/29 [00:13<00:00,  2.19it/s, est. speed input: 1548.85 toks/s, output: 407.09 toks/s]
[2025-01-06 17:17:48,538][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:17:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/14 [00:02<00:33,  2.58s/it, est. speed input: 290.26 toks/s, output: 11.22 toks/s]Processed prompts:  14%|█▍        | 2/14 [00:04<00:23,  1.93s/it, est. speed input: 418.11 toks/s, output: 24.94 toks/s]Processed prompts:  21%|██▏       | 3/14 [00:04<00:15,  1.40s/it, est. speed input: 550.04 toks/s, output: 40.73 toks/s]Processed prompts:  29%|██▊       | 4/14 [00:05<00:12,  1.28s/it, est. speed input: 608.47 toks/s, output: 55.08 toks/s]Processed prompts:  36%|███▌      | 5/14 [00:06<00:08,  1.07it/s, est. speed input: 729.12 toks/s, output: 74.75 toks/s]Processed prompts:  50%|█████     | 7/14 [00:07<00:05,  1.34it/s, est. speed input: 877.93 toks/s, output: 107.45 toks/s]Processed prompts:  57%|█████▋    | 8/14 [00:07<00:03,  1.74it/s, est. speed input: 993.26 toks/s, output: 130.84 toks/s]Processed prompts:  71%|███████▏  | 10/14 [00:07<00:01,  2.52it/s, est. speed input: 1188.87 toks/s, output: 175.06 toks/s]Processed prompts: 100%|██████████| 14/14 [00:07<00:00,  1.80it/s, est. speed input: 1678.46 toks/s, output: 277.73 toks/s]
[2025-01-06 17:18:04,541][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:18:11,889][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:18:11,890][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:18:12,251][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:18:25,827][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:18:25,828][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:18:26,197][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:18:26,339][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:18:33,651][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:18:51,223][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:18:51,224][root][INFO] - Iteration 50 took 2m 22s. Generation: 67.04%, Training: 32.96%. Estimated time remaining: 3h 18m 3s. Estimated total time for complete run: 5h 3m 8s.
[2025-01-06 17:18:51,559][root][INFO] - Loading VLLM model.
WARNING 01-06 17:18:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:18:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:18:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:18:52 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:18:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:19:10 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:19:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:19:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:19:32 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:19:32,845][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:19:33,093][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:19:33,094][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:19:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:00,  3.90s/it, est. speed input: 129.65 toks/s, output: 6.93 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:24,  2.80s/it, est. speed input: 170.30 toks/s, output: 14.84 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:30,  1.08s/it, est. speed input: 334.05 toks/s, output: 35.22 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:24,  1.09it/s, est. speed input: 382.37 toks/s, output: 43.31 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:06<00:18,  1.42it/s, est. speed input: 444.33 toks/s, output: 53.23 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:15,  1.58it/s, est. speed input: 484.72 toks/s, output: 61.57 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:19,  1.21it/s, est. speed input: 471.42 toks/s, output: 65.34 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:13,  1.64it/s, est. speed input: 524.20 toks/s, output: 77.62 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:06,  3.15it/s, est. speed input: 677.07 toks/s, output: 114.19 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:02,  5.40it/s, est. speed input: 878.39 toks/s, output: 163.94 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:03,  4.77it/s, est. speed input: 900.02 toks/s, output: 172.14 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:02,  5.05it/s, est. speed input: 938.63 toks/s, output: 183.80 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:09<00:01,  6.05it/s, est. speed input: 1021.23 toks/s, output: 208.69 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:02,  5.08it/s, est. speed input: 1038.01 toks/s, output: 217.09 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:10<00:01,  5.13it/s, est. speed input: 1067.93 toks/s, output: 228.58 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:01,  6.18it/s, est. speed input: 1170.76 toks/s, output: 266.70 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:01,  4.79it/s, est. speed input: 1172.65 toks/s, output: 273.83 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:01,  4.64it/s, est. speed input: 1192.14 toks/s, output: 285.55 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.80it/s, est. speed input: 1412.88 toks/s, output: 372.98 toks/s]
[2025-01-06 17:19:44,990][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:19:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:03,  3.03s/it, est. speed input: 232.82 toks/s, output: 8.92 toks/s]Processed prompts:  68%|██████▊   | 15/22 [00:03<00:01,  6.57it/s, est. speed input: 3206.07 toks/s, output: 137.38 toks/s]Processed prompts: 100%|██████████| 22/22 [00:06<00:00,  3.49it/s, est. speed input: 2354.35 toks/s, output: 160.17 toks/s]
[2025-01-06 17:19:51,717][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:19:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:06<01:01,  6.82s/it, est. speed input: 102.56 toks/s, output: 29.34 toks/s]Processed prompts: 100%|██████████| 10/10 [00:06<00:00,  1.47it/s, est. speed input: 1025.55 toks/s, output: 293.43 toks/s]
[2025-01-06 17:19:58,965][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:19:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:05<02:30,  5.18s/it, est. speed input: 98.47 toks/s, output: 8.69 toks/s]Processed prompts:   7%|▋         | 2/30 [00:05<01:07,  2.40s/it, est. speed input: 259.47 toks/s, output: 17.37 toks/s]Processed prompts:  10%|█         | 3/30 [00:05<00:38,  1.41s/it, est. speed input: 412.11 toks/s, output: 26.42 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:09<00:55,  2.12s/it, est. speed input: 371.35 toks/s, output: 29.84 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:09<00:38,  1.53s/it, est. speed input: 425.81 toks/s, output: 41.42 toks/s]Processed prompts:  20%|██        | 6/30 [00:10<00:31,  1.29s/it, est. speed input: 483.60 toks/s, output: 51.68 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:10<00:24,  1.05s/it, est. speed input: 546.17 toks/s, output: 62.94 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:11<00:17,  1.26it/s, est. speed input: 579.79 toks/s, output: 75.59 toks/s]Processed prompts:  30%|███       | 9/30 [00:12<00:17,  1.23it/s, est. speed input: 617.58 toks/s, output: 84.72 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:12<00:12,  1.54it/s, est. speed input: 660.34 toks/s, output: 97.49 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:12<00:10,  1.83it/s, est. speed input: 699.27 toks/s, output: 109.94 toks/s]Processed prompts:  40%|████      | 12/30 [00:12<00:08,  2.06it/s, est. speed input: 734.26 toks/s, output: 122.07 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:13<00:06,  2.65it/s, est. speed input: 780.44 toks/s, output: 136.06 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.28it/s, est. speed input: 1705.22 toks/s, output: 393.96 toks/s]
[2025-01-06 17:20:12,678][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:20:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:42,  2.82s/it, est. speed input: 337.97 toks/s, output: 10.27 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:04<00:26,  1.92s/it, est. speed input: 376.84 toks/s, output: 22.64 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:04<00:16,  1.24s/it, est. speed input: 551.43 toks/s, output: 37.25 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:04<00:10,  1.17it/s, est. speed input: 718.34 toks/s, output: 52.59 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:05<00:05,  1.72it/s, est. speed input: 976.63 toks/s, output: 81.05 toks/s]Processed prompts:  44%|████▍     | 7/16 [00:06<00:05,  1.66it/s, est. speed input: 1015.04 toks/s, output: 93.06 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:06<00:02,  2.37it/s, est. speed input: 1247.77 toks/s, output: 129.13 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:06<00:02,  2.65it/s, est. speed input: 1345.15 toks/s, output: 146.80 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:06<00:01,  3.02it/s, est. speed input: 1444.28 toks/s, output: 165.36 toks/s]Processed prompts:  81%|████████▏ | 13/16 [00:07<00:00,  3.51it/s, est. speed input: 1615.82 toks/s, output: 201.30 toks/s]Processed prompts:  88%|████████▊ | 14/16 [00:07<00:00,  3.15it/s, est. speed input: 1649.25 toks/s, output: 215.79 toks/s]Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.04it/s, est. speed input: 1888.46 toks/s, output: 266.28 toks/s]
[2025-01-06 17:20:29,059][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:20:36,531][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:20:36,532][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:20:36,897][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:20:50,336][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:20:50,337][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:20:50,667][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:20:50,815][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:20:58,295][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:21:16,385][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:21:16,386][root][INFO] - Iteration 51 took 2m 25s. Generation: 67.23%, Training: 32.77%. Estimated time remaining: 3h 22m 10s. Estimated total time for complete run: 5h 9m 40s.
[2025-01-06 17:21:16,704][root][INFO] - Loading VLLM model.
WARNING 01-06 17:21:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:21:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:21:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:21:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:21:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:21:36 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:21:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:21:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:21:58 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:21:58,434][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:21:58,690][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:21:58,691][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:21:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:12,  4.26s/it, est. speed input: 118.59 toks/s, output: 6.34 toks/s]Processed prompts:   6%|▋         | 2/32 [00:07<01:41,  3.37s/it, est. speed input: 144.02 toks/s, output: 14.26 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:00,  2.07s/it, est. speed input: 200.97 toks/s, output: 24.14 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.35s/it, est. speed input: 260.05 toks/s, output: 34.50 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:27,  1.02s/it, est. speed input: 307.42 toks/s, output: 44.07 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:18,  1.37it/s, est. speed input: 361.77 toks/s, output: 54.80 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:13,  1.84it/s, est. speed input: 414.23 toks/s, output: 65.50 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:13,  1.72it/s, est. speed input: 439.20 toks/s, output: 73.05 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:07,  2.76it/s, est. speed input: 534.78 toks/s, output: 95.73 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:03,  4.93it/s, est. speed input: 685.19 toks/s, output: 131.92 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:02,  5.86it/s, est. speed input: 773.84 toks/s, output: 154.36 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:09<00:02,  7.06it/s, est. speed input: 863.08 toks/s, output: 177.84 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:11<00:03,  3.51it/s, est. speed input: 860.60 toks/s, output: 185.30 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:11<00:02,  4.39it/s, est. speed input: 934.60 toks/s, output: 211.59 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:11<00:02,  4.03it/s, est. speed input: 950.45 toks/s, output: 220.89 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:12<00:01,  4.15it/s, est. speed input: 998.03 toks/s, output: 243.99 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.62it/s, est. speed input: 1321.85 toks/s, output: 373.24 toks/s]
[2025-01-06 17:22:11,356][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:22:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:27,  3.50s/it, est. speed input: 212.79 toks/s, output: 7.71 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:03<00:21,  1.06it/s, est. speed input: 610.24 toks/s, output: 23.04 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:06<00:00,  4.85it/s, est. speed input: 2549.66 toks/s, output: 133.80 toks/s]Processed prompts: 100%|██████████| 26/26 [00:06<00:00,  3.85it/s, est. speed input: 2653.30 toks/s, output: 163.05 toks/s]
[2025-01-06 17:22:18,560][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:22:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:04<00:20,  4.12s/it, est. speed input: 169.70 toks/s, output: 35.20 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:05<00:09,  2.43s/it, est. speed input: 260.55 toks/s, output: 64.30 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.12it/s, est. speed input: 781.61 toks/s, output: 213.39 toks/s]
[2025-01-06 17:22:24,370][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:22:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:06<03:02,  6.30s/it, est. speed input: 80.89 toks/s, output: 10.47 toks/s]Processed prompts:   7%|▋         | 2/30 [00:07<01:27,  3.14s/it, est. speed input: 202.45 toks/s, output: 20.47 toks/s]Processed prompts:  10%|█         | 3/30 [00:08<01:05,  2.43s/it, est. speed input: 274.41 toks/s, output: 29.28 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:09<00:41,  1.60s/it, est. speed input: 368.98 toks/s, output: 40.92 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:09<00:29,  1.20s/it, est. speed input: 403.63 toks/s, output: 51.88 toks/s]Processed prompts:  20%|██        | 6/30 [00:10<00:24,  1.02s/it, est. speed input: 445.19 toks/s, output: 61.90 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:10<00:16,  1.39it/s, est. speed input: 489.93 toks/s, output: 74.77 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:11<00:21,  1.05it/s, est. speed input: 488.31 toks/s, output: 79.87 toks/s]Processed prompts:  30%|███       | 9/30 [00:12<00:18,  1.13it/s, est. speed input: 516.08 toks/s, output: 90.03 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:13<00:15,  1.26it/s, est. speed input: 545.55 toks/s, output: 100.94 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.26it/s, est. speed input: 1614.89 toks/s, output: 401.78 toks/s]
[2025-01-06 17:22:38,125][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:22:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:18,  3.75s/it, est. speed input: 163.79 toks/s, output: 7.72 toks/s]Processed prompts:   9%|▉         | 2/22 [00:05<00:56,  2.83s/it, est. speed input: 264.51 toks/s, output: 17.70 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:06<00:32,  1.72s/it, est. speed input: 356.41 toks/s, output: 29.99 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:06<00:20,  1.13s/it, est. speed input: 490.03 toks/s, output: 42.72 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:07<00:15,  1.11it/s, est. speed input: 581.06 toks/s, output: 54.10 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:07<00:07,  2.10it/s, est. speed input: 836.71 toks/s, output: 82.28 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:07<00:05,  2.57it/s, est. speed input: 949.83 toks/s, output: 95.49 toks/s]Processed prompts:  41%|████      | 9/22 [00:07<00:05,  2.22it/s, est. speed input: 996.86 toks/s, output: 104.00 toks/s]Processed prompts:  50%|█████     | 11/22 [00:08<00:04,  2.42it/s, est. speed input: 1132.55 toks/s, output: 127.03 toks/s]Processed prompts:  55%|█████▍    | 12/22 [00:08<00:03,  2.84it/s, est. speed input: 1220.25 toks/s, output: 142.07 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:09<00:02,  3.02it/s, est. speed input: 1288.94 toks/s, output: 155.67 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:09<00:02,  3.14it/s, est. speed input: 1351.37 toks/s, output: 169.25 toks/s]Processed prompts:  73%|███████▎  | 16/22 [00:09<00:01,  4.20it/s, est. speed input: 1510.43 toks/s, output: 201.31 toks/s]Processed prompts:  77%|███████▋  | 17/22 [00:09<00:01,  4.17it/s, est. speed input: 1569.33 toks/s, output: 215.72 toks/s]Processed prompts:  82%|████████▏ | 18/22 [00:10<00:00,  4.43it/s, est. speed input: 1635.26 toks/s, output: 231.61 toks/s]Processed prompts: 100%|██████████| 22/22 [00:10<00:00,  2.18it/s, est. speed input: 2007.88 toks/s, output: 310.96 toks/s]
[2025-01-06 17:22:56,726][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:23:04,203][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:23:04,203][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:23:04,570][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:23:18,578][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:23:18,579][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:23:18,935][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:23:19,085][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:23:26,474][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:23:44,856][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:23:44,857][root][INFO] - Iteration 52 took 2m 28s. Generation: 67.49%, Training: 32.51%. Estimated time remaining: 3h 26m 45s. Estimated total time for complete run: 5h 16m 44s.
[2025-01-06 17:23:45,178][root][INFO] - Loading VLLM model.
WARNING 01-06 17:23:45 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:23:45 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:23:45 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:23:46 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 17:23:50 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:24:04 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:24:05 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:24:05 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:24:26 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:24:26,680][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:24:26,920][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:24:26,922][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:24:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:22,  6.54s/it, est. speed input: 77.26 toks/s, output: 10.86 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:22,  2.76s/it, est. speed input: 151.73 toks/s, output: 21.63 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:51,  1.77s/it, est. speed input: 209.26 toks/s, output: 31.35 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:31,  1.14s/it, est. speed input: 272.54 toks/s, output: 42.23 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:25,  1.05it/s, est. speed input: 314.67 toks/s, output: 51.09 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:21,  1.21it/s, est. speed input: 351.63 toks/s, output: 60.11 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:14,  1.69it/s, est. speed input: 405.26 toks/s, output: 72.00 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:10,  2.21it/s, est. speed input: 455.14 toks/s, output: 83.48 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:05,  3.68it/s, est. speed input: 559.74 toks/s, output: 107.51 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:05,  3.99it/s, est. speed input: 603.09 toks/s, output: 118.34 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:03,  4.98it/s, est. speed input: 692.83 toks/s, output: 140.99 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:02,  6.70it/s, est. speed input: 788.87 toks/s, output: 165.79 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:02,  6.97it/s, est. speed input: 830.95 toks/s, output: 177.30 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:02,  5.05it/s, est. speed input: 849.74 toks/s, output: 184.70 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  5.27it/s, est. speed input: 917.53 toks/s, output: 206.65 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:02,  5.63it/s, est. speed input: 953.51 toks/s, output: 218.74 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:01,  6.01it/s, est. speed input: 988.97 toks/s, output: 230.99 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:01,  6.51it/s, est. speed input: 1056.55 toks/s, output: 255.52 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:00,  7.55it/s, est. speed input: 1128.67 toks/s, output: 282.23 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:00,  6.10it/s, est. speed input: 1144.75 toks/s, output: 291.55 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:00,  5.51it/s, est. speed input: 1164.35 toks/s, output: 302.38 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.72it/s, est. speed input: 1372.01 toks/s, output: 385.54 toks/s]
[2025-01-06 17:24:39,152][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:24:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:02<00:42,  2.53s/it, est. speed input: 258.19 toks/s, output: 9.90 toks/s]Processed prompts:  33%|███▎      | 6/18 [00:02<00:03,  3.03it/s, est. speed input: 1544.06 toks/s, output: 60.32 toks/s]Processed prompts:  94%|█████████▍| 17/18 [00:03<00:00,  8.70it/s, est. speed input: 3832.22 toks/s, output: 164.90 toks/s]Processed prompts: 100%|██████████| 18/18 [00:03<00:00,  5.66it/s, est. speed input: 3905.45 toks/s, output: 175.90 toks/s]
[2025-01-06 17:24:42,755][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:24:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/14 [00:07<01:38,  7.57s/it, est. speed input: 92.33 toks/s, output: 23.78 toks/s]Processed prompts:  14%|█▍        | 2/14 [00:08<00:42,  3.52s/it, est. speed input: 169.47 toks/s, output: 46.06 toks/s]Processed prompts: 100%|██████████| 14/14 [00:08<00:00,  1.70it/s, est. speed input: 1186.24 toks/s, output: 336.99 toks/s]
[2025-01-06 17:24:51,444][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:24:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:04<01:50,  4.08s/it, est. speed input: 233.79 toks/s, output: 7.11 toks/s]Processed prompts:   7%|▋         | 2/28 [00:06<01:26,  3.31s/it, est. speed input: 278.30 toks/s, output: 15.90 toks/s]Processed prompts:  11%|█         | 3/28 [00:09<01:10,  2.82s/it, est. speed input: 314.65 toks/s, output: 25.40 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:09<00:41,  1.75s/it, est. speed input: 414.78 toks/s, output: 38.59 toks/s]Processed prompts:  21%|██▏       | 6/28 [00:09<00:20,  1.10it/s, est. speed input: 599.80 toks/s, output: 63.92 toks/s]Processed prompts:  25%|██▌       | 7/28 [00:09<00:15,  1.33it/s, est. speed input: 675.93 toks/s, output: 75.72 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:10<00:17,  1.17it/s, est. speed input: 694.45 toks/s, output: 82.81 toks/s]Processed prompts:  32%|███▏      | 9/28 [00:11<00:14,  1.36it/s, est. speed input: 728.45 toks/s, output: 94.62 toks/s]Processed prompts:  36%|███▌      | 10/28 [00:11<00:11,  1.54it/s, est. speed input: 760.58 toks/s, output: 106.48 toks/s]Processed prompts:  39%|███▉      | 11/28 [00:12<00:10,  1.58it/s, est. speed input: 780.75 toks/s, output: 117.18 toks/s]Processed prompts:  43%|████▎     | 12/28 [00:12<00:07,  2.08it/s, est. speed input: 813.62 toks/s, output: 131.84 toks/s]Processed prompts: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s, est. speed input: 1720.27 toks/s, output: 384.95 toks/s]
[2025-01-06 17:25:04,610][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:25:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:02<00:37,  2.69s/it, est. speed input: 278.16 toks/s, output: 10.78 toks/s]Processed prompts:  13%|█▎        | 2/15 [00:03<00:22,  1.74s/it, est. speed input: 451.32 toks/s, output: 23.40 toks/s]Processed prompts:  20%|██        | 3/15 [00:04<00:17,  1.46s/it, est. speed input: 542.44 toks/s, output: 36.83 toks/s]Processed prompts:  27%|██▋       | 4/15 [00:05<00:10,  1.01it/s, est. speed input: 659.85 toks/s, output: 54.34 toks/s]Processed prompts:  33%|███▎      | 5/15 [00:06<00:09,  1.00it/s, est. speed input: 706.47 toks/s, output: 66.85 toks/s]Processed prompts:  40%|████      | 6/15 [00:06<00:06,  1.35it/s, est. speed input: 829.06 toks/s, output: 86.22 toks/s]Processed prompts:  47%|████▋     | 7/15 [00:06<00:04,  1.81it/s, est. speed input: 952.58 toks/s, output: 106.18 toks/s]Processed prompts:  53%|█████▎    | 8/15 [00:06<00:03,  2.29it/s, est. speed input: 1066.83 toks/s, output: 125.81 toks/s]Processed prompts:  60%|██████    | 9/15 [00:06<00:02,  2.73it/s, est. speed input: 1171.32 toks/s, output: 145.09 toks/s]Processed prompts:  73%|███████▎  | 11/15 [00:07<00:00,  4.39it/s, est. speed input: 1415.76 toks/s, output: 188.67 toks/s]Processed prompts:  80%|████████  | 12/15 [00:07<00:00,  4.04it/s, est. speed input: 1485.51 toks/s, output: 205.23 toks/s]Processed prompts:  87%|████████▋ | 13/15 [00:07<00:00,  4.24it/s, est. speed input: 1571.15 toks/s, output: 224.82 toks/s]Processed prompts:  93%|█████████▎| 14/15 [00:07<00:00,  4.62it/s, est. speed input: 1660.12 toks/s, output: 245.70 toks/s]Processed prompts: 100%|██████████| 15/15 [00:07<00:00,  1.92it/s, est. speed input: 1782.49 toks/s, output: 271.36 toks/s]
[2025-01-06 17:25:21,024][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:25:28,289][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:25:28,290][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:25:28,664][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:25:41,811][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:25:41,812][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:25:42,230][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:25:42,370][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:25:49,656][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:26:08,170][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:26:08,171][root][INFO] - Iteration 53 took 2m 23s. Generation: 66.99%, Training: 33.01%. Estimated time remaining: 3h 13m 22s. Estimated total time for complete run: 5h 5m 44s.
[2025-01-06 17:26:08,486][root][INFO] - Loading VLLM model.
WARNING 01-06 17:26:08 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:26:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:26:09 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:26:09 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:26:14 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:26:28 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:26:28 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:26:28 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:26:50 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:26:50,468][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:26:50,750][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:26:50,751][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:26:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:45,  7.27s/it, est. speed input: 69.42 toks/s, output: 10.04 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:00,  2.10s/it, est. speed input: 192.80 toks/s, output: 29.14 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:46,  1.65s/it, est. speed input: 233.29 toks/s, output: 37.65 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:31,  1.16s/it, est. speed input: 286.08 toks/s, output: 48.27 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:09<00:24,  1.06it/s, est. speed input: 325.40 toks/s, output: 57.46 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:13,  1.81it/s, est. speed input: 422.27 toks/s, output: 79.23 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.22it/s, est. speed input: 467.70 toks/s, output: 90.04 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:04,  4.16it/s, est. speed input: 614.56 toks/s, output: 125.04 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:03,  4.56it/s, est. speed input: 692.25 toks/s, output: 145.69 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:03,  4.78it/s, est. speed input: 729.81 toks/s, output: 156.08 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:02,  5.68it/s, est. speed input: 809.10 toks/s, output: 178.22 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:02,  6.14it/s, est. speed input: 847.81 toks/s, output: 189.52 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  6.31it/s, est. speed input: 883.14 toks/s, output: 200.37 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:11<00:01,  7.76it/s, est. speed input: 961.49 toks/s, output: 224.12 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:11<00:01,  7.11it/s, est. speed input: 1023.05 toks/s, output: 245.13 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:00,  8.92it/s, est. speed input: 1101.11 toks/s, output: 271.42 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:00,  7.19it/s, est. speed input: 1150.05 toks/s, output: 291.92 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:12<00:00,  6.10it/s, est. speed input: 1166.06 toks/s, output: 301.33 toks/s]Processed prompts:  91%|█████████ | 29/32 [00:12<00:00,  6.21it/s, est. speed input: 1192.88 toks/s, output: 313.92 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s, est. speed input: 1316.26 toks/s, output: 362.79 toks/s]
[2025-01-06 17:27:03,490][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:27:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/19 [00:02<00:48,  2.72s/it, est. speed input: 247.40 toks/s, output: 9.94 toks/s]Processed prompts:  95%|█████████▍| 18/19 [00:03<00:00,  8.02it/s, est. speed input: 4059.87 toks/s, output: 176.08 toks/s]Processed prompts: 100%|██████████| 19/19 [00:03<00:00,  5.06it/s, est. speed input: 3466.85 toks/s, output: 164.40 toks/s]
[2025-01-06 17:27:07,655][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:27:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:07<01:28,  7.40s/it, est. speed input: 94.46 toks/s, output: 25.13 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:07<00:20,  2.02s/it, est. speed input: 272.79 toks/s, output: 74.15 toks/s]Processed prompts:  31%|███       | 4/13 [00:07<00:12,  1.38s/it, est. speed input: 356.82 toks/s, output: 98.27 toks/s]Processed prompts: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s, est. speed input: 1159.62 toks/s, output: 327.96 toks/s]
[2025-01-06 17:27:15,979][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:27:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:07<03:17,  7.04s/it, est. speed input: 135.52 toks/s, output: 11.08 toks/s]Processed prompts:   7%|▋         | 2/29 [00:07<01:29,  3.33s/it, est. speed input: 212.62 toks/s, output: 21.74 toks/s]Processed prompts:  10%|█         | 3/29 [00:08<00:51,  1.98s/it, est. speed input: 316.04 toks/s, output: 32.73 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:08<00:35,  1.43s/it, est. speed input: 403.90 toks/s, output: 43.00 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:08<00:22,  1.05it/s, est. speed input: 506.96 toks/s, output: 55.04 toks/s]Processed prompts:  21%|██        | 6/29 [00:09<00:16,  1.40it/s, est. speed input: 593.83 toks/s, output: 66.25 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:09<00:12,  1.73it/s, est. speed input: 649.67 toks/s, output: 77.16 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:09<00:10,  1.94it/s, est. speed input: 721.80 toks/s, output: 87.44 toks/s]Processed prompts:  31%|███       | 9/29 [00:09<00:07,  2.51it/s, est. speed input: 807.87 toks/s, output: 99.62 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:11<00:12,  1.54it/s, est. speed input: 783.10 toks/s, output: 103.20 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:11<00:08,  2.04it/s, est. speed input: 819.28 toks/s, output: 116.48 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:11<00:06,  2.55it/s, est. speed input: 868.36 toks/s, output: 129.38 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:12<00:08,  1.79it/s, est. speed input: 879.33 toks/s, output: 134.88 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:12<00:04,  2.84it/s, est. speed input: 974.69 toks/s, output: 163.30 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:12<00:03,  3.34it/s, est. speed input: 1018.59 toks/s, output: 177.16 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.28it/s, est. speed input: 1772.17 toks/s, output: 381.32 toks/s]
[2025-01-06 17:27:29,314][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:27:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:03<00:35,  3.89s/it, est. speed input: 237.34 toks/s, output: 23.63 toks/s]Processed prompts:  20%|██        | 2/10 [00:04<00:14,  1.77s/it, est. speed input: 449.44 toks/s, output: 46.43 toks/s]Processed prompts:  30%|███       | 3/10 [00:04<00:07,  1.05s/it, est. speed input: 648.20 toks/s, output: 69.35 toks/s]Processed prompts:  40%|████      | 4/10 [00:04<00:04,  1.39it/s, est. speed input: 826.74 toks/s, output: 91.71 toks/s]Processed prompts:  50%|█████     | 5/10 [00:04<00:02,  1.95it/s, est. speed input: 1002.77 toks/s, output: 114.87 toks/s]Processed prompts:  70%|███████   | 7/10 [00:05<00:01,  2.83it/s, est. speed input: 1275.65 toks/s, output: 158.04 toks/s]Processed prompts:  80%|████████  | 8/10 [00:05<00:00,  2.57it/s, est. speed input: 1335.37 toks/s, output: 173.78 toks/s]Processed prompts: 100%|██████████| 10/10 [00:06<00:00,  2.87it/s, est. speed input: 1515.60 toks/s, output: 216.37 toks/s]Processed prompts: 100%|██████████| 10/10 [00:06<00:00,  1.61it/s, est. speed input: 1515.60 toks/s, output: 216.37 toks/s]
[2025-01-06 17:27:44,229][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:27:51,640][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:27:51,640][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:27:52,054][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:28:05,120][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:28:05,121][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:28:05,485][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:28:05,639][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:28:13,049][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:28:30,785][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:28:30,786][root][INFO] - Iteration 54 took 2m 22s. Generation: 67.19%, Training: 32.81%. Estimated time remaining: 3h 9m 30s. Estimated total time for complete run: 5h 4m 14s.
[2025-01-06 17:28:31,121][root][INFO] - Loading VLLM model.
WARNING 01-06 17:28:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:28:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:28:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:28:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 17:28:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:28:50 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:28:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:28:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:29:12 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:29:12,519][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:29:12,790][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:29:12,791][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:29:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:08,  6.07s/it, est. speed input: 83.24 toks/s, output: 10.22 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:24,  2.82s/it, est. speed input: 152.89 toks/s, output: 20.13 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:45,  1.58s/it, est. speed input: 225.34 toks/s, output: 30.64 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:32,  1.16s/it, est. speed input: 279.08 toks/s, output: 39.79 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:16,  1.62it/s, est. speed input: 403.59 toks/s, output: 60.87 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:11,  2.10it/s, est. speed input: 464.31 toks/s, output: 71.71 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:10,  2.33it/s, est. speed input: 510.09 toks/s, output: 80.93 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:07,  3.09it/s, est. speed input: 607.96 toks/s, output: 101.00 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:08<00:02,  6.21it/s, est. speed input: 837.63 toks/s, output: 148.93 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:03,  4.80it/s, est. speed input: 888.49 toks/s, output: 164.06 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:03,  3.71it/s, est. speed input: 915.33 toks/s, output: 177.33 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:03,  3.85it/s, est. speed input: 945.99 toks/s, output: 188.51 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:02,  4.03it/s, est. speed input: 976.31 toks/s, output: 200.00 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:02,  4.22it/s, est. speed input: 1006.06 toks/s, output: 211.74 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:01,  6.87it/s, est. speed input: 1134.07 toks/s, output: 255.35 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:10<00:00,  7.50it/s, est. speed input: 1204.47 toks/s, output: 282.63 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:11<00:00,  5.64it/s, est. speed input: 1234.90 toks/s, output: 302.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.79it/s, est. speed input: 1411.29 toks/s, output: 372.30 toks/s]
[2025-01-06 17:29:24,683][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:29:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:03,  3.00s/it, est. speed input: 247.94 toks/s, output: 8.99 toks/s]Processed prompts:  91%|█████████ | 20/22 [00:03<00:00,  8.81it/s, est. speed input: 4329.89 toks/s, output: 184.60 toks/s]Processed prompts: 100%|██████████| 22/22 [00:04<00:00,  5.26it/s, est. speed input: 3556.41 toks/s, output: 178.36 toks/s]
[2025-01-06 17:29:29,283][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:29:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  10%|█         | 1/10 [00:05<00:51,  5.73s/it, est. speed input: 121.92 toks/s, output: 28.61 toks/s]Processed prompts:  20%|██        | 2/10 [00:06<00:22,  2.83s/it, est. speed input: 214.23 toks/s, output: 54.55 toks/s]Processed prompts:  30%|███       | 3/10 [00:06<00:11,  1.63s/it, est. speed input: 311.08 toks/s, output: 82.48 toks/s]Processed prompts: 100%|██████████| 10/10 [00:06<00:00,  1.48it/s, est. speed input: 1036.88 toks/s, output: 290.15 toks/s]
[2025-01-06 17:29:36,476][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:29:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:08<03:57,  8.47s/it, est. speed input: 82.56 toks/s, output: 12.28 toks/s]Processed prompts:   7%|▋         | 2/29 [00:09<01:53,  4.22s/it, est. speed input: 144.01 toks/s, output: 23.69 toks/s]Processed prompts:  10%|█         | 3/29 [00:10<01:04,  2.47s/it, est. speed input: 233.11 toks/s, output: 35.98 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:10<00:38,  1.53s/it, est. speed input: 324.22 toks/s, output: 48.84 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:10<00:25,  1.07s/it, est. speed input: 407.42 toks/s, output: 61.02 toks/s]Processed prompts:  21%|██        | 6/29 [00:10<00:19,  1.16it/s, est. speed input: 454.52 toks/s, output: 72.13 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:11<00:16,  1.32it/s, est. speed input: 516.48 toks/s, output: 82.72 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:12<00:08,  2.24it/s, est. speed input: 681.66 toks/s, output: 119.26 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:12<00:08,  2.11it/s, est. speed input: 706.28 toks/s, output: 128.74 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:04,  3.46it/s, est. speed input: 858.16 toks/s, output: 170.21 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:13<00:03,  3.71it/s, est. speed input: 899.29 toks/s, output: 183.04 toks/s]Processed prompts: 100%|██████████| 29/29 [00:13<00:00,  2.21it/s, est. speed input: 1662.79 toks/s, output: 395.95 toks/s]
[2025-01-06 17:29:50,203][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:29:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|▉         | 1/11 [00:02<00:21,  2.13s/it, est. speed input: 448.01 toks/s, output: 13.62 toks/s]Processed prompts:  18%|█▊        | 2/11 [00:02<00:12,  1.37s/it, est. speed input: 571.74 toks/s, output: 29.04 toks/s]Processed prompts:  27%|██▋       | 3/11 [00:03<00:08,  1.03s/it, est. speed input: 711.01 toks/s, output: 46.01 toks/s]Processed prompts:  36%|███▋      | 4/11 [00:04<00:07,  1.07s/it, est. speed input: 741.58 toks/s, output: 60.53 toks/s]Processed prompts:  45%|████▌     | 5/11 [00:04<00:04,  1.38it/s, est. speed input: 922.86 toks/s, output: 85.08 toks/s]Processed prompts:  55%|█████▍    | 6/11 [00:04<00:02,  1.93it/s, est. speed input: 1092.54 toks/s, output: 109.21 toks/s]Processed prompts:  73%|███████▎  | 8/11 [00:05<00:00,  3.43it/s, est. speed input: 1444.28 toks/s, output: 159.82 toks/s]Processed prompts:  82%|████████▏ | 9/11 [00:05<00:00,  2.92it/s, est. speed input: 1489.94 toks/s, output: 174.49 toks/s]Processed prompts:  91%|█████████ | 10/11 [00:06<00:00,  2.20it/s, est. speed input: 1463.08 toks/s, output: 185.34 toks/s]Processed prompts: 100%|██████████| 11/11 [00:06<00:00,  1.74it/s, est. speed input: 1614.29 toks/s, output: 217.05 toks/s]
[2025-01-06 17:30:05,289][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 17:30:12,634][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:30:12,635][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:30:12,997][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:30:26,365][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:30:26,366][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:30:26,685][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:30:26,872][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:30:34,311][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:30:52,010][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:30:52,011][root][INFO] - Iteration 55 took 2m 21s. Generation: 66.81%, Training: 33.19%. Estimated time remaining: 3h 4m 11s. Estimated total time for complete run: 5h 1m 16s.
[2025-01-06 17:30:52,347][root][INFO] - Loading VLLM model.
WARNING 01-06 17:30:52 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:30:52 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:30:53 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:30:53 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:30:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:31:11 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:31:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:31:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:31:34 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:31:34,098][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:31:34,346][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:31:34,347][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:31:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:03<02:01,  3.93s/it, est. speed input: 128.59 toks/s, output: 6.87 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:37,  3.24s/it, est. speed input: 151.18 toks/s, output: 14.97 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:57,  2.00s/it, est. speed input: 210.25 toks/s, output: 25.26 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:36,  1.30s/it, est. speed input: 271.70 toks/s, output: 36.05 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:12,  1.94it/s, est. speed input: 465.54 toks/s, output: 69.67 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:10,  2.19it/s, est. speed input: 514.70 toks/s, output: 79.37 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:08<00:09,  2.47it/s, est. speed input: 561.17 toks/s, output: 89.14 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:07,  2.97it/s, est. speed input: 612.55 toks/s, output: 99.95 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:08,  2.59it/s, est. speed input: 633.94 toks/s, output: 106.93 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:08,  2.39it/s, est. speed input: 654.08 toks/s, output: 114.52 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:07,  2.58it/s, est. speed input: 685.74 toks/s, output: 124.51 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:05,  3.08it/s, est. speed input: 725.42 toks/s, output: 136.16 toks/s]Processed prompts:  50%|█████     | 16/32 [00:09<00:03,  4.29it/s, est. speed input: 808.74 toks/s, output: 160.55 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:02,  5.87it/s, est. speed input: 896.68 toks/s, output: 186.44 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:10<00:00, 11.64it/s, est. speed input: 1131.46 toks/s, output: 255.71 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:10<00:00, 11.35it/s, est. speed input: 1207.54 toks/s, output: 280.34 toks/s]Processed prompts:  84%|████████▍ | 27/32 [00:11<00:01,  4.77it/s, est. speed input: 1180.06 toks/s, output: 286.55 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.77it/s, est. speed input: 1398.56 toks/s, output: 373.09 toks/s]
[2025-01-06 17:31:46,331][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:31:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:28,  3.56s/it, est. speed input: 196.83 toks/s, output: 8.15 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:04<00:00,  7.18it/s, est. speed input: 3774.92 toks/s, output: 174.12 toks/s]Processed prompts: 100%|██████████| 26/26 [00:06<00:00,  3.99it/s, est. speed input: 2729.09 toks/s, output: 151.33 toks/s]
[2025-01-06 17:31:53,262][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:31:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:27,  5.43s/it, est. speed input: 128.65 toks/s, output: 36.81 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 771.85 toks/s, output: 220.84 toks/s]
[2025-01-06 17:31:59,138][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:31:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:13,  8.19s/it, est. speed input: 116.44 toks/s, output: 10.99 toks/s]Processed prompts:   9%|▉         | 3/32 [00:09<01:12,  2.50s/it, est. speed input: 283.37 toks/s, output: 31.20 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:09<00:51,  1.83s/it, est. speed input: 364.05 toks/s, output: 41.30 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:10<00:35,  1.32s/it, est. speed input: 448.53 toks/s, output: 52.27 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:10<00:24,  1.05it/s, est. speed input: 509.69 toks/s, output: 63.65 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:11<00:17,  1.37it/s, est. speed input: 595.52 toks/s, output: 82.14 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:12<00:18,  1.23it/s, est. speed input: 601.08 toks/s, output: 88.50 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:12<00:14,  1.54it/s, est. speed input: 647.68 toks/s, output: 100.66 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:12<00:12,  1.70it/s, est. speed input: 680.29 toks/s, output: 111.11 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:13<00:09,  2.17it/s, est. speed input: 726.64 toks/s, output: 123.80 toks/s]Processed prompts:  41%|████      | 13/32 [00:13<00:08,  2.32it/s, est. speed input: 759.43 toks/s, output: 134.61 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:06,  2.90it/s, est. speed input: 803.74 toks/s, output: 147.50 toks/s]Processed prompts:  50%|█████     | 16/32 [00:13<00:04,  3.80it/s, est. speed input: 885.46 toks/s, output: 172.41 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.31it/s, est. speed input: 1710.96 toks/s, output: 403.35 toks/s]
[2025-01-06 17:32:13,554][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:32:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/13 [00:04<00:49,  4.11s/it, est. speed input: 231.90 toks/s, output: 18.96 toks/s]Processed prompts:  23%|██▎       | 3/13 [00:04<00:12,  1.27s/it, est. speed input: 615.08 toks/s, output: 54.16 toks/s]Processed prompts:  38%|███▊      | 5/13 [00:05<00:06,  1.23it/s, est. speed input: 879.04 toks/s, output: 86.80 toks/s]Processed prompts:  46%|████▌     | 6/13 [00:05<00:05,  1.34it/s, est. speed input: 949.11 toks/s, output: 102.38 toks/s]Processed prompts:  62%|██████▏   | 8/13 [00:06<00:02,  2.14it/s, est. speed input: 1226.24 toks/s, output: 146.83 toks/s]Processed prompts:  77%|███████▋  | 10/13 [00:07<00:01,  2.04it/s, est. speed input: 1310.54 toks/s, output: 174.09 toks/s]Processed prompts: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s, est. speed input: 1699.97 toks/s, output: 256.05 toks/s]
[2025-01-06 17:32:29,610][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:32:37,006][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:32:37,006][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:32:37,379][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:32:51,068][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:32:51,070][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:32:51,425][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:32:51,577][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 17:32:58,975][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:33:16,784][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:33:16,785][root][INFO] - Iteration 56 took 2m 24s. Generation: 67.29%, Training: 32.71%. Estimated time remaining: 3h 9m 20s. Estimated total time for complete run: 5h 8m 50s.
[2025-01-06 17:33:17,097][root][INFO] - Loading VLLM model.
WARNING 01-06 17:33:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:33:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:33:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:33:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:33:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:33:36 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:33:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:33:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:33:58 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:33:58,440][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:33:58,692][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:33:58,693][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:33:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:44,  5.32s/it, est. speed input: 94.93 toks/s, output: 8.84 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:31,  3.04s/it, est. speed input: 149.50 toks/s, output: 17.47 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:49,  1.70s/it, est. speed input: 220.42 toks/s, output: 27.79 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.32s/it, est. speed input: 265.21 toks/s, output: 36.37 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:16,  1.53it/s, est. speed input: 392.13 toks/s, output: 58.50 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:15,  1.66it/s, est. speed input: 431.04 toks/s, output: 66.94 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:12,  1.93it/s, est. speed input: 474.85 toks/s, output: 76.63 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:08,  2.75it/s, est. speed input: 570.70 toks/s, output: 97.87 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:06,  3.11it/s, est. speed input: 614.66 toks/s, output: 108.44 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:07,  2.63it/s, est. speed input: 632.19 toks/s, output: 115.38 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:05,  3.19it/s, est. speed input: 675.51 toks/s, output: 127.08 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:06,  2.89it/s, est. speed input: 696.58 toks/s, output: 135.37 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:04,  3.65it/s, est. speed input: 768.35 toks/s, output: 158.05 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:02,  4.90it/s, est. speed input: 849.37 toks/s, output: 183.61 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:02,  5.47it/s, est. speed input: 887.75 toks/s, output: 196.24 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:11<00:03,  3.10it/s, est. speed input: 872.01 toks/s, output: 198.58 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:11<00:01,  5.51it/s, est. speed input: 992.04 toks/s, output: 242.99 toks/s]Processed prompts:  81%|████████▏ | 26/32 [00:11<00:00,  7.81it/s, est. speed input: 1106.13 toks/s, output: 286.85 toks/s]Processed prompts:  88%|████████▊ | 28/32 [00:12<00:00,  7.74it/s, est. speed input: 1165.25 toks/s, output: 313.56 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s, est. speed input: 1331.70 toks/s, output: 379.48 toks/s]
[2025-01-06 17:34:11,252][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:34:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:02<00:37,  2.37s/it, est. speed input: 249.67 toks/s, output: 10.14 toks/s]Processed prompts:  12%|█▏        | 2/17 [00:02<00:16,  1.09s/it, est. speed input: 477.82 toks/s, output: 20.72 toks/s]Processed prompts:  94%|█████████▍| 16/17 [00:02<00:00,  9.63it/s, est. speed input: 3828.62 toks/s, output: 172.43 toks/s]Processed prompts: 100%|██████████| 17/17 [00:03<00:00,  4.99it/s, est. speed input: 3410.70 toks/s, output: 166.35 toks/s]
[2025-01-06 17:34:15,070][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:34:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/15 [00:06<01:33,  6.65s/it, est. speed input: 105.07 toks/s, output: 22.25 toks/s]Processed prompts:  13%|█▎        | 2/15 [00:08<00:48,  3.74s/it, est. speed input: 167.27 toks/s, output: 41.16 toks/s]Processed prompts:  20%|██        | 3/15 [00:08<00:24,  2.08s/it, est. speed input: 247.87 toks/s, output: 64.18 toks/s]Processed prompts: 100%|██████████| 15/15 [00:08<00:00,  1.77it/s, est. speed input: 1234.66 toks/s, output: 346.43 toks/s]
[2025-01-06 17:34:24,034][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:34:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:57,  4.21s/it, est. speed input: 214.25 toks/s, output: 6.90 toks/s]Processed prompts:  10%|█         | 3/29 [00:07<01:00,  2.32s/it, est. speed input: 373.29 toks/s, output: 19.67 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:07<00:38,  1.55s/it, est. speed input: 493.04 toks/s, output: 31.45 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:07<00:25,  1.07s/it, est. speed input: 609.73 toks/s, output: 43.17 toks/s]Processed prompts:  21%|██        | 6/29 [00:08<00:26,  1.13s/it, est. speed input: 630.19 toks/s, output: 50.34 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:09<00:21,  1.01it/s, est. speed input: 684.06 toks/s, output: 60.51 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:10<00:18,  1.14it/s, est. speed input: 735.32 toks/s, output: 71.03 toks/s]Processed prompts:  31%|███       | 9/29 [00:10<00:14,  1.41it/s, est. speed input: 760.88 toks/s, output: 83.26 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:12<00:12,  1.42it/s, est. speed input: 810.15 toks/s, output: 101.82 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:06,  2.37it/s, est. speed input: 935.53 toks/s, output: 144.02 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:12<00:05,  2.71it/s, est. speed input: 980.06 toks/s, output: 158.21 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.30it/s, est. speed input: 1800.19 toks/s, output: 380.47 toks/s]
[2025-01-06 17:34:37,193][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:34:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:02<00:22,  2.80s/it, est. speed input: 340.92 toks/s, output: 22.16 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:03<00:06,  1.13s/it, est. speed input: 681.25 toks/s, output: 58.44 toks/s]Processed prompts:  56%|█████▌    | 5/9 [00:04<00:02,  1.48it/s, est. speed input: 1040.23 toks/s, output: 104.80 toks/s]Processed prompts:  78%|███████▊  | 7/9 [00:04<00:00,  2.24it/s, est. speed input: 1389.40 toks/s, output: 156.02 toks/s]Processed prompts:  89%|████████▉ | 8/9 [00:05<00:00,  1.65it/s, est. speed input: 1275.98 toks/s, output: 159.86 toks/s]Processed prompts: 100%|██████████| 9/9 [00:05<00:00,  1.55it/s, est. speed input: 1440.46 toks/s, output: 194.35 toks/s]
[2025-01-06 17:34:51,797][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 17:34:59,428][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:34:59,428][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:34:59,793][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:35:13,103][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:35:13,104][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:35:13,462][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:35:13,607][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:35:21,013][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:35:38,825][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:35:38,826][root][INFO] - Iteration 57 took 2m 22s. Generation: 66.77%, Training: 33.23%. Estimated time remaining: 3h 1m 8s. Estimated total time for complete run: 5h 3m 1s.
[2025-01-06 17:35:39,143][root][INFO] - Loading VLLM model.
WARNING 01-06 17:35:39 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:35:39 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:35:39 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:35:39 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 17:35:44 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:35:58 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:35:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:35:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:36:20 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:36:20,495][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:36:20,769][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:36:20,770][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:36:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:20,  6.48s/it, est. speed input: 77.98 toks/s, output: 9.73 toks/s]Processed prompts:   6%|▋         | 2/32 [00:07<01:30,  3.02s/it, est. speed input: 142.74 toks/s, output: 19.22 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:59,  2.04s/it, est. speed input: 190.55 toks/s, output: 28.17 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:42,  1.53s/it, est. speed input: 232.31 toks/s, output: 37.38 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:27,  1.02s/it, est. speed input: 286.70 toks/s, output: 48.60 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:18,  1.41it/s, est. speed input: 339.87 toks/s, output: 59.79 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:13,  1.74it/s, est. speed input: 412.80 toks/s, output: 77.76 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.10it/s, est. speed input: 455.06 toks/s, output: 88.81 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:09,  2.42it/s, est. speed input: 493.67 toks/s, output: 99.52 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:10<00:08,  2.48it/s, est. speed input: 523.66 toks/s, output: 109.07 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:06,  2.93it/s, est. speed input: 561.54 toks/s, output: 120.46 toks/s]Processed prompts:  41%|████      | 13/32 [00:11<00:07,  2.45it/s, est. speed input: 577.56 toks/s, output: 128.09 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:11<00:05,  3.33it/s, est. speed input: 647.17 toks/s, output: 151.90 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.89it/s, est. speed input: 683.17 toks/s, output: 164.45 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:03,  4.01it/s, est. speed input: 738.76 toks/s, output: 186.44 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:12<00:02,  4.61it/s, est. speed input: 773.09 toks/s, output: 199.58 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:12<00:01,  5.86it/s, est. speed input: 840.98 toks/s, output: 226.09 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:12<00:02,  4.80it/s, est. speed input: 857.74 toks/s, output: 235.55 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.47it/s, est. speed input: 1247.59 toks/s, output: 389.95 toks/s]
[2025-01-06 17:36:34,187][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:36:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:02<00:56,  2.99s/it, est. speed input: 249.30 toks/s, output: 9.70 toks/s]Processed prompts:  85%|████████▌ | 17/20 [00:03<00:00,  7.58it/s, est. speed input: 3865.81 toks/s, output: 161.25 toks/s]Processed prompts: 100%|██████████| 20/20 [00:06<00:00,  3.22it/s, est. speed input: 2283.13 toks/s, output: 156.17 toks/s]
[2025-01-06 17:36:40,819][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:36:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:06<01:16,  6.94s/it, est. speed input: 100.68 toks/s, output: 25.93 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:07<00:32,  3.23s/it, est. speed input: 184.69 toks/s, output: 50.20 toks/s]Processed prompts: 100%|██████████| 12/12 [00:07<00:00,  1.59it/s, est. speed input: 1108.09 toks/s, output: 314.41 toks/s]
[2025-01-06 17:36:48,814][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:36:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:01,  4.20s/it, est. speed input: 227.17 toks/s, output: 6.67 toks/s]Processed prompts:   7%|▋         | 2/30 [00:07<01:37,  3.50s/it, est. speed input: 264.72 toks/s, output: 14.98 toks/s]Processed prompts:  10%|█         | 3/30 [00:09<01:19,  2.93s/it, est. speed input: 302.44 toks/s, output: 24.09 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:10<00:52,  2.03s/it, est. speed input: 377.11 toks/s, output: 35.58 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:10<00:34,  1.38s/it, est. speed input: 436.95 toks/s, output: 48.00 toks/s]Processed prompts:  20%|██        | 6/30 [00:10<00:24,  1.03s/it, est. speed input: 469.79 toks/s, output: 59.74 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:11<00:18,  1.23it/s, est. speed input: 517.97 toks/s, output: 71.40 toks/s]Processed prompts:  30%|███       | 9/30 [00:12<00:15,  1.39it/s, est. speed input: 584.60 toks/s, output: 90.76 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:12<00:13,  1.53it/s, est. speed input: 638.13 toks/s, output: 102.04 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:12<00:09,  1.94it/s, est. speed input: 685.68 toks/s, output: 115.64 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:13<00:06,  2.74it/s, est. speed input: 759.40 toks/s, output: 141.98 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:13<00:02,  4.82it/s, est. speed input: 910.08 toks/s, output: 185.36 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.25it/s, est. speed input: 1715.88 toks/s, output: 395.13 toks/s]
[2025-01-06 17:37:02,787][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:37:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:02<00:23,  2.13s/it, est. speed input: 324.51 toks/s, output: 11.72 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:03<00:18,  1.82s/it, est. speed input: 432.58 toks/s, output: 27.02 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:04<00:06,  1.20it/s, est. speed input: 821.97 toks/s, output: 64.14 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:05<00:06,  1.14it/s, est. speed input: 851.03 toks/s, output: 76.89 toks/s]Processed prompts:  50%|█████     | 6/12 [00:05<00:03,  1.52it/s, est. speed input: 1003.86 toks/s, output: 99.81 toks/s]Processed prompts:  58%|█████▊    | 7/12 [00:05<00:02,  1.73it/s, est. speed input: 1101.15 toks/s, output: 119.24 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:05<00:01,  2.25it/s, est. speed input: 1231.73 toks/s, output: 143.10 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:06<00:01,  2.76it/s, est. speed input: 1353.21 toks/s, output: 166.14 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:06<00:00,  3.12it/s, est. speed input: 1457.07 toks/s, output: 188.25 toks/s]Processed prompts: 100%|██████████| 12/12 [00:06<00:00,  3.76it/s, est. speed input: 1621.92 toks/s, output: 233.17 toks/s]Processed prompts: 100%|██████████| 12/12 [00:06<00:00,  1.79it/s, est. speed input: 1621.92 toks/s, output: 233.17 toks/s]
[2025-01-06 17:37:18,210][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 17:37:25,527][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:37:25,527][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:37:25,936][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:37:39,745][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:37:39,747][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:37:40,206][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:37:40,355][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 17:37:48,289][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:38:06,393][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:38:06,394][root][INFO] - Iteration 58 took 2m 27s. Generation: 67.25%, Training: 32.75%. Estimated time remaining: 3h 10m 28s. Estimated total time for complete run: 5h 14m 48s.
[2025-01-06 17:38:06,723][root][INFO] - Loading VLLM model.
WARNING 01-06 17:38:06 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:38:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:38:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:38:07 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:38:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:38:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:38:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:38:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:38:48 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:38:48,501][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:38:48,755][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:38:48,756][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:38:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:55,  5.66s/it, est. speed input: 89.29 toks/s, output: 9.90 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:21,  2.72s/it, est. speed input: 159.94 toks/s, output: 19.48 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:55,  1.93s/it, est. speed input: 207.36 toks/s, output: 28.33 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:40,  1.44s/it, est. speed input: 252.73 toks/s, output: 37.91 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:21,  1.22it/s, est. speed input: 355.12 toks/s, output: 59.19 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:19,  1.30it/s, est. speed input: 385.71 toks/s, output: 67.98 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.16it/s, est. speed input: 487.80 toks/s, output: 92.62 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:09,  2.37it/s, est. speed input: 525.65 toks/s, output: 103.05 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:05,  3.47it/s, est. speed input: 618.97 toks/s, output: 127.47 toks/s]Processed prompts:  41%|████      | 13/32 [00:10<00:05,  3.30it/s, est. speed input: 647.20 toks/s, output: 136.74 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:04,  3.69it/s, est. speed input: 685.30 toks/s, output: 148.30 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:04,  3.92it/s, est. speed input: 719.65 toks/s, output: 159.42 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:03,  4.56it/s, est. speed input: 758.83 toks/s, output: 171.77 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:01,  7.79it/s, est. speed input: 888.76 toks/s, output: 212.12 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:01,  9.30it/s, est. speed input: 970.31 toks/s, output: 238.44 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:11<00:01,  5.91it/s, est. speed input: 1008.01 toks/s, output: 255.76 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:12<00:01,  4.02it/s, est. speed input: 1000.48 toks/s, output: 259.78 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s, est. speed input: 1333.94 toks/s, output: 391.85 toks/s]
[2025-01-06 17:39:01,312][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:39:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:15,  3.27s/it, est. speed input: 198.86 toks/s, output: 8.26 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:03<00:01,  6.14it/s, est. speed input: 3143.77 toks/s, output: 132.34 toks/s]Processed prompts:  83%|████████▎ | 20/24 [00:03<00:00,  6.90it/s, est. speed input: 3560.39 toks/s, output: 163.21 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:04<00:00,  5.80it/s, est. speed input: 3359.48 toks/s, output: 185.93 toks/s]Processed prompts: 100%|██████████| 24/24 [00:04<00:00,  5.06it/s, est. speed input: 3494.18 toks/s, output: 204.65 toks/s]
[2025-01-06 17:39:06,489][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:39:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:04<00:34,  5.00s/it, est. speed input: 139.80 toks/s, output: 31.80 toks/s]Processed prompts:  25%|██▌       | 2/8 [00:05<00:15,  2.53s/it, est. speed input: 240.82 toks/s, output: 60.12 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:06<00:07,  1.49s/it, est. speed input: 346.71 toks/s, output: 90.77 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.32it/s, est. speed input: 924.53 toks/s, output: 256.10 toks/s]
[2025-01-06 17:39:12,997][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:39:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:06<02:57,  6.11s/it, est. speed input: 149.62 toks/s, output: 10.15 toks/s]Processed prompts:   7%|▋         | 2/30 [00:07<01:29,  3.19s/it, est. speed input: 257.16 toks/s, output: 19.82 toks/s]Processed prompts:  10%|█         | 3/30 [00:07<00:51,  1.89s/it, est. speed input: 371.23 toks/s, output: 30.52 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:08<00:34,  1.32s/it, est. speed input: 469.80 toks/s, output: 40.81 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:09<00:33,  1.36s/it, est. speed input: 499.22 toks/s, output: 47.60 toks/s]Processed prompts:  20%|██        | 6/30 [00:10<00:32,  1.35s/it, est. speed input: 501.82 toks/s, output: 55.46 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:10<00:22,  1.04it/s, est. speed input: 558.58 toks/s, output: 68.55 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:12<00:27,  1.26s/it, est. speed input: 530.19 toks/s, output: 73.23 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:13<00:15,  1.31it/s, est. speed input: 618.58 toks/s, output: 100.48 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.26it/s, est. speed input: 1647.49 toks/s, output: 401.29 toks/s]
[2025-01-06 17:39:26,827][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:39:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:03<00:48,  3.06s/it, est. speed input: 311.57 toks/s, output: 9.47 toks/s]Processed prompts:  12%|█▏        | 2/17 [00:03<00:22,  1.51s/it, est. speed input: 489.24 toks/s, output: 19.81 toks/s]Processed prompts:  18%|█▊        | 3/17 [00:04<00:19,  1.41s/it, est. speed input: 557.30 toks/s, output: 30.19 toks/s]Processed prompts:  24%|██▎       | 4/17 [00:05<00:15,  1.18s/it, est. speed input: 645.62 toks/s, output: 43.26 toks/s]Processed prompts:  29%|██▉       | 5/17 [00:05<00:10,  1.17it/s, est. speed input: 778.05 toks/s, output: 59.30 toks/s]Processed prompts:  35%|███▌      | 6/17 [00:06<00:07,  1.46it/s, est. speed input: 885.56 toks/s, output: 74.60 toks/s]Processed prompts:  41%|████      | 7/17 [00:06<00:05,  1.91it/s, est. speed input: 1007.91 toks/s, output: 91.54 toks/s]Processed prompts:  59%|█████▉    | 10/17 [00:07<00:03,  2.05it/s, est. speed input: 1169.07 toks/s, output: 129.43 toks/s]Processed prompts:  65%|██████▍   | 11/17 [00:08<00:03,  1.88it/s, est. speed input: 1163.17 toks/s, output: 142.54 toks/s]Processed prompts: 100%|██████████| 17/17 [00:08<00:00,  2.00it/s, est. speed input: 1838.40 toks/s, output: 283.78 toks/s]
[2025-01-06 17:39:44,131][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 17:39:51,828][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:39:51,829][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:39:52,192][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:40:05,954][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:40:05,955][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:40:06,409][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:40:06,557][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:40:14,120][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:40:32,426][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:40:32,428][root][INFO] - Iteration 59 took 2m 26s. Generation: 66.82%, Training: 33.18%. Estimated time remaining: 3h 4m 46s. Estimated total time for complete run: 5h 11m 32s.
[2025-01-06 17:40:32,760][root][INFO] - Loading VLLM model.
WARNING 01-06 17:40:32 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:40:32 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:40:33 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:40:33 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:40:38 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:40:52 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:40:52 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:40:52 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:41:14 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:41:14,496][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:41:14,737][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:41:14,739][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:41:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:53,  5.61s/it, est. speed input: 90.09 toks/s, output: 9.99 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:20,  2.70s/it, est. speed input: 161.22 toks/s, output: 19.63 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:29,  1.06s/it, est. speed input: 313.82 toks/s, output: 40.39 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:22,  1.22it/s, est. speed input: 376.02 toks/s, output: 49.89 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:19,  1.34it/s, est. speed input: 414.65 toks/s, output: 57.61 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:16,  1.54it/s, est. speed input: 457.39 toks/s, output: 66.63 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:09,  2.54it/s, est. speed input: 573.11 toks/s, output: 89.28 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:11,  2.00it/s, est. speed input: 577.08 toks/s, output: 94.05 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:11,  1.76it/s, est. speed input: 584.40 toks/s, output: 100.36 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:10,  1.82it/s, est. speed input: 605.46 toks/s, output: 109.50 toks/s]Processed prompts:  41%|████      | 13/32 [00:10<00:08,  2.13it/s, est. speed input: 638.97 toks/s, output: 121.08 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:07,  2.52it/s, est. speed input: 673.95 toks/s, output: 133.17 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:03,  4.05it/s, est. speed input: 761.19 toks/s, output: 160.90 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:10<00:02,  5.37it/s, est. speed input: 841.67 toks/s, output: 187.59 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:11<00:02,  4.38it/s, est. speed input: 885.51 toks/s, output: 207.44 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:11<00:02,  4.14it/s, est. speed input: 906.41 toks/s, output: 218.29 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:11<00:02,  4.65it/s, est. speed input: 939.56 toks/s, output: 232.23 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:12<00:01,  5.69it/s, est. speed input: 1005.45 toks/s, output: 260.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.65it/s, est. speed input: 1340.57 toks/s, output: 393.29 toks/s]
[2025-01-06 17:41:27,216][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:41:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:09,  3.17s/it, est. speed input: 235.16 toks/s, output: 8.52 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:03<00:00,  7.08it/s, est. speed input: 3509.26 toks/s, output: 147.70 toks/s]Processed prompts: 100%|██████████| 23/23 [00:06<00:00,  3.56it/s, est. speed input: 2459.07 toks/s, output: 161.54 toks/s]
[2025-01-06 17:41:34,122][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:41:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:04<00:35,  4.42s/it, est. speed input: 158.11 toks/s, output: 28.73 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:04<00:13,  1.93s/it, est. speed input: 303.30 toks/s, output: 56.62 toks/s]Processed prompts:  33%|███▎      | 3/9 [00:06<00:10,  1.83s/it, est. speed input: 331.59 toks/s, output: 72.90 toks/s]Processed prompts: 100%|██████████| 9/9 [00:06<00:00,  1.42it/s, est. speed input: 994.71 toks/s, output: 262.63 toks/s]
[2025-01-06 17:41:40,902][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:41:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:07<03:36,  7.75s/it, est. speed input: 123.13 toks/s, output: 12.00 toks/s]Processed prompts:  10%|█         | 3/29 [00:08<01:00,  2.31s/it, est. speed input: 334.07 toks/s, output: 34.43 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:08<00:38,  1.55s/it, est. speed input: 432.21 toks/s, output: 46.69 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:09<00:29,  1.24s/it, est. speed input: 478.56 toks/s, output: 56.70 toks/s]Processed prompts:  21%|██        | 6/29 [00:09<00:22,  1.05it/s, est. speed input: 559.91 toks/s, output: 67.99 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:10<00:23,  1.07s/it, est. speed input: 539.11 toks/s, output: 74.05 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:11<00:18,  1.13it/s, est. speed input: 577.71 toks/s, output: 85.46 toks/s]Processed prompts:  31%|███       | 9/29 [00:11<00:13,  1.46it/s, est. speed input: 626.13 toks/s, output: 98.42 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:12<00:09,  1.89it/s, est. speed input: 703.87 toks/s, output: 121.89 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:12<00:07,  2.13it/s, est. speed input: 742.76 toks/s, output: 134.39 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:12<00:06,  2.65it/s, est. speed input: 790.30 toks/s, output: 148.50 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:12<00:04,  3.27it/s, est. speed input: 837.41 toks/s, output: 162.68 toks/s]Processed prompts: 100%|██████████| 29/29 [00:12<00:00,  2.25it/s, est. speed input: 1641.67 toks/s, output: 395.17 toks/s]
[2025-01-06 17:41:54,346][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:41:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   8%|▊         | 1/12 [00:03<00:39,  3.63s/it, est. speed input: 262.72 toks/s, output: 19.00 toks/s]Processed prompts:  17%|█▋        | 2/12 [00:04<00:17,  1.72s/it, est. speed input: 424.93 toks/s, output: 37.41 toks/s]Processed prompts:  25%|██▌       | 3/12 [00:05<00:12,  1.44s/it, est. speed input: 479.68 toks/s, output: 52.38 toks/s]Processed prompts:  33%|███▎      | 4/12 [00:05<00:07,  1.02it/s, est. speed input: 616.31 toks/s, output: 73.31 toks/s]Processed prompts:  42%|████▏     | 5/12 [00:05<00:04,  1.48it/s, est. speed input: 773.43 toks/s, output: 95.53 toks/s]Processed prompts:  50%|█████     | 6/12 [00:05<00:03,  1.67it/s, est. speed input: 874.94 toks/s, output: 113.44 toks/s]Processed prompts:  67%|██████▋   | 8/12 [00:06<00:01,  2.49it/s, est. speed input: 1119.87 toks/s, output: 156.42 toks/s]Processed prompts:  75%|███████▌  | 9/12 [00:06<00:01,  2.69it/s, est. speed input: 1217.43 toks/s, output: 176.73 toks/s]Processed prompts:  83%|████████▎ | 10/12 [00:07<00:00,  2.63it/s, est. speed input: 1282.72 toks/s, output: 194.90 toks/s]Processed prompts: 100%|██████████| 12/12 [00:07<00:00,  1.70it/s, est. speed input: 1552.55 toks/s, output: 251.47 toks/s]
[2025-01-06 17:42:10,311][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 17:42:18,001][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:42:18,002][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:42:18,382][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:42:32,309][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:42:32,311][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:42:32,723][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:42:33,143][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:42:41,046][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:42:58,649][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:42:58,650][root][INFO] - Iteration 60 took 2m 26s. Generation: 66.84%, Training: 33.16%. Estimated time remaining: 3h 2m 44s. Estimated total time for complete run: 5h 11m 56s.
[2025-01-06 17:42:59,172][root][INFO] - Loading VLLM model.
WARNING 01-06 17:42:59 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:42:59 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:43:00 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:43:00 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:43:05 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:43:18 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:43:19 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:43:19 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:43:41 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:43:41,258][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:43:41,619][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:43:41,624][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:43:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:34,  6.93s/it, est. speed input: 72.92 toks/s, output: 10.40 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:55,  1.90s/it, est. speed input: 209.85 toks/s, output: 30.75 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.32s/it, est. speed input: 271.20 toks/s, output: 40.68 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:27,  1.03s/it, est. speed input: 319.86 toks/s, output: 49.66 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:19,  1.33it/s, est. speed input: 376.12 toks/s, output: 60.08 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:13,  1.82it/s, est. speed input: 433.12 toks/s, output: 70.82 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:14,  1.71it/s, est. speed input: 457.71 toks/s, output: 77.61 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.14it/s, est. speed input: 503.50 toks/s, output: 88.18 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:09,  2.29it/s, est. speed input: 565.13 toks/s, output: 105.40 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:07,  2.52it/s, est. speed input: 599.75 toks/s, output: 115.79 toks/s]Processed prompts:  41%|████      | 13/32 [00:10<00:07,  2.68it/s, est. speed input: 630.40 toks/s, output: 125.89 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:06,  2.65it/s, est. speed input: 654.42 toks/s, output: 135.23 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:11<00:05,  3.03it/s, est. speed input: 687.90 toks/s, output: 146.75 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.54it/s, est. speed input: 723.09 toks/s, output: 158.85 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:11<00:05,  2.82it/s, est. speed input: 733.46 toks/s, output: 166.43 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:11<00:03,  4.27it/s, est. speed input: 807.45 toks/s, output: 193.55 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:12<00:02,  5.33it/s, est. speed input: 875.34 toks/s, output: 219.97 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:12<00:01,  5.75it/s, est. speed input: 907.68 toks/s, output: 233.17 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:12<00:01,  4.77it/s, est. speed input: 924.49 toks/s, output: 243.08 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.55it/s, est. speed input: 1286.21 toks/s, output: 386.34 toks/s]
[2025-01-06 17:43:54,647][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:43:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:30,  3.63s/it, est. speed input: 175.91 toks/s, output: 8.00 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:03<00:00,  8.21it/s, est. speed input: 4131.10 toks/s, output: 173.99 toks/s]Processed prompts: 100%|██████████| 26/26 [00:06<00:00,  3.84it/s, est. speed input: 2699.23 toks/s, output: 162.63 toks/s]
[2025-01-06 17:44:01,866][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:44:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:25,  5.11s/it, est. speed input: 136.77 toks/s, output: 36.39 toks/s]Processed prompts:  33%|███▎      | 2/6 [00:05<00:09,  2.29s/it, est. speed input: 257.51 toks/s, output: 71.10 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.11it/s, est. speed input: 772.50 toks/s, output: 218.45 toks/s]
[2025-01-06 17:44:07,848][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:44:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:07<03:46,  7.80s/it, est. speed input: 122.31 toks/s, output: 11.67 toks/s]Processed prompts:   7%|▋         | 2/30 [00:08<01:36,  3.45s/it, est. speed input: 232.52 toks/s, output: 23.03 toks/s]Processed prompts:  10%|█         | 3/30 [00:09<01:06,  2.46s/it, est. speed input: 301.33 toks/s, output: 32.64 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:10<00:51,  1.96s/it, est. speed input: 332.96 toks/s, output: 42.36 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:11<00:39,  1.56s/it, est. speed input: 368.90 toks/s, output: 53.00 toks/s]Processed prompts:  20%|██        | 6/30 [00:11<00:25,  1.07s/it, est. speed input: 425.60 toks/s, output: 66.34 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:13<00:31,  1.36s/it, est. speed input: 415.39 toks/s, output: 71.43 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.20it/s, est. speed input: 1586.58 toks/s, output: 409.12 toks/s]
[2025-01-06 17:44:22,023][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:44:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:03<00:59,  3.98s/it, est. speed input: 188.35 toks/s, output: 14.82 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:04<00:29,  2.14s/it, est. speed input: 352.83 toks/s, output: 29.20 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:05<00:17,  1.37s/it, est. speed input: 501.90 toks/s, output: 44.56 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:05<00:10,  1.15it/s, est. speed input: 669.02 toks/s, output: 61.86 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:05<00:05,  1.92it/s, est. speed input: 949.58 toks/s, output: 93.60 toks/s]Processed prompts:  50%|█████     | 8/16 [00:06<00:03,  2.49it/s, est. speed input: 1178.70 toks/s, output: 124.60 toks/s]Processed prompts:  56%|█████▋    | 9/16 [00:07<00:03,  2.09it/s, est. speed input: 1191.54 toks/s, output: 133.66 toks/s]Processed prompts:  69%|██████▉   | 11/16 [00:07<00:01,  2.97it/s, est. speed input: 1412.37 toks/s, output: 173.39 toks/s]Processed prompts:  81%|████████▏ | 13/16 [00:08<00:01,  2.82it/s, est. speed input: 1514.45 toks/s, output: 202.28 toks/s]Processed prompts: 100%|██████████| 16/16 [00:08<00:00,  1.99it/s, est. speed input: 1844.44 toks/s, output: 276.78 toks/s]
[2025-01-06 17:44:39,151][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 17:44:46,486][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:44:46,486][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:44:46,940][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:45:01,013][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:45:01,014][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:45:01,376][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:45:01,520][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]
[2025-01-06 17:45:09,282][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:45:27,386][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:45:27,387][root][INFO] - Iteration 61 took 2m 28s. Generation: 67.46%, Training: 32.54%. Estimated time remaining: 3h 5m 37s. Estimated total time for complete run: 5h 17m 18s.
[2025-01-06 17:45:27,709][root][INFO] - Loading VLLM model.
WARNING 01-06 17:45:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:45:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:45:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:45:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:45:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:45:47 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:45:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:45:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:46:09 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:46:09,838][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:46:10,083][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:46:10,084][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:46:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:47,  5.39s/it, est. speed input: 93.63 toks/s, output: 8.16 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:09,  2.33s/it, est. speed input: 181.19 toks/s, output: 16.32 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:48,  1.66s/it, est. speed input: 234.90 toks/s, output: 23.72 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:25,  1.08it/s, est. speed input: 351.91 toks/s, output: 40.56 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:18,  1.38it/s, est. speed input: 409.91 toks/s, output: 50.06 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:17,  1.44it/s, est. speed input: 440.54 toks/s, output: 57.45 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:13,  1.76it/s, est. speed input: 487.91 toks/s, output: 67.27 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:21,  1.06it/s, est. speed input: 448.75 toks/s, output: 68.13 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:16,  1.33it/s, est. speed input: 484.68 toks/s, output: 79.56 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:10<00:13,  1.60it/s, est. speed input: 516.71 toks/s, output: 90.69 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:10,  1.92it/s, est. speed input: 549.59 toks/s, output: 102.21 toks/s]Processed prompts:  41%|████      | 13/32 [00:11<00:08,  2.25it/s, est. speed input: 581.38 toks/s, output: 113.80 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:03,  4.14it/s, est. speed input: 700.08 toks/s, output: 153.27 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:11<00:03,  4.38it/s, est. speed input: 760.90 toks/s, output: 176.62 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:12<00:03,  3.56it/s, est. speed input: 771.24 toks/s, output: 184.79 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:12<00:02,  4.14it/s, est. speed input: 805.24 toks/s, output: 198.60 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:12<00:01,  5.09it/s, est. speed input: 868.36 toks/s, output: 225.41 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.50it/s, est. speed input: 1263.04 toks/s, output: 381.73 toks/s]
[2025-01-06 17:46:23,345][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:46:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:06,  3.02s/it, est. speed input: 195.39 toks/s, output: 7.93 toks/s]Processed prompts:   9%|▊         | 2/23 [00:03<00:27,  1.33s/it, est. speed input: 410.08 toks/s, output: 16.10 toks/s]Processed prompts:  57%|█████▋    | 13/23 [00:03<00:01,  7.15it/s, est. speed input: 2742.77 toks/s, output: 112.43 toks/s]Processed prompts:  78%|███████▊  | 18/23 [00:03<00:00, 10.21it/s, est. speed input: 3658.20 toks/s, output: 155.42 toks/s]Processed prompts: 100%|██████████| 23/23 [00:06<00:00,  3.87it/s, est. speed input: 2571.94 toks/s, output: 145.37 toks/s]Processed prompts: 100%|██████████| 23/23 [00:06<00:00,  3.69it/s, est. speed input: 2571.94 toks/s, output: 145.37 toks/s]
[2025-01-06 17:46:30,002][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:46:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:06<00:48,  6.04s/it, est. speed input: 115.73 toks/s, output: 30.46 toks/s]Processed prompts:  22%|██▏       | 2/9 [00:06<00:19,  2.74s/it, est. speed input: 216.07 toks/s, output: 59.35 toks/s]Processed prompts: 100%|██████████| 9/9 [00:06<00:00,  1.39it/s, est. speed input: 972.27 toks/s, output: 275.72 toks/s]
[2025-01-06 17:46:36,908][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:46:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<02:08,  4.58s/it, est. speed input: 111.29 toks/s, output: 8.29 toks/s]Processed prompts:   7%|▋         | 2/29 [00:06<01:25,  3.18s/it, est. speed input: 215.98 toks/s, output: 16.97 toks/s]Processed prompts:  10%|█         | 3/29 [00:08<01:06,  2.57s/it, est. speed input: 280.07 toks/s, output: 26.18 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:10<00:57,  2.30s/it, est. speed input: 321.01 toks/s, output: 35.42 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:11<00:41,  1.75s/it, est. speed input: 344.10 toks/s, output: 47.26 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:11<00:21,  1.01it/s, est. speed input: 491.83 toks/s, output: 73.57 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:12<00:19,  1.08it/s, est. speed input: 517.68 toks/s, output: 84.02 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:12<00:10,  1.77it/s, est. speed input: 605.52 toks/s, output: 112.58 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:13<00:09,  1.91it/s, est. speed input: 640.73 toks/s, output: 124.47 toks/s]Processed prompts: 100%|██████████| 29/29 [00:13<00:00,  2.21it/s, est. speed input: 1605.88 toks/s, output: 399.18 toks/s]
[2025-01-06 17:46:50,557][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:46:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/17 [00:03<00:58,  3.63s/it, est. speed input: 195.65 toks/s, output: 12.40 toks/s]Processed prompts:  18%|█▊        | 3/17 [00:05<00:20,  1.46s/it, est. speed input: 406.91 toks/s, output: 34.59 toks/s]Processed prompts:  29%|██▉       | 5/17 [00:06<00:11,  1.03it/s, est. speed input: 647.52 toks/s, output: 61.35 toks/s]Processed prompts:  41%|████      | 7/17 [00:07<00:08,  1.23it/s, est. speed input: 805.88 toks/s, output: 87.99 toks/s]Processed prompts:  53%|█████▎    | 9/17 [00:08<00:05,  1.45it/s, est. speed input: 942.88 toks/s, output: 118.26 toks/s]Processed prompts:  59%|█████▉    | 10/17 [00:08<00:04,  1.60it/s, est. speed input: 1012.17 toks/s, output: 136.07 toks/s]Processed prompts: 100%|██████████| 17/17 [00:08<00:00,  1.97it/s, est. speed input: 1778.37 toks/s, output: 297.17 toks/s]
[2025-01-06 17:47:08,372][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]
[2025-01-06 17:47:16,252][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:47:16,252][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:47:16,670][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:47:30,507][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:47:30,508][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:47:30,854][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:47:31,052][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 17:47:38,663][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:47:57,007][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:47:57,008][root][INFO] - Iteration 62 took 2m 29s. Generation: 67.40%, Training: 32.60%. Estimated time remaining: 3h 5m 0s. Estimated total time for complete run: 5h 19m 11s.
[2025-01-06 17:47:57,384][root][INFO] - Loading VLLM model.
WARNING 01-06 17:47:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:47:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:47:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:47:58 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:48:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:48:16 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:48:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:48:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:48:39 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:48:39,169][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:48:39,413][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:48:39,414][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:48:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<03:05,  5.98s/it, est. speed input: 84.49 toks/s, output: 9.87 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:30,  3.02s/it, est. speed input: 145.63 toks/s, output: 19.32 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:50,  1.75s/it, est. speed input: 211.32 toks/s, output: 29.71 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.33s/it, est. speed input: 257.14 toks/s, output: 38.70 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:26,  1.03it/s, est. speed input: 308.31 toks/s, output: 48.96 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:20,  1.30it/s, est. speed input: 353.66 toks/s, output: 58.94 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:13,  1.81it/s, est. speed input: 407.57 toks/s, output: 70.45 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:10,  2.35it/s, est. speed input: 457.69 toks/s, output: 81.57 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:14,  1.61it/s, est. speed input: 460.31 toks/s, output: 86.09 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:11,  1.88it/s, est. speed input: 494.50 toks/s, output: 96.65 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:09,  2.15it/s, est. speed input: 551.35 toks/s, output: 116.27 toks/s]Processed prompts:  41%|████      | 13/32 [00:11<00:08,  2.34it/s, est. speed input: 580.92 toks/s, output: 127.33 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:11<00:06,  2.78it/s, est. speed input: 616.18 toks/s, output: 139.79 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.32it/s, est. speed input: 677.85 toks/s, output: 163.17 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:12<00:04,  3.64it/s, est. speed input: 708.92 toks/s, output: 175.56 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:03,  3.59it/s, est. speed input: 733.01 toks/s, output: 186.68 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:12<00:02,  4.30it/s, est. speed input: 792.90 toks/s, output: 212.20 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.51it/s, est. speed input: 1265.33 toks/s, output: 399.57 toks/s]
[2025-01-06 17:48:52,675][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:48:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:34,  3.65s/it, est. speed input: 178.28 toks/s, output: 7.41 toks/s]Processed prompts:   7%|▋         | 2/27 [00:03<00:39,  1.56s/it, est. speed input: 371.75 toks/s, output: 14.92 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:03<00:00, 11.61it/s, est. speed input: 4316.64 toks/s, output: 180.78 toks/s]Processed prompts: 100%|██████████| 27/27 [00:05<00:00,  5.28it/s, est. speed input: 3719.72 toks/s, output: 191.18 toks/s]
[2025-01-06 17:48:58,211][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:48:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.07s/it, est. speed input: 137.97 toks/s, output: 39.48 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 689.81 toks/s, output: 197.37 toks/s]
[2025-01-06 17:49:03,697][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:49:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:22, 12.33s/it, est. speed input: 56.69 toks/s, output: 12.73 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<03:00,  6.01s/it, est. speed input: 100.50 toks/s, output: 24.44 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:46,  3.67s/it, est. speed input: 141.69 toks/s, output: 36.35 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:03,  2.27s/it, est. speed input: 187.44 toks/s, output: 49.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1584.81 toks/s, output: 424.88 toks/s]
[2025-01-06 17:49:19,181][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:49:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:06<01:44,  6.17s/it, est. speed input: 154.69 toks/s, output: 16.21 toks/s]Processed prompts:  17%|█▋        | 3/18 [00:06<00:25,  1.71s/it, est. speed input: 441.79 toks/s, output: 47.70 toks/s]Processed prompts:  28%|██▊       | 5/18 [00:06<00:11,  1.15it/s, est. speed input: 720.32 toks/s, output: 79.88 toks/s]Processed prompts:  33%|███▎      | 6/18 [00:06<00:08,  1.43it/s, est. speed input: 834.17 toks/s, output: 94.43 toks/s]Processed prompts:  39%|███▉      | 7/18 [00:07<00:06,  1.78it/s, est. speed input: 945.71 toks/s, output: 109.47 toks/s]Processed prompts:  44%|████▍     | 8/18 [00:07<00:04,  2.24it/s, est. speed input: 1057.11 toks/s, output: 125.07 toks/s]Processed prompts:  50%|█████     | 9/18 [00:08<00:05,  1.64it/s, est. speed input: 1042.44 toks/s, output: 129.55 toks/s]Processed prompts:  56%|█████▌    | 10/18 [00:08<00:04,  1.79it/s, est. speed input: 1100.97 toks/s, output: 143.79 toks/s]Processed prompts:  61%|██████    | 11/18 [00:09<00:03,  1.78it/s, est. speed input: 1133.11 toks/s, output: 156.57 toks/s]Processed prompts: 100%|██████████| 18/18 [00:09<00:00,  1.95it/s, est. speed input: 1856.00 toks/s, output: 308.14 toks/s]
[2025-01-06 17:49:37,546][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:49:45,147][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:49:45,147][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:49:45,514][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:49:59,769][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:49:59,771][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:50:00,087][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:50:00,235][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 17:50:07,876][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:50:26,801][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:50:26,802][root][INFO] - Iteration 63 took 2m 29s. Generation: 67.02%, Training: 32.98%. Estimated time remaining: 3h 2m 53s. Estimated total time for complete run: 5h 19m 33s.
[2025-01-06 17:50:27,097][root][INFO] - Loading VLLM model.
WARNING 01-06 17:50:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:50:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:50:27 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:50:27 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:50:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:50:46 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:50:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:50:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:51:08 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 17:51:08,464][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:51:08,712][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:51:08,713][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:51:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:34,  4.97s/it, est. speed input: 101.58 toks/s, output: 8.85 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:20,  2.68s/it, est. speed input: 166.99 toks/s, output: 17.53 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.22s/it, est. speed input: 299.90 toks/s, output: 36.08 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:31,  1.15s/it, est. speed input: 326.38 toks/s, output: 43.30 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:25,  1.04it/s, est. speed input: 366.17 toks/s, output: 52.81 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:17,  1.39it/s, est. speed input: 419.18 toks/s, output: 64.27 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:14,  1.60it/s, est. speed input: 456.92 toks/s, output: 74.08 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:17,  1.30it/s, est. speed input: 457.30 toks/s, output: 79.49 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:15,  1.38it/s, est. speed input: 477.86 toks/s, output: 88.76 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:11<00:13,  1.51it/s, est. speed input: 500.99 toks/s, output: 98.94 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:10,  1.83it/s, est. speed input: 533.31 toks/s, output: 111.06 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:11<00:06,  2.74it/s, est. speed input: 605.89 toks/s, output: 137.20 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.96it/s, est. speed input: 682.78 toks/s, output: 164.86 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:12<00:05,  2.95it/s, est. speed input: 688.02 toks/s, output: 171.82 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:03,  3.52it/s, est. speed input: 722.17 toks/s, output: 185.90 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:12<00:03,  4.19it/s, est. speed input: 755.91 toks/s, output: 200.03 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.51it/s, est. speed input: 1269.68 toks/s, output: 403.77 toks/s]
[2025-01-06 17:51:21,878][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:51:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:15,  3.29s/it, est. speed input: 217.65 toks/s, output: 8.21 toks/s]Processed prompts:  62%|██████▎   | 15/24 [00:03<00:01,  5.98it/s, est. speed input: 3029.56 toks/s, output: 125.94 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:05<00:00,  4.78it/s, est. speed input: 2851.35 toks/s, output: 166.49 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.43it/s, est. speed input: 2410.82 toks/s, output: 193.69 toks/s]
[2025-01-06 17:51:29,316][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:51:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:05<00:41,  5.92s/it, est. speed input: 118.07 toks/s, output: 32.43 toks/s]Processed prompts:  38%|███▊      | 3/8 [00:06<00:08,  1.61s/it, est. speed input: 342.64 toks/s, output: 95.91 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 913.66 toks/s, output: 259.30 toks/s]
[2025-01-06 17:51:35,860][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:51:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:08<03:44,  8.03s/it, est. speed input: 118.84 toks/s, output: 12.33 toks/s]Processed prompts:   7%|▋         | 2/29 [00:08<01:39,  3.67s/it, est. speed input: 220.65 toks/s, output: 24.17 toks/s]Processed prompts:  10%|█         | 3/29 [00:11<01:27,  3.36s/it, est. speed input: 245.87 toks/s, output: 32.13 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:12<00:55,  2.22s/it, est. speed input: 278.21 toks/s, output: 45.21 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:12<00:40,  1.68s/it, est. speed input: 316.93 toks/s, output: 57.30 toks/s]Processed prompts:  21%|██        | 6/29 [00:12<00:26,  1.15s/it, est. speed input: 368.44 toks/s, output: 71.52 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:13<00:20,  1.07it/s, est. speed input: 407.04 toks/s, output: 83.80 toks/s]Processed prompts: 100%|██████████| 29/29 [00:13<00:00,  2.16it/s, est. speed input: 1533.18 toks/s, output: 411.26 toks/s]
[2025-01-06 17:51:49,866][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:51:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▋         | 1/16 [00:02<00:40,  2.71s/it, est. speed input: 277.08 toks/s, output: 9.97 toks/s]Processed prompts:  12%|█▎        | 2/16 [00:04<00:27,  1.98s/it, est. speed input: 408.12 toks/s, output: 22.51 toks/s]Processed prompts:  19%|█▉        | 3/16 [00:05<00:24,  1.88s/it, est. speed input: 448.13 toks/s, output: 35.41 toks/s]Processed prompts:  25%|██▌       | 4/16 [00:06<00:14,  1.20s/it, est. speed input: 591.87 toks/s, output: 54.24 toks/s]Processed prompts:  38%|███▊      | 6/16 [00:06<00:06,  1.54it/s, est. speed input: 860.01 toks/s, output: 90.83 toks/s]Processed prompts:  50%|█████     | 8/16 [00:07<00:04,  1.91it/s, est. speed input: 1041.08 toks/s, output: 122.36 toks/s]Processed prompts:  62%|██████▎   | 10/16 [00:07<00:02,  2.41it/s, est. speed input: 1199.73 toks/s, output: 158.46 toks/s]Processed prompts:  75%|███████▌  | 12/16 [00:08<00:01,  2.67it/s, est. speed input: 1318.31 toks/s, output: 192.61 toks/s]Processed prompts: 100%|██████████| 16/16 [00:08<00:00,  1.95it/s, est. speed input: 1758.31 toks/s, output: 290.07 toks/s]
[2025-01-06 17:52:07,349][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 17:52:14,935][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:52:14,935][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:52:15,310][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:52:29,480][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:52:29,481][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:52:29,934][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:52:30,097][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:52:37,756][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:52:56,158][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:52:56,159][root][INFO] - Iteration 64 took 2m 29s. Generation: 67.22%, Training: 32.78%. Estimated time remaining: 2h 59m 27s. Estimated total time for complete run: 5h 18m 37s.
[2025-01-06 17:52:56,556][root][INFO] - Loading VLLM model.
WARNING 01-06 17:52:56 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:52:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:52:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:52:57 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:53:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:53:15 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:53:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:53:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:53:38 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:53:38,350][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:53:38,604][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:53:38,606][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:53:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:38,  5.12s/it, est. speed input: 98.67 toks/s, output: 8.60 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:32,  3.09s/it, est. speed input: 148.68 toks/s, output: 17.08 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:57,  2.00s/it, est. speed input: 202.20 toks/s, output: 26.69 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:38,  1.39s/it, est. speed input: 254.08 toks/s, output: 36.73 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:25,  1.05it/s, est. speed input: 311.05 toks/s, output: 47.67 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:08<00:12,  1.93it/s, est. speed input: 457.22 toks/s, output: 77.86 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:12,  1.83it/s, est. speed input: 479.19 toks/s, output: 85.40 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:11,  1.88it/s, est. speed input: 506.64 toks/s, output: 94.51 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:10<00:09,  2.14it/s, est. speed input: 541.89 toks/s, output: 105.35 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:07,  2.64it/s, est. speed input: 583.31 toks/s, output: 117.53 toks/s]Processed prompts:  41%|████      | 13/32 [00:10<00:06,  3.09it/s, est. speed input: 621.32 toks/s, output: 129.28 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:04,  4.25it/s, est. speed input: 700.34 toks/s, output: 153.66 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:06,  2.42it/s, est. speed input: 685.37 toks/s, output: 155.82 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:11<00:05,  2.97it/s, est. speed input: 721.19 toks/s, output: 169.27 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:05,  2.49it/s, est. speed input: 728.07 toks/s, output: 176.93 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:12<00:03,  3.89it/s, est. speed input: 800.30 toks/s, output: 206.10 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.52it/s, est. speed input: 1273.81 toks/s, output: 394.20 toks/s]
[2025-01-06 17:53:51,734][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:53:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:32,  3.56s/it, est. speed input: 209.42 toks/s, output: 7.31 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:03<00:16,  1.40it/s, est. speed input: 758.67 toks/s, output: 29.37 toks/s]Processed prompts:  81%|████████▏ | 22/27 [00:03<00:00, 10.16it/s, est. speed input: 3922.31 toks/s, output: 164.89 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.80it/s, est. speed input: 2670.12 toks/s, output: 180.45 toks/s]
[2025-01-06 17:53:59,328][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:53:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.06s/it, est. speed input: 138.01 toks/s, output: 39.49 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 690.01 toks/s, output: 197.43 toks/s]
[2025-01-06 17:54:04,847][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:54:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:08<04:14,  8.48s/it, est. speed input: 112.49 toks/s, output: 11.79 toks/s]Processed prompts:   6%|▋         | 2/31 [00:11<02:38,  5.47s/it, est. speed input: 123.59 toks/s, output: 21.69 toks/s]Processed prompts:  10%|▉         | 3/31 [00:12<01:32,  3.29s/it, est. speed input: 192.78 toks/s, output: 33.96 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:12<00:55,  2.06s/it, est. speed input: 245.20 toks/s, output: 47.04 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:13<00:26,  1.05s/it, est. speed input: 365.98 toks/s, output: 72.81 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:13<00:19,  1.26it/s, est. speed input: 416.30 toks/s, output: 85.94 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:13<00:11,  1.97it/s, est. speed input: 497.16 toks/s, output: 111.37 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:13<00:08,  2.37it/s, est. speed input: 529.49 toks/s, output: 124.12 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:13<00:07,  2.60it/s, est. speed input: 587.77 toks/s, output: 135.72 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:14<00:06,  2.93it/s, est. speed input: 628.12 toks/s, output: 147.77 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.20it/s, est. speed input: 1544.76 toks/s, output: 417.73 toks/s]
[2025-01-06 17:54:19,442][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:54:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   6%|▌         | 1/18 [00:04<01:11,  4.22s/it, est. speed input: 174.19 toks/s, output: 13.03 toks/s]Processed prompts:  11%|█         | 2/18 [00:05<00:35,  2.24s/it, est. speed input: 332.38 toks/s, output: 25.78 toks/s]Processed prompts:  17%|█▋        | 3/18 [00:05<00:22,  1.48s/it, est. speed input: 467.09 toks/s, output: 39.23 toks/s]Processed prompts:  22%|██▏       | 4/18 [00:05<00:14,  1.03s/it, est. speed input: 600.44 toks/s, output: 53.75 toks/s]Processed prompts:  28%|██▊       | 5/18 [00:06<00:09,  1.31it/s, est. speed input: 724.97 toks/s, output: 68.50 toks/s]Processed prompts:  39%|███▉      | 7/18 [00:06<00:04,  2.44it/s, est. speed input: 1007.17 toks/s, output: 101.67 toks/s]Processed prompts:  44%|████▍     | 8/18 [00:06<00:04,  2.21it/s, est. speed input: 1032.60 toks/s, output: 112.01 toks/s]Processed prompts:  50%|█████     | 9/18 [00:07<00:04,  1.94it/s, est. speed input: 1064.31 toks/s, output: 121.92 toks/s]Processed prompts:  56%|█████▌    | 10/18 [00:08<00:05,  1.36it/s, est. speed input: 1015.10 toks/s, output: 126.57 toks/s]Processed prompts: 100%|██████████| 18/18 [00:09<00:00,  2.00it/s, est. speed input: 1856.09 toks/s, output: 303.43 toks/s]
[2025-01-06 17:54:37,698][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
[2025-01-06 17:54:45,544][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:54:45,545][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:54:45,915][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:55:00,275][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:55:00,277][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:55:00,650][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:55:00,801][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:55:08,409][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:55:26,529][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:55:26,530][root][INFO] - Iteration 65 took 2m 30s. Generation: 67.43%, Training: 32.57%. Estimated time remaining: 2h 59m 7s. Estimated total time for complete run: 5h 20m 47s.
[2025-01-06 17:55:26,864][root][INFO] - Loading VLLM model.
WARNING 01-06 17:55:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:55:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:55:27 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:55:27 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 17:55:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:55:46 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:55:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:55:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:56:08 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:56:08,940][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:56:09,187][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:56:09,189][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:56:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:37,  5.07s/it, est. speed input: 99.65 toks/s, output: 8.68 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:21,  2.72s/it, est. speed input: 164.33 toks/s, output: 17.25 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:06<00:34,  1.23s/it, est. speed input: 295.60 toks/s, output: 35.56 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:30,  1.14s/it, est. speed input: 324.55 toks/s, output: 42.93 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:28,  1.11s/it, est. speed input: 344.13 toks/s, output: 50.43 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:21,  1.18it/s, est. speed input: 389.82 toks/s, output: 61.64 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:20,  1.15it/s, est. speed input: 404.46 toks/s, output: 69.28 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:11,  1.91it/s, est. speed input: 493.58 toks/s, output: 94.12 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:10<00:09,  2.16it/s, est. speed input: 528.28 toks/s, output: 105.27 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:07,  2.56it/s, est. speed input: 566.36 toks/s, output: 117.29 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:11<00:06,  2.84it/s, est. speed input: 625.40 toks/s, output: 138.44 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:12<00:10,  1.61it/s, est. speed input: 591.70 toks/s, output: 137.71 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.48it/s, est. speed input: 1254.24 toks/s, output: 400.72 toks/s]
[2025-01-06 17:56:22,544][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:56:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:37,  3.73s/it, est. speed input: 185.28 toks/s, output: 7.76 toks/s]Processed prompts:  52%|█████▏    | 14/27 [00:03<00:02,  4.97it/s, est. speed input: 2533.66 toks/s, output: 105.94 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:04<00:01,  6.55it/s, est. speed input: 3243.43 toks/s, output: 151.13 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:06<00:00,  3.78it/s, est. speed input: 2552.71 toks/s, output: 157.00 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.60it/s, est. speed input: 2519.30 toks/s, output: 210.30 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.56it/s, est. speed input: 2519.30 toks/s, output: 210.30 toks/s]
[2025-01-06 17:56:30,547][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:56:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.06s/it, est. speed input: 138.11 toks/s, output: 39.52 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 690.51 toks/s, output: 197.57 toks/s]
[2025-01-06 17:56:36,044][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:56:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:30, 10.66s/it, est. speed input: 89.49 toks/s, output: 12.38 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:33,  5.11s/it, est. speed input: 160.65 toks/s, output: 23.91 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:36,  3.34s/it, est. speed input: 198.72 toks/s, output: 34.84 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:07,  2.41s/it, est. speed input: 252.47 toks/s, output: 45.87 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:14<00:47,  1.75s/it, est. speed input: 290.42 toks/s, output: 57.74 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1534.47 toks/s, output: 425.87 toks/s]
[2025-01-06 17:56:51,247][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:56:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:03<01:15,  3.60s/it, est. speed input: 207.99 toks/s, output: 8.05 toks/s]Processed prompts:   9%|▉         | 2/22 [00:04<00:41,  2.08s/it, est. speed input: 324.48 toks/s, output: 17.32 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:06<00:38,  2.05s/it, est. speed input: 369.74 toks/s, output: 26.53 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:06<00:24,  1.38s/it, est. speed input: 458.55 toks/s, output: 40.09 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:07<00:19,  1.13s/it, est. speed input: 542.61 toks/s, output: 52.21 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:08<00:09,  1.53it/s, est. speed input: 728.16 toks/s, output: 80.88 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:08<00:08,  1.70it/s, est. speed input: 806.00 toks/s, output: 93.67 toks/s]Processed prompts:  41%|████      | 9/22 [00:08<00:07,  1.80it/s, est. speed input: 870.64 toks/s, output: 106.01 toks/s]Processed prompts:  50%|█████     | 11/22 [00:09<00:04,  2.55it/s, est. speed input: 1016.48 toks/s, output: 136.36 toks/s]Processed prompts:  59%|█████▉    | 13/22 [00:10<00:03,  2.64it/s, est. speed input: 1134.20 toks/s, output: 162.04 toks/s]Processed prompts:  64%|██████▎   | 14/22 [00:10<00:02,  2.74it/s, est. speed input: 1192.12 toks/s, output: 176.49 toks/s]Processed prompts: 100%|██████████| 22/22 [00:10<00:00,  2.13it/s, est. speed input: 1911.49 toks/s, output: 331.46 toks/s]
[2025-01-06 17:57:10,959][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 17:57:18,514][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:57:18,515][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:57:18,899][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 17:57:33,381][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 17:57:33,382][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 17:57:33,802][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 17:57:33,943][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-06 17:57:41,795][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 17:58:00,505][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 17:58:00,506][root][INFO] - Iteration 66 took 2m 33s. Generation: 67.73%, Training: 32.27%. Estimated time remaining: 3h 4m 14s. Estimated total time for complete run: 5h 28m 28s.
[2025-01-06 17:58:00,842][root][INFO] - Loading VLLM model.
WARNING 01-06 17:58:01 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 17:58:01 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 17:58:01 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 17:58:01 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 17:58:06 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 17:58:20 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 17:58:20 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 17:58:20 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 17:58:42 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 17:58:42,897][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 17:58:43,145][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 17:58:43,147][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:58:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:30,  4.87s/it, est. speed input: 103.76 toks/s, output: 9.04 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:29,  2.99s/it, est. speed input: 154.36 toks/s, output: 17.73 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:04,  2.24s/it, est. speed input: 192.14 toks/s, output: 26.76 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:42,  1.53s/it, est. speed input: 242.13 toks/s, output: 37.64 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:09<00:33,  1.24s/it, est. speed input: 278.50 toks/s, output: 47.43 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:09<00:22,  1.17it/s, est. speed input: 330.25 toks/s, output: 59.73 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:20,  1.20it/s, est. speed input: 354.74 toks/s, output: 68.34 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:10<00:17,  1.40it/s, est. speed input: 387.48 toks/s, output: 78.93 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:15,  1.44it/s, est. speed input: 410.33 toks/s, output: 88.30 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:11<00:11,  1.85it/s, est. speed input: 448.08 toks/s, output: 100.88 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:12<00:12,  1.65it/s, est. speed input: 461.87 toks/s, output: 109.09 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:12<00:09,  2.15it/s, est. speed input: 498.13 toks/s, output: 122.48 toks/s]Processed prompts:  41%|████      | 13/32 [00:12<00:07,  2.39it/s, est. speed input: 526.23 toks/s, output: 134.26 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:12<00:05,  3.21it/s, est. speed input: 589.38 toks/s, output: 159.89 toks/s]Processed prompts:  50%|█████     | 16/32 [00:12<00:04,  3.78it/s, est. speed input: 622.75 toks/s, output: 173.57 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:13<00:03,  4.45it/s, est. speed input: 655.90 toks/s, output: 187.33 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.44it/s, est. speed input: 1234.59 toks/s, output: 416.52 toks/s]
[2025-01-06 17:58:56,705][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 17:58:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:14,  3.24s/it, est. speed input: 229.63 toks/s, output: 7.71 toks/s]Processed prompts:   8%|▊         | 2/24 [00:03<00:31,  1.42s/it, est. speed input: 393.08 toks/s, output: 15.63 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:03<00:01,  6.75it/s, est. speed input: 2618.11 toks/s, output: 106.69 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:03<00:00,  8.30it/s, est. speed input: 3389.99 toks/s, output: 145.54 toks/s]Processed prompts:  96%|█████████▌| 23/24 [00:06<00:00,  3.87it/s, est. speed input: 2551.52 toks/s, output: 149.12 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.55it/s, est. speed input: 2535.31 toks/s, output: 171.31 toks/s]
[2025-01-06 17:59:03,895][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:59:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:06<00:42,  6.10s/it, est. speed input: 114.59 toks/s, output: 32.79 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 916.63 toks/s, output: 262.27 toks/s]
[2025-01-06 17:59:10,486][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:59:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:09<04:49,  9.98s/it, est. speed input: 95.62 toks/s, output: 12.63 toks/s]Processed prompts:   7%|▋         | 2/30 [00:10<02:03,  4.42s/it, est. speed input: 157.44 toks/s, output: 24.86 toks/s]Processed prompts:  10%|█         | 3/30 [00:11<01:14,  2.76s/it, est. speed input: 230.97 toks/s, output: 36.32 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:11<00:44,  1.71s/it, est. speed input: 290.08 toks/s, output: 49.22 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:11<00:28,  1.13s/it, est. speed input: 348.15 toks/s, output: 62.07 toks/s]Processed prompts:  20%|██        | 6/30 [00:11<00:21,  1.10it/s, est. speed input: 414.29 toks/s, output: 73.18 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:12<00:21,  1.08it/s, est. speed input: 422.93 toks/s, output: 81.74 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:13<00:20,  1.08it/s, est. speed input: 444.90 toks/s, output: 90.67 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.16it/s, est. speed input: 1581.95 toks/s, output: 408.02 toks/s]
[2025-01-06 17:59:24,868][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 17:59:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:04<01:22,  4.33s/it, est. speed input: 173.26 toks/s, output: 10.86 toks/s]Processed prompts:  10%|█         | 2/20 [00:05<00:49,  2.73s/it, est. speed input: 278.53 toks/s, output: 22.03 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:06<00:27,  1.64s/it, est. speed input: 415.39 toks/s, output: 35.49 toks/s]Processed prompts:  20%|██        | 4/20 [00:06<00:19,  1.20s/it, est. speed input: 522.75 toks/s, output: 48.11 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:06<00:12,  1.23it/s, est. speed input: 651.62 toks/s, output: 62.88 toks/s]Processed prompts:  30%|███       | 6/20 [00:07<00:08,  1.64it/s, est. speed input: 764.77 toks/s, output: 76.87 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:07<00:05,  2.24it/s, est. speed input: 884.74 toks/s, output: 91.83 toks/s]Processed prompts:  40%|████      | 8/20 [00:08<00:07,  1.67it/s, est. speed input: 901.38 toks/s, output: 99.05 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:08<00:05,  2.02it/s, est. speed input: 979.97 toks/s, output: 113.92 toks/s]Processed prompts:  50%|█████     | 10/20 [00:08<00:04,  2.38it/s, est. speed input: 1037.54 toks/s, output: 128.99 toks/s]Processed prompts:  55%|█████▌    | 11/20 [00:09<00:03,  2.38it/s, est. speed input: 1094.36 toks/s, output: 142.14 toks/s]Processed prompts:  65%|██████▌   | 13/20 [00:09<00:02,  3.26it/s, est. speed input: 1254.46 toks/s, output: 175.08 toks/s]Processed prompts:  70%|███████   | 14/20 [00:09<00:01,  3.16it/s, est. speed input: 1307.74 toks/s, output: 189.32 toks/s]Processed prompts: 100%|██████████| 20/20 [00:09<00:00,  2.04it/s, est. speed input: 1887.35 toks/s, output: 311.45 toks/s]
[2025-01-06 17:59:43,936][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 17:59:51,619][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 17:59:51,620][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 17:59:51,992][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:00:06,402][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:00:06,403][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:00:06,779][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:00:06,925][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:00:14,569][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:00:33,450][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:00:33,451][root][INFO] - Iteration 67 took 2m 32s. Generation: 67.53%, Training: 32.47%. Estimated time remaining: 2h 59m 29s. Estimated total time for complete run: 5h 26m 16s.
[2025-01-06 18:00:33,824][root][INFO] - Loading VLLM model.
WARNING 01-06 18:00:34 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:00:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:00:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:00:34 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:00:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:00:53 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:00:53 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:00:53 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:01:15 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:01:15,409][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:01:15,654][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:01:15,655][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:01:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 118.98 toks/s, output: 7.07 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:07,  2.24s/it, est. speed input: 198.69 toks/s, output: 14.56 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:49,  1.72s/it, est. speed input: 244.68 toks/s, output: 22.13 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:38,  1.38s/it, est. speed input: 286.51 toks/s, output: 30.49 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:27,  1.02s/it, est. speed input: 339.34 toks/s, output: 40.32 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:22,  1.16it/s, est. speed input: 379.69 toks/s, output: 49.50 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:22,  1.13it/s, est. speed input: 395.94 toks/s, output: 56.90 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:16,  1.46it/s, est. speed input: 439.89 toks/s, output: 68.16 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:09<00:09,  2.39it/s, est. speed input: 535.59 toks/s, output: 92.06 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:09<00:07,  2.87it/s, est. speed input: 580.42 toks/s, output: 103.86 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:10<00:07,  2.65it/s, est. speed input: 604.29 toks/s, output: 112.68 toks/s]Processed prompts:  41%|████      | 13/32 [00:10<00:08,  2.25it/s, est. speed input: 616.54 toks/s, output: 120.21 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:07,  2.40it/s, est. speed input: 643.07 toks/s, output: 130.80 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.87it/s, est. speed input: 726.71 toks/s, output: 158.20 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:11<00:03,  3.84it/s, est. speed input: 754.07 toks/s, output: 169.26 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:06,  2.07it/s, est. speed input: 726.51 toks/s, output: 169.92 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.55it/s, est. speed input: 1287.85 toks/s, output: 392.57 toks/s]
[2025-01-06 18:01:28,638][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:01:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:21,  3.39s/it, est. speed input: 219.59 toks/s, output: 7.96 toks/s]Processed prompts:  80%|████████  | 20/25 [00:03<00:00,  7.25it/s, est. speed input: 3712.92 toks/s, output: 156.90 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:06<00:00,  3.47it/s, est. speed input: 2415.49 toks/s, output: 153.42 toks/s]Processed prompts: 100%|██████████| 25/25 [00:06<00:00,  3.59it/s, est. speed input: 2522.54 toks/s, output: 182.17 toks/s]
[2025-01-06 18:01:36,055][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:01:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:05<00:35,  5.88s/it, est. speed input: 118.83 toks/s, output: 34.00 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s, est. speed input: 831.78 toks/s, output: 237.99 toks/s]
[2025-01-06 18:01:42,356][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:01:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:11<05:52, 11.74s/it, est. speed input: 81.27 toks/s, output: 12.86 toks/s]Processed prompts:   6%|▋         | 2/31 [00:13<02:50,  5.88s/it, est. speed input: 122.27 toks/s, output: 24.56 toks/s]Processed prompts:  10%|▉         | 3/31 [00:14<01:43,  3.70s/it, est. speed input: 160.82 toks/s, output: 36.38 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.12it/s, est. speed input: 1573.29 toks/s, output: 419.27 toks/s]
[2025-01-06 18:01:57,542][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:01:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:06<01:54,  6.02s/it, est. speed input: 158.50 toks/s, output: 14.45 toks/s]Processed prompts:  10%|█         | 2/20 [00:06<00:52,  2.89s/it, est. speed input: 283.88 toks/s, output: 28.27 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:07<00:29,  1.71s/it, est. speed input: 407.83 toks/s, output: 42.75 toks/s]Processed prompts:  20%|██        | 4/20 [00:07<00:18,  1.18s/it, est. speed input: 516.47 toks/s, output: 56.71 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:07<00:12,  1.23it/s, est. speed input: 632.36 toks/s, output: 71.85 toks/s]Processed prompts:  30%|███       | 6/20 [00:08<00:13,  1.01it/s, est. speed input: 645.06 toks/s, output: 79.00 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:09<00:10,  1.24it/s, est. speed input: 717.57 toks/s, output: 93.71 toks/s]Processed prompts:  40%|████      | 8/20 [00:09<00:06,  1.72it/s, est. speed input: 789.40 toks/s, output: 111.18 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:09<00:05,  1.94it/s, est. speed input: 857.43 toks/s, output: 125.94 toks/s]Processed prompts:  50%|█████     | 10/20 [00:10<00:05,  1.99it/s, est. speed input: 910.74 toks/s, output: 139.61 toks/s]Processed prompts: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s, est. speed input: 1841.32 toks/s, output: 334.72 toks/s]
[2025-01-06 18:02:17,171][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:02:24,703][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:02:24,703][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:02:25,067][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:02:39,366][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:02:39,367][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:02:39,737][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:02:39,880][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:02:47,501][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:03:06,843][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:03:06,844][root][INFO] - Iteration 68 took 2m 33s. Generation: 67.48%, Training: 32.52%. Estimated time remaining: 2h 57m 53s. Estimated total time for complete run: 5h 27m 14s.
[2025-01-06 18:03:07,180][root][INFO] - Loading VLLM model.
WARNING 01-06 18:03:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:03:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:03:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:03:07 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:03:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:03:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:03:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:03:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:03:49 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:03:49,225][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:03:49,520][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:03:49,521][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:03:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:28,  4.78s/it, est. speed input: 105.60 toks/s, output: 9.20 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:28,  2.96s/it, est. speed input: 156.38 toks/s, output: 17.96 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:58,  2.01s/it, est. speed input: 206.59 toks/s, output: 27.68 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:36,  1.30s/it, est. speed input: 267.11 toks/s, output: 38.88 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:25,  1.07it/s, est. speed input: 322.01 toks/s, output: 49.74 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:10<00:36,  1.39s/it, est. speed input: 299.84 toks/s, output: 52.25 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:10<00:24,  1.02it/s, est. speed input: 344.40 toks/s, output: 65.18 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:11<00:24,  1.01s/it, est. speed input: 356.20 toks/s, output: 73.27 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:20,  1.11it/s, est. speed input: 378.98 toks/s, output: 83.88 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:12<00:15,  1.43it/s, est. speed input: 412.74 toks/s, output: 96.93 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:12<00:12,  1.71it/s, est. speed input: 442.00 toks/s, output: 109.25 toks/s]Processed prompts:  41%|████      | 13/32 [00:12<00:07,  2.53it/s, est. speed input: 507.91 toks/s, output: 135.93 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:05,  3.03it/s, est. speed input: 541.53 toks/s, output: 149.74 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.44it/s, est. speed input: 1229.86 toks/s, output: 422.76 toks/s]
[2025-01-06 18:04:03,104][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:04:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:16,  3.33s/it, est. speed input: 223.75 toks/s, output: 8.11 toks/s]Processed prompts:  54%|█████▍    | 13/24 [00:03<00:02,  5.16it/s, est. speed input: 2668.18 toks/s, output: 108.37 toks/s]Processed prompts:  79%|███████▉  | 19/24 [00:03<00:00,  7.60it/s, est. speed input: 3706.30 toks/s, output: 155.30 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.45it/s, est. speed input: 2540.29 toks/s, output: 174.33 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.53it/s, est. speed input: 2540.29 toks/s, output: 174.33 toks/s]
[2025-01-06 18:04:10,359][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:04:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:06<00:42,  6.10s/it, est. speed input: 114.51 toks/s, output: 32.76 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 916.03 toks/s, output: 262.10 toks/s]
[2025-01-06 18:04:16,890][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:04:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:11<05:34, 11.52s/it, est. speed input: 82.79 toks/s, output: 13.36 toks/s]Processed prompts:   7%|▋         | 2/30 [00:12<02:25,  5.19s/it, est. speed input: 134.63 toks/s, output: 26.14 toks/s]Processed prompts:  10%|█         | 3/30 [00:12<01:17,  2.87s/it, est. speed input: 210.39 toks/s, output: 39.54 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:12<00:46,  1.78s/it, est. speed input: 264.47 toks/s, output: 52.88 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:12<00:29,  1.20s/it, est. speed input: 316.32 toks/s, output: 65.95 toks/s]Processed prompts:  20%|██        | 6/30 [00:12<00:19,  1.21it/s, est. speed input: 353.70 toks/s, output: 79.20 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:12<00:13,  1.65it/s, est. speed input: 403.64 toks/s, output: 92.12 toks/s]Processed prompts:  30%|███       | 9/30 [00:13<00:11,  1.80it/s, est. speed input: 474.93 toks/s, output: 112.70 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, est. speed input: 1564.66 toks/s, output: 414.36 toks/s]
[2025-01-06 18:04:31,395][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:04:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   7%|▋         | 1/14 [00:04<01:00,  4.67s/it, est. speed input: 193.31 toks/s, output: 19.05 toks/s]Processed prompts:  14%|█▍        | 2/14 [00:05<00:27,  2.33s/it, est. speed input: 308.66 toks/s, output: 36.97 toks/s]Processed prompts:  21%|██▏       | 3/14 [00:05<00:15,  1.40s/it, est. speed input: 461.06 toks/s, output: 55.89 toks/s]Processed prompts:  29%|██▊       | 4/14 [00:06<00:10,  1.02s/it, est. speed input: 584.11 toks/s, output: 73.49 toks/s]Processed prompts:  36%|███▌      | 5/14 [00:06<00:06,  1.31it/s, est. speed input: 705.95 toks/s, output: 92.25 toks/s]Processed prompts:  43%|████▎     | 6/14 [00:06<00:04,  1.85it/s, est. speed input: 830.42 toks/s, output: 113.06 toks/s]Processed prompts:  50%|█████     | 7/14 [00:06<00:03,  2.30it/s, est. speed input: 911.34 toks/s, output: 132.29 toks/s]Processed prompts:  57%|█████▋    | 8/14 [00:07<00:02,  2.23it/s, est. speed input: 983.62 toks/s, output: 147.46 toks/s]Processed prompts:  64%|██████▍   | 9/14 [00:07<00:02,  2.50it/s, est. speed input: 1072.10 toks/s, output: 166.19 toks/s]Processed prompts:  71%|███████▏  | 10/14 [00:07<00:01,  2.71it/s, est. speed input: 1125.35 toks/s, output: 185.08 toks/s]Processed prompts: 100%|██████████| 14/14 [00:07<00:00,  1.78it/s, est. speed input: 1602.45 toks/s, output: 285.24 toks/s]
[2025-01-06 18:04:48,834][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 18:04:56,318][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:04:56,318][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:04:56,685][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:05:11,202][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:05:11,203][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:05:11,716][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:05:11,869][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:05:19,491][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:05:37,575][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:05:37,576][root][INFO] - Iteration 69 took 2m 30s. Generation: 67.56%, Training: 32.44%. Estimated time remaining: 2h 49m 42s. Estimated total time for complete run: 5h 21m 33s.
[2025-01-06 18:05:38,014][root][INFO] - Loading VLLM model.
WARNING 01-06 18:05:38 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:05:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:05:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:05:38 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:05:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:05:57 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:05:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:05:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:06:19 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:06:19,926][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:06:20,209][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:06:20,210][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:06:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:34,  4.99s/it, est. speed input: 101.18 toks/s, output: 8.82 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:21,  2.72s/it, est. speed input: 164.80 toks/s, output: 17.46 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:04,  2.23s/it, est. speed input: 195.20 toks/s, output: 25.51 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:09<00:51,  1.84s/it, est. speed input: 223.96 toks/s, output: 34.48 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:09<00:35,  1.32s/it, est. speed input: 268.35 toks/s, output: 45.80 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:09<00:24,  1.08it/s, est. speed input: 316.56 toks/s, output: 57.88 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:17,  1.44it/s, est. speed input: 361.36 toks/s, output: 69.61 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:10,  2.26it/s, est. speed input: 448.56 toks/s, output: 92.97 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:11,  1.85it/s, est. speed input: 460.98 toks/s, output: 99.77 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:06,  2.93it/s, est. speed input: 546.20 toks/s, output: 126.09 toks/s]Processed prompts:  41%|████      | 13/32 [00:11<00:06,  2.74it/s, est. speed input: 569.02 toks/s, output: 135.47 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:12<00:05,  3.06it/s, est. speed input: 627.13 toks/s, output: 157.63 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:12<00:02,  4.75it/s, est. speed input: 739.05 toks/s, output: 198.54 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:12<00:02,  5.05it/s, est. speed input: 771.23 toks/s, output: 211.31 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:12<00:02,  4.05it/s, est. speed input: 784.21 toks/s, output: 219.66 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.48it/s, est. speed input: 1254.70 toks/s, output: 405.99 toks/s]
[2025-01-06 18:06:33,555][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:06:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:14,  3.24s/it, est. speed input: 229.33 toks/s, output: 8.01 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:03<00:18,  1.12it/s, est. speed input: 612.93 toks/s, output: 24.20 toks/s]Processed prompts:  67%|██████▋   | 16/24 [00:03<00:00,  8.22it/s, est. speed input: 3214.62 toks/s, output: 133.09 toks/s]Processed prompts:  92%|█████████▏| 22/24 [00:05<00:00,  5.78it/s, est. speed input: 3071.83 toks/s, output: 168.85 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.54it/s, est. speed input: 2514.67 toks/s, output: 178.04 toks/s]
[2025-01-06 18:06:40,766][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:06:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:06<00:42,  6.10s/it, est. speed input: 114.53 toks/s, output: 32.77 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 916.15 toks/s, output: 262.13 toks/s]
[2025-01-06 18:06:47,320][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:06:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:09<04:47,  9.59s/it, est. speed input: 99.50 toks/s, output: 12.10 toks/s]Processed prompts:  10%|▉         | 3/31 [00:10<01:21,  2.92s/it, est. speed input: 266.31 toks/s, output: 34.34 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:13<01:13,  2.71s/it, est. speed input: 272.72 toks/s, output: 41.82 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:13<00:48,  1.85s/it, est. speed input: 323.54 toks/s, output: 55.06 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:13<00:36,  1.48s/it, est. speed input: 376.18 toks/s, output: 66.16 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:14<00:25,  1.08s/it, est. speed input: 438.45 toks/s, output: 79.12 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:14<00:18,  1.24it/s, est. speed input: 467.96 toks/s, output: 92.01 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.17it/s, est. speed input: 1604.59 toks/s, output: 414.34 toks/s]
[2025-01-06 18:07:02,117][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:07:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:06<02:12,  6.97s/it, est. speed input: 107.50 toks/s, output: 15.64 toks/s]Processed prompts:  10%|█         | 2/20 [00:08<01:02,  3.49s/it, est. speed input: 186.93 toks/s, output: 30.18 toks/s]Processed prompts:  20%|██        | 4/20 [00:09<00:27,  1.71s/it, est. speed input: 364.80 toks/s, output: 58.03 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:09<00:18,  1.23s/it, est. speed input: 460.61 toks/s, output: 74.89 toks/s]Processed prompts:  30%|███       | 6/20 [00:10<00:16,  1.21s/it, est. speed input: 500.69 toks/s, output: 85.63 toks/s]Processed prompts: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s, est. speed input: 1734.53 toks/s, output: 348.79 toks/s]
[2025-01-06 18:07:22,513][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:07:30,177][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:07:30,178][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:07:30,551][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:07:44,952][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:07:44,953][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:07:45,412][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:07:45,559][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-06 18:07:53,393][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:08:12,731][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:08:12,733][root][INFO] - Iteration 70 took 2m 35s. Generation: 67.46%, Training: 32.54%. Estimated time remaining: 2h 56m 33s. Estimated total time for complete run: 5h 30m 59s.
[2025-01-06 18:08:13,090][root][INFO] - Loading VLLM model.
WARNING 01-06 18:08:13 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:08:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:08:13 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:08:13 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 18:08:18 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:08:32 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:08:33 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:08:33 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:08:54 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:08:54,655][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:08:54,913][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:08:54,914][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:08:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:44,  5.32s/it, est. speed input: 94.94 toks/s, output: 8.27 toks/s]Processed prompts:   6%|▋         | 2/32 [00:07<01:51,  3.70s/it, est. speed input: 127.95 toks/s, output: 16.59 toks/s]Processed prompts:   9%|▉         | 3/32 [00:08<01:01,  2.12s/it, est. speed input: 186.40 toks/s, output: 27.31 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:39,  1.40s/it, est. speed input: 240.07 toks/s, output: 37.79 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:29,  1.09s/it, est. speed input: 281.42 toks/s, output: 47.26 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:09<00:19,  1.32it/s, est. speed input: 333.68 toks/s, output: 58.59 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:16,  1.50it/s, est. speed input: 369.98 toks/s, output: 67.92 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:10<00:20,  1.16it/s, est. speed input: 372.91 toks/s, output: 73.01 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:17,  1.28it/s, est. speed input: 397.49 toks/s, output: 82.65 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:11<00:09,  2.25it/s, est. speed input: 479.84 toks/s, output: 108.49 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:12<00:09,  2.02it/s, est. speed input: 495.95 toks/s, output: 116.79 toks/s]Processed prompts:  41%|████      | 13/32 [00:12<00:09,  1.98it/s, est. speed input: 514.90 toks/s, output: 126.27 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:13<00:06,  2.78it/s, est. speed input: 578.90 toks/s, output: 151.70 toks/s]Processed prompts:  50%|█████     | 16/32 [00:13<00:05,  2.77it/s, est. speed input: 600.73 toks/s, output: 162.45 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.38it/s, est. speed input: 1201.42 toks/s, output: 400.35 toks/s]
[2025-01-06 18:09:08,837][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:09:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:30,  3.62s/it, est. speed input: 205.92 toks/s, output: 7.74 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:03<00:03,  4.40it/s, est. speed input: 2268.13 toks/s, output: 93.23 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:04<00:01,  4.89it/s, est. speed input: 2683.78 toks/s, output: 126.82 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:07<00:01,  2.67it/s, est. speed input: 1973.18 toks/s, output: 147.88 toks/s]Processed prompts:  92%|█████████▏| 24/26 [00:08<00:00,  3.10it/s, est. speed input: 2124.91 toks/s, output: 194.82 toks/s]Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  3.21it/s, est. speed input: 2308.70 toks/s, output: 244.17 toks/s]
[2025-01-06 18:09:17,376][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:09:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:27,  5.45s/it, est. speed input: 128.21 toks/s, output: 36.68 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 769.21 toks/s, output: 220.09 toks/s]
[2025-01-06 18:09:23,251][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:09:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:10<05:22, 10.76s/it, est. speed input: 88.64 toks/s, output: 12.73 toks/s]Processed prompts:   6%|▋         | 2/31 [00:11<02:16,  4.71s/it, est. speed input: 169.81 toks/s, output: 25.10 toks/s]Processed prompts:  10%|▉         | 3/31 [00:13<01:42,  3.67s/it, est. speed input: 209.35 toks/s, output: 34.31 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:13<01:02,  2.31s/it, est. speed input: 274.59 toks/s, output: 47.49 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:14<00:43,  1.65s/it, est. speed input: 300.68 toks/s, output: 59.77 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, est. speed input: 1518.04 toks/s, output: 421.18 toks/s]
[2025-01-06 18:09:38,187][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:09:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:06<02:11,  6.56s/it, est. speed input: 114.10 toks/s, output: 14.47 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:06<00:32,  1.78s/it, est. speed input: 361.59 toks/s, output: 42.75 toks/s]Processed prompts:  19%|█▉        | 4/21 [00:07<00:24,  1.41s/it, est. speed input: 426.94 toks/s, output: 54.25 toks/s]Processed prompts:  24%|██▍       | 5/21 [00:07<00:17,  1.07s/it, est. speed input: 528.01 toks/s, output: 67.70 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:07<00:11,  1.31it/s, est. speed input: 639.78 toks/s, output: 82.87 toks/s]Processed prompts:  33%|███▎      | 7/21 [00:08<00:09,  1.51it/s, est. speed input: 719.18 toks/s, output: 95.23 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:09<00:09,  1.34it/s, est. speed input: 749.46 toks/s, output: 103.59 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:09<00:05,  2.04it/s, est. speed input: 893.20 toks/s, output: 134.87 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:09<00:03,  2.51it/s, est. speed input: 978.20 toks/s, output: 151.64 toks/s]Processed prompts:  57%|█████▋    | 12/21 [00:10<00:03,  2.52it/s, est. speed input: 1033.98 toks/s, output: 164.97 toks/s]Processed prompts:  62%|██████▏   | 13/21 [00:10<00:02,  3.13it/s, est. speed input: 1117.42 toks/s, output: 182.39 toks/s]Processed prompts: 100%|██████████| 21/21 [00:10<00:00,  2.02it/s, est. speed input: 1829.13 toks/s, output: 336.29 toks/s]
[2025-01-06 18:09:58,553][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:10:06,295][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:10:06,295][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:10:06,660][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:10:21,496][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:10:21,498][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:10:21,850][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:10:21,996][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
[2025-01-06 18:10:29,953][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:10:48,723][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:10:48,724][root][INFO] - Iteration 71 took 2m 35s. Generation: 67.74%, Training: 32.26%. Estimated time remaining: 2h 55m 44s. Estimated total time for complete run: 5h 32m 46s.
[2025-01-06 18:10:49,192][root][INFO] - Loading VLLM model.
WARNING 01-06 18:10:49 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:10:49 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:10:49 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:10:50 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 18:10:54 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:11:08 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:11:09 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:11:09 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:11:30 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:11:31,025][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:11:31,291][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:11:31,292][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:11:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:33,  4.95s/it, est. speed input: 101.98 toks/s, output: 8.89 toks/s]Processed prompts:   6%|▋         | 2/32 [00:07<01:47,  3.59s/it, est. speed input: 133.11 toks/s, output: 17.40 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:58,  2.03s/it, est. speed input: 195.15 toks/s, output: 28.73 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:43,  1.55s/it, est. speed input: 235.87 toks/s, output: 38.30 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:27,  1.03s/it, est. speed input: 291.04 toks/s, output: 50.14 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:18,  1.40it/s, est. speed input: 344.95 toks/s, output: 61.93 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:20,  1.21it/s, est. speed input: 359.34 toks/s, output: 68.41 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:10<00:15,  1.55it/s, est. speed input: 400.26 toks/s, output: 79.95 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:13,  1.67it/s, est. speed input: 429.03 toks/s, output: 89.77 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:11,  1.87it/s, est. speed input: 459.87 toks/s, output: 100.44 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:06,  3.06it/s, est. speed input: 542.74 toks/s, output: 126.37 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:12<00:09,  1.97it/s, est. speed input: 555.78 toks/s, output: 138.35 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:13<00:07,  2.13it/s, est. speed input: 580.28 toks/s, output: 150.14 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.45it/s, est. speed input: 1237.87 toks/s, output: 410.58 toks/s]
[2025-01-06 18:11:44,815][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:11:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:28,  3.55s/it, est. speed input: 209.56 toks/s, output: 7.61 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:03<00:22,  1.04it/s, est. speed input: 585.33 toks/s, output: 23.00 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:03<00:01,  7.97it/s, est. speed input: 3157.25 toks/s, output: 129.87 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:05<00:00,  6.28it/s, est. speed input: 3144.64 toks/s, output: 164.21 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:05<00:00,  6.74it/s, est. speed input: 3373.81 toks/s, output: 205.51 toks/s]Processed prompts: 100%|██████████| 26/26 [00:07<00:00,  3.68it/s, est. speed input: 2644.99 toks/s, output: 183.40 toks/s]
[2025-01-06 18:11:52,296][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:11:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:27,  5.44s/it, est. speed input: 128.40 toks/s, output: 36.74 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 770.35 toks/s, output: 220.42 toks/s]
[2025-01-06 18:11:58,149][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:11:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:12<06:07, 12.25s/it, est. speed input: 57.06 toks/s, output: 13.14 toks/s]Processed prompts:   6%|▋         | 2/31 [00:13<02:46,  5.74s/it, est. speed input: 104.06 toks/s, output: 25.46 toks/s]Processed prompts:  10%|▉         | 3/31 [00:14<01:41,  3.62s/it, est. speed input: 144.24 toks/s, output: 37.28 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.13it/s, est. speed input: 1526.95 toks/s, output: 422.46 toks/s]
[2025-01-06 18:12:13,226][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:12:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:06<02:05,  6.27s/it, est. speed input: 119.64 toks/s, output: 14.04 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:06<00:30,  1.67s/it, est. speed input: 383.29 toks/s, output: 41.86 toks/s]Processed prompts:  19%|█▉        | 4/21 [00:07<00:22,  1.33s/it, est. speed input: 481.50 toks/s, output: 52.98 toks/s]Processed prompts:  24%|██▍       | 5/21 [00:08<00:20,  1.26s/it, est. speed input: 532.67 toks/s, output: 62.16 toks/s]Processed prompts:  29%|██▊       | 6/21 [00:08<00:13,  1.09it/s, est. speed input: 630.79 toks/s, output: 77.31 toks/s]Processed prompts:  33%|███▎      | 7/21 [00:08<00:09,  1.51it/s, est. speed input: 734.86 toks/s, output: 93.02 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:09<00:10,  1.26it/s, est. speed input: 751.88 toks/s, output: 100.53 toks/s]Processed prompts:  43%|████▎     | 9/21 [00:10<00:08,  1.42it/s, est. speed input: 808.16 toks/s, output: 113.95 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:10<00:05,  1.85it/s, est. speed input: 888.12 toks/s, output: 130.83 toks/s]Processed prompts:  52%|█████▏    | 11/21 [00:10<00:04,  2.20it/s, est. speed input: 957.53 toks/s, output: 146.72 toks/s]Processed prompts: 100%|██████████| 21/21 [00:10<00:00,  2.00it/s, est. speed input: 1843.30 toks/s, output: 337.13 toks/s]
[2025-01-06 18:12:33,518][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 18:12:40,835][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:12:40,835][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:12:41,213][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:12:55,831][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:12:55,832][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:12:56,256][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:12:56,406][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:13:03,931][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:13:22,747][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:13:22,748][root][INFO] - Iteration 72 took 2m 34s. Generation: 67.94%, Training: 32.06%. Estimated time remaining: 2h 48m 58s. Estimated total time for complete run: 5h 28m 35s.
[2025-01-06 18:13:23,073][root][INFO] - Loading VLLM model.
WARNING 01-06 18:13:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:13:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:13:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:13:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:13:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:13:42 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:13:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:13:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:14:04 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:14:04,783][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:14:05,111][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:14:05,113][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:14:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:28,  4.81s/it, est. speed input: 105.07 toks/s, output: 9.15 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:19,  2.65s/it, est. speed input: 169.93 toks/s, output: 18.00 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:00,  2.08s/it, est. speed input: 206.32 toks/s, output: 26.42 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.35s/it, est. speed input: 266.76 toks/s, output: 37.64 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:25,  1.04it/s, est. speed input: 321.59 toks/s, output: 48.53 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:09<00:28,  1.09s/it, est. speed input: 329.35 toks/s, output: 54.57 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:16,  1.50it/s, est. speed input: 418.21 toks/s, output: 77.95 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:16,  1.42it/s, est. speed input: 434.50 toks/s, output: 85.94 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:12,  1.81it/s, est. speed input: 476.13 toks/s, output: 98.81 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:10<00:10,  2.04it/s, est. speed input: 507.88 toks/s, output: 110.08 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:07,  2.57it/s, est. speed input: 547.15 toks/s, output: 123.06 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:11<00:04,  4.10it/s, est. speed input: 630.93 toks/s, output: 150.28 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:04,  3.43it/s, est. speed input: 676.82 toks/s, output: 169.88 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:12<00:05,  2.54it/s, est. speed input: 676.19 toks/s, output: 175.49 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.52it/s, est. speed input: 1272.79 toks/s, output: 411.77 toks/s]
[2025-01-06 18:14:18,257][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:14:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:24,  3.52s/it, est. speed input: 198.77 toks/s, output: 8.23 toks/s]Processed prompts:  40%|████      | 10/25 [00:03<00:04,  3.71it/s, est. speed input: 1898.05 toks/s, output: 80.02 toks/s]Processed prompts:  56%|█████▌    | 14/25 [00:03<00:02,  5.04it/s, est. speed input: 2488.91 toks/s, output: 111.71 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:04<00:01,  5.23it/s, est. speed input: 2673.81 toks/s, output: 132.17 toks/s]Processed prompts:  76%|███████▌  | 19/25 [00:05<00:01,  4.19it/s, est. speed input: 2537.91 toks/s, output: 144.84 toks/s]Processed prompts:  84%|████████▍ | 21/25 [00:05<00:00,  4.89it/s, est. speed input: 2685.24 toks/s, output: 174.81 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:05<00:00,  5.90it/s, est. speed input: 2919.81 toks/s, output: 219.88 toks/s]Processed prompts: 100%|██████████| 25/25 [00:07<00:00,  3.46it/s, est. speed input: 2449.48 toks/s, output: 204.35 toks/s]
[2025-01-06 18:14:25,930][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:14:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:05<00:35,  5.88s/it, est. speed input: 118.89 toks/s, output: 34.02 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s, est. speed input: 832.14 toks/s, output: 238.10 toks/s]
[2025-01-06 18:14:32,258][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:14:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:25, 12.43s/it, est. speed input: 76.76 toks/s, output: 12.71 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:46,  5.55s/it, est. speed input: 145.00 toks/s, output: 24.93 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:39,  3.42s/it, est. speed input: 203.71 toks/s, output: 36.51 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:07,  2.41s/it, est. speed input: 238.64 toks/s, output: 47.78 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, est. speed input: 1593.18 toks/s, output: 423.04 toks/s]
[2025-01-06 18:14:47,712][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:14:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▌         | 1/20 [00:06<02:04,  6.55s/it, est. speed input: 145.58 toks/s, output: 15.11 toks/s]Processed prompts:  10%|█         | 2/20 [00:07<00:54,  3.01s/it, est. speed input: 269.49 toks/s, output: 29.66 toks/s]Processed prompts:  15%|█▌        | 3/20 [00:08<00:36,  2.14s/it, est. speed input: 349.93 toks/s, output: 42.43 toks/s]Processed prompts:  20%|██        | 4/20 [00:08<00:23,  1.49s/it, est. speed input: 439.98 toks/s, output: 57.19 toks/s]Processed prompts:  25%|██▌       | 5/20 [00:08<00:15,  1.01s/it, est. speed input: 540.36 toks/s, output: 73.52 toks/s]Processed prompts:  35%|███▌      | 7/20 [00:08<00:06,  1.88it/s, est. speed input: 744.17 toks/s, output: 107.09 toks/s]Processed prompts:  40%|████      | 8/20 [00:10<00:08,  1.43it/s, est. speed input: 752.73 toks/s, output: 113.62 toks/s]Processed prompts:  45%|████▌     | 9/20 [00:10<00:06,  1.75it/s, est. speed input: 827.81 toks/s, output: 130.16 toks/s]Processed prompts: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s, est. speed input: 1786.98 toks/s, output: 340.08 toks/s]
[2025-01-06 18:15:08,043][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 18:15:15,399][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:15:15,399][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:15:15,837][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:15:30,129][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:15:30,130][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:15:30,484][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:15:30,744][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 18:15:38,266][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:15:57,485][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:15:57,486][root][INFO] - Iteration 73 took 2m 34s. Generation: 67.95%, Training: 32.05%. Estimated time remaining: 2h 47m 55s. Estimated total time for complete run: 5h 30m 6s.
[2025-01-06 18:15:57,809][root][INFO] - Loading VLLM model.
WARNING 01-06 18:15:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:15:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:15:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:15:58 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:16:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:16:17 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:16:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:16:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:16:39 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:16:39,127][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:16:39,370][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:16:39,371][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:16:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:39,  5.13s/it, est. speed input: 98.40 toks/s, output: 8.18 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:05,  2.18s/it, est. speed input: 192.28 toks/s, output: 16.37 toks/s]Processed prompts:   9%|▉         | 3/32 [00:08<01:14,  2.57s/it, est. speed input: 182.90 toks/s, output: 21.97 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:48,  1.74s/it, est. speed input: 231.09 toks/s, output: 32.72 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:22,  1.14it/s, est. speed input: 338.18 toks/s, output: 55.69 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:20,  1.22it/s, est. speed input: 366.54 toks/s, output: 64.29 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:10<00:16,  1.45it/s, est. speed input: 403.90 toks/s, output: 74.78 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:17,  1.28it/s, est. speed input: 413.13 toks/s, output: 81.44 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:11<00:14,  1.57it/s, est. speed input: 447.20 toks/s, output: 92.98 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:10,  1.98it/s, est. speed input: 505.68 toks/s, output: 114.74 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:12<00:08,  2.06it/s, est. speed input: 548.39 toks/s, output: 134.58 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:13<00:07,  2.30it/s, est. speed input: 576.32 toks/s, output: 146.91 toks/s]Processed prompts:  50%|█████     | 16/32 [00:13<00:05,  2.78it/s, est. speed input: 609.07 toks/s, output: 160.56 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.41it/s, est. speed input: 1214.56 toks/s, output: 400.60 toks/s]
[2025-01-06 18:16:53,147][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:16:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:28,  3.55s/it, est. speed input: 196.80 toks/s, output: 7.61 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:03<00:22,  1.04it/s, est. speed input: 585.05 toks/s, output: 23.02 toks/s]Processed prompts:  69%|██████▉   | 18/26 [00:03<00:00,  8.27it/s, est. speed input: 3262.29 toks/s, output: 136.57 toks/s]Processed prompts:  85%|████████▍ | 22/26 [00:06<00:00,  4.35it/s, est. speed input: 2566.84 toks/s, output: 142.78 toks/s]Processed prompts:  96%|█████████▌| 25/26 [00:07<00:00,  3.42it/s, est. speed input: 2330.45 toks/s, output: 191.99 toks/s]Processed prompts: 100%|██████████| 26/26 [00:07<00:00,  3.40it/s, est. speed input: 2427.56 toks/s, output: 218.11 toks/s]
[2025-01-06 18:17:01,247][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:17:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:27,  5.45s/it, est. speed input: 128.18 toks/s, output: 36.67 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 769.01 toks/s, output: 220.03 toks/s]
[2025-01-06 18:17:07,118][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:17:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<06:09, 11.91s/it, est. speed input: 80.08 toks/s, output: 12.67 toks/s]Processed prompts:   6%|▋         | 2/32 [00:12<02:37,  5.26s/it, est. speed input: 132.00 toks/s, output: 24.91 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:33,  3.24s/it, est. speed input: 176.14 toks/s, output: 36.47 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:10,  2.53s/it, est. speed input: 206.07 toks/s, output: 46.40 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1562.97 toks/s, output: 424.61 toks/s]
[2025-01-06 18:17:22,470][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:17:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:05<02:01,  5.81s/it, est. speed input: 164.25 toks/s, output: 12.74 toks/s]Processed prompts:   9%|▉         | 2/22 [00:06<00:58,  2.91s/it, est. speed input: 285.16 toks/s, output: 24.96 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:08<00:45,  2.38s/it, est. speed input: 339.05 toks/s, output: 35.42 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:09<00:34,  1.89s/it, est. speed input: 398.28 toks/s, output: 47.70 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:09<00:15,  1.05it/s, est. speed input: 561.61 toks/s, output: 79.26 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:11<00:16,  1.07s/it, est. speed input: 577.29 toks/s, output: 87.30 toks/s]Processed prompts: 100%|██████████| 22/22 [00:11<00:00,  1.96it/s, est. speed input: 1794.66 toks/s, output: 354.79 toks/s]
[2025-01-06 18:17:43,371][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:17:50,745][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:17:50,745][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:17:51,142][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:18:05,890][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:18:05,891][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:18:06,365][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:18:06,521][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 18:18:13,982][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:18:33,483][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:18:33,484][root][INFO] - Iteration 74 took 2m 35s. Generation: 67.78%, Training: 32.22%. Estimated time remaining: 2h 48m 0s. Estimated total time for complete run: 5h 32m 47s.
[2025-01-06 18:18:33,807][root][INFO] - Loading VLLM model.
WARNING 01-06 18:18:34 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:18:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:18:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:18:34 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]

INFO 01-06 18:18:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:18:53 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:18:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:18:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:19:15 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:19:15,823][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:19:16,080][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:19:16,081][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:19:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:50,  7.43s/it, est. speed input: 67.96 toks/s, output: 11.17 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:48,  3.62s/it, est. speed input: 120.40 toks/s, output: 21.70 toks/s]Processed prompts:   9%|▉         | 3/32 [00:10<01:18,  2.71s/it, est. speed input: 151.18 toks/s, output: 30.83 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:11<01:00,  2.16s/it, est. speed input: 178.11 toks/s, output: 40.47 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:11<00:29,  1.12s/it, est. speed input: 258.53 toks/s, output: 65.36 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:11<00:16,  1.47it/s, est. speed input: 338.70 toks/s, output: 90.96 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:12<00:16,  1.37it/s, est. speed input: 354.29 toks/s, output: 98.53 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:13<00:10,  1.98it/s, est. speed input: 422.03 toks/s, output: 123.91 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:13<00:09,  2.02it/s, est. speed input: 444.90 toks/s, output: 134.13 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:05,  3.05it/s, est. speed input: 514.14 toks/s, output: 161.59 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s, est. speed input: 1171.58 toks/s, output: 422.09 toks/s]
[2025-01-06 18:19:30,313][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:19:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:03<01:13,  3.36s/it, est. speed input: 221.41 toks/s, output: 8.63 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:03<00:02,  4.35it/s, est. speed input: 2306.02 toks/s, output: 93.70 toks/s]Processed prompts:  74%|███████▍  | 17/23 [00:04<00:01,  5.31it/s, est. speed input: 2880.99 toks/s, output: 139.56 toks/s]Processed prompts:  91%|█████████▏| 21/23 [00:06<00:00,  3.03it/s, est. speed input: 2204.57 toks/s, output: 168.20 toks/s]Processed prompts: 100%|██████████| 23/23 [00:07<00:00,  3.15it/s, est. speed input: 2298.69 toks/s, output: 212.64 toks/s]
[2025-01-06 18:19:38,072][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:19:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  11%|█         | 1/9 [00:06<00:51,  6.49s/it, est. speed input: 107.73 toks/s, output: 30.82 toks/s]Processed prompts: 100%|██████████| 9/9 [00:06<00:00,  1.39it/s, est. speed input: 969.50 toks/s, output: 277.40 toks/s]
[2025-01-06 18:19:44,991][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:19:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:12<05:53, 12.62s/it, est. speed input: 75.53 toks/s, output: 14.03 toks/s]Processed prompts:   7%|▋         | 2/29 [00:12<02:22,  5.26s/it, est. speed input: 149.80 toks/s, output: 27.96 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:13<00:57,  2.31s/it, est. speed input: 256.99 toks/s, output: 53.13 toks/s]Processed prompts: 100%|██████████| 29/29 [00:13<00:00,  2.09it/s, est. speed input: 1519.03 toks/s, output: 414.04 toks/s]
[2025-01-06 18:19:59,362][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:19:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/22 [00:06<02:21,  6.73s/it, est. speed input: 111.48 toks/s, output: 13.82 toks/s]Processed prompts:  14%|█▎        | 3/22 [00:07<00:36,  1.90s/it, est. speed input: 343.92 toks/s, output: 40.64 toks/s]Processed prompts:  18%|█▊        | 4/22 [00:08<00:31,  1.76s/it, est. speed input: 395.06 toks/s, output: 49.38 toks/s]Processed prompts:  23%|██▎       | 5/22 [00:09<00:22,  1.34s/it, est. speed input: 477.56 toks/s, output: 62.84 toks/s]Processed prompts:  27%|██▋       | 6/22 [00:09<00:15,  1.01it/s, est. speed input: 566.65 toks/s, output: 77.60 toks/s]Processed prompts:  32%|███▏      | 7/22 [00:09<00:11,  1.29it/s, est. speed input: 647.09 toks/s, output: 91.85 toks/s]Processed prompts:  36%|███▋      | 8/22 [00:10<00:11,  1.20it/s, est. speed input: 678.32 toks/s, output: 101.22 toks/s]Processed prompts:  41%|████      | 9/22 [00:11<00:09,  1.41it/s, est. speed input: 738.00 toks/s, output: 115.33 toks/s]Processed prompts: 100%|██████████| 22/22 [00:11<00:00,  1.99it/s, est. speed input: 1820.20 toks/s, output: 349.95 toks/s]
[2025-01-06 18:20:20,446][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:20:27,891][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:20:27,891][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:20:28,258][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:20:43,172][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:20:43,173][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:20:43,521][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:20:43,673][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:20:51,435][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:21:10,835][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:21:10,836][root][INFO] - Iteration 75 took 2m 37s. Generation: 67.74%, Training: 32.26%. Estimated time remaining: 2h 48m 16s. Estimated total time for complete run: 5h 35m 41s.
[2025-01-06 18:21:11,172][root][INFO] - Loading VLLM model.
WARNING 01-06 18:21:11 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:21:11 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:21:12 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:21:12 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:21:16 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:21:30 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:21:31 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:21:31 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:21:52 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:21:52,876][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:21:53,123][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:21:53,124][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:21:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:43,  7.20s/it, est. speed input: 70.17 toks/s, output: 11.12 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<02:00,  4.02s/it, est. speed input: 112.26 toks/s, output: 21.12 toks/s]Processed prompts:   9%|▉         | 3/32 [00:11<01:33,  3.23s/it, est. speed input: 134.33 toks/s, output: 30.06 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:11<01:00,  2.16s/it, est. speed input: 171.24 toks/s, output: 42.13 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:12<00:39,  1.46s/it, est. speed input: 210.05 toks/s, output: 54.82 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:35,  1.35s/it, est. speed input: 230.26 toks/s, output: 63.99 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:13<00:29,  1.17s/it, est. speed input: 253.35 toks/s, output: 74.54 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:14<00:19,  1.21it/s, est. speed input: 287.42 toks/s, output: 88.22 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.28it/s, est. speed input: 1149.62 toks/s, output: 429.68 toks/s]
[2025-01-06 18:22:07,629][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:22:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:31,  3.51s/it, est. speed input: 212.46 toks/s, output: 6.56 toks/s]Processed prompts:   7%|▋         | 2/27 [00:03<00:40,  1.63s/it, est. speed input: 389.21 toks/s, output: 13.59 toks/s]Processed prompts:  52%|█████▏    | 14/27 [00:03<00:02,  6.43it/s, est. speed input: 2615.78 toks/s, output: 102.89 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:05<00:01,  4.64it/s, est. speed input: 2533.59 toks/s, output: 132.69 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:08<00:00,  3.08it/s, est. speed input: 2136.06 toks/s, output: 168.35 toks/s]Processed prompts: 100%|██████████| 27/27 [00:08<00:00,  3.27it/s, est. speed input: 2407.07 toks/s, output: 241.12 toks/s]
[2025-01-06 18:22:16,328][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:22:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.09s/it, est. speed input: 137.41 toks/s, output: 39.32 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it, est. speed input: 687.04 toks/s, output: 196.58 toks/s]
[2025-01-06 18:22:21,835][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:22:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:09<04:46,  9.86s/it, est. speed input: 96.73 toks/s, output: 13.08 toks/s]Processed prompts:   7%|▋         | 2/30 [00:13<03:00,  6.45s/it, est. speed input: 105.16 toks/s, output: 23.56 toks/s]Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.15it/s, est. speed input: 1433.17 toks/s, output: 424.07 toks/s]
[2025-01-06 18:22:36,310][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:22:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:06<02:14,  6.10s/it, est. speed input: 122.55 toks/s, output: 12.78 toks/s]Processed prompts:   9%|▊         | 2/23 [00:06<01:01,  2.94s/it, est. speed input: 219.54 toks/s, output: 25.06 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:08<00:44,  2.21s/it, est. speed input: 300.04 toks/s, output: 35.85 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:08<00:28,  1.50s/it, est. speed input: 397.12 toks/s, output: 49.44 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:09<00:24,  1.34s/it, est. speed input: 452.79 toks/s, output: 60.13 toks/s]Processed prompts:  30%|███       | 7/23 [00:10<00:13,  1.17it/s, est. speed input: 562.69 toks/s, output: 87.38 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:10<00:10,  1.45it/s, est. speed input: 640.07 toks/s, output: 102.39 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:10<00:07,  1.78it/s, est. speed input: 696.00 toks/s, output: 117.41 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:10<00:04,  2.96it/s, est. speed input: 863.36 toks/s, output: 150.49 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:11<00:03,  3.05it/s, est. speed input: 925.07 toks/s, output: 164.06 toks/s]Processed prompts: 100%|██████████| 23/23 [00:11<00:00,  2.03it/s, est. speed input: 1791.67 toks/s, output: 357.22 toks/s]
[2025-01-06 18:22:57,633][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:23:05,137][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:23:05,137][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:23:05,517][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:23:20,957][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:23:20,958][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:23:21,454][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:23:21,598][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 18:23:28,950][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:23:47,695][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:23:47,697][root][INFO] - Iteration 76 took 2m 36s. Generation: 67.97%, Training: 32.03%. Estimated time remaining: 2h 44m 36s. Estimated total time for complete run: 5h 34m 38s.
[2025-01-06 18:23:48,035][root][INFO] - Loading VLLM model.
WARNING 01-06 18:23:48 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:23:48 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:23:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:23:48 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:23:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:24:07 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:24:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:24:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:24:29 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:24:29,410][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:24:29,670][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:24:29,672][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:24:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:42,  7.17s/it, est. speed input: 70.47 toks/s, output: 11.58 toks/s]Processed prompts:   6%|▋         | 2/32 [00:10<02:20,  4.68s/it, est. speed input: 99.96 toks/s, output: 21.28 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:11<01:03,  2.25s/it, est. speed input: 172.44 toks/s, output: 43.37 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:11<00:31,  1.23s/it, est. speed input: 255.07 toks/s, output: 70.04 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:11<00:23,  1.06it/s, est. speed input: 294.94 toks/s, output: 83.18 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:12<00:17,  1.36it/s, est. speed input: 332.78 toks/s, output: 95.96 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:12<00:11,  1.99it/s, est. speed input: 403.02 toks/s, output: 120.51 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:13<00:13,  1.59it/s, est. speed input: 409.33 toks/s, output: 125.86 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, est. speed input: 1182.70 toks/s, output: 432.39 toks/s]
[2025-01-06 18:24:43,777][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:24:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:03<01:46,  3.82s/it, est. speed input: 185.11 toks/s, output: 6.55 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:46,  1.71s/it, est. speed input: 359.01 toks/s, output: 13.35 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:04<00:01,  7.94it/s, est. speed input: 3119.19 toks/s, output: 125.55 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:05<00:00,  5.45it/s, est. speed input: 2920.97 toks/s, output: 159.32 toks/s]Processed prompts:  97%|█████████▋| 28/29 [00:07<00:00,  3.88it/s, est. speed input: 2573.11 toks/s, output: 189.64 toks/s]Processed prompts: 100%|██████████| 29/29 [00:07<00:00,  3.65it/s, est. speed input: 2666.85 toks/s, output: 214.82 toks/s]
[2025-01-06 18:24:52,180][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:24:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/it, est. speed input: 160.10 toks/s, output: 45.81 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it, est. speed input: 480.27 toks/s, output: 137.41 toks/s]
[2025-01-06 18:24:56,961][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:24:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:58, 11.56s/it, est. speed input: 60.45 toks/s, output: 12.63 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:20,  6.69s/it, est. speed input: 94.14 toks/s, output: 23.30 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1532.29 toks/s, output: 427.33 toks/s]
[2025-01-06 18:25:12,302][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:25:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:06<02:50,  6.82s/it, est. speed input: 109.95 toks/s, output: 11.29 toks/s]Processed prompts:   8%|▊         | 2/26 [00:07<01:21,  3.39s/it, est. speed input: 218.18 toks/s, output: 22.15 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:09<01:01,  2.68s/it, est. speed input: 275.72 toks/s, output: 31.65 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:10<00:45,  2.07s/it, est. speed input: 335.36 toks/s, output: 42.72 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:11<00:30,  1.44s/it, est. speed input: 408.48 toks/s, output: 56.01 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:11<00:14,  1.30it/s, est. speed input: 568.68 toks/s, output: 84.01 toks/s]Processed prompts:  31%|███       | 8/26 [00:11<00:13,  1.35it/s, est. speed input: 617.01 toks/s, output: 94.58 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:12<00:12,  1.34it/s, est. speed input: 654.96 toks/s, output: 104.61 toks/s]Processed prompts: 100%|██████████| 26/26 [00:12<00:00,  2.04it/s, est. speed input: 1910.62 toks/s, output: 371.22 toks/s]
[2025-01-06 18:25:35,223][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:25:42,588][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:25:42,589][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:25:42,974][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:25:58,166][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:25:58,168][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:25:58,515][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:25:58,667][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 18:26:06,366][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:26:26,139][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:26:26,140][root][INFO] - Iteration 77 took 2m 38s. Generation: 67.77%, Training: 32.23%. Estimated time remaining: 2h 45m 20s. Estimated total time for complete run: 5h 38m 0s.
[2025-01-06 18:26:26,475][root][INFO] - Loading VLLM model.
WARNING 01-06 18:26:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:26:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:26:27 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:26:27 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:26:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:26:45 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:26:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:26:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:27:07 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:27:07,919][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:27:08,163][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:27:08,164][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:27:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<04:02,  7.83s/it, est. speed input: 64.47 toks/s, output: 10.60 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:34,  5.16s/it, est. speed input: 90.76 toks/s, output: 19.86 toks/s]Processed prompts:   9%|▉         | 3/32 [00:11<01:27,  3.02s/it, est. speed input: 130.64 toks/s, output: 31.65 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:12<00:57,  2.05s/it, est. speed input: 165.95 toks/s, output: 42.97 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:13<00:48,  1.78s/it, est. speed input: 187.61 toks/s, output: 52.16 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:14<00:27,  1.12s/it, est. speed input: 245.28 toks/s, output: 74.87 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:14<00:15,  1.49it/s, est. speed input: 313.12 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1109.53 toks/s, output: 417.17 toks/s]
[2025-01-06 18:27:23,175][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:27:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<01:57,  4.06s/it, est. speed input: 182.37 toks/s, output: 6.64 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:48,  1.74s/it, est. speed input: 355.57 toks/s, output: 13.40 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:04<00:01,  7.15it/s, est. speed input: 2870.17 toks/s, output: 115.10 toks/s]Processed prompts:  70%|███████   | 21/30 [00:04<00:01,  8.62it/s, est. speed input: 3374.13 toks/s, output: 143.68 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:05<00:00,  6.79it/s, est. speed input: 3340.92 toks/s, output: 161.96 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  3.10it/s, est. speed input: 2485.24 toks/s, output: 176.07 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.63it/s, est. speed input: 2665.35 toks/s, output: 224.43 toks/s]
[2025-01-06 18:27:31,904][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:27:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it, est. speed input: 174.45 toks/s, output: 49.91 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it, est. speed input: 348.87 toks/s, output: 99.82 toks/s]
[2025-01-06 18:27:36,323][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:27:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:35, 12.75s/it, est. speed input: 54.80 toks/s, output: 13.09 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:01,  6.04s/it, est. speed input: 85.78 toks/s, output: 25.26 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:40,  3.47s/it, est. speed input: 131.51 toks/s, output: 38.05 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:01,  2.19s/it, est. speed input: 176.85 toks/s, output: 51.01 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1474.98 toks/s, output: 430.88 toks/s]
[2025-01-06 18:27:51,548][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:27:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:09<04:06,  9.85s/it, est. speed input: 76.18 toks/s, output: 13.71 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:10<01:00,  2.63s/it, est. speed input: 242.97 toks/s, output: 40.79 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:10<00:28,  1.35s/it, est. speed input: 401.00 toks/s, output: 67.12 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:11<00:24,  1.21s/it, est. speed input: 456.31 toks/s, output: 76.68 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:12<00:21,  1.11s/it, est. speed input: 503.24 toks/s, output: 86.37 toks/s]Processed prompts:  31%|███       | 8/26 [00:12<00:15,  1.13it/s, est. speed input: 568.04 toks/s, output: 99.54 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:12<00:11,  1.45it/s, est. speed input: 618.22 toks/s, output: 113.32 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:12<00:08,  1.86it/s, est. speed input: 685.22 toks/s, output: 127.42 toks/s]Processed prompts: 100%|██████████| 26/26 [00:12<00:00,  2.03it/s, est. speed input: 1842.21 toks/s, output: 376.79 toks/s]
[2025-01-06 18:28:14,478][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-06 18:28:22,167][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:28:22,168][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:28:22,541][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:28:38,241][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:28:38,242][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:28:38,747][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:28:38,978][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 18:28:46,707][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:29:05,867][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:29:05,868][root][INFO] - Iteration 78 took 2m 39s. Generation: 67.74%, Training: 32.26%. Estimated time remaining: 2h 45m 25s. Estimated total time for complete run: 5h 40m 45s.
[2025-01-06 18:29:06,196][root][INFO] - Loading VLLM model.
WARNING 01-06 18:29:06 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:29:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:29:06 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:29:06 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:29:11 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:29:25 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:29:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:29:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:29:47 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:29:47,865][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:29:48,115][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:29:48,116][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:29:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:39,  5.15s/it, est. speed input: 98.03 toks/s, output: 8.54 toks/s]Processed prompts:   6%|▋         | 2/32 [00:06<01:23,  2.79s/it, est. speed input: 160.60 toks/s, output: 17.01 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:06,  2.29s/it, est. speed input: 189.87 toks/s, output: 24.94 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:08<00:45,  1.63s/it, est. speed input: 234.65 toks/s, output: 35.08 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:10<00:29,  1.15s/it, est. speed input: 301.00 toks/s, output: 53.25 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:11<00:29,  1.17s/it, est. speed input: 313.38 toks/s, output: 61.08 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:11<00:22,  1.08it/s, est. speed input: 348.62 toks/s, output: 73.17 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:15,  1.45it/s, est. speed input: 388.81 toks/s, output: 86.32 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:12<00:15,  1.46it/s, est. speed input: 408.32 toks/s, output: 95.73 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:12<00:09,  2.18it/s, est. speed input: 475.78 toks/s, output: 121.14 toks/s]Processed prompts:  41%|████      | 13/32 [00:12<00:07,  2.56it/s, est. speed input: 508.34 toks/s, output: 133.96 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:07,  2.57it/s, est. speed input: 531.42 toks/s, output: 144.77 toks/s]Processed prompts:  50%|█████     | 16/32 [00:13<00:04,  3.86it/s, est. speed input: 599.93 toks/s, output: 172.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.38it/s, est. speed input: 1199.83 toks/s, output: 410.06 toks/s]
[2025-01-06 18:30:02,044][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:30:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:03<01:42,  3.78s/it, est. speed input: 197.06 toks/s, output: 7.14 toks/s]Processed prompts:  11%|█         | 3/28 [00:03<00:25,  1.02s/it, est. speed input: 562.62 toks/s, output: 21.35 toks/s]Processed prompts:  57%|█████▋    | 16/28 [00:04<00:01,  7.19it/s, est. speed input: 2800.74 toks/s, output: 116.34 toks/s]Processed prompts:  75%|███████▌  | 21/28 [00:05<00:01,  5.16it/s, est. speed input: 2670.08 toks/s, output: 144.14 toks/s]Processed prompts:  86%|████████▌ | 24/28 [00:06<00:00,  4.20it/s, est. speed input: 2527.80 toks/s, output: 172.40 toks/s]Processed prompts:  93%|█████████▎| 26/28 [00:08<00:00,  3.10it/s, est. speed input: 2259.41 toks/s, output: 189.05 toks/s]Processed prompts: 100%|██████████| 28/28 [00:08<00:00,  3.41it/s, est. speed input: 2440.62 toks/s, output: 237.71 toks/s]
[2025-01-06 18:30:10,708][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:30:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it, est. speed input: 145.93 toks/s, output: 41.76 toks/s]Processed prompts: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it, est. speed input: 583.71 toks/s, output: 167.01 toks/s]
[2025-01-06 18:30:15,939][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:30:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:13<07:07, 13.80s/it, est. speed input: 69.15 toks/s, output: 13.19 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:09,  6.33s/it, est. speed input: 98.30 toks/s, output: 25.65 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1519.57 toks/s, output: 428.50 toks/s]
[2025-01-06 18:30:31,355][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:30:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:07<02:53,  7.90s/it, est. speed input: 120.74 toks/s, output: 14.30 toks/s]Processed prompts:   9%|▊         | 2/23 [00:08<01:12,  3.45s/it, est. speed input: 231.62 toks/s, output: 28.28 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:09<00:48,  2.43s/it, est. speed input: 281.38 toks/s, output: 40.12 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:09<00:21,  1.20s/it, est. speed input: 457.86 toks/s, output: 68.49 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:10<00:16,  1.05it/s, est. speed input: 535.34 toks/s, output: 82.34 toks/s]Processed prompts:  30%|███       | 7/23 [00:10<00:11,  1.38it/s, est. speed input: 598.49 toks/s, output: 97.27 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:11<00:10,  1.49it/s, est. speed input: 655.81 toks/s, output: 109.21 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:11<00:07,  1.98it/s, est. speed input: 734.98 toks/s, output: 124.92 toks/s]Processed prompts:  43%|████▎     | 10/23 [00:11<00:05,  2.59it/s, est. speed input: 812.82 toks/s, output: 140.63 toks/s]Processed prompts:  48%|████▊     | 11/23 [00:11<00:03,  3.10it/s, est. speed input: 884.22 toks/s, output: 155.61 toks/s]Processed prompts:  52%|█████▏    | 12/23 [00:11<00:03,  3.62it/s, est. speed input: 953.94 toks/s, output: 170.65 toks/s]Processed prompts: 100%|██████████| 23/23 [00:11<00:00,  1.99it/s, est. speed input: 1820.29 toks/s, output: 360.74 toks/s]
[2025-01-06 18:30:53,388][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 18:31:00,805][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:31:00,805][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:31:01,214][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:31:16,182][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:31:16,183][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:31:16,629][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:31:16,796][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 18:31:24,532][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:31:43,771][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:31:43,772][root][INFO] - Iteration 79 took 2m 37s. Generation: 68.00%, Training: 32.00%. Estimated time remaining: 2h 38m 54s. Estimated total time for complete run: 5h 36m 51s.
[2025-01-06 18:31:44,105][root][INFO] - Loading VLLM model.
WARNING 01-06 18:31:44 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:31:44 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:31:44 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:31:44 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:31:49 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:32:03 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:32:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:32:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:32:25 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:32:25,792][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:32:26,042][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:32:26,043][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:32:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<04:03,  7.84s/it, est. speed input: 64.37 toks/s, output: 10.58 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:56,  3.90s/it, est. speed input: 112.43 toks/s, output: 20.59 toks/s]Processed prompts:   9%|▉         | 3/32 [00:10<01:27,  3.02s/it, est. speed input: 138.14 toks/s, output: 29.27 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:11<00:41,  1.52s/it, est. speed input: 215.89 toks/s, output: 51.90 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:12<00:33,  1.30s/it, est. speed input: 243.32 toks/s, output: 61.83 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:13<00:28,  1.12s/it, est. speed input: 269.02 toks/s, output: 71.99 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:13<00:22,  1.08it/s, est. speed input: 297.01 toks/s, output: 83.15 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:13<00:15,  1.46it/s, est. speed input: 331.67 toks/s, output: 96.18 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:13<00:12,  1.77it/s, est. speed input: 360.87 toks/s, output: 107.97 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:14<00:10,  2.01it/s, est. speed input: 387.77 toks/s, output: 119.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1128.01 toks/s, output: 412.60 toks/s]
[2025-01-06 18:32:40,838][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:32:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:03<01:25,  3.57s/it, est. speed input: 208.46 toks/s, output: 8.11 toks/s]Processed prompts:  44%|████▍     | 11/25 [00:03<00:03,  4.09it/s, est. speed input: 2198.00 toks/s, output: 87.85 toks/s]Processed prompts:  68%|██████▊   | 17/25 [00:04<00:01,  5.48it/s, est. speed input: 2875.87 toks/s, output: 130.36 toks/s]Processed prompts:  84%|████████▍ | 21/25 [00:04<00:00,  6.25it/s, est. speed input: 3238.51 toks/s, output: 169.76 toks/s]Processed prompts:  96%|█████████▌| 24/25 [00:06<00:00,  4.33it/s, est. speed input: 2881.51 toks/s, output: 187.49 toks/s]Processed prompts: 100%|██████████| 25/25 [00:07<00:00,  3.52it/s, est. speed input: 2571.75 toks/s, output: 189.26 toks/s]
[2025-01-06 18:32:48,404][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:32:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  14%|█▍        | 1/7 [00:05<00:35,  5.87s/it, est. speed input: 119.10 toks/s, output: 34.08 toks/s]Processed prompts: 100%|██████████| 7/7 [00:05<00:00,  1.19it/s, est. speed input: 833.65 toks/s, output: 238.53 toks/s]
[2025-01-06 18:32:54,681][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:32:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:20, 12.27s/it, est. speed input: 56.96 toks/s, output: 12.79 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:02,  6.09s/it, est. speed input: 117.75 toks/s, output: 24.43 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:47,  3.69s/it, est. speed input: 158.18 toks/s, output: 36.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1548.80 toks/s, output: 426.56 toks/s]
[2025-01-06 18:33:10,094][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:33:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:07<02:36,  7.10s/it, est. speed input: 128.15 toks/s, output: 13.80 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:08<00:44,  2.22s/it, est. speed input: 321.50 toks/s, output: 39.13 toks/s]Processed prompts:  17%|█▋        | 4/23 [00:09<00:35,  1.87s/it, est. speed input: 381.92 toks/s, output: 49.79 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:10<00:27,  1.54s/it, est. speed input: 442.52 toks/s, output: 61.86 toks/s]Processed prompts:  26%|██▌       | 6/23 [00:10<00:20,  1.20s/it, est. speed input: 493.49 toks/s, output: 75.83 toks/s]Processed prompts:  30%|███       | 7/23 [00:10<00:14,  1.10it/s, est. speed input: 548.87 toks/s, output: 90.70 toks/s]Processed prompts:  35%|███▍      | 8/23 [00:11<00:10,  1.38it/s, est. speed input: 618.45 toks/s, output: 105.34 toks/s]Processed prompts:  39%|███▉      | 9/23 [00:11<00:07,  1.75it/s, est. speed input: 689.47 toks/s, output: 120.61 toks/s]Processed prompts: 100%|██████████| 23/23 [00:11<00:00,  1.99it/s, est. speed input: 1790.60 toks/s, output: 362.92 toks/s]
[2025-01-06 18:33:31,582][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 18:33:39,074][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:33:39,074][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:33:39,446][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:33:54,290][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:33:54,291][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:33:54,629][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:33:54,788][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-06 18:34:02,574][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:34:22,010][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:34:22,011][root][INFO] - Iteration 80 took 2m 38s. Generation: 68.04%, Training: 31.96%. Estimated time remaining: 2h 36m 59s. Estimated total time for complete run: 5h 37m 34s.
[2025-01-06 18:34:22,339][root][INFO] - Loading VLLM model.
WARNING 01-06 18:34:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:34:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:34:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:34:23 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:34:27 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:34:41 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:34:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:34:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:35:04 model_runner.py:1335] Graph capturing finished in 23 secs.
[2025-01-06 18:35:04,882][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:35:05,152][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:35:05,153][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:35:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:10,  6.13s/it, est. speed input: 82.33 toks/s, output: 10.92 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:54,  3.83s/it, est. speed input: 120.97 toks/s, output: 20.48 toks/s]Processed prompts:   9%|▉         | 3/32 [00:08<01:01,  2.13s/it, est. speed input: 178.94 toks/s, output: 32.72 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:26,  1.03it/s, est. speed input: 292.39 toks/s, output: 57.09 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:18,  1.38it/s, est. speed input: 346.53 toks/s, output: 69.08 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:09<00:17,  1.43it/s, est. speed input: 377.04 toks/s, output: 77.54 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:13,  1.81it/s, est. speed input: 421.69 toks/s, output: 89.14 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:09<00:10,  2.16it/s, est. speed input: 462.31 toks/s, output: 100.30 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:11<00:12,  1.70it/s, est. speed input: 491.72 toks/s, output: 113.48 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:11<00:09,  2.05it/s, est. speed input: 527.82 toks/s, output: 126.21 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:12<00:06,  2.58it/s, est. speed input: 612.92 toks/s, output: 159.64 toks/s]Processed prompts:  50%|█████     | 16/32 [00:12<00:06,  2.51it/s, est. speed input: 631.01 toks/s, output: 169.62 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.49it/s, est. speed input: 1258.14 toks/s, output: 418.24 toks/s]
[2025-01-06 18:35:18,444][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:35:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:03<01:16,  3.32s/it, est. speed input: 224.54 toks/s, output: 8.14 toks/s]Processed prompts:  62%|██████▎   | 15/24 [00:03<00:01,  5.93it/s, est. speed input: 3050.16 toks/s, output: 124.68 toks/s]Processed prompts:  88%|████████▊ | 21/24 [00:04<00:00,  6.37it/s, est. speed input: 3505.21 toks/s, output: 164.75 toks/s]Processed prompts: 100%|██████████| 24/24 [00:06<00:00,  3.60it/s, est. speed input: 2581.00 toks/s, output: 168.29 toks/s]
[2025-01-06 18:35:25,575][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:35:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  12%|█▎        | 1/8 [00:06<00:42,  6.10s/it, est. speed input: 114.60 toks/s, output: 32.79 toks/s]Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.31it/s, est. speed input: 916.74 toks/s, output: 262.30 toks/s]
[2025-01-06 18:35:32,107][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:35:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:37, 10.87s/it, est. speed input: 87.73 toks/s, output: 12.23 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:11,  6.38s/it, est. speed input: 117.18 toks/s, output: 22.61 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:49,  3.76s/it, est. speed input: 176.62 toks/s, output: 34.96 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:05,  2.35s/it, est. speed input: 221.35 toks/s, output: 47.94 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, est. speed input: 1596.08 toks/s, output: 422.85 toks/s]
[2025-01-06 18:35:47,597][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:35:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   5%|▍         | 1/21 [00:06<02:01,  6.07s/it, est. speed input: 157.20 toks/s, output: 13.84 toks/s]Processed prompts:  10%|▉         | 2/21 [00:07<01:06,  3.48s/it, est. speed input: 246.84 toks/s, output: 26.52 toks/s]Processed prompts:  14%|█▍        | 3/21 [00:08<00:39,  2.19s/it, est. speed input: 341.22 toks/s, output: 40.66 toks/s]Processed prompts:  33%|███▎      | 7/21 [00:09<00:12,  1.14it/s, est. speed input: 676.54 toks/s, output: 94.12 toks/s]Processed prompts:  38%|███▊      | 8/21 [00:10<00:10,  1.19it/s, est. speed input: 720.71 toks/s, output: 106.25 toks/s]Processed prompts:  48%|████▊     | 10/21 [00:10<00:06,  1.80it/s, est. speed input: 870.68 toks/s, output: 141.86 toks/s]Processed prompts: 100%|██████████| 21/21 [00:10<00:00,  1.96it/s, est. speed input: 1811.35 toks/s, output: 347.05 toks/s]
[2025-01-06 18:36:08,694][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:36:16,034][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:36:16,034][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:36:16,404][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:36:30,710][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:36:30,711][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:36:31,062][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:36:31,214][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:36:38,841][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:36:58,305][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:36:58,306][root][INFO] - Iteration 81 took 2m 36s. Generation: 68.17%, Training: 31.83%. Estimated time remaining: 2h 30m 13s. Estimated total time for complete run: 5h 33m 25s.
[2025-01-06 18:36:58,668][root][INFO] - Loading VLLM model.
WARNING 01-06 18:36:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:36:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:36:59 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:36:59 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:37:04 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:37:18 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:37:18 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:37:18 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:37:40 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:37:40,419][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:37:40,692][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:37:40,694][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:37:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:33,  6.90s/it, est. speed input: 73.18 toks/s, output: 11.45 toks/s]Processed prompts:   6%|▋         | 2/32 [00:09<02:13,  4.46s/it, est. speed input: 104.60 toks/s, output: 21.13 toks/s]Processed prompts:   9%|▉         | 3/32 [00:11<01:29,  3.09s/it, est. speed input: 136.26 toks/s, output: 31.84 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:12<01:08,  2.44s/it, est. speed input: 160.89 toks/s, output: 42.13 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:12<00:43,  1.62s/it, est. speed input: 198.45 toks/s, output: 55.57 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:34,  1.31s/it, est. speed input: 225.66 toks/s, output: 66.88 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:13<00:25,  1.00s/it, est. speed input: 256.20 toks/s, output: 79.43 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:13<00:17,  1.40it/s, est. speed input: 290.62 toks/s, output: 93.23 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s, est. speed input: 1162.43 toks/s, output: 438.50 toks/s]
[2025-01-06 18:37:55,066][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:37:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:57,  4.20s/it, est. speed input: 177.49 toks/s, output: 6.91 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:04<00:05,  3.43it/s, est. speed input: 1864.61 toks/s, output: 74.89 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:04<00:02,  4.75it/s, est. speed input: 2385.40 toks/s, output: 102.35 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:05<00:02,  5.20it/s, est. speed input: 2622.99 toks/s, output: 120.51 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:05<00:01,  5.20it/s, est. speed input: 2711.74 toks/s, output: 133.52 toks/s]Processed prompts:  76%|███████▌  | 22/29 [00:05<00:01,  5.05it/s, est. speed input: 2752.56 toks/s, output: 148.90 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:06<00:00,  5.86it/s, est. speed input: 2920.10 toks/s, output: 172.94 toks/s]Processed prompts:  90%|████████▉ | 26/29 [00:08<00:01,  2.50it/s, est. speed input: 2344.92 toks/s, output: 168.16 toks/s]Processed prompts:  93%|█████████▎| 27/29 [00:08<00:00,  2.68it/s, est. speed input: 2372.03 toks/s, output: 186.90 toks/s]Processed prompts: 100%|██████████| 29/29 [00:08<00:00,  3.64it/s, est. speed input: 2516.76 toks/s, output: 231.51 toks/s]Processed prompts: 100%|██████████| 29/29 [00:08<00:00,  3.42it/s, est. speed input: 2516.76 toks/s, output: 231.51 toks/s]
[2025-01-06 18:38:03,991][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:38:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.02s/it, est. speed input: 173.96 toks/s, output: 45.54 toks/s]Processed prompts:  67%|██████▋   | 2/3 [00:04<00:01,  1.84s/it, est. speed input: 322.80 toks/s, output: 88.44 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it, est. speed input: 484.19 toks/s, output: 134.61 toks/s]
[2025-01-06 18:38:08,740][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:38:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:48, 11.24s/it, est. speed input: 84.78 toks/s, output: 12.63 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<03:03,  6.13s/it, est. speed input: 138.24 toks/s, output: 23.63 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:49,  3.76s/it, est. speed input: 176.78 toks/s, output: 35.68 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1503.78 toks/s, output: 429.11 toks/s]
[2025-01-06 18:38:23,967][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:38:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:05<02:17,  5.75s/it, est. speed input: 130.44 toks/s, output: 10.78 toks/s]Processed prompts:   8%|▊         | 2/25 [00:07<01:19,  3.47s/it, est. speed input: 223.50 toks/s, output: 21.12 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:08<00:52,  2.38s/it, est. speed input: 305.35 toks/s, output: 32.40 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:09<00:34,  1.65s/it, est. speed input: 391.23 toks/s, output: 44.84 toks/s]Processed prompts:  20%|██        | 5/25 [00:09<00:22,  1.14s/it, est. speed input: 482.39 toks/s, output: 58.21 toks/s]Processed prompts:  24%|██▍       | 6/25 [00:09<00:17,  1.11it/s, est. speed input: 556.74 toks/s, output: 70.40 toks/s]Processed prompts:  28%|██▊       | 7/25 [00:10<00:13,  1.31it/s, est. speed input: 622.68 toks/s, output: 82.33 toks/s]Processed prompts:  32%|███▏      | 8/25 [00:11<00:14,  1.20it/s, est. speed input: 635.44 toks/s, output: 91.22 toks/s]Processed prompts:  40%|████      | 10/25 [00:11<00:07,  1.97it/s, est. speed input: 782.06 toks/s, output: 120.49 toks/s]Processed prompts:  44%|████▍     | 11/25 [00:12<00:06,  2.07it/s, est. speed input: 834.70 toks/s, output: 132.99 toks/s]Processed prompts: 100%|██████████| 25/25 [00:12<00:00,  2.07it/s, est. speed input: 1889.31 toks/s, output: 364.70 toks/s]
[2025-01-06 18:38:46,468][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:38:53,870][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:38:53,870][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:38:54,240][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:39:09,824][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:39:09,825][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:39:10,182][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:39:10,329][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:39:18,126][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:39:37,382][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:39:37,383][root][INFO] - Iteration 82 took 2m 39s. Generation: 67.90%, Training: 32.10%. Estimated time remaining: 2h 33m 30s. Estimated total time for complete run: 5h 39m 21s.
[2025-01-06 18:39:37,792][root][INFO] - Loading VLLM model.
WARNING 01-06 18:39:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:39:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:39:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:39:38 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:39:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:39:57 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:39:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:39:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:40:19 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:40:19,263][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:40:19,503][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:40:19,504][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:40:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:34,  6.92s/it, est. speed input: 72.93 toks/s, output: 11.41 toks/s]Processed prompts:   6%|▋         | 2/32 [00:09<02:14,  4.47s/it, est. speed input: 104.33 toks/s, output: 21.07 toks/s]Processed prompts:   9%|▉         | 3/32 [00:10<01:25,  2.94s/it, est. speed input: 140.40 toks/s, output: 32.25 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:10<00:51,  1.85s/it, est. speed input: 184.24 toks/s, output: 45.15 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:11<00:33,  1.24s/it, est. speed input: 226.82 toks/s, output: 57.94 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:11<00:27,  1.04s/it, est. speed input: 257.20 toks/s, output: 68.50 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:12<00:20,  1.19it/s, est. speed input: 289.66 toks/s, output: 80.06 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:13<00:19,  1.20it/s, est. speed input: 310.19 toks/s, output: 89.30 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:13<00:13,  1.66it/s, est. speed input: 346.28 toks/s, output: 102.93 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:13<00:10,  2.10it/s, est. speed input: 379.15 toks/s, output: 115.85 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:13<00:06,  3.14it/s, est. speed input: 445.70 toks/s, output: 142.24 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, est. speed input: 1180.77 toks/s, output: 433.58 toks/s]
[2025-01-06 18:40:33,653][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:40:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:38,  3.79s/it, est. speed input: 196.51 toks/s, output: 7.65 toks/s]Processed prompts:  70%|███████   | 19/27 [00:04<00:01,  6.04it/s, est. speed input: 3303.34 toks/s, output: 136.13 toks/s]Processed prompts:  81%|████████▏ | 22/27 [00:04<00:00,  5.87it/s, est. speed input: 3346.95 toks/s, output: 158.12 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:05<00:00,  4.82it/s, est. speed input: 3111.88 toks/s, output: 170.89 toks/s]Processed prompts:  96%|█████████▋| 26/27 [00:06<00:00,  3.58it/s, est. speed input: 2763.15 toks/s, output: 182.68 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.55it/s, est. speed input: 2751.17 toks/s, output: 200.98 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.77it/s, est. speed input: 2751.17 toks/s, output: 200.98 toks/s]
[2025-01-06 18:40:41,257][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:40:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.17s/it, est. speed input: 135.31 toks/s, output: 38.71 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it, est. speed input: 676.49 toks/s, output: 193.56 toks/s]
[2025-01-06 18:40:46,827][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:40:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<06:07, 11.86s/it, est. speed input: 80.43 toks/s, output: 12.73 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:14,  6.49s/it, est. speed input: 113.27 toks/s, output: 23.78 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:45,  3.64s/it, est. speed input: 158.58 toks/s, output: 36.88 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1530.32 toks/s, output: 427.92 toks/s]
[2025-01-06 18:41:02,188][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:41:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:07<03:01,  7.57s/it, est. speed input: 99.07 toks/s, output: 12.81 toks/s]Processed prompts:   8%|▊         | 2/25 [00:08<01:25,  3.71s/it, est. speed input: 198.49 toks/s, output: 24.93 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:09<00:53,  2.42s/it, est. speed input: 280.70 toks/s, output: 36.86 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:10<00:35,  1.69s/it, est. speed input: 359.58 toks/s, output: 49.38 toks/s]Processed prompts:  20%|██        | 5/25 [00:10<00:27,  1.38s/it, est. speed input: 400.85 toks/s, output: 60.74 toks/s]Processed prompts:  28%|██▊       | 7/25 [00:11<00:14,  1.23it/s, est. speed input: 531.60 toks/s, output: 88.07 toks/s]Processed prompts:  32%|███▏      | 8/25 [00:12<00:13,  1.23it/s, est. speed input: 574.74 toks/s, output: 98.33 toks/s]Processed prompts:  40%|████      | 10/25 [00:12<00:07,  2.02it/s, est. speed input: 705.10 toks/s, output: 129.15 toks/s]Processed prompts: 100%|██████████| 25/25 [00:12<00:00,  2.02it/s, est. speed input: 1845.12 toks/s, output: 371.61 toks/s]
[2025-01-06 18:41:25,107][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 18:41:32,428][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:41:32,428][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:41:32,804][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:41:47,954][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:41:47,955][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:41:48,296][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:41:48,440][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:41:55,947][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:42:15,572][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:42:15,573][root][INFO] - Iteration 83 took 2m 38s. Generation: 68.00%, Training: 32.00%. Estimated time remaining: 2h 28m 59s. Estimated total time for complete run: 5h 37m 28s.
[2025-01-06 18:42:15,941][root][INFO] - Loading VLLM model.
WARNING 01-06 18:42:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:42:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:42:16 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:42:16 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 18:42:21 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:42:35 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:42:35 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:42:35 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:42:57 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:42:57,893][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:42:58,132][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:42:58,134][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:42:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:38,  7.03s/it, est. speed input: 71.80 toks/s, output: 9.53 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:52,  5.75s/it, est. speed input: 84.96 toks/s, output: 18.09 toks/s]Processed prompts:   9%|▉         | 3/32 [00:12<01:32,  3.18s/it, est. speed input: 126.19 toks/s, output: 30.40 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:12<00:55,  1.99s/it, est. speed input: 165.86 toks/s, output: 42.53 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:13<00:44,  1.64s/it, est. speed input: 191.49 toks/s, output: 52.25 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:14<00:35,  1.37s/it, est. speed input: 215.64 toks/s, output: 62.34 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:14<00:28,  1.13s/it, est. speed input: 240.71 toks/s, output: 73.20 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1096.51 toks/s, output: 412.21 toks/s]
[2025-01-06 18:43:13,356][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:43:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:37,  3.75s/it, est. speed input: 198.63 toks/s, output: 7.47 toks/s]Processed prompts:  44%|████▍     | 12/27 [00:03<00:03,  4.24it/s, est. speed input: 2252.32 toks/s, output: 89.91 toks/s]Processed prompts:  63%|██████▎   | 17/27 [00:04<00:01,  5.57it/s, est. speed input: 2874.04 toks/s, output: 124.81 toks/s]Processed prompts:  78%|███████▊  | 21/27 [00:04<00:01,  5.70it/s, est. speed input: 3088.14 toks/s, output: 151.28 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:05<00:00,  5.35it/s, est. speed input: 3115.10 toks/s, output: 177.72 toks/s]Processed prompts:  96%|█████████▋| 26/27 [00:07<00:00,  3.05it/s, est. speed input: 2537.96 toks/s, output: 174.31 toks/s]Processed prompts: 100%|██████████| 27/27 [00:07<00:00,  3.59it/s, est. speed input: 2635.16 toks/s, output: 200.91 toks/s]
[2025-01-06 18:43:21,299][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:43:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.06s/it, est. speed input: 138.02 toks/s, output: 39.49 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 690.05 toks/s, output: 197.44 toks/s]
[2025-01-06 18:43:26,797][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:43:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:43, 14.94s/it, est. speed input: 46.79 toks/s, output: 13.39 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, est. speed input: 1532.06 toks/s, output: 428.42 toks/s]
[2025-01-06 18:43:42,254][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:43:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:07<03:06,  7.45s/it, est. speed input: 127.99 toks/s, output: 12.07 toks/s]Processed prompts:   8%|▊         | 2/26 [00:08<01:25,  3.56s/it, est. speed input: 230.27 toks/s, output: 23.65 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:08<00:46,  2.03s/it, est. speed input: 337.10 toks/s, output: 36.04 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:08<00:30,  1.40s/it, est. speed input: 404.37 toks/s, output: 47.58 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:09<00:23,  1.10s/it, est. speed input: 477.15 toks/s, output: 58.47 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:09<00:16,  1.21it/s, est. speed input: 561.00 toks/s, output: 70.80 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:10<00:12,  1.56it/s, est. speed input: 620.54 toks/s, output: 83.12 toks/s]Processed prompts:  31%|███       | 8/26 [00:10<00:08,  2.08it/s, est. speed input: 706.12 toks/s, output: 96.37 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:10<00:06,  2.53it/s, est. speed input: 783.48 toks/s, output: 108.93 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:10<00:04,  3.20it/s, est. speed input: 864.90 toks/s, output: 122.28 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:11<00:05,  2.71it/s, est. speed input: 912.08 toks/s, output: 131.88 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:12<00:08,  1.59it/s, est. speed input: 899.18 toks/s, output: 135.08 toks/s]Processed prompts: 100%|██████████| 26/26 [00:12<00:00,  2.12it/s, est. speed input: 1954.10 toks/s, output: 363.74 toks/s]
[2025-01-06 18:44:05,455][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:44:13,058][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:44:13,059][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:44:13,444][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:44:28,641][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:44:28,642][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:44:28,990][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:44:29,141][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:44:36,612][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:44:56,363][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:44:56,364][root][INFO] - Iteration 84 took 2m 40s. Generation: 68.25%, Training: 31.75%. Estimated time remaining: 2h 31m 51s. Estimated total time for complete run: 5h 43m 1s.
[2025-01-06 18:44:56,679][root][INFO] - Loading VLLM model.
WARNING 01-06 18:44:56 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:44:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:44:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:44:57 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:45:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:45:16 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:45:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:45:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:45:38 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:45:38,579][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:45:38,823][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:45:38,824][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:45:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:54,  7.57s/it, est. speed input: 66.73 toks/s, output: 10.31 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:57,  3.93s/it, est. speed input: 112.91 toks/s, output: 20.01 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:09<00:44,  1.57s/it, est. speed input: 217.43 toks/s, output: 41.76 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:10<00:38,  1.41s/it, est. speed input: 244.00 toks/s, output: 49.67 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:11<00:34,  1.34s/it, est. speed input: 262.66 toks/s, output: 57.39 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:11<00:25,  1.02s/it, est. speed input: 299.56 toks/s, output: 69.06 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:11<00:17,  1.35it/s, est. speed input: 339.39 toks/s, output: 81.49 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:12<00:13,  1.74it/s, est. speed input: 375.49 toks/s, output: 93.27 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:12<00:09,  2.24it/s, est. speed input: 412.25 toks/s, output: 105.39 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:12<00:09,  2.20it/s, est. speed input: 436.61 toks/s, output: 114.99 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:12<00:08,  2.50it/s, est. speed input: 466.22 toks/s, output: 126.25 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:05,  3.38it/s, est. speed input: 529.77 toks/s, output: 150.31 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:13<00:04,  3.52it/s, est. speed input: 557.18 toks/s, output: 161.67 toks/s]Processed prompts:  50%|█████     | 16/32 [00:13<00:04,  3.39it/s, est. speed input: 580.47 toks/s, output: 172.27 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s, est. speed input: 1160.89 toks/s, output: 402.15 toks/s]
[2025-01-06 18:45:53,224][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:45:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:03<01:53,  3.91s/it, est. speed input: 190.69 toks/s, output: 6.40 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:46,  1.68s/it, est. speed input: 357.22 toks/s, output: 12.93 toks/s]Processed prompts:  10%|█         | 3/30 [00:04<00:25,  1.04it/s, est. speed input: 527.64 toks/s, output: 19.59 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:04<00:01,  9.09it/s, est. speed input: 2721.83 toks/s, output: 109.67 toks/s]Processed prompts:  70%|███████   | 21/30 [00:04<00:01,  8.30it/s, est. speed input: 3030.59 toks/s, output: 142.13 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:05<00:00,  6.54it/s, est. speed input: 3042.45 toks/s, output: 165.25 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  3.53it/s, est. speed input: 2495.69 toks/s, output: 169.49 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.70it/s, est. speed input: 2665.99 toks/s, output: 218.80 toks/s]
[2025-01-06 18:46:01,775][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:46:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:03<00:03,  3.71s/it, est. speed input: 188.30 toks/s, output: 49.57 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it, est. speed input: 351.29 toks/s, output: 96.49 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it, est. speed input: 351.29 toks/s, output: 96.49 toks/s]
[2025-01-06 18:46:06,177][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:46:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:24, 14.34s/it, est. speed input: 65.34 toks/s, output: 13.39 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:05,  6.19s/it, est. speed input: 97.59 toks/s, output: 26.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1478.00 toks/s, output: 431.07 toks/s]
[2025-01-06 18:46:21,501][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:46:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:07<03:41,  7.63s/it, est. speed input: 98.30 toks/s, output: 10.09 toks/s]Processed prompts:   7%|▋         | 2/30 [00:11<02:39,  5.69s/it, est. speed input: 142.38 toks/s, output: 19.06 toks/s]Processed prompts:  10%|█         | 3/30 [00:12<01:31,  3.40s/it, est. speed input: 210.12 toks/s, output: 30.92 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:12<00:55,  2.15s/it, est. speed input: 280.68 toks/s, output: 43.37 toks/s]Processed prompts:  20%|██        | 6/30 [00:12<00:24,  1.04s/it, est. speed input: 423.21 toks/s, output: 68.93 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:14<00:18,  1.21it/s, est. speed input: 511.81 toks/s, output: 89.30 toks/s]Processed prompts:  30%|███       | 9/30 [00:14<00:14,  1.43it/s, est. speed input: 553.81 toks/s, output: 101.24 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:14<00:08,  2.25it/s, est. speed input: 680.40 toks/s, output: 127.71 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, est. speed input: 1904.11 toks/s, output: 390.16 toks/s]
[2025-01-06 18:46:46,716][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:46:54,139][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:46:54,139][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:46:54,518][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:47:09,619][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:47:09,621][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:47:09,972][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:47:10,124][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 18:47:17,681][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:47:37,521][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:47:37,522][root][INFO] - Iteration 85 took 2m 41s. Generation: 68.36%, Training: 31.64%. Estimated time remaining: 2h 29m 57s. Estimated total time for complete run: 5h 43m 48s.
[2025-01-06 18:47:37,835][root][INFO] - Loading VLLM model.
WARNING 01-06 18:47:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:47:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:47:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:47:38 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 18:47:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:47:57 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:47:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:47:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:48:19 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:48:19,608][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:48:19,851][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:48:19,854][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:48:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:29,  6.75s/it, est. speed input: 74.76 toks/s, output: 9.92 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:45,  3.52s/it, est. speed input: 126.06 toks/s, output: 19.35 toks/s]Processed prompts:   9%|▉         | 3/32 [00:09<01:20,  2.79s/it, est. speed input: 152.47 toks/s, output: 27.78 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:10<00:50,  1.80s/it, est. speed input: 197.60 toks/s, output: 39.32 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:11<00:41,  1.56s/it, est. speed input: 222.67 toks/s, output: 48.33 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:11<00:28,  1.08s/it, est. speed input: 263.43 toks/s, output: 60.60 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:12<00:24,  1.03it/s, est. speed input: 288.77 toks/s, output: 70.25 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:13<00:23,  1.01it/s, est. speed input: 304.51 toks/s, output: 78.61 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:13<00:16,  1.41it/s, est. speed input: 339.99 toks/s, output: 91.86 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:13<00:14,  1.53it/s, est. speed input: 363.28 toks/s, output: 102.44 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:14<00:10,  1.95it/s, est. speed input: 394.23 toks/s, output: 115.25 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.27it/s, est. speed input: 1146.82 toks/s, output: 413.31 toks/s]
[2025-01-06 18:48:34,410][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:48:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:53,  4.05s/it, est. speed input: 183.93 toks/s, output: 7.16 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:04<00:02,  5.61it/s, est. speed input: 2941.78 toks/s, output: 119.07 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:06<00:01,  4.68it/s, est. speed input: 2851.84 toks/s, output: 143.17 toks/s]Processed prompts:  97%|█████████▋| 28/29 [00:07<00:00,  4.37it/s, est. speed input: 2823.14 toks/s, output: 199.26 toks/s]Processed prompts: 100%|██████████| 29/29 [00:07<00:00,  3.69it/s, est. speed input: 2690.04 toks/s, output: 208.63 toks/s]
[2025-01-06 18:48:42,688][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:48:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it, est. speed input: 160.21 toks/s, output: 45.84 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, est. speed input: 480.60 toks/s, output: 137.51 toks/s]
[2025-01-06 18:48:47,451][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:48:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:13<07:02, 13.64s/it, est. speed input: 69.96 toks/s, output: 13.20 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:09,  6.33s/it, est. speed input: 111.27 toks/s, output: 25.58 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1506.24 toks/s, output: 429.45 toks/s]
[2025-01-06 18:49:02,824][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:49:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:09<04:03,  9.38s/it, est. speed input: 101.75 toks/s, output: 12.91 toks/s]Processed prompts:   7%|▋         | 2/27 [00:09<01:38,  3.96s/it, est. speed input: 200.06 toks/s, output: 25.69 toks/s]Processed prompts:  11%|█         | 3/27 [00:10<01:01,  2.58s/it, est. speed input: 273.28 toks/s, output: 36.95 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:11<00:42,  1.86s/it, est. speed input: 339.69 toks/s, output: 48.43 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:11<00:30,  1.37s/it, est. speed input: 389.39 toks/s, output: 60.63 toks/s]Processed prompts:  22%|██▏       | 6/27 [00:11<00:19,  1.05it/s, est. speed input: 465.02 toks/s, output: 74.22 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:13<00:21,  1.10s/it, est. speed input: 472.63 toks/s, output: 81.49 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  2.04it/s, est. speed input: 1879.89 toks/s, output: 382.99 toks/s]
[2025-01-06 18:49:26,839][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 18:49:34,158][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:49:34,158][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:49:34,559][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:49:49,709][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:49:49,711][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:49:50,065][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:49:50,214][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 18:49:57,882][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:50:17,618][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:50:17,619][root][INFO] - Iteration 86 took 2m 40s. Generation: 68.19%, Training: 31.81%. Estimated time remaining: 2h 25m 1s. Estimated total time for complete run: 5h 41m 32s.
[2025-01-06 18:50:17,967][root][INFO] - Loading VLLM model.
WARNING 01-06 18:50:18 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:50:18 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:50:18 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:50:18 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:50:24 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:50:38 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:50:39 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:50:39 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:51:00 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:51:00,321][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:51:00,571][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:51:00,576][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:51:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<04:01,  7.78s/it, est. speed input: 64.90 toks/s, output: 10.67 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:56,  3.87s/it, est. speed input: 113.23 toks/s, output: 20.74 toks/s]Processed prompts:   9%|▉         | 3/32 [00:10<01:24,  2.90s/it, est. speed input: 141.97 toks/s, output: 29.71 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:11<01:02,  2.23s/it, est. speed input: 170.05 toks/s, output: 39.57 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:12<00:48,  1.79s/it, est. speed input: 195.95 toks/s, output: 49.74 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:32,  1.24s/it, est. speed input: 232.19 toks/s, output: 62.45 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:13<00:23,  1.07it/s, est. speed input: 264.45 toks/s, output: 74.44 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:13<00:18,  1.30it/s, est. speed input: 293.22 toks/s, output: 85.86 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:13<00:13,  1.74it/s, est. speed input: 326.29 toks/s, output: 98.64 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:14<00:11,  1.88it/s, est. speed input: 351.53 toks/s, output: 109.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1124.86 toks/s, output: 415.84 toks/s]
[2025-01-06 18:51:15,432][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:51:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:36,  3.70s/it, est. speed input: 201.45 toks/s, output: 7.30 toks/s]Processed prompts:   7%|▋         | 2/27 [00:03<00:39,  1.59s/it, est. speed input: 391.60 toks/s, output: 14.72 toks/s]Processed prompts:  41%|████      | 11/27 [00:04<00:03,  4.59it/s, est. speed input: 1913.66 toks/s, output: 79.74 toks/s]Processed prompts:  48%|████▊     | 13/27 [00:04<00:02,  4.93it/s, est. speed input: 2121.71 toks/s, output: 93.35 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:05<00:03,  3.27it/s, est. speed input: 1910.62 toks/s, output: 96.57 toks/s]Processed prompts:  63%|██████▎   | 17/27 [00:06<00:02,  3.73it/s, est. speed input: 2055.04 toks/s, output: 120.72 toks/s]Processed prompts:  67%|██████▋   | 18/27 [00:06<00:02,  3.67it/s, est. speed input: 2076.52 toks/s, output: 131.20 toks/s]Processed prompts:  70%|███████   | 19/27 [00:06<00:02,  3.58it/s, est. speed input: 2088.64 toks/s, output: 142.07 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:07<00:02,  2.91it/s, est. speed input: 2020.98 toks/s, output: 149.14 toks/s]Processed prompts:  78%|███████▊  | 21/27 [00:07<00:01,  3.20it/s, est. speed input: 2064.58 toks/s, output: 164.21 toks/s]Processed prompts:  81%|████████▏ | 22/27 [00:07<00:01,  3.60it/s, est. speed input: 2116.45 toks/s, output: 180.27 toks/s]Processed prompts:  85%|████████▌ | 23/27 [00:08<00:01,  2.57it/s, est. speed input: 2025.72 toks/s, output: 186.69 toks/s]Processed prompts:  93%|█████████▎| 25/27 [00:08<00:00,  3.53it/s, est. speed input: 2132.73 toks/s, output: 224.34 toks/s]Processed prompts: 100%|██████████| 27/27 [00:08<00:00,  3.11it/s, est. speed input: 2281.44 toks/s, output: 268.00 toks/s]
[2025-01-06 18:51:24,542][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:51:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.07s/it, est. speed input: 137.75 toks/s, output: 39.41 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 688.70 toks/s, output: 197.05 toks/s]
[2025-01-06 18:51:30,031][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:51:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:10, 10.01s/it, est. speed input: 95.30 toks/s, output: 12.09 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:58,  5.95s/it, est. speed input: 145.49 toks/s, output: 22.34 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:47,  3.69s/it, est. speed input: 184.60 toks/s, output: 34.13 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:09,  2.49s/it, est. speed input: 211.15 toks/s, output: 46.20 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1537.53 toks/s, output: 425.53 toks/s]
[2025-01-06 18:51:45,305][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:51:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/25 [00:07<03:11,  7.98s/it, est. speed input: 93.95 toks/s, output: 13.15 toks/s]Processed prompts:   8%|▊         | 2/25 [00:10<01:51,  4.84s/it, est. speed input: 160.42 toks/s, output: 24.67 toks/s]Processed prompts:  12%|█▏        | 3/25 [00:11<01:05,  2.97s/it, est. speed input: 233.97 toks/s, output: 38.20 toks/s]Processed prompts:  16%|█▌        | 4/25 [00:11<00:39,  1.87s/it, est. speed input: 312.65 toks/s, output: 52.80 toks/s]Processed prompts:  20%|██        | 5/25 [00:12<00:29,  1.47s/it, est. speed input: 371.29 toks/s, output: 65.22 toks/s]Processed prompts:  32%|███▏      | 8/25 [00:12<00:10,  1.55it/s, est. speed input: 587.58 toks/s, output: 109.88 toks/s]Processed prompts: 100%|██████████| 25/25 [00:12<00:00,  1.98it/s, est. speed input: 1805.87 toks/s, output: 378.81 toks/s]
[2025-01-06 18:52:08,639][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:52:16,130][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:52:16,131][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:52:16,515][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:52:31,869][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:52:31,870][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:52:32,239][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:52:32,398][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:52:39,823][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:52:59,496][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:52:59,497][root][INFO] - Iteration 87 took 2m 41s. Generation: 68.49%, Training: 31.51%. Estimated time remaining: 2h 26m 7s. Estimated total time for complete run: 5h 45m 20s.
[2025-01-06 18:52:59,845][root][INFO] - Loading VLLM model.
WARNING 01-06 18:53:00 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:53:00 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:53:00 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:53:00 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:53:05 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:53:19 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:53:19 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:53:19 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:53:41 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 18:53:41,335][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:53:41,585][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:53:41,586][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:53:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<04:51,  9.40s/it, est. speed input: 53.70 toks/s, output: 11.70 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:24,  4.82s/it, est. speed input: 91.63 toks/s, output: 22.41 toks/s]Processed prompts:   9%|▉         | 3/32 [00:12<01:32,  3.18s/it, est. speed input: 123.65 toks/s, output: 33.05 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:13<01:03,  2.27s/it, est. speed input: 154.02 toks/s, output: 44.07 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:29,  1.14s/it, est. speed input: 226.32 toks/s, output: 69.47 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:14<00:26,  1.04s/it, est. speed input: 249.27 toks/s, output: 79.19 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:14<00:20,  1.18it/s, est. speed input: 277.84 toks/s, output: 90.98 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1111.30 toks/s, output: 421.07 toks/s]
[2025-01-06 18:53:56,605][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:53:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:03<01:52,  3.88s/it, est. speed input: 180.91 toks/s, output: 6.19 toks/s]Processed prompts:   7%|▋         | 2/30 [00:03<00:46,  1.67s/it, est. speed input: 355.09 toks/s, output: 12.51 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:04<00:17,  1.49it/s, est. speed input: 699.01 toks/s, output: 25.47 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:04<00:01,  7.56it/s, est. speed input: 2575.16 toks/s, output: 103.72 toks/s]Processed prompts:  60%|██████    | 18/30 [00:04<00:01,  7.85it/s, est. speed input: 2772.83 toks/s, output: 117.14 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:06<00:02,  3.50it/s, est. speed input: 2204.09 toks/s, output: 114.18 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:07<00:02,  3.40it/s, est. speed input: 2211.73 toks/s, output: 138.26 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:07<00:01,  3.66it/s, est. speed input: 2271.73 toks/s, output: 153.23 toks/s]Processed prompts:  87%|████████▋ | 26/30 [00:07<00:00,  4.63it/s, est. speed input: 2454.33 toks/s, output: 199.06 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:08<00:00,  4.19it/s, est. speed input: 2434.41 toks/s, output: 209.90 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:08<00:00,  3.20it/s, est. speed input: 2336.21 toks/s, output: 216.27 toks/s]Processed prompts: 100%|██████████| 30/30 [00:08<00:00,  3.37it/s, est. speed input: 2479.11 toks/s, output: 258.93 toks/s]
[2025-01-06 18:54:05,954][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:54:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it, est. speed input: 175.01 toks/s, output: 50.07 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it, est. speed input: 349.99 toks/s, output: 100.14 toks/s]
[2025-01-06 18:54:10,365][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:54:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.71s/it, est. speed input: 34.68 toks/s, output: 13.60 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1439.90 toks/s, output: 435.18 toks/s]
[2025-01-06 18:54:25,598][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:54:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:06<03:00,  6.96s/it, est. speed input: 107.82 toks/s, output: 11.36 toks/s]Processed prompts:   7%|▋         | 2/27 [00:07<01:20,  3.21s/it, est. speed input: 198.81 toks/s, output: 22.40 toks/s]Processed prompts:  11%|█         | 3/27 [00:08<00:53,  2.22s/it, est. speed input: 262.09 toks/s, output: 32.50 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:09<00:37,  1.62s/it, est. speed input: 344.69 toks/s, output: 43.35 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:09<00:27,  1.27s/it, est. speed input: 418.56 toks/s, output: 54.36 toks/s]Processed prompts:  22%|██▏       | 6/27 [00:10<00:22,  1.05s/it, est. speed input: 464.77 toks/s, output: 65.35 toks/s]Processed prompts:  30%|██▉       | 8/27 [00:11<00:15,  1.20it/s, est. speed input: 579.00 toks/s, output: 86.40 toks/s]Processed prompts:  33%|███▎      | 9/27 [00:12<00:15,  1.15it/s, est. speed input: 594.03 toks/s, output: 95.48 toks/s]Processed prompts: 100%|██████████| 27/27 [00:12<00:00,  2.11it/s, est. speed input: 1869.58 toks/s, output: 376.89 toks/s]
[2025-01-06 18:54:49,083][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:54:56,527][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:54:56,527][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:54:56,904][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:55:12,612][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:55:12,614][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:55:12,994][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:55:13,145][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 18:55:20,643][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:55:39,747][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:55:39,748][root][INFO] - Iteration 88 took 2m 40s. Generation: 68.29%, Training: 31.71%. Estimated time remaining: 2h 19m 58s. Estimated total time for complete run: 5h 41m 52s.
[2025-01-06 18:55:40,110][root][INFO] - Loading VLLM model.
WARNING 01-06 18:55:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:55:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:55:40 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:55:40 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 18:55:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:55:59 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:56:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:56:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:56:21 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:56:21,953][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:56:22,224][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:56:22,225][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:56:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:07<04:01,  7.80s/it, est. speed input: 64.77 toks/s, output: 11.93 toks/s]Processed prompts:   6%|▋         | 2/32 [00:08<01:54,  3.81s/it, est. speed input: 114.57 toks/s, output: 23.03 toks/s]Processed prompts:   9%|▉         | 3/32 [00:09<01:15,  2.60s/it, est. speed input: 151.76 toks/s, output: 33.36 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:10<00:48,  1.73s/it, est. speed input: 194.51 toks/s, output: 45.26 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:11<00:43,  1.59s/it, est. speed input: 215.30 toks/s, output: 53.80 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:38,  1.49s/it, est. speed input: 232.61 toks/s, output: 62.64 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:13<00:31,  1.26s/it, est. speed input: 255.80 toks/s, output: 73.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s, est. speed input: 1169.31 toks/s, output: 435.31 toks/s]
[2025-01-06 18:56:36,523][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:56:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:39,  3.82s/it, est. speed input: 194.82 toks/s, output: 7.58 toks/s]Processed prompts:  44%|████▍     | 12/27 [00:04<00:03,  3.81it/s, est. speed input: 2055.12 toks/s, output: 85.90 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:04<00:02,  4.62it/s, est. speed input: 2426.35 toks/s, output: 109.11 toks/s]Processed prompts:  63%|██████▎   | 17/27 [00:05<00:02,  4.24it/s, est. speed input: 2422.18 toks/s, output: 121.44 toks/s]Processed prompts:  70%|███████   | 19/27 [00:05<00:02,  3.67it/s, est. speed input: 2347.58 toks/s, output: 132.66 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:06<00:01,  3.58it/s, est. speed input: 2345.40 toks/s, output: 142.79 toks/s]Processed prompts:  78%|███████▊  | 21/27 [00:06<00:01,  3.92it/s, est. speed input: 2414.43 toks/s, output: 157.31 toks/s]Processed prompts:  81%|████████▏ | 22/27 [00:06<00:01,  3.06it/s, est. speed input: 2303.07 toks/s, output: 162.70 toks/s]Processed prompts:  85%|████████▌ | 23/27 [00:07<00:01,  2.95it/s, est. speed input: 2283.58 toks/s, output: 175.06 toks/s]Processed prompts:  89%|████████▉ | 24/27 [00:08<00:01,  2.23it/s, est. speed input: 2149.77 toks/s, output: 181.16 toks/s]Processed prompts:  93%|█████████▎| 25/27 [00:08<00:00,  2.62it/s, est. speed input: 2192.40 toks/s, output: 201.16 toks/s]Processed prompts: 100%|██████████| 27/27 [00:08<00:00,  3.23it/s, est. speed input: 2370.32 toks/s, output: 248.96 toks/s]
[2025-01-06 18:56:45,345][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:56:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.09s/it, est. speed input: 137.23 toks/s, output: 39.26 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it, est. speed input: 686.11 toks/s, output: 196.31 toks/s]
[2025-01-06 18:56:50,855][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:56:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:18, 12.20s/it, est. speed input: 78.22 toks/s, output: 12.79 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:57,  5.92s/it, est. speed input: 139.08 toks/s, output: 24.57 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:48,  3.73s/it, est. speed input: 175.63 toks/s, output: 36.18 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1529.07 toks/s, output: 426.90 toks/s]
[2025-01-06 18:57:06,203][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:57:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/24 [00:07<02:44,  7.16s/it, est. speed input: 104.69 toks/s, output: 12.98 toks/s]Processed prompts:   8%|▊         | 2/24 [00:09<01:32,  4.19s/it, est. speed input: 161.71 toks/s, output: 24.69 toks/s]Processed prompts:  12%|█▎        | 3/24 [00:09<00:52,  2.52s/it, est. speed input: 250.31 toks/s, output: 38.35 toks/s]Processed prompts:  17%|█▋        | 4/24 [00:10<00:32,  1.62s/it, est. speed input: 319.22 toks/s, output: 52.61 toks/s]Processed prompts:  21%|██        | 5/24 [00:11<00:29,  1.54s/it, est. speed input: 345.95 toks/s, output: 62.21 toks/s]Processed prompts:  25%|██▌       | 6/24 [00:11<00:19,  1.06s/it, est. speed input: 424.51 toks/s, output: 77.58 toks/s]Processed prompts:  29%|██▉       | 7/24 [00:11<00:13,  1.30it/s, est. speed input: 499.70 toks/s, output: 92.66 toks/s]Processed prompts:  38%|███▊      | 9/24 [00:11<00:06,  2.30it/s, est. speed input: 653.56 toks/s, output: 123.81 toks/s]Processed prompts:  42%|████▏     | 10/24 [00:12<00:05,  2.70it/s, est. speed input: 722.54 toks/s, output: 138.40 toks/s]Processed prompts: 100%|██████████| 24/24 [00:12<00:00,  1.98it/s, est. speed input: 1823.01 toks/s, output: 369.18 toks/s]
[2025-01-06 18:57:29,196][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 18:57:36,572][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 18:57:36,572][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 18:57:36,953][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 18:57:52,303][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 18:57:52,304][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 18:57:52,726][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 18:57:52,869][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 18:58:00,284][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 18:58:19,696][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 18:58:19,697][root][INFO] - Iteration 89 took 2m 39s. Generation: 68.30%, Training: 31.70%. Estimated time remaining: 2h 16m 40s. Estimated total time for complete run: 5h 41m 13s.
[2025-01-06 18:58:20,017][root][INFO] - Loading VLLM model.
WARNING 01-06 18:58:20 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 18:58:20 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 18:58:20 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 18:58:20 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 18:58:25 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 18:58:39 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 18:58:40 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 18:58:40 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 18:59:01 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 18:59:01,698][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 18:59:01,978][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 18:59:01,979][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:59:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<06:08, 11.89s/it, est. speed input: 42.46 toks/s, output: 12.69 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:50,  5.68s/it, est. speed input: 76.40 toks/s, output: 24.51 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:49,  3.78s/it, est. speed input: 102.77 toks/s, output: 35.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1091.99 toks/s, output: 427.20 toks/s]
[2025-01-06 18:59:17,251][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 18:59:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:03<01:44,  3.73s/it, est. speed input: 199.79 toks/s, output: 6.17 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:46,  1.73s/it, est. speed input: 359.53 toks/s, output: 12.79 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:04<00:08,  2.68it/s, est. speed input: 1191.67 toks/s, output: 46.86 toks/s]Processed prompts:  31%|███       | 9/29 [00:04<00:05,  3.34it/s, est. speed input: 1443.86 toks/s, output: 60.36 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:05<00:05,  3.11it/s, est. speed input: 1522.93 toks/s, output: 70.55 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:05<00:05,  3.10it/s, est. speed input: 1605.56 toks/s, output: 84.74 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:06<00:04,  3.06it/s, est. speed input: 1636.09 toks/s, output: 93.10 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:06<00:05,  2.58it/s, est. speed input: 1597.03 toks/s, output: 99.02 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:07<00:04,  2.94it/s, est. speed input: 1661.70 toks/s, output: 111.14 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:08<00:07,  1.57it/s, est. speed input: 1449.38 toks/s, output: 108.48 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:09<00:06,  1.77it/s, est. speed input: 1473.58 toks/s, output: 121.92 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:09<00:04,  2.15it/s, est. speed input: 1524.13 toks/s, output: 137.51 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:09<00:03,  2.71it/s, est. speed input: 1584.35 toks/s, output: 154.04 toks/s]Processed prompts:  72%|███████▏  | 21/29 [00:09<00:03,  2.40it/s, est. speed input: 1573.29 toks/s, output: 164.85 toks/s]Processed prompts:  76%|███████▌  | 22/29 [00:10<00:02,  3.05it/s, est. speed input: 1629.17 toks/s, output: 182.44 toks/s]Processed prompts:  79%|███████▉  | 23/29 [00:10<00:01,  3.59it/s, est. speed input: 1677.28 toks/s, output: 199.30 toks/s]Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.85it/s, est. speed input: 2112.94 toks/s, output: 317.40 toks/s]
[2025-01-06 18:59:27,849][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:59:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it, est. speed input: 160.22 toks/s, output: 45.84 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, est. speed input: 480.62 toks/s, output: 137.52 toks/s]
[2025-01-06 18:59:32,647][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:59:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:36, 14.74s/it, est. speed input: 34.61 toks/s, output: 13.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1441.57 toks/s, output: 434.31 toks/s]
[2025-01-06 18:59:47,879][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 18:59:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:10<04:27, 10.28s/it, est. speed input: 72.86 toks/s, output: 13.62 toks/s]Processed prompts:   7%|▋         | 2/27 [00:11<01:58,  4.74s/it, est. speed input: 152.90 toks/s, output: 26.58 toks/s]Processed prompts:  11%|█         | 3/27 [00:11<01:10,  2.93s/it, est. speed input: 205.81 toks/s, output: 39.18 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:12<00:42,  1.83s/it, est. speed input: 282.23 toks/s, output: 53.10 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:12<00:28,  1.30s/it, est. speed input: 351.25 toks/s, output: 66.21 toks/s]Processed prompts:  22%|██▏       | 6/27 [00:12<00:20,  1.05it/s, est. speed input: 418.37 toks/s, output: 79.42 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:13<00:16,  1.19it/s, est. speed input: 455.71 toks/s, output: 90.84 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  2.03it/s, est. speed input: 1781.88 toks/s, output: 391.37 toks/s]
[2025-01-06 19:00:11,902][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:00:19,325][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:00:19,325][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:00:19,707][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:00:35,754][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:00:35,755][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:00:36,150][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:00:36,309][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 19:00:43,847][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:01:03,417][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:01:03,418][root][INFO] - Iteration 90 took 2m 43s. Generation: 68.42%, Training: 31.58%. Estimated time remaining: 2h 21m 59s. Estimated total time for complete run: 5h 49m 16s.
[2025-01-06 19:01:03,788][root][INFO] - Loading VLLM model.
WARNING 01-06 19:01:04 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:01:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:01:04 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:01:04 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 19:01:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:01:23 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:01:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:01:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:01:45 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:01:45,333][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:01:45,587][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:01:45,588][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:01:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:49, 11.26s/it, est. speed input: 44.85 toks/s, output: 12.43 toks/s]Processed prompts:   6%|▋         | 2/32 [00:12<02:36,  5.20s/it, est. speed input: 82.63 toks/s, output: 24.22 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:39,  3.42s/it, est. speed input: 112.14 toks/s, output: 35.08 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:13<00:41,  1.53s/it, est. speed input: 184.56 toks/s, output: 60.96 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:14<00:36,  1.39s/it, est. speed input: 206.00 toks/s, output: 70.30 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1098.64 toks/s, output: 423.82 toks/s]
[2025-01-06 19:02:00,763][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:02:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:09,  4.31s/it, est. speed input: 172.92 toks/s, output: 6.73 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:04<00:13,  1.82it/s, est. speed input: 998.27 toks/s, output: 40.08 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:05<00:09,  2.30it/s, est. speed input: 1252.80 toks/s, output: 57.12 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:05<00:08,  2.48it/s, est. speed input: 1363.19 toks/s, output: 70.54 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:06<00:06,  2.76it/s, est. speed input: 1476.51 toks/s, output: 86.21 toks/s]Processed prompts:  48%|████▊     | 15/31 [00:06<00:05,  2.99it/s, est. speed input: 1577.24 toks/s, output: 102.97 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:07<00:04,  3.23it/s, est. speed input: 1638.96 toks/s, output: 113.05 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:07<00:04,  3.35it/s, est. speed input: 1681.76 toks/s, output: 122.47 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:07<00:04,  3.21it/s, est. speed input: 1700.70 toks/s, output: 130.88 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:07<00:03,  3.61it/s, est. speed input: 1755.16 toks/s, output: 142.42 toks/s]Processed prompts:  65%|██████▍   | 20/31 [00:08<00:02,  3.77it/s, est. speed input: 1796.63 toks/s, output: 153.19 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:08<00:02,  4.23it/s, est. speed input: 1852.19 toks/s, output: 165.42 toks/s]Processed prompts:  71%|███████   | 22/31 [00:09<00:03,  2.67it/s, est. speed input: 1783.50 toks/s, output: 168.58 toks/s]Processed prompts:  74%|███████▍  | 23/31 [00:09<00:02,  3.23it/s, est. speed input: 1836.80 toks/s, output: 182.90 toks/s]Processed prompts:  77%|███████▋  | 24/31 [00:09<00:02,  3.11it/s, est. speed input: 1847.21 toks/s, output: 193.84 toks/s]Processed prompts:  81%|████████  | 25/31 [00:09<00:02,  2.86it/s, est. speed input: 1844.46 toks/s, output: 204.24 toks/s]Processed prompts:  84%|████████▍ | 26/31 [00:10<00:01,  3.29it/s, est. speed input: 1882.25 toks/s, output: 219.27 toks/s]Processed prompts:  87%|████████▋ | 27/31 [00:10<00:01,  3.93it/s, est. speed input: 1928.06 toks/s, output: 235.63 toks/s]Processed prompts: 100%|██████████| 31/31 [00:10<00:00,  3.00it/s, est. speed input: 2209.00 toks/s, output: 312.41 toks/s]
[2025-01-06 19:02:11,552][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:02:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.90 toks/s, output: 55.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.90 toks/s, output: 55.77 toks/s]
[2025-01-06 19:02:15,552][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:02:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:14<07:11, 14.38s/it, est. speed input: 35.45 toks/s, output: 13.90 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.15it/s, est. speed input: 1414.30 toks/s, output: 430.98 toks/s]
[2025-01-06 19:02:30,424][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:02:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:07<03:47,  7.84s/it, est. speed input: 95.62 toks/s, output: 10.45 toks/s]Processed prompts:   7%|▋         | 2/30 [00:11<02:24,  5.15s/it, est. speed input: 135.00 toks/s, output: 19.80 toks/s]Processed prompts:  10%|█         | 3/30 [00:11<01:19,  2.96s/it, est. speed input: 214.27 toks/s, output: 31.78 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:11<00:48,  1.85s/it, est. speed input: 293.35 toks/s, output: 43.98 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:12<00:37,  1.51s/it, est. speed input: 348.14 toks/s, output: 53.87 toks/s]Processed prompts:  20%|██        | 6/30 [00:12<00:24,  1.03s/it, est. speed input: 404.63 toks/s, output: 66.57 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:13<00:16,  1.37it/s, est. speed input: 521.08 toks/s, output: 88.34 toks/s]Processed prompts:  30%|███       | 9/30 [00:13<00:13,  1.53it/s, est. speed input: 573.50 toks/s, output: 99.41 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:14<00:10,  1.95it/s, est. speed input: 635.72 toks/s, output: 112.30 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:14<00:08,  2.31it/s, est. speed input: 692.56 toks/s, output: 124.55 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.10it/s, est. speed input: 1901.61 toks/s, output: 390.88 toks/s]
[2025-01-06 19:02:55,741][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:03:03,206][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:03:03,207][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:03:03,591][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:03:19,646][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:03:19,648][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:03:20,108][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:03:20,296][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 19:03:27,731][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:03:47,024][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:03:47,025][root][INFO] - Iteration 91 took 2m 43s. Generation: 68.53%, Training: 31.47%. Estimated time remaining: 2h 19m 0s. Estimated total time for complete run: 5h 49m 1s.
[2025-01-06 19:03:47,390][root][INFO] - Loading VLLM model.
WARNING 01-06 19:03:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:03:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:03:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:03:48 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:03:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:04:06 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:04:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:04:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:04:28 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:04:28,890][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:04:29,158][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:04:29,160][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:04:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<04:51,  9.42s/it, est. speed input: 53.62 toks/s, output: 11.79 toks/s]Processed prompts:   6%|▋         | 2/32 [00:10<02:19,  4.65s/it, est. speed input: 94.07 toks/s, output: 22.73 toks/s]Processed prompts:   9%|▉         | 3/32 [00:11<01:29,  3.09s/it, est. speed input: 126.61 toks/s, output: 33.26 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:12<00:58,  2.10s/it, est. speed input: 161.08 toks/s, output: 44.81 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:14<00:51,  1.91s/it, est. speed input: 179.00 toks/s, output: 53.45 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:14<00:36,  1.40s/it, est. speed input: 208.40 toks/s, output: 65.62 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1111.42 toks/s, output: 423.25 toks/s]
[2025-01-06 19:04:44,170][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:04:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:03<01:44,  3.89s/it, est. speed input: 191.74 toks/s, output: 7.21 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:04<00:07,  2.68it/s, est. speed input: 1438.65 toks/s, output: 57.76 toks/s]Processed prompts:  39%|███▉      | 11/28 [00:04<00:04,  3.57it/s, est. speed input: 1833.82 toks/s, output: 77.66 toks/s]Processed prompts:  50%|█████     | 14/28 [00:04<00:03,  4.49it/s, est. speed input: 2169.86 toks/s, output: 98.98 toks/s]Processed prompts:  57%|█████▋    | 16/28 [00:05<00:03,  3.60it/s, est. speed input: 2087.35 toks/s, output: 106.99 toks/s]Processed prompts:  64%|██████▍   | 18/28 [00:06<00:03,  3.30it/s, est. speed input: 2076.12 toks/s, output: 124.13 toks/s]Processed prompts:  68%|██████▊   | 19/28 [00:06<00:02,  3.59it/s, est. speed input: 2141.21 toks/s, output: 136.99 toks/s]Processed prompts:  75%|███████▌  | 21/28 [00:06<00:01,  4.11it/s, est. speed input: 2256.44 toks/s, output: 162.46 toks/s]Processed prompts:  79%|███████▊  | 22/28 [00:06<00:01,  4.39it/s, est. speed input: 2312.46 toks/s, output: 176.00 toks/s]Processed prompts:  82%|████████▏ | 23/28 [00:07<00:01,  4.24it/s, est. speed input: 2330.21 toks/s, output: 187.59 toks/s]Processed prompts:  89%|████████▉ | 25/28 [00:08<00:00,  3.27it/s, est. speed input: 2269.90 toks/s, output: 205.45 toks/s]Processed prompts:  93%|█████████▎| 26/28 [00:08<00:00,  3.58it/s, est. speed input: 2310.39 toks/s, output: 222.58 toks/s]Processed prompts:  96%|█████████▋| 27/28 [00:08<00:00,  3.29it/s, est. speed input: 2291.60 toks/s, output: 235.71 toks/s]Processed prompts: 100%|██████████| 28/28 [00:08<00:00,  3.23it/s, est. speed input: 2377.47 toks/s, output: 258.77 toks/s]
[2025-01-06 19:04:53,291][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:04:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it, est. speed input: 146.08 toks/s, output: 41.80 toks/s]Processed prompts: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it, est. speed input: 584.29 toks/s, output: 167.18 toks/s]
[2025-01-06 19:04:58,498][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:04:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:14<06:46, 14.02s/it, est. speed input: 49.85 toks/s, output: 14.26 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.14it/s, est. speed input: 1423.87 toks/s, output: 427.85 toks/s]
[2025-01-06 19:05:13,035][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:05:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:08<03:35,  8.60s/it, est. speed input: 87.19 toks/s, output: 13.14 toks/s]Processed prompts:   8%|▊         | 2/26 [00:10<01:47,  4.49s/it, est. speed input: 146.87 toks/s, output: 25.16 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:10<00:59,  2.58s/it, est. speed input: 233.32 toks/s, output: 38.70 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:11<00:43,  1.97s/it, est. speed input: 277.41 toks/s, output: 50.04 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:11<00:27,  1.33s/it, est. speed input: 354.10 toks/s, output: 64.13 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:12<00:20,  1.03s/it, est. speed input: 402.04 toks/s, output: 76.84 toks/s]Processed prompts:  31%|███       | 8/26 [00:12<00:10,  1.76it/s, est. speed input: 550.29 toks/s, output: 106.09 toks/s]Processed prompts:  35%|███▍      | 9/26 [00:12<00:07,  2.15it/s, est. speed input: 618.85 toks/s, output: 120.03 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:12<00:05,  2.69it/s, est. speed input: 688.04 toks/s, output: 134.31 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:12<00:03,  4.12it/s, est. speed input: 828.70 toks/s, output: 163.68 toks/s]Processed prompts: 100%|██████████| 26/26 [00:12<00:00,  2.03it/s, est. speed input: 1805.98 toks/s, output: 381.91 toks/s]
[2025-01-06 19:05:36,707][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:05:44,077][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:05:44,077][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:05:44,456][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:05:59,955][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:05:59,956][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:06:00,297][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:06:00,445][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:06:07,826][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:06:26,889][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:06:26,890][root][INFO] - Iteration 92 took 2m 39s. Generation: 68.52%, Training: 31.48%. Estimated time remaining: 2h 8m 22s. Estimated total time for complete run: 5h 41m 2s.
[2025-01-06 19:06:27,198][root][INFO] - Loading VLLM model.
WARNING 01-06 19:06:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:06:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:06:27 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:06:28 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:06:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:06:46 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:06:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:06:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:07:08 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:07:08,786][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:07:09,032][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:07:09,033][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:07:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<05:04,  9.82s/it, est. speed input: 51.41 toks/s, output: 12.83 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<03:02,  6.09s/it, est. speed input: 75.90 toks/s, output: 23.30 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:13<01:04,  2.31s/it, est. speed input: 150.49 toks/s, output: 50.66 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:13<00:46,  1.72s/it, est. speed input: 182.78 toks/s, output: 63.19 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:13<00:32,  1.24s/it, est. speed input: 216.78 toks/s, output: 76.48 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:14<00:22,  1.10it/s, est. speed input: 250.07 toks/s, output: 89.70 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.26it/s, est. speed input: 1138.96 toks/s, output: 441.77 toks/s]
[2025-01-06 19:07:23,683][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:07:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:01,  4.18s/it, est. speed input: 178.11 toks/s, output: 6.93 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:04<00:06,  3.11it/s, est. speed input: 1681.23 toks/s, output: 67.71 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:04<00:04,  4.02it/s, est. speed input: 2068.52 toks/s, output: 87.11 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:05<00:03,  4.00it/s, est. speed input: 2189.96 toks/s, output: 102.58 toks/s]Processed prompts:  60%|██████    | 18/30 [00:06<00:03,  3.68it/s, est. speed input: 2185.25 toks/s, output: 115.07 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:06<00:02,  3.72it/s, est. speed input: 2235.59 toks/s, output: 134.45 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:06<00:01,  4.23it/s, est. speed input: 2359.10 toks/s, output: 158.20 toks/s]Processed prompts:  77%|███████▋  | 23/30 [00:07<00:01,  4.44it/s, est. speed input: 2410.52 toks/s, output: 170.31 toks/s]Processed prompts:  80%|████████  | 24/30 [00:07<00:01,  3.47it/s, est. speed input: 2327.49 toks/s, output: 175.00 toks/s]Processed prompts:  83%|████████▎ | 25/30 [00:07<00:01,  3.59it/s, est. speed input: 2350.39 toks/s, output: 187.80 toks/s]Processed prompts:  90%|█████████ | 27/30 [00:07<00:00,  5.00it/s, est. speed input: 2497.95 toks/s, output: 221.35 toks/s]Processed prompts:  93%|█████████▎| 28/30 [00:09<00:00,  2.69it/s, est. speed input: 2297.70 toks/s, output: 218.47 toks/s]Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.33it/s, est. speed input: 2462.97 toks/s, output: 262.85 toks/s]
[2025-01-06 19:07:33,155][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:07:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:03<00:03,  4.00s/it, est. speed input: 174.93 toks/s, output: 50.05 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it, est. speed input: 349.84 toks/s, output: 100.10 toks/s]
[2025-01-06 19:07:37,547][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:07:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:38, 14.79s/it, est. speed input: 34.49 toks/s, output: 13.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1470.29 toks/s, output: 432.76 toks/s]
[2025-01-06 19:07:52,860][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:07:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:10<04:39, 10.75s/it, est. speed input: 69.76 toks/s, output: 13.67 toks/s]Processed prompts:   7%|▋         | 2/27 [00:11<02:04,  4.96s/it, est. speed input: 146.10 toks/s, output: 26.66 toks/s]Processed prompts:  11%|█         | 3/27 [00:13<01:24,  3.51s/it, est. speed input: 197.85 toks/s, output: 37.89 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:13<00:34,  1.55s/it, est. speed input: 322.27 toks/s, output: 67.08 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  1.99it/s, est. speed input: 1812.39 toks/s, output: 392.15 toks/s]
[2025-01-06 19:08:17,333][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:08:24,759][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:08:24,759][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:08:25,140][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:08:40,963][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:08:40,964][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:08:41,367][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:08:41,515][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 19:08:49,083][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:09:08,641][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:09:08,642][root][INFO] - Iteration 93 took 2m 41s. Generation: 68.19%, Training: 31.81%. Estimated time remaining: 2h 9m 42s. Estimated total time for complete run: 5h 45m 4s.
[2025-01-06 19:09:08,990][root][INFO] - Loading VLLM model.
WARNING 01-06 19:09:09 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:09:09 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:09:09 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:09:09 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:09:14 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:09:28 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:09:28 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:09:28 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:09:50 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:09:50,500][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:09:50,759][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:09:50,760][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:09:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:14, 10.15s/it, est. speed input: 49.73 toks/s, output: 12.70 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:37,  5.24s/it, est. speed input: 84.47 toks/s, output: 24.09 toks/s]Processed prompts:   9%|▉         | 3/32 [00:12<01:28,  3.04s/it, est. speed input: 122.50 toks/s, output: 36.71 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:13,  2.61s/it, est. speed input: 141.05 toks/s, output: 45.67 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1128.33 toks/s, output: 436.67 toks/s]
[2025-01-06 19:10:05,547][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:10:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:37,  3.73s/it, est. speed input: 199.51 toks/s, output: 7.23 toks/s]Processed prompts:   7%|▋         | 2/27 [00:03<00:40,  1.60s/it, est. speed input: 387.94 toks/s, output: 14.58 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:04<00:11,  1.91it/s, est. speed input: 901.10 toks/s, output: 36.04 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:04<00:07,  2.71it/s, est. speed input: 1181.57 toks/s, output: 51.43 toks/s]Processed prompts:  30%|██▉       | 8/27 [00:04<00:06,  2.89it/s, est. speed input: 1265.91 toks/s, output: 58.54 toks/s]Processed prompts:  33%|███▎      | 9/27 [00:04<00:05,  3.41it/s, est. speed input: 1386.42 toks/s, output: 67.35 toks/s]Processed prompts:  37%|███▋      | 10/27 [00:05<00:06,  2.71it/s, est. speed input: 1373.99 toks/s, output: 71.90 toks/s]Processed prompts:  41%|████      | 11/27 [00:05<00:05,  2.89it/s, est. speed input: 1435.93 toks/s, output: 80.77 toks/s]Processed prompts:  44%|████▍     | 12/27 [00:05<00:04,  3.41it/s, est. speed input: 1525.64 toks/s, output: 91.50 toks/s]Processed prompts:  48%|████▊     | 13/27 [00:06<00:05,  2.74it/s, est. speed input: 1511.37 toks/s, output: 97.74 toks/s]Processed prompts:  52%|█████▏    | 14/27 [00:06<00:03,  3.44it/s, est. speed input: 1601.28 toks/s, output: 110.46 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:07<00:04,  2.60it/s, est. speed input: 1562.76 toks/s, output: 116.60 toks/s]Processed prompts:  63%|██████▎   | 17/27 [00:07<00:02,  3.75it/s, est. speed input: 1709.86 toks/s, output: 144.02 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:08<00:02,  3.39it/s, est. speed input: 1778.19 toks/s, output: 174.96 toks/s]Processed prompts:  81%|████████▏ | 22/27 [00:09<00:01,  3.00it/s, est. speed input: 1780.18 toks/s, output: 196.72 toks/s]Processed prompts:  85%|████████▌ | 23/27 [00:09<00:01,  3.15it/s, est. speed input: 1811.90 toks/s, output: 212.73 toks/s]Processed prompts: 100%|██████████| 27/27 [00:09<00:00,  2.87it/s, est. speed input: 2127.17 toks/s, output: 297.78 toks/s]
[2025-01-06 19:10:15,418][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:10:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.07s/it, est. speed input: 137.87 toks/s, output: 39.45 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it, est. speed input: 689.31 toks/s, output: 197.23 toks/s]
[2025-01-06 19:10:20,908][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:10:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:12<06:41, 12.94s/it, est. speed input: 73.73 toks/s, output: 12.98 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:02,  6.08s/it, est. speed input: 134.20 toks/s, output: 25.11 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:42,  3.55s/it, est. speed input: 194.04 toks/s, output: 37.63 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:01,  2.19s/it, est. speed input: 226.82 toks/s, output: 50.79 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1526.74 toks/s, output: 427.46 toks/s]
[2025-01-06 19:10:36,281][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:10:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/23 [00:09<03:29,  9.53s/it, est. speed input: 100.05 toks/s, output: 15.31 toks/s]Processed prompts:  13%|█▎        | 3/23 [00:11<01:02,  3.14s/it, est. speed input: 252.13 toks/s, output: 42.20 toks/s]Processed prompts:  22%|██▏       | 5/23 [00:12<00:30,  1.70s/it, est. speed input: 397.11 toks/s, output: 72.10 toks/s]Processed prompts: 100%|██████████| 23/23 [00:12<00:00,  1.91it/s, est. speed input: 1758.69 toks/s, output: 371.78 toks/s]
[2025-01-06 19:10:59,227][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:11:06,582][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:11:06,583][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:11:07,053][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:11:22,768][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:11:22,770][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:11:23,133][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:11:23,279][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:11:30,721][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:11:50,050][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:11:50,051][root][INFO] - Iteration 94 took 2m 41s. Generation: 68.43%, Training: 31.57%. Estimated time remaining: 2h 6m 16s. Estimated total time for complete run: 5h 44m 20s.
[2025-01-06 19:11:50,370][root][INFO] - Loading VLLM model.
WARNING 01-06 19:11:50 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:11:50 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:11:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:11:52 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:11:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:12:10 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:12:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:12:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:12:32 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:12:32,944][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:12:33,186][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:12:33,187][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:12:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:28, 10.61s/it, est. speed input: 47.60 toks/s, output: 12.16 toks/s]Processed prompts:   6%|▋         | 2/32 [00:11<02:29,  4.97s/it, est. speed input: 86.84 toks/s, output: 23.64 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:56,  4.01s/it, est. speed input: 104.46 toks/s, output: 32.41 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:11,  2.54s/it, est. speed input: 136.57 toks/s, output: 45.30 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1092.51 toks/s, output: 423.89 toks/s]
[2025-01-06 19:12:48,446][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:12:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:03<01:49,  3.92s/it, est. speed input: 189.83 toks/s, output: 6.62 toks/s]Processed prompts:  10%|█         | 3/29 [00:04<00:28,  1.08s/it, est. speed input: 546.40 toks/s, output: 20.05 toks/s]Processed prompts:  31%|███       | 9/29 [00:04<00:06,  3.09it/s, est. speed input: 1443.29 toks/s, output: 58.65 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:05<00:08,  2.25it/s, est. speed input: 1292.53 toks/s, output: 58.60 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:05<00:06,  2.58it/s, est. speed input: 1391.05 toks/s, output: 68.92 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:05<00:05,  2.99it/s, est. speed input: 1486.43 toks/s, output: 79.35 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:06<00:04,  3.55it/s, est. speed input: 1631.55 toks/s, output: 98.50 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:07<00:05,  2.78it/s, est. speed input: 1584.60 toks/s, output: 103.37 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:07<00:02,  4.08it/s, est. speed input: 1771.25 toks/s, output: 130.13 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:07<00:02,  3.81it/s, est. speed input: 1792.99 toks/s, output: 139.42 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:08<00:03,  2.89it/s, est. speed input: 1746.21 toks/s, output: 144.96 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:08<00:02,  3.20it/s, est. speed input: 1792.43 toks/s, output: 158.12 toks/s]Processed prompts:  72%|███████▏  | 21/29 [00:08<00:02,  3.54it/s, est. speed input: 1838.29 toks/s, output: 171.63 toks/s]Processed prompts:  76%|███████▌  | 22/29 [00:09<00:03,  2.16it/s, est. speed input: 1733.33 toks/s, output: 173.66 toks/s]Processed prompts:  79%|███████▉  | 23/29 [00:09<00:02,  2.12it/s, est. speed input: 1721.52 toks/s, output: 185.12 toks/s]Processed prompts: 100%|██████████| 29/29 [00:09<00:00,  2.92it/s, est. speed input: 2170.75 toks/s, output: 306.10 toks/s]
[2025-01-06 19:12:58,799][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:12:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it, est. speed input: 160.30 toks/s, output: 45.87 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, est. speed input: 480.89 toks/s, output: 137.59 toks/s]
[2025-01-06 19:13:03,570][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:13:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:37, 14.75s/it, est. speed input: 34.59 toks/s, output: 13.56 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1453.41 toks/s, output: 434.02 toks/s]
[2025-01-06 19:13:18,821][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:13:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:07<03:33,  7.90s/it, est. speed input: 94.94 toks/s, output: 11.52 toks/s]Processed prompts:   7%|▋         | 2/28 [00:08<01:32,  3.54s/it, est. speed input: 203.03 toks/s, output: 22.76 toks/s]Processed prompts:  11%|█         | 3/28 [00:11<01:18,  3.12s/it, est. speed input: 241.22 toks/s, output: 30.86 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:12<00:55,  2.30s/it, est. speed input: 299.50 toks/s, output: 42.21 toks/s]Processed prompts:  18%|█▊        | 5/28 [00:12<00:35,  1.56s/it, est. speed input: 370.78 toks/s, output: 55.46 toks/s]Processed prompts:  21%|██▏       | 6/28 [00:12<00:25,  1.15s/it, est. speed input: 435.97 toks/s, output: 68.25 toks/s]Processed prompts:  25%|██▌       | 7/28 [00:13<00:20,  1.01it/s, est. speed input: 485.54 toks/s, output: 79.43 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:13<00:15,  1.33it/s, est. speed input: 547.52 toks/s, output: 92.81 toks/s]Processed prompts: 100%|██████████| 28/28 [00:13<00:00,  2.06it/s, est. speed input: 1831.50 toks/s, output: 387.68 toks/s]
[2025-01-06 19:13:43,612][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 19:13:50,906][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:13:50,906][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:13:51,386][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:14:07,332][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:14:07,333][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:14:07,658][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:14:07,827][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:14:15,187][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:14:34,512][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:14:34,513][root][INFO] - Iteration 95 took 2m 44s. Generation: 68.97%, Training: 31.03%. Estimated time remaining: 2h 10m 3s. Estimated total time for complete run: 5h 50m 51s.
[2025-01-06 19:14:34,849][root][INFO] - Loading VLLM model.
WARNING 01-06 19:14:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:14:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:14:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:14:35 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:14:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:14:54 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:14:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:14:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:15:16 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:15:16,225][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:15:16,467][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:15:16,468][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:15:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:28, 10.60s/it, est. speed input: 47.64 toks/s, output: 12.17 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:58,  5.96s/it, est. speed input: 75.90 toks/s, output: 22.77 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:36,  3.34s/it, est. speed input: 111.88 toks/s, output: 35.52 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:10,  2.52s/it, est. speed input: 136.43 toks/s, output: 45.99 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1091.38 toks/s, output: 424.19 toks/s]
[2025-01-06 19:15:31,750][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:15:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:54,  4.10s/it, est. speed input: 181.80 toks/s, output: 7.08 toks/s]Processed prompts:  10%|█         | 3/29 [00:04<00:28,  1.10s/it, est. speed input: 525.78 toks/s, output: 21.15 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:04<00:16,  1.46it/s, est. speed input: 768.68 toks/s, output: 34.72 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:05<00:07,  2.66it/s, est. speed input: 1165.48 toks/s, output: 59.16 toks/s]Processed prompts:  31%|███       | 9/29 [00:05<00:09,  2.06it/s, est. speed input: 1110.66 toks/s, output: 61.42 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:06<00:07,  2.34it/s, est. speed input: 1225.56 toks/s, output: 78.39 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:06<00:06,  2.74it/s, est. speed input: 1312.80 toks/s, output: 89.64 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:07<00:05,  2.96it/s, est. speed input: 1373.15 toks/s, output: 99.62 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:07<00:04,  3.53it/s, est. speed input: 1455.37 toks/s, output: 111.33 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:08<00:06,  2.02it/s, est. speed input: 1352.05 toks/s, output: 111.69 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:08<00:03,  3.26it/s, est. speed input: 1513.57 toks/s, output: 140.87 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:08<00:02,  3.76it/s, est. speed input: 1578.01 toks/s, output: 154.27 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:08<00:01,  5.10it/s, est. speed input: 1716.87 toks/s, output: 182.57 toks/s]Processed prompts:  72%|███████▏  | 21/29 [00:08<00:01,  4.78it/s, est. speed input: 1751.28 toks/s, output: 193.84 toks/s]Processed prompts:  76%|███████▌  | 22/29 [00:09<00:02,  3.39it/s, est. speed input: 1725.23 toks/s, output: 199.99 toks/s]Processed prompts:  86%|████████▌ | 25/29 [00:10<00:01,  3.71it/s, est. speed input: 1820.27 toks/s, output: 238.34 toks/s]Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s, est. speed input: 2110.89 toks/s, output: 316.73 toks/s]
[2025-01-06 19:15:42,396][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:15:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it, est. speed input: 160.26 toks/s, output: 45.85 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, est. speed input: 480.76 toks/s, output: 137.56 toks/s]
[2025-01-06 19:15:47,192][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:15:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:38, 14.78s/it, est. speed input: 34.50 toks/s, output: 13.53 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1462.63 toks/s, output: 432.95 toks/s]
[2025-01-06 19:16:02,465][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:16:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:13<06:01, 13.38s/it, est. speed input: 71.33 toks/s, output: 13.98 toks/s]Processed prompts:   7%|▋         | 2/28 [00:13<02:28,  5.70s/it, est. speed input: 139.22 toks/s, output: 27.73 toks/s]Processed prompts:  11%|█         | 3/28 [00:14<01:21,  3.27s/it, est. speed input: 188.77 toks/s, output: 41.19 toks/s]Processed prompts: 100%|██████████| 28/28 [00:14<00:00,  1.99it/s, est. speed input: 1779.03 toks/s, output: 396.27 toks/s]
[2025-01-06 19:16:27,700][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:16:35,060][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:16:35,060][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:16:35,436][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:16:51,485][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:16:51,486][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:16:51,969][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:16:52,112][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:16:59,820][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:17:19,604][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:17:19,605][root][INFO] - Iteration 96 took 2m 45s. Generation: 68.47%, Training: 31.53%. Estimated time remaining: 2h 8m 38s. Estimated total time for complete run: 5h 52m 11s.
[2025-01-06 19:17:19,994][root][INFO] - Loading VLLM model.
WARNING 01-06 19:17:20 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:17:20 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:17:20 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:17:20 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:17:25 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:17:39 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:17:39 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:17:39 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:18:01 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:18:01,690][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:18:01,970][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:18:01,971][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:18:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:27, 10.58s/it, est. speed input: 47.73 toks/s, output: 12.19 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:58,  5.95s/it, est. speed input: 76.03 toks/s, output: 22.81 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:53,  3.93s/it, est. speed input: 102.31 toks/s, output: 33.97 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1091.26 toks/s, output: 425.63 toks/s]
[2025-01-06 19:18:17,248][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:18:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:03<01:33,  3.73s/it, est. speed input: 199.81 toks/s, output: 7.78 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:04<00:17,  1.27it/s, est. speed input: 741.01 toks/s, output: 30.34 toks/s]Processed prompts:  19%|█▉        | 5/26 [00:04<00:16,  1.31it/s, est. speed input: 786.69 toks/s, output: 36.32 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:05<00:12,  1.57it/s, est. speed input: 883.79 toks/s, output: 45.28 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:05<00:09,  2.03it/s, est. speed input: 1004.53 toks/s, output: 55.67 toks/s]Processed prompts:  38%|███▊      | 10/26 [00:05<00:04,  3.87it/s, est. speed input: 1379.65 toks/s, output: 88.33 toks/s]Processed prompts:  42%|████▏     | 11/26 [00:05<00:03,  3.84it/s, est. speed input: 1443.51 toks/s, output: 96.87 toks/s]Processed prompts:  46%|████▌     | 12/26 [00:05<00:03,  3.73it/s, est. speed input: 1497.64 toks/s, output: 105.54 toks/s]Processed prompts:  54%|█████▍    | 14/26 [00:06<00:02,  5.50it/s, est. speed input: 1713.62 toks/s, output: 130.65 toks/s]Processed prompts:  62%|██████▏   | 16/26 [00:07<00:02,  3.41it/s, est. speed input: 1686.71 toks/s, output: 141.43 toks/s]Processed prompts:  65%|██████▌   | 17/26 [00:07<00:02,  3.63it/s, est. speed input: 1741.26 toks/s, output: 154.08 toks/s]Processed prompts:  73%|███████▎  | 19/26 [00:07<00:02,  3.29it/s, est. speed input: 1773.05 toks/s, output: 174.55 toks/s]Processed prompts:  77%|███████▋  | 20/26 [00:09<00:03,  1.89it/s, est. speed input: 1590.12 toks/s, output: 170.11 toks/s]Processed prompts: 100%|██████████| 26/26 [00:09<00:00,  2.79it/s, est. speed input: 2067.40 toks/s, output: 298.65 toks/s]
[2025-01-06 19:18:27,016][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:18:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  17%|█▋        | 1/6 [00:05<00:27,  5.44s/it, est. speed input: 128.39 toks/s, output: 36.73 toks/s]Processed prompts: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s, est. speed input: 770.28 toks/s, output: 220.40 toks/s]
[2025-01-06 19:18:32,873][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:18:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<06:08, 11.89s/it, est. speed input: 80.22 toks/s, output: 12.87 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:17,  6.58s/it, est. speed input: 112.09 toks/s, output: 23.94 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1492.21 toks/s, output: 430.77 toks/s]
[2025-01-06 19:18:48,147][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:18:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▍         | 1/26 [00:10<04:29, 10.77s/it, est. speed input: 88.60 toks/s, output: 14.30 toks/s]Processed prompts:   8%|▊         | 2/26 [00:10<01:49,  4.56s/it, est. speed input: 173.84 toks/s, output: 28.43 toks/s]Processed prompts:  12%|█▏        | 3/26 [00:11<00:58,  2.55s/it, est. speed input: 257.09 toks/s, output: 42.50 toks/s]Processed prompts:  15%|█▌        | 4/26 [00:12<00:43,  1.97s/it, est. speed input: 312.48 toks/s, output: 53.73 toks/s]Processed prompts:  23%|██▎       | 6/26 [00:12<00:21,  1.06s/it, est. speed input: 449.91 toks/s, output: 81.21 toks/s]Processed prompts:  27%|██▋       | 7/26 [00:12<00:16,  1.19it/s, est. speed input: 498.29 toks/s, output: 94.92 toks/s]Processed prompts: 100%|██████████| 26/26 [00:12<00:00,  2.00it/s, est. speed input: 1752.13 toks/s, output: 387.42 toks/s]
[2025-01-06 19:19:12,138][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:19:19,492][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:19:19,492][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:19:19,941][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:19:35,569][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:19:35,570][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:19:35,928][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:19:36,084][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:19:43,471][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:20:02,899][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:20:02,900][root][INFO] - Iteration 97 took 2m 43s. Generation: 68.83%, Training: 31.17%. Estimated time remaining: 2h 2m 5s. Estimated total time for complete run: 5h 48m 21s.
[2025-01-06 19:20:03,261][root][INFO] - Loading VLLM model.
WARNING 01-06 19:20:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:20:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:20:03 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:20:04 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:20:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:20:22 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:20:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:20:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:20:45 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:20:45,101][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:20:45,348][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:20:45,350][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:20:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<05:02,  9.75s/it, est. speed input: 51.81 toks/s, output: 12.41 toks/s]Processed prompts:   6%|▋         | 2/32 [00:10<02:08,  4.30s/it, est. speed input: 98.75 toks/s, output: 24.44 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:51,  3.83s/it, est. speed input: 112.16 toks/s, output: 32.20 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:14,  2.66s/it, est. speed input: 140.56 toks/s, output: 44.19 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1124.43 toks/s, output: 433.84 toks/s]
[2025-01-06 19:21:00,152][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:21:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:03<01:51,  3.97s/it, est. speed input: 187.73 toks/s, output: 6.80 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:45,  1.70s/it, est. speed input: 365.05 toks/s, output: 13.72 toks/s]Processed prompts:  31%|███       | 9/29 [00:04<00:06,  3.18it/s, est. speed input: 1425.52 toks/s, output: 58.45 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:04<00:03,  4.52it/s, est. speed input: 1856.04 toks/s, output: 83.74 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:05<00:04,  3.63it/s, est. speed input: 1830.88 toks/s, output: 90.53 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:06<00:03,  3.54it/s, est. speed input: 1891.86 toks/s, output: 106.08 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:06<00:03,  3.50it/s, est. speed input: 1917.14 toks/s, output: 115.32 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:06<00:03,  3.40it/s, est. speed input: 1933.72 toks/s, output: 124.77 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:07<00:03,  2.62it/s, est. speed input: 1848.75 toks/s, output: 129.54 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:07<00:02,  3.11it/s, est. speed input: 1916.70 toks/s, output: 144.37 toks/s]Processed prompts:  79%|███████▉  | 23/29 [00:08<00:01,  4.08it/s, est. speed input: 2072.06 toks/s, output: 185.84 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:08<00:01,  3.00it/s, est. speed input: 1991.34 toks/s, output: 191.05 toks/s]Processed prompts:  86%|████████▌ | 25/29 [00:09<00:01,  2.71it/s, est. speed input: 1964.31 toks/s, output: 202.08 toks/s]Processed prompts: 100%|██████████| 29/29 [00:09<00:00,  3.07it/s, est. speed input: 2278.72 toks/s, output: 286.89 toks/s]
[2025-01-06 19:21:10,061][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:21:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/it, est. speed input: 160.12 toks/s, output: 45.81 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it, est. speed input: 480.34 toks/s, output: 137.43 toks/s]
[2025-01-06 19:21:14,831][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:21:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:41, 14.89s/it, est. speed input: 34.25 toks/s, output: 13.43 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1502.77 toks/s, output: 429.80 toks/s]
[2025-01-06 19:21:30,197][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:21:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:12<05:42, 12.70s/it, est. speed input: 75.10 toks/s, output: 13.62 toks/s]Processed prompts:   7%|▋         | 2/28 [00:13<02:24,  5.55s/it, est. speed input: 143.97 toks/s, output: 26.86 toks/s]Processed prompts:  11%|█         | 3/28 [00:14<01:25,  3.44s/it, est. speed input: 202.03 toks/s, output: 39.25 toks/s]Processed prompts: 100%|██████████| 28/28 [00:14<00:00,  1.98it/s, est. speed input: 1827.84 toks/s, output: 392.18 toks/s]
[2025-01-06 19:21:55,732][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:22:03,065][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:22:03,066][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:22:03,453][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:22:19,423][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:22:19,425][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:22:19,806][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:22:19,988][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:22:27,406][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:22:47,336][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:22:47,337][root][INFO] - Iteration 98 took 2m 44s. Generation: 68.53%, Training: 31.47%. Estimated time remaining: 2h 1m 47s. Estimated total time for complete run: 5h 50m 47s.
[2025-01-06 19:22:47,682][root][INFO] - Loading VLLM model.
WARNING 01-06 19:22:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:22:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:22:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:22:48 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:22:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:23:07 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:23:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:23:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:23:29 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:23:29,520][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:23:29,781][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:23:29,782][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:23:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:28, 10.59s/it, est. speed input: 47.69 toks/s, output: 12.18 toks/s]Processed prompts:   6%|▋         | 2/32 [00:12<02:37,  5.24s/it, est. speed input: 83.54 toks/s, output: 23.41 toks/s]Processed prompts:   9%|▉         | 3/32 [00:12<01:30,  3.12s/it, est. speed input: 119.52 toks/s, output: 35.26 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:15,  2.70s/it, est. speed input: 137.00 toks/s, output: 43.88 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1095.98 toks/s, output: 423.68 toks/s]
[2025-01-06 19:23:44,994][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:23:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:57,  4.19s/it, est. speed input: 177.73 toks/s, output: 6.92 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:48,  1.79s/it, est. speed input: 346.12 toks/s, output: 13.94 toks/s]Processed prompts:  10%|█         | 3/29 [00:04<00:27,  1.05s/it, est. speed input: 500.20 toks/s, output: 21.04 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:04<00:19,  1.28it/s, est. speed input: 615.70 toks/s, output: 27.89 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:04<00:06,  3.21it/s, est. speed input: 1055.59 toks/s, output: 52.83 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:05<00:03,  5.39it/s, est. speed input: 1458.02 toks/s, output: 77.80 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:05<00:04,  4.12it/s, est. speed input: 1529.27 toks/s, output: 89.59 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:06<00:03,  4.46it/s, est. speed input: 1679.28 toks/s, output: 107.40 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:06<00:03,  4.61it/s, est. speed input: 1748.00 toks/s, output: 116.41 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:06<00:02,  4.47it/s, est. speed input: 1794.79 toks/s, output: 124.73 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:07<00:05,  2.09it/s, est. speed input: 1583.24 toks/s, output: 119.09 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:08<00:04,  2.54it/s, est. speed input: 1649.45 toks/s, output: 132.96 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:08<00:03,  3.00it/s, est. speed input: 1708.41 toks/s, output: 146.55 toks/s]Processed prompts:  72%|███████▏  | 21/29 [00:08<00:01,  4.27it/s, est. speed input: 1844.16 toks/s, output: 175.48 toks/s]Processed prompts:  76%|███████▌  | 22/29 [00:08<00:01,  4.13it/s, est. speed input: 1872.61 toks/s, output: 187.25 toks/s]Processed prompts:  79%|███████▉  | 23/29 [00:09<00:01,  3.94it/s, est. speed input: 1895.53 toks/s, output: 199.14 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:09<00:02,  2.43it/s, est. speed input: 1807.35 toks/s, output: 201.81 toks/s]Processed prompts: 100%|██████████| 29/29 [00:09<00:00,  2.91it/s, est. speed input: 2164.69 toks/s, output: 300.38 toks/s]
[2025-01-06 19:23:55,404][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:23:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.36s/it, est. speed input: 160.18 toks/s, output: 45.83 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it, est. speed input: 480.52 toks/s, output: 137.49 toks/s]
[2025-01-06 19:24:00,170][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:24:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:38, 14.78s/it, est. speed input: 34.52 toks/s, output: 13.54 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1463.09 toks/s, output: 433.11 toks/s]
[2025-01-06 19:24:15,462][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:24:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:13<06:18, 13.50s/it, est. speed input: 70.65 toks/s, output: 13.48 toks/s]Processed prompts:   7%|▋         | 2/29 [00:14<02:46,  6.16s/it, est. speed input: 131.25 toks/s, output: 26.29 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  2.00it/s, est. speed input: 1791.68 toks/s, output: 397.94 toks/s]
[2025-01-06 19:24:41,369][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:24:48,753][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:24:48,753][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:24:49,132][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:25:08,462][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:25:08,463][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:25:08,826][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:25:08,967][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:25:16,459][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:25:36,247][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:25:36,248][root][INFO] - Iteration 99 took 2m 48s. Generation: 67.42%, Training: 32.58%. Estimated time remaining: 2h 8m 30s. Estimated total time for complete run: 6h 0m 20s.
[2025-01-06 19:25:36,583][root][INFO] - Loading VLLM model.
WARNING 01-06 19:25:36 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:25:36 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:25:37 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:25:37 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 19:25:42 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:25:56 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:25:56 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:25:56 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:26:18 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:26:18,322][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:26:18,567][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:26:18,568][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:26:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:29, 10.63s/it, est. speed input: 47.52 toks/s, output: 12.14 toks/s]Processed prompts:   6%|▋         | 2/32 [00:12<02:49,  5.65s/it, est. speed input: 78.97 toks/s, output: 22.99 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:33,  3.23s/it, est. speed input: 115.27 toks/s, output: 35.38 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:13<00:56,  2.02s/it, est. speed input: 151.70 toks/s, output: 47.99 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:13<00:37,  1.37s/it, est. speed input: 186.49 toks/s, output: 60.34 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:14<00:32,  1.26s/it, est. speed input: 207.98 toks/s, output: 69.60 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:14<00:22,  1.11it/s, est. speed input: 240.02 toks/s, output: 82.43 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1097.19 toks/s, output: 421.90 toks/s]
[2025-01-06 19:26:33,774][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:26:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:39,  3.84s/it, est. speed input: 194.18 toks/s, output: 7.56 toks/s]Processed prompts:  11%|█         | 3/27 [00:03<00:24,  1.03s/it, est. speed input: 566.97 toks/s, output: 22.83 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:05<00:18,  1.22it/s, est. speed input: 712.81 toks/s, output: 37.39 toks/s]Processed prompts:  22%|██▏       | 6/27 [00:05<00:15,  1.32it/s, est. speed input: 771.46 toks/s, output: 45.49 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:06<00:13,  1.51it/s, est. speed input: 836.53 toks/s, output: 54.90 toks/s]Processed prompts:  37%|███▋      | 10/27 [00:06<00:05,  3.07it/s, est. speed input: 1173.99 toks/s, output: 91.22 toks/s]Processed prompts:  44%|████▍     | 12/27 [00:07<00:05,  2.70it/s, est. speed input: 1231.22 toks/s, output: 106.94 toks/s]Processed prompts:  48%|████▊     | 13/27 [00:07<00:04,  3.06it/s, est. speed input: 1307.84 toks/s, output: 119.34 toks/s]Processed prompts:  52%|█████▏    | 14/27 [00:08<00:05,  2.30it/s, est. speed input: 1268.24 toks/s, output: 123.30 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:08<00:04,  2.69it/s, est. speed input: 1331.62 toks/s, output: 136.96 toks/s]Processed prompts:  59%|█████▉    | 16/27 [00:08<00:03,  2.91it/s, est. speed input: 1377.25 toks/s, output: 149.37 toks/s]Processed prompts:  63%|██████▎   | 17/27 [00:09<00:04,  2.38it/s, est. speed input: 1364.22 toks/s, output: 156.86 toks/s]Processed prompts:  70%|███████   | 19/27 [00:09<00:03,  2.55it/s, est. speed input: 1416.02 toks/s, output: 181.06 toks/s]Processed prompts:  74%|███████▍  | 20/27 [00:10<00:02,  2.66it/s, est. speed input: 1443.78 toks/s, output: 194.79 toks/s]Processed prompts: 100%|██████████| 27/27 [00:10<00:00,  2.63it/s, est. speed input: 1950.79 toks/s, output: 330.93 toks/s]
[2025-01-06 19:26:44,487][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:26:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.08s/it, est. speed input: 137.62 toks/s, output: 39.38 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it, est. speed input: 688.09 toks/s, output: 196.88 toks/s]
[2025-01-06 19:26:49,973][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:26:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:39, 14.81s/it, est. speed input: 34.43 toks/s, output: 13.50 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1481.41 toks/s, output: 432.09 toks/s]
[2025-01-06 19:27:05,306][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:27:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:09<04:05,  9.43s/it, est. speed input: 101.02 toks/s, output: 13.14 toks/s]Processed prompts:   7%|▋         | 2/27 [00:11<02:00,  4.83s/it, est. speed input: 172.72 toks/s, output: 25.18 toks/s]Processed prompts:  11%|█         | 3/27 [00:11<01:11,  2.98s/it, est. speed input: 242.01 toks/s, output: 37.81 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:12<00:43,  1.88s/it, est. speed input: 317.24 toks/s, output: 51.56 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:13<00:37,  1.68s/it, est. speed input: 357.12 toks/s, output: 61.40 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  2.02it/s, est. speed input: 1791.04 toks/s, output: 390.87 toks/s]
[2025-01-06 19:27:30,137][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 19:27:37,584][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:27:37,585][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:27:37,962][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:27:53,961][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:27:53,962][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:27:54,418][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:27:54,573][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 19:28:01,942][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:28:21,445][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:28:21,446][root][INFO] - Iteration 100 took 2m 45s. Generation: 68.85%, Training: 31.15%. Estimated time remaining: 1h 57m 50s. Estimated total time for complete run: 5h 52m 25s.
[2025-01-06 19:28:21,778][root][INFO] - Loading VLLM model.
WARNING 01-06 19:28:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:28:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:28:22 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:28:22 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:28:27 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:28:41 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:28:41 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:28:41 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:29:03 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:29:03,311][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:29:03,626][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:29:03,627][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:29:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:13<07:12, 13.94s/it, est. speed input: 36.24 toks/s, output: 13.27 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:08,  6.27s/it, est. speed input: 68.06 toks/s, output: 25.95 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1088.97 toks/s, output: 430.26 toks/s]
[2025-01-06 19:29:18,898][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:29:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:03<01:47,  3.98s/it, est. speed input: 187.20 toks/s, output: 7.29 toks/s]Processed prompts:  11%|█         | 3/28 [00:04<00:28,  1.15s/it, est. speed input: 519.92 toks/s, output: 21.63 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:04<00:19,  1.21it/s, est. speed input: 661.45 toks/s, output: 29.30 toks/s]Processed prompts:  18%|█▊        | 5/28 [00:04<00:13,  1.71it/s, est. speed input: 805.08 toks/s, output: 37.55 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:05<00:06,  2.93it/s, est. speed input: 1161.73 toks/s, output: 60.39 toks/s]Processed prompts:  32%|███▏      | 9/28 [00:05<00:06,  2.90it/s, est. speed input: 1222.23 toks/s, output: 67.42 toks/s]Processed prompts:  36%|███▌      | 10/28 [00:05<00:05,  3.28it/s, est. speed input: 1316.06 toks/s, output: 76.65 toks/s]Processed prompts:  39%|███▉      | 11/28 [00:06<00:09,  1.80it/s, est. speed input: 1177.57 toks/s, output: 76.02 toks/s]Processed prompts:  43%|████▎     | 12/28 [00:07<00:07,  2.27it/s, est. speed input: 1262.51 toks/s, output: 88.56 toks/s]Processed prompts:  46%|████▋     | 13/28 [00:07<00:06,  2.48it/s, est. speed input: 1311.15 toks/s, output: 99.26 toks/s]Processed prompts:  57%|█████▋    | 16/28 [00:08<00:04,  2.85it/s, est. speed input: 1435.42 toks/s, output: 130.12 toks/s]Processed prompts:  61%|██████    | 17/28 [00:09<00:05,  1.84it/s, est. speed input: 1320.64 toks/s, output: 130.63 toks/s]Processed prompts:  68%|██████▊   | 19/28 [00:09<00:03,  2.52it/s, est. speed input: 1431.16 toks/s, output: 162.68 toks/s]Processed prompts:  71%|███████▏  | 20/28 [00:10<00:03,  2.37it/s, est. speed input: 1432.44 toks/s, output: 173.93 toks/s]Processed prompts: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, est. speed input: 2005.96 toks/s, output: 327.93 toks/s]
[2025-01-06 19:29:29,776][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:29:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:04<00:14,  4.78s/it, est. speed input: 146.32 toks/s, output: 41.86 toks/s]Processed prompts: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it, est. speed input: 585.23 toks/s, output: 167.45 toks/s]
[2025-01-06 19:29:34,970][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:29:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:37, 14.74s/it, est. speed input: 34.59 toks/s, output: 13.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1456.82 toks/s, output: 434.10 toks/s]
[2025-01-06 19:29:50,235][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:29:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:11<05:10, 11.50s/it, est. speed input: 82.99 toks/s, output: 13.40 toks/s]Processed prompts:   7%|▋         | 2/28 [00:13<02:34,  5.93s/it, est. speed input: 125.91 toks/s, output: 25.51 toks/s]Processed prompts:  11%|█         | 3/28 [00:14<01:26,  3.44s/it, est. speed input: 189.59 toks/s, output: 38.90 toks/s]Processed prompts: 100%|██████████| 28/28 [00:14<00:00,  2.00it/s, est. speed input: 1761.46 toks/s, output: 395.79 toks/s]
[2025-01-06 19:30:15,822][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:30:23,170][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:30:23,171][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:30:23,582][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:30:39,809][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:30:39,811][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:30:40,215][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:30:40,361][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:30:47,851][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:31:07,480][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:31:07,482][root][INFO] - Iteration 101 took 2m 46s. Generation: 68.80%, Training: 31.20%. Estimated time remaining: 1h 56m 51s. Estimated total time for complete run: 5h 54m 12s.
[2025-01-06 19:31:07,819][root][INFO] - Loading VLLM model.
WARNING 01-06 19:31:08 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:31:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:31:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:31:08 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:31:13 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:31:27 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:31:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:31:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:31:49 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:31:49,266][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:31:49,512][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:31:49,513][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:31:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<05:06,  9.88s/it, est. speed input: 51.09 toks/s, output: 11.94 toks/s]Processed prompts:   6%|▋         | 2/32 [00:12<02:54,  5.80s/it, est. speed input: 78.74 toks/s, output: 22.22 toks/s]Processed prompts:   9%|▉         | 3/32 [00:13<01:36,  3.34s/it, est. speed input: 114.45 toks/s, output: 34.67 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:13<00:59,  2.11s/it, est. speed input: 149.99 toks/s, output: 47.30 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:14<00:48,  1.79s/it, est. speed input: 171.78 toks/s, output: 56.94 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1099.35 toks/s, output: 424.30 toks/s]
[2025-01-06 19:32:04,692][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:32:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/28 [00:03<01:47,  3.98s/it, est. speed input: 187.26 toks/s, output: 7.29 toks/s]Processed prompts:   7%|▋         | 2/28 [00:04<00:44,  1.70s/it, est. speed input: 364.52 toks/s, output: 14.68 toks/s]Processed prompts:  11%|█         | 3/28 [00:04<00:29,  1.19s/it, est. speed input: 478.40 toks/s, output: 21.83 toks/s]Processed prompts:  14%|█▍        | 4/28 [00:04<00:19,  1.25it/s, est. speed input: 610.86 toks/s, output: 30.34 toks/s]Processed prompts:  18%|█▊        | 5/28 [00:05<00:13,  1.76it/s, est. speed input: 733.75 toks/s, output: 39.16 toks/s]Processed prompts:  21%|██▏       | 6/28 [00:05<00:10,  2.18it/s, est. speed input: 840.88 toks/s, output: 47.58 toks/s]Processed prompts:  25%|██▌       | 7/28 [00:05<00:09,  2.31it/s, est. speed input: 916.04 toks/s, output: 55.34 toks/s]Processed prompts:  29%|██▊       | 8/28 [00:05<00:07,  2.61it/s, est. speed input: 998.88 toks/s, output: 64.22 toks/s]Processed prompts:  32%|███▏      | 9/28 [00:06<00:06,  2.88it/s, est. speed input: 1075.98 toks/s, output: 73.39 toks/s]Processed prompts:  36%|███▌      | 10/28 [00:06<00:05,  3.26it/s, est. speed input: 1155.59 toks/s, output: 83.21 toks/s]Processed prompts:  39%|███▉      | 11/28 [00:06<00:04,  3.45it/s, est. speed input: 1223.74 toks/s, output: 92.82 toks/s]Processed prompts:  43%|████▎     | 12/28 [00:07<00:05,  3.07it/s, est. speed input: 1258.46 toks/s, output: 100.89 toks/s]Processed prompts:  46%|████▋     | 13/28 [00:07<00:03,  3.81it/s, est. speed input: 1338.63 toks/s, output: 112.90 toks/s]Processed prompts:  50%|█████     | 14/28 [00:07<00:04,  3.29it/s, est. speed input: 1365.89 toks/s, output: 121.27 toks/s]Processed prompts:  54%|█████▎    | 15/28 [00:08<00:06,  1.91it/s, est. speed input: 1289.18 toks/s, output: 122.80 toks/s]Processed prompts:  57%|█████▋    | 16/28 [00:09<00:06,  1.79it/s, est. speed input: 1279.80 toks/s, output: 131.18 toks/s]Processed prompts:  61%|██████    | 17/28 [00:09<00:05,  2.00it/s, est. speed input: 1308.93 toks/s, output: 143.68 toks/s]Processed prompts:  64%|██████▍   | 18/28 [00:09<00:04,  2.30it/s, est. speed input: 1346.72 toks/s, output: 157.43 toks/s]Processed prompts:  68%|██████▊   | 19/28 [00:10<00:04,  1.96it/s, est. speed input: 1330.20 toks/s, output: 166.16 toks/s]Processed prompts: 100%|██████████| 28/28 [00:10<00:00,  2.64it/s, est. speed input: 1962.68 toks/s, output: 335.98 toks/s]
[2025-01-06 19:32:15,726][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:32:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:04<00:14,  4.78s/it, est. speed input: 146.31 toks/s, output: 41.86 toks/s]Processed prompts: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it, est. speed input: 585.21 toks/s, output: 167.44 toks/s]
[2025-01-06 19:32:20,933][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:32:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.70s/it, est. speed input: 34.70 toks/s, output: 13.61 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1436.74 toks/s, output: 435.38 toks/s]
[2025-01-06 19:32:36,160][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:32:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:13<05:51, 13.53s/it, est. speed input: 70.43 toks/s, output: 14.78 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  2.00it/s, est. speed input: 1722.36 toks/s, output: 399.04 toks/s]
[2025-01-06 19:33:01,636][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:33:09,019][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:33:09,019][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:33:09,405][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:33:25,646][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:33:25,647][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:33:25,995][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:33:26,141][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:33:33,563][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:33:53,001][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:33:53,002][root][INFO] - Iteration 102 took 2m 45s. Generation: 68.75%, Training: 31.25%. Estimated time remaining: 1h 52m 59s. Estimated total time for complete run: 5h 53m 6s.
[2025-01-06 19:33:53,393][root][INFO] - Loading VLLM model.
WARNING 01-06 19:33:53 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:33:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:33:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:33:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:33:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:34:12 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:34:13 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:34:13 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:34:34 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:34:34,946][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:34:35,190][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:34:35,191][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:34:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:29, 10.61s/it, est. speed input: 47.58 toks/s, output: 13.10 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<03:09,  6.32s/it, est. speed input: 72.54 toks/s, output: 23.92 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:41,  3.49s/it, est. speed input: 107.89 toks/s, output: 37.67 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:14<01:01,  2.20s/it, est. speed input: 141.52 toks/s, output: 51.07 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s, est. speed input: 1132.12 toks/s, output: 443.39 toks/s]
[2025-01-06 19:34:49,928][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:34:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<01:58,  4.09s/it, est. speed input: 182.27 toks/s, output: 6.61 toks/s]Processed prompts:  10%|█         | 3/30 [00:04<00:30,  1.14s/it, est. speed input: 517.94 toks/s, output: 19.71 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:04<00:24,  1.05it/s, est. speed input: 593.90 toks/s, output: 25.86 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:05<00:09,  2.35it/s, est. speed input: 1007.17 toks/s, output: 50.25 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:05<00:08,  2.46it/s, est. speed input: 1079.76 toks/s, output: 56.82 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:05<00:05,  3.62it/s, est. speed input: 1318.85 toks/s, output: 75.05 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:05<00:05,  3.30it/s, est. speed input: 1354.92 toks/s, output: 80.87 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:06<00:04,  3.74it/s, est. speed input: 1498.24 toks/s, output: 97.53 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:06<00:04,  3.46it/s, est. speed input: 1526.86 toks/s, output: 104.63 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:04,  3.41it/s, est. speed input: 1566.06 toks/s, output: 113.09 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:07<00:03,  3.77it/s, est. speed input: 1629.04 toks/s, output: 123.58 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:08<00:05,  2.49it/s, est. speed input: 1563.34 toks/s, output: 126.35 toks/s]Processed prompts:  60%|██████    | 18/30 [00:09<00:07,  1.65it/s, est. speed input: 1448.39 toks/s, output: 127.13 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:09<00:06,  1.76it/s, est. speed input: 1455.39 toks/s, output: 138.31 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:10<00:07,  1.42it/s, est. speed input: 1383.96 toks/s, output: 143.59 toks/s]Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.80it/s, est. speed input: 2079.42 toks/s, output: 330.29 toks/s]
[2025-01-06 19:35:01,096][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:35:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.00s/it, est. speed input: 174.66 toks/s, output: 49.97 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it, est. speed input: 349.30 toks/s, output: 99.94 toks/s]
[2025-01-06 19:35:05,504][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:35:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.69s/it, est. speed input: 47.58 toks/s, output: 13.61 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1428.52 toks/s, output: 435.61 toks/s]
[2025-01-06 19:35:20,711][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:35:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:13<06:42, 13.87s/it, est. speed input: 68.79 toks/s, output: 13.27 toks/s]Processed prompts:   7%|▋         | 2/30 [00:14<02:55,  6.26s/it, est. speed input: 128.85 toks/s, output: 25.93 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.03it/s, est. speed input: 1792.86 toks/s, output: 404.09 toks/s]
[2025-01-06 19:35:47,354][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:35:54,714][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:35:54,715][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:35:55,132][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:36:11,500][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:36:11,501][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:36:11,848][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:36:12,005][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:36:19,467][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:36:38,981][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:36:38,982][root][INFO] - Iteration 103 took 2m 45s. Generation: 68.76%, Training: 31.24%. Estimated time remaining: 1h 51m 12s. Estimated total time for complete run: 5h 54m 5s.
[2025-01-06 19:36:39,348][root][INFO] - Loading VLLM model.
WARNING 01-06 19:36:39 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:36:39 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:36:40 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:36:40 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 19:36:44 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:36:58 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:36:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:36:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:37:21 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:37:21,119][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:37:21,462][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:37:21,463][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:37:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:28, 10.61s/it, est. speed input: 47.60 toks/s, output: 12.35 toks/s]Processed prompts:   6%|▋         | 2/32 [00:13<02:58,  5.96s/it, est. speed input: 75.86 toks/s, output: 23.06 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:52,  3.88s/it, est. speed input: 102.92 toks/s, output: 34.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1097.78 toks/s, output: 428.45 toks/s]
[2025-01-06 19:37:36,645][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:37:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:07,  4.25s/it, est. speed input: 175.01 toks/s, output: 6.59 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:05<00:16,  1.53it/s, est. speed input: 879.52 toks/s, output: 37.19 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:06<00:18,  1.33it/s, est. speed input: 838.27 toks/s, output: 40.74 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:06<00:17,  1.33it/s, est. speed input: 854.51 toks/s, output: 47.74 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:07<00:12,  1.67it/s, est. speed input: 969.91 toks/s, output: 66.25 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:08<00:12,  1.64it/s, est. speed input: 984.29 toks/s, output: 74.15 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:08<00:09,  2.02it/s, est. speed input: 1057.40 toks/s, output: 86.20 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:09<00:13,  1.32it/s, est. speed input: 970.30 toks/s, output: 87.70 toks/s] Processed prompts:  45%|████▌     | 14/31 [00:10<00:10,  1.64it/s, est. speed input: 1023.80 toks/s, output: 100.81 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:10<00:05,  2.62it/s, est. speed input: 1152.68 toks/s, output: 129.20 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:10<00:03,  3.61it/s, est. speed input: 1270.99 toks/s, output: 156.75 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:11<00:04,  2.93it/s, est. speed input: 1271.86 toks/s, output: 164.64 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:11<00:03,  2.91it/s, est. speed input: 1323.28 toks/s, output: 187.28 toks/s]Processed prompts: 100%|██████████| 31/31 [00:11<00:00,  2.63it/s, est. speed input: 1956.27 toks/s, output: 357.21 toks/s]
[2025-01-06 19:37:48,891][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:37:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.92 toks/s, output: 55.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.92 toks/s, output: 55.77 toks/s]
[2025-01-06 19:37:52,900][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:37:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:33, 14.63s/it, est. speed input: 34.85 toks/s, output: 13.67 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1403.92 toks/s, output: 437.36 toks/s]
[2025-01-06 19:38:08,011][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:38:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:12<06:02, 12.95s/it, est. speed input: 73.69 toks/s, output: 13.44 toks/s]Processed prompts:   7%|▋         | 2/29 [00:14<02:47,  6.20s/it, est. speed input: 118.12 toks/s, output: 25.93 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  2.01it/s, est. speed input: 1759.98 toks/s, output: 400.23 toks/s]
[2025-01-06 19:38:34,297][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:38:41,709][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:38:41,709][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:38:42,095][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:38:58,752][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:38:58,754][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:38:59,141][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:38:59,289][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:39:06,609][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:39:25,918][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:39:25,919][root][INFO] - Iteration 104 took 2m 46s. Generation: 68.99%, Training: 31.01%. Estimated time remaining: 1h 50m 28s. Estimated total time for complete run: 5h 56m 7s.
[2025-01-06 19:39:26,297][root][INFO] - Loading VLLM model.
WARNING 01-06 19:39:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:39:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:39:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:39:27 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:39:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:39:45 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:39:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:39:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:40:07 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:40:07,726][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:40:07,973][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:40:07,974][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:40:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:24, 10.46s/it, est. speed input: 48.29 toks/s, output: 12.14 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:26,  6.89s/it, est. speed input: 68.03 toks/s, output: 22.03 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1088.50 toks/s, output: 426.17 toks/s]
[2025-01-06 19:40:23,290][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:40:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:17,  4.44s/it, est. speed input: 167.66 toks/s, output: 6.53 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:57,  1.90s/it, est. speed input: 326.36 toks/s, output: 13.14 toks/s]Processed prompts:   9%|▉         | 3/32 [00:04<00:32,  1.11s/it, est. speed input: 471.00 toks/s, output: 19.82 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:22,  1.27it/s, est. speed input: 591.91 toks/s, output: 26.43 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:11,  2.33it/s, est. speed input: 850.78 toks/s, output: 41.12 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:05<00:08,  2.81it/s, est. speed input: 949.42 toks/s, output: 48.39 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:05<00:07,  3.19it/s, est. speed input: 1047.09 toks/s, output: 55.51 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:19,  1.20it/s, est. speed input: 855.45 toks/s, output: 52.24 toks/s] Processed prompts:  31%|███▏      | 10/32 [00:08<00:16,  1.34it/s, est. speed input: 890.03 toks/s, output: 61.30 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:15,  1.39it/s, est. speed input: 907.00 toks/s, output: 69.82 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:11,  1.60it/s, est. speed input: 963.31 toks/s, output: 88.44 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:10<00:10,  1.76it/s, est. speed input: 998.80 toks/s, output: 99.48 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:08,  1.97it/s, est. speed input: 1037.11 toks/s, output: 111.04 toks/s]Processed prompts:  50%|█████     | 16/32 [00:11<00:07,  2.18it/s, est. speed input: 1073.87 toks/s, output: 122.72 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:11<00:06,  2.40it/s, est. speed input: 1110.54 toks/s, output: 134.67 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:11<00:03,  3.38it/s, est. speed input: 1211.35 toks/s, output: 161.91 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:12<00:03,  3.02it/s, est. speed input: 1228.67 toks/s, output: 172.06 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:12<00:03,  3.37it/s, est. speed input: 1269.57 toks/s, output: 185.60 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s, est. speed input: 1938.21 toks/s, output: 365.15 toks/s]
[2025-01-06 19:40:35,994][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:40:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:32, 14.59s/it, est. speed input: 34.96 toks/s, output: 13.71 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1403.49 toks/s, output: 438.63 toks/s]
[2025-01-06 19:40:51,079][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:40:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:38, 15.28s/it, est. speed input: 62.45 toks/s, output: 13.09 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.03it/s, est. speed input: 1802.19 toks/s, output: 405.86 toks/s]
[2025-01-06 19:41:18,264][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:41:26,060][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:41:26,060][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:41:26,474][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:41:43,291][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:41:43,293][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:41:43,683][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:41:43,828][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 19:41:51,347][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:42:10,882][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:42:10,883][root][INFO] - Iteration 105 took 2m 44s. Generation: 68.02%, Training: 31.98%. Estimated time remaining: 1h 43m 30s. Estimated total time for complete run: 5h 51m 55s.
[2025-01-06 19:42:11,209][root][INFO] - Loading VLLM model.
WARNING 01-06 19:42:11 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:42:11 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:42:11 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:42:11 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:42:16 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:42:30 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:42:31 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:42:31 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:42:52 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:42:52,858][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:42:53,117][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:42:53,118][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:42:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:59, 11.59s/it, est. speed input: 43.59 toks/s, output: 13.29 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:07,  6.26s/it, est. speed input: 71.58 toks/s, output: 24.80 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:41,  3.51s/it, est. speed input: 105.60 toks/s, output: 38.34 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1126.38 toks/s, output: 442.61 toks/s]
[2025-01-06 19:43:07,959][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:43:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<01:58,  4.09s/it, est. speed input: 182.27 toks/s, output: 6.61 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:49,  1.75s/it, est. speed input: 354.46 toks/s, output: 13.32 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:04<00:22,  1.14it/s, est. speed input: 606.84 toks/s, output: 25.86 toks/s]Processed prompts:  20%|██        | 6/30 [00:05<00:14,  1.65it/s, est. speed input: 808.18 toks/s, output: 40.33 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:15,  1.52it/s, est. speed input: 823.07 toks/s, output: 46.25 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:06<00:12,  1.70it/s, est. speed input: 884.05 toks/s, output: 55.17 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:13,  1.51it/s, est. speed input: 882.42 toks/s, output: 61.60 toks/s]Processed prompts:  40%|████      | 12/30 [00:08<00:06,  2.61it/s, est. speed input: 1112.43 toks/s, output: 95.86 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:08<00:05,  2.95it/s, est. speed input: 1180.58 toks/s, output: 107.32 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:08<00:06,  2.44it/s, est. speed input: 1177.61 toks/s, output: 113.64 toks/s]Processed prompts:  50%|█████     | 15/30 [00:09<00:07,  2.09it/s, est. speed input: 1170.85 toks/s, output: 120.56 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:09<00:05,  2.49it/s, est. speed input: 1225.45 toks/s, output: 133.62 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:10<00:05,  2.19it/s, est. speed input: 1225.59 toks/s, output: 141.86 toks/s]Processed prompts:  60%|██████    | 18/30 [00:10<00:04,  2.47it/s, est. speed input: 1264.27 toks/s, output: 154.63 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:11<00:05,  2.18it/s, est. speed input: 1263.70 toks/s, output: 163.58 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:11<00:03,  2.50it/s, est. speed input: 1301.00 toks/s, output: 177.46 toks/s]Processed prompts: 100%|██████████| 30/30 [00:11<00:00,  2.62it/s, est. speed input: 1952.14 toks/s, output: 352.30 toks/s]
[2025-01-06 19:43:19,886][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:43:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:03<00:03,  4.00s/it, est. speed input: 174.89 toks/s, output: 50.04 toks/s]Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it, est. speed input: 349.77 toks/s, output: 100.08 toks/s]
[2025-01-06 19:43:24,291][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:43:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:34, 14.67s/it, est. speed input: 34.75 toks/s, output: 13.63 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1417.29 toks/s, output: 436.11 toks/s]
[2025-01-06 19:43:39,452][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:43:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:11<05:24, 11.59s/it, est. speed input: 82.28 toks/s, output: 13.02 toks/s]Processed prompts:   7%|▋         | 2/29 [00:14<02:53,  6.41s/it, est. speed input: 132.66 toks/s, output: 24.40 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, est. speed input: 1767.13 toks/s, output: 399.82 toks/s]
[2025-01-06 19:44:05,814][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 19:44:13,150][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:44:13,151][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:44:13,540][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:44:30,081][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:44:30,082][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:44:30,447][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:44:30,587][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:44:38,112][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:44:57,555][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:44:57,556][root][INFO] - Iteration 106 took 2m 46s. Generation: 68.86%, Training: 31.14%. Estimated time remaining: 1h 44m 22s. Estimated total time for complete run: 5h 55m 34s.
[2025-01-06 19:44:57,882][root][INFO] - Loading VLLM model.
WARNING 01-06 19:44:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:44:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:44:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:44:58 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:45:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:45:17 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:45:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:45:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:45:39 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:45:39,720][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:45:39,982][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:45:39,983][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:45:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<05:08,  9.94s/it, est. speed input: 50.79 toks/s, output: 12.77 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:17,  6.57s/it, est. speed input: 71.36 toks/s, output: 22.89 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:45,  3.65s/it, est. speed input: 105.72 toks/s, output: 36.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1127.63 toks/s, output: 441.28 toks/s]
[2025-01-06 19:45:54,834][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:45:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:10,  4.35s/it, est. speed input: 171.40 toks/s, output: 6.67 toks/s]Processed prompts:   6%|▋         | 2/31 [00:05<01:10,  2.42s/it, est. speed input: 274.62 toks/s, output: 14.04 toks/s]Processed prompts:  10%|▉         | 3/31 [00:05<00:44,  1.58s/it, est. speed input: 360.00 toks/s, output: 22.20 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:07<00:29,  1.12s/it, est. speed input: 485.14 toks/s, output: 36.71 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:08<00:24,  1.01it/s, est. speed input: 535.04 toks/s, output: 45.56 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:08<00:18,  1.28it/s, est. speed input: 606.69 toks/s, output: 56.33 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:09<00:12,  1.75it/s, est. speed input: 727.89 toks/s, output: 76.56 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:09<00:09,  2.15it/s, est. speed input: 797.07 toks/s, output: 88.24 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:09<00:09,  2.10it/s, est. speed input: 830.53 toks/s, output: 96.97 toks/s]Processed prompts:  45%|████▌     | 14/31 [00:10<00:04,  3.59it/s, est. speed input: 1028.11 toks/s, output: 133.97 toks/s]Processed prompts:  48%|████▊     | 15/31 [00:10<00:05,  2.90it/s, est. speed input: 1038.86 toks/s, output: 140.52 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:11<00:05,  2.52it/s, est. speed input: 1051.99 toks/s, output: 148.19 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:11<00:05,  2.69it/s, est. speed input: 1089.74 toks/s, output: 159.58 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:11<00:04,  3.19it/s, est. speed input: 1140.17 toks/s, output: 172.95 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:12<00:04,  2.95it/s, est. speed input: 1163.22 toks/s, output: 182.90 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:12<00:02,  3.94it/s, est. speed input: 1256.25 toks/s, output: 210.36 toks/s]Processed prompts: 100%|██████████| 31/31 [00:12<00:00,  2.51it/s, est. speed input: 1856.97 toks/s, output: 371.97 toks/s]
[2025-01-06 19:46:07,647][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:46:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.23 toks/s, output: 55.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.23 toks/s, output: 55.57 toks/s]
[2025-01-06 19:46:11,648][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:46:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:32, 14.60s/it, est. speed input: 34.93 toks/s, output: 13.70 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1381.00 toks/s, output: 438.28 toks/s]
[2025-01-06 19:46:26,743][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:46:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:36, 15.22s/it, est. speed input: 62.68 toks/s, output: 13.14 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.04it/s, est. speed input: 1767.50 toks/s, output: 407.30 toks/s]
[2025-01-06 19:46:53,985][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:47:01,497][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:47:01,498][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:47:01,908][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:47:18,714][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:47:18,716][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:47:19,101][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:47:19,241][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-06 19:47:26,992][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:47:46,461][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:47:46,462][root][INFO] - Iteration 107 took 2m 48s. Generation: 68.84%, Training: 31.16%. Estimated time remaining: 1h 46m 19s. Estimated total time for complete run: 6h 0m 19s.
[2025-01-06 19:47:46,770][root][INFO] - Loading VLLM model.
WARNING 01-06 19:47:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:47:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:47:47 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:47:47 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 19:47:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:48:06 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:48:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:48:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:48:28 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:48:28,080][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:48:28,416][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:48:28,418][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:48:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:48, 11.25s/it, est. speed input: 44.91 toks/s, output: 12.63 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:20,  6.68s/it, est. speed input: 68.54 toks/s, output: 23.21 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1096.54 toks/s, output: 430.34 toks/s]
[2025-01-06 19:48:43,627][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:48:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:52,  4.01s/it, est. speed input: 186.01 toks/s, output: 6.74 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:46,  1.72s/it, est. speed input: 361.79 toks/s, output: 13.60 toks/s]Processed prompts:  14%|█▍        | 4/29 [00:04<00:20,  1.21it/s, est. speed input: 633.64 toks/s, output: 26.58 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:05<00:16,  1.47it/s, est. speed input: 735.53 toks/s, output: 33.96 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:05<00:09,  2.21it/s, est. speed input: 955.78 toks/s, output: 50.23 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:05<00:08,  2.51it/s, est. speed input: 1046.64 toks/s, output: 58.66 toks/s]Processed prompts:  31%|███       | 9/29 [00:05<00:06,  2.92it/s, est. speed input: 1140.47 toks/s, output: 67.71 toks/s]Processed prompts:  34%|███▍      | 10/29 [00:06<00:07,  2.46it/s, est. speed input: 1153.60 toks/s, output: 73.56 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:06<00:06,  2.93it/s, est. speed input: 1235.64 toks/s, output: 83.84 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:06<00:04,  3.57it/s, est. speed input: 1322.84 toks/s, output: 94.71 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:07<00:04,  3.41it/s, est. speed input: 1366.97 toks/s, output: 103.33 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:07<00:03,  3.96it/s, est. speed input: 1440.88 toks/s, output: 114.40 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:07<00:04,  2.85it/s, est. speed input: 1428.30 toks/s, output: 120.15 toks/s]Processed prompts:  55%|█████▌    | 16/29 [00:07<00:03,  3.59it/s, est. speed input: 1502.94 toks/s, output: 133.03 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:08<00:02,  4.42it/s, est. speed input: 1576.59 toks/s, output: 146.03 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:08<00:01,  6.00it/s, est. speed input: 1720.63 toks/s, output: 172.38 toks/s]Processed prompts:  69%|██████▉   | 20/29 [00:08<00:02,  3.17it/s, est. speed input: 1656.09 toks/s, output: 174.29 toks/s]Processed prompts:  79%|███████▉  | 23/29 [00:09<00:01,  4.42it/s, est. speed input: 1818.42 toks/s, output: 216.41 toks/s]Processed prompts:  83%|████████▎ | 24/29 [00:09<00:01,  4.64it/s, est. speed input: 1863.69 toks/s, output: 230.59 toks/s]Processed prompts:  86%|████████▌ | 25/29 [00:10<00:01,  3.30it/s, est. speed input: 1824.20 toks/s, output: 236.27 toks/s]Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.84it/s, est. speed input: 2116.05 toks/s, output: 314.62 toks/s]
[2025-01-06 19:48:54,315][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:48:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.38s/it, est. speed input: 159.67 toks/s, output: 45.68 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it, est. speed input: 478.95 toks/s, output: 137.04 toks/s]
[2025-01-06 19:48:59,117][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:48:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:39, 14.84s/it, est. speed input: 34.38 toks/s, output: 13.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1482.75 toks/s, output: 431.36 toks/s]
[2025-01-06 19:49:14,425][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:49:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:14<06:48, 14.60s/it, est. speed input: 65.33 toks/s, output: 13.70 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  1.99it/s, est. speed input: 1810.65 toks/s, output: 397.18 toks/s]
[2025-01-06 19:49:41,034][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 19:49:48,679][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:49:48,679][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:49:49,062][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:50:05,144][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:50:05,145][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:50:05,496][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:50:05,647][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:50:13,039][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:50:33,085][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:50:33,086][root][INFO] - Iteration 108 took 2m 46s. Generation: 68.68%, Training: 31.32%. Estimated time remaining: 1h 38m 41s. Estimated total time for complete run: 5h 55m 27s.
[2025-01-06 19:50:33,428][root][INFO] - Loading VLLM model.
WARNING 01-06 19:50:33 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:50:33 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:50:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:50:34 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:50:38 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:50:52 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:50:53 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:50:53 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:51:14 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:51:15,013][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:51:15,261][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:51:15,262][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:51:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:45, 15.01s/it, est. speed input: 33.64 toks/s, output: 13.32 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1076.39 toks/s, output: 426.29 toks/s]
[2025-01-06 19:51:30,753][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:51:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:14,  4.33s/it, est. speed input: 172.24 toks/s, output: 6.24 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:55,  1.85s/it, est. speed input: 335.02 toks/s, output: 12.59 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:05<00:20,  1.35it/s, est. speed input: 693.60 toks/s, output: 29.72 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:15,  1.68it/s, est. speed input: 808.16 toks/s, output: 37.54 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:16,  1.48it/s, est. speed input: 810.38 toks/s, output: 42.38 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:06<00:12,  1.93it/s, est. speed input: 911.73 toks/s, output: 51.98 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:07<00:09,  2.44it/s, est. speed input: 1053.16 toks/s, output: 68.69 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:07<00:06,  3.28it/s, est. speed input: 1215.74 toks/s, output: 88.40 toks/s]Processed prompts:  41%|████      | 13/32 [00:07<00:05,  3.37it/s, est. speed input: 1270.94 toks/s, output: 97.13 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:07<00:05,  3.46it/s, est. speed input: 1323.41 toks/s, output: 106.10 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:07<00:04,  4.04it/s, est. speed input: 1395.72 toks/s, output: 116.81 toks/s]Processed prompts:  50%|█████     | 16/32 [00:08<00:05,  3.10it/s, est. speed input: 1396.09 toks/s, output: 122.67 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:09<00:05,  2.57it/s, est. speed input: 1412.91 toks/s, output: 136.72 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:09<00:04,  2.95it/s, est. speed input: 1463.98 toks/s, output: 149.01 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:09<00:03,  3.10it/s, est. speed input: 1498.93 toks/s, output: 160.14 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:10<00:03,  3.35it/s, est. speed input: 1538.13 toks/s, output: 172.04 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:10<00:02,  3.55it/s, est. speed input: 1629.56 toks/s, output: 205.54 toks/s]Processed prompts:  78%|███████▊  | 25/32 [00:11<00:02,  3.44it/s, est. speed input: 1648.69 toks/s, output: 217.17 toks/s]Processed prompts: 100%|██████████| 32/32 [00:11<00:00,  2.82it/s, est. speed input: 2100.84 toks/s, output: 339.63 toks/s]
[2025-01-06 19:51:42,552][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:51:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:38, 14.79s/it, est. speed input: 34.49 toks/s, output: 13.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1461.31 toks/s, output: 432.74 toks/s]
[2025-01-06 19:51:57,837][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:51:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<08:09, 15.80s/it, est. speed input: 60.27 toks/s, output: 12.66 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.03it/s, est. speed input: 1878.75 toks/s, output: 405.15 toks/s]
[2025-01-06 19:52:25,866][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 19:52:33,570][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:52:33,570][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:52:33,944][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:52:50,590][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:52:50,592][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:52:50,997][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:52:51,167][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 19:52:58,541][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:53:18,621][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:53:18,622][root][INFO] - Iteration 109 took 2m 45s. Generation: 68.04%, Training: 31.96%. Estimated time remaining: 1h 33m 36s. Estimated total time for complete run: 5h 53m 8s.
[2025-01-06 19:53:19,391][root][INFO] - Loading VLLM model.
WARNING 01-06 19:53:19 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:53:19 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:53:20 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:53:20 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:53:25 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:53:38 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:53:39 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:53:39 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:54:00 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:54:00,774][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:54:01,060][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:54:01,062][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:54:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:13<07:05, 13.73s/it, est. speed input: 36.77 toks/s, output: 13.03 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:11,  6.40s/it, est. speed input: 67.35 toks/s, output: 25.27 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.13it/s, est. speed input: 1077.47 toks/s, output: 425.32 toks/s]
[2025-01-06 19:54:16,521][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:54:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:10,  4.34s/it, est. speed input: 171.39 toks/s, output: 6.68 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:04<00:16,  1.55it/s, est. speed input: 897.39 toks/s, output: 37.36 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:05<00:10,  2.12it/s, est. speed input: 1138.55 toks/s, output: 52.36 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:05<00:09,  2.39it/s, est. speed input: 1234.91 toks/s, output: 59.69 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:06<00:10,  2.07it/s, est. speed input: 1212.76 toks/s, output: 63.34 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:06<00:09,  2.20it/s, est. speed input: 1258.49 toks/s, output: 70.97 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:06<00:07,  2.52it/s, est. speed input: 1327.38 toks/s, output: 80.20 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:07<00:07,  2.46it/s, est. speed input: 1350.88 toks/s, output: 87.62 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:07<00:03,  4.43it/s, est. speed input: 1617.77 toks/s, output: 122.19 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:07<00:03,  4.60it/s, est. speed input: 1674.34 toks/s, output: 132.21 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:07<00:03,  4.03it/s, est. speed input: 1693.36 toks/s, output: 139.93 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:08<00:02,  4.78it/s, est. speed input: 1857.25 toks/s, output: 172.42 toks/s]Processed prompts:  71%|███████   | 22/31 [00:08<00:02,  4.03it/s, est. speed input: 1854.56 toks/s, output: 179.97 toks/s]Processed prompts:  74%|███████▍  | 23/31 [00:09<00:02,  3.50it/s, est. speed input: 1849.97 toks/s, output: 188.25 toks/s]Processed prompts:  77%|███████▋  | 24/31 [00:09<00:02,  3.01it/s, est. speed input: 1834.17 toks/s, output: 196.42 toks/s]Processed prompts:  81%|████████  | 25/31 [00:10<00:02,  2.59it/s, est. speed input: 1808.76 toks/s, output: 204.61 toks/s]Processed prompts:  87%|████████▋ | 27/31 [00:10<00:01,  3.84it/s, est. speed input: 1922.51 toks/s, output: 239.15 toks/s]Processed prompts: 100%|██████████| 31/31 [00:10<00:00,  2.96it/s, est. speed input: 2202.63 toks/s, output: 315.02 toks/s]
[2025-01-06 19:54:27,449][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:54:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 193.94 toks/s, output: 55.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 193.94 toks/s, output: 55.49 toks/s]
[2025-01-06 19:54:31,464][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:54:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:39, 14.83s/it, est. speed input: 34.39 toks/s, output: 13.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1474.27 toks/s, output: 431.49 toks/s]
[2025-01-06 19:54:46,795][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:54:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:43, 15.46s/it, est. speed input: 61.72 toks/s, output: 12.94 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.01it/s, est. speed input: 1860.36 toks/s, output: 401.08 toks/s]
[2025-01-06 19:55:14,556][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:55:21,960][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:55:21,963][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:55:22,359][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:55:38,806][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:55:38,807][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:55:39,177][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:55:39,338][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-06 19:55:46,894][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:56:06,977][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:56:06,978][root][INFO] - Iteration 110 took 2m 48s. Generation: 68.76%, Training: 31.24%. Estimated time remaining: 1h 36m 48s. Estimated total time for complete run: 5h 59m 9s.
[2025-01-06 19:56:07,339][root][INFO] - Loading VLLM model.
WARNING 01-06 19:56:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:56:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:56:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:56:08 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 19:56:13 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:56:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:56:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:56:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:56:49 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 19:56:49,149][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:56:49,433][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:56:49,434][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:56:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:51, 11.35s/it, est. speed input: 44.48 toks/s, output: 12.51 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:15,  6.51s/it, est. speed input: 69.75 toks/s, output: 23.20 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<01:47,  3.70s/it, est. speed input: 102.14 toks/s, output: 36.14 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1089.48 toks/s, output: 427.16 toks/s]
[2025-01-06 19:57:04,709][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:57:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:02,  4.22s/it, est. speed input: 176.37 toks/s, output: 6.87 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:04<00:23,  1.13it/s, est. speed input: 654.64 toks/s, output: 26.81 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:04<00:17,  1.40it/s, est. speed input: 773.08 toks/s, output: 33.63 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:05<00:10,  2.06it/s, est. speed input: 1045.91 toks/s, output: 52.83 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:05<00:07,  2.78it/s, est. speed input: 1256.19 toks/s, output: 71.33 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:06<00:06,  2.72it/s, est. speed input: 1294.37 toks/s, output: 78.19 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:06<00:04,  3.86it/s, est. speed input: 1499.03 toks/s, output: 99.69 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:07<00:06,  2.32it/s, est. speed input: 1378.73 toks/s, output: 98.62 toks/s]Processed prompts:  50%|█████     | 15/30 [00:07<00:05,  2.65it/s, est. speed input: 1440.70 toks/s, output: 109.98 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:08<00:06,  2.20it/s, est. speed input: 1410.48 toks/s, output: 115.85 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:08<00:05,  2.49it/s, est. speed input: 1455.74 toks/s, output: 127.83 toks/s]Processed prompts:  60%|██████    | 18/30 [00:08<00:04,  2.73it/s, est. speed input: 1494.71 toks/s, output: 139.67 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:09<00:03,  3.32it/s, est. speed input: 1554.89 toks/s, output: 153.58 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:09<00:04,  2.28it/s, est. speed input: 1506.96 toks/s, output: 158.60 toks/s]Processed prompts:  70%|███████   | 21/30 [00:10<00:04,  2.23it/s, est. speed input: 1509.68 toks/s, output: 169.26 toks/s]Processed prompts:  73%|███████▎  | 22/30 [00:10<00:03,  2.30it/s, est. speed input: 1523.09 toks/s, output: 181.59 toks/s]Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s, est. speed input: 2076.91 toks/s, output: 330.28 toks/s]
[2025-01-06 19:57:15,968][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:57:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it, est. speed input: 174.28 toks/s, output: 49.87 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it, est. speed input: 348.55 toks/s, output: 99.73 toks/s]
[2025-01-06 19:57:20,382][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:57:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:36, 14.72s/it, est. speed input: 47.47 toks/s, output: 13.58 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1438.14 toks/s, output: 434.63 toks/s]
[2025-01-06 19:57:35,582][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 19:57:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:14<06:46, 14.51s/it, est. speed input: 65.77 toks/s, output: 13.79 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  2.00it/s, est. speed input: 1778.31 toks/s, output: 399.82 toks/s]
[2025-01-06 19:58:02,378][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 19:58:09,834][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 19:58:09,835][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 19:58:10,231][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 19:58:26,686][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 19:58:26,688][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 19:58:27,032][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 19:58:27,209][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 19:58:34,531][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 19:58:54,077][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 19:58:54,078][root][INFO] - Iteration 111 took 2m 47s. Generation: 68.98%, Training: 31.02%. Estimated time remaining: 1h 31m 21s. Estimated total time for complete run: 5h 56m 28s.
[2025-01-06 19:58:54,470][root][INFO] - Loading VLLM model.
WARNING 01-06 19:58:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 19:58:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 19:58:55 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 19:58:55 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 19:59:00 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 19:59:13 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 19:59:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 19:59:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 19:59:35 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 19:59:35,766][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 19:59:36,006][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 19:59:36,007][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:59:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:54, 11.43s/it, est. speed input: 44.17 toks/s, output: 12.42 toks/s]Processed prompts:   9%|▉         | 3/32 [00:14<02:02,  4.22s/it, est. speed input: 102.12 toks/s, output: 32.69 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.16it/s, est. speed input: 1089.27 toks/s, output: 423.64 toks/s]
[2025-01-06 19:59:51,310][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 19:59:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:04<01:54,  4.09s/it, est. speed input: 181.96 toks/s, output: 6.59 toks/s]Processed prompts:   7%|▋         | 2/29 [00:04<00:47,  1.75s/it, est. speed input: 353.90 toks/s, output: 13.31 toks/s]Processed prompts:  10%|█         | 3/29 [00:04<00:31,  1.20s/it, est. speed input: 470.18 toks/s, output: 19.99 toks/s]Processed prompts:  17%|█▋        | 5/29 [00:04<00:13,  1.82it/s, est. speed input: 766.73 toks/s, output: 36.24 toks/s]Processed prompts:  24%|██▍       | 7/29 [00:06<00:12,  1.77it/s, est. speed input: 864.87 toks/s, output: 46.78 toks/s]Processed prompts:  28%|██▊       | 8/29 [00:06<00:09,  2.16it/s, est. speed input: 965.52 toks/s, output: 56.71 toks/s]Processed prompts:  31%|███       | 9/29 [00:06<00:10,  1.83it/s, est. speed input: 961.74 toks/s, output: 62.53 toks/s]Processed prompts:  38%|███▊      | 11/29 [00:07<00:07,  2.45it/s, est. speed input: 1106.54 toks/s, output: 83.33 toks/s]Processed prompts:  41%|████▏     | 12/29 [00:07<00:06,  2.50it/s, est. speed input: 1148.55 toks/s, output: 92.67 toks/s]Processed prompts:  45%|████▍     | 13/29 [00:08<00:06,  2.37it/s, est. speed input: 1170.60 toks/s, output: 101.21 toks/s]Processed prompts:  48%|████▊     | 14/29 [00:08<00:06,  2.43it/s, est. speed input: 1204.98 toks/s, output: 111.31 toks/s]Processed prompts:  52%|█████▏    | 15/29 [00:08<00:04,  2.94it/s, est. speed input: 1269.63 toks/s, output: 124.24 toks/s]Processed prompts:  59%|█████▊    | 17/29 [00:09<00:04,  2.65it/s, est. speed input: 1312.42 toks/s, output: 142.87 toks/s]Processed prompts:  62%|██████▏   | 18/29 [00:10<00:05,  2.17it/s, est. speed input: 1292.20 toks/s, output: 149.92 toks/s]Processed prompts:  66%|██████▌   | 19/29 [00:11<00:05,  1.93it/s, est. speed input: 1278.71 toks/s, output: 158.54 toks/s]Processed prompts: 100%|██████████| 29/29 [00:11<00:00,  2.62it/s, est. speed input: 1946.85 toks/s, output: 338.48 toks/s]
[2025-01-06 20:00:02,838][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:00:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:04<00:08,  4.41s/it, est. speed input: 158.59 toks/s, output: 45.38 toks/s]Processed prompts: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it, est. speed input: 475.75 toks/s, output: 136.12 toks/s]
[2025-01-06 20:00:07,647][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:00:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:36, 14.74s/it, est. speed input: 34.60 toks/s, output: 13.57 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1428.31 toks/s, output: 434.18 toks/s]
[2025-01-06 20:00:22,898][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:00:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:14<06:45, 14.48s/it, est. speed input: 65.87 toks/s, output: 13.81 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  2.00it/s, est. speed input: 1753.14 toks/s, output: 400.43 toks/s]
[2025-01-06 20:00:49,648][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:00:57,058][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:00:57,058][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:00:57,436][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:01:13,755][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:01:13,757][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:01:14,086][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:01:14,227][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:01:21,542][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:01:41,236][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:01:41,237][root][INFO] - Iteration 112 took 2m 47s. Generation: 69.03%, Training: 30.97%. Estimated time remaining: 1h 28m 41s. Estimated total time for complete run: 5h 56m 36s.
[2025-01-06 20:01:41,558][root][INFO] - Loading VLLM model.
WARNING 01-06 20:01:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:01:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:01:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:01:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 20:01:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:02:01 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:02:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:02:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:02:23 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:02:23,580][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:02:23,820][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:02:23,821][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:02:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:11<05:58, 11.55s/it, est. speed input: 43.72 toks/s, output: 12.29 toks/s]Processed prompts:   6%|▋         | 2/32 [00:15<03:24,  6.81s/it, est. speed input: 67.17 toks/s, output: 22.74 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1074.60 toks/s, output: 421.73 toks/s]
[2025-01-06 20:02:39,341][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:02:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:10,  4.34s/it, est. speed input: 171.73 toks/s, output: 6.68 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:05<00:27,  1.00s/it, est. speed input: 593.93 toks/s, output: 25.53 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:05<00:21,  1.20it/s, est. speed input: 689.91 toks/s, output: 32.61 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:05<00:16,  1.53it/s, est. speed input: 796.57 toks/s, output: 40.65 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:05<00:13,  1.80it/s, est. speed input: 880.77 toks/s, output: 48.32 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:06<00:11,  2.02it/s, est. speed input: 950.00 toks/s, output: 55.97 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:07<00:13,  1.68it/s, est. speed input: 944.19 toks/s, output: 60.99 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:07<00:09,  2.17it/s, est. speed input: 1028.40 toks/s, output: 71.52 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:07<00:07,  2.55it/s, est. speed input: 1096.19 toks/s, output: 81.36 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:08<00:10,  1.81it/s, est. speed input: 1062.95 toks/s, output: 85.52 toks/s]Processed prompts:  48%|████▊     | 15/31 [00:08<00:04,  3.45it/s, est. speed input: 1290.68 toks/s, output: 122.46 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:09<00:03,  3.85it/s, est. speed input: 1397.21 toks/s, output: 144.22 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:09<00:04,  2.75it/s, est. speed input: 1362.00 toks/s, output: 148.01 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:09<00:03,  3.27it/s, est. speed input: 1422.88 toks/s, output: 161.87 toks/s]Processed prompts:  65%|██████▍   | 20/31 [00:10<00:03,  2.79it/s, est. speed input: 1422.28 toks/s, output: 169.85 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:11<00:04,  2.43it/s, est. speed input: 1416.97 toks/s, output: 178.09 toks/s]Processed prompts:  71%|███████   | 22/31 [00:11<00:03,  2.47it/s, est. speed input: 1434.24 toks/s, output: 189.57 toks/s]Processed prompts: 100%|██████████| 31/31 [00:11<00:00,  2.71it/s, est. speed input: 2020.85 toks/s, output: 347.11 toks/s]
[2025-01-06 20:02:51,213][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:02:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.13 toks/s, output: 55.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.13 toks/s, output: 55.55 toks/s]
[2025-01-06 20:02:55,223][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:02:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:33, 14.63s/it, est. speed input: 34.86 toks/s, output: 13.67 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1391.17 toks/s, output: 437.43 toks/s]
[2025-01-06 20:03:10,353][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:03:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:37, 15.25s/it, est. speed input: 62.54 toks/s, output: 13.11 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.03it/s, est. speed input: 1778.20 toks/s, output: 406.43 toks/s]
[2025-01-06 20:03:38,150][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:03:45,649][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:03:45,650][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:03:46,042][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:04:02,605][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:04:02,606][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:04:02,970][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:04:03,185][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:04:10,544][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:04:30,117][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:04:30,118][root][INFO] - Iteration 113 took 2m 48s. Generation: 69.14%, Training: 30.86%. Estimated time remaining: 1h 29m 32s. Estimated total time for complete run: 6h 0m 16s.
[2025-01-06 20:04:30,495][root][INFO] - Loading VLLM model.
WARNING 01-06 20:04:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:04:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:04:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:04:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 20:04:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:04:49 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:04:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:04:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:05:12 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:05:12,047][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:05:12,291][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:05:12,292][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:05:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:44, 14.97s/it, est. speed input: 33.73 toks/s, output: 13.36 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, est. speed input: 1079.44 toks/s, output: 427.50 toks/s]
[2025-01-06 20:05:27,733][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:05:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:17,  4.44s/it, est. speed input: 167.62 toks/s, output: 6.52 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:05,  2.19s/it, est. speed input: 295.07 toks/s, output: 13.47 toks/s]Processed prompts:   9%|▉         | 3/32 [00:05<00:39,  1.35s/it, est. speed input: 413.61 toks/s, output: 20.91 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:24,  1.13it/s, est. speed input: 534.09 toks/s, output: 28.86 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:27,  1.01s/it, est. speed input: 546.34 toks/s, output: 33.89 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:07<00:21,  1.20it/s, est. speed input: 611.71 toks/s, output: 42.43 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:12,  2.00it/s, est. speed input: 782.23 toks/s, output: 62.35 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:09,  2.44it/s, est. speed input: 862.78 toks/s, output: 72.46 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:10,  2.19it/s, est. speed input: 891.54 toks/s, output: 79.35 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:08<00:09,  2.23it/s, est. speed input: 932.87 toks/s, output: 87.89 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:04,  3.96it/s, est. speed input: 1152.73 toks/s, output: 122.26 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:09<00:04,  3.97it/s, est. speed input: 1201.46 toks/s, output: 131.97 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:06,  2.63it/s, est. speed input: 1178.13 toks/s, output: 135.25 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:10<00:05,  2.56it/s, est. speed input: 1201.83 toks/s, output: 144.28 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:10<00:03,  3.86it/s, est. speed input: 1325.27 toks/s, output: 171.20 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:10<00:02,  4.48it/s, est. speed input: 1381.78 toks/s, output: 184.32 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:11<00:03,  3.10it/s, est. speed input: 1371.04 toks/s, output: 189.79 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:12<00:04,  2.39it/s, est. speed input: 1354.31 toks/s, output: 195.48 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s, est. speed input: 1969.95 toks/s, output: 360.78 toks/s]
[2025-01-06 20:05:40,273][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:05:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:31, 14.57s/it, est. speed input: 35.00 toks/s, output: 13.73 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1379.42 toks/s, output: 439.22 toks/s]
[2025-01-06 20:05:55,334][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:05:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<08:02, 15.57s/it, est. speed input: 61.28 toks/s, output: 12.85 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s, est. speed input: 1801.35 toks/s, output: 411.08 toks/s]
[2025-01-06 20:06:23,355][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 20:06:30,839][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:06:30,839][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:06:31,247][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:06:48,030][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:06:48,031][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:06:48,411][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:06:48,555][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:06:56,123][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:07:15,513][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:07:15,514][root][INFO] - Iteration 114 took 2m 45s. Generation: 68.36%, Training: 31.64%. Estimated time remaining: 1h 19m 21s. Estimated total time for complete run: 5h 52m 50s.
[2025-01-06 20:07:15,857][root][INFO] - Loading VLLM model.
WARNING 01-06 20:07:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:07:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:07:16 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:07:16 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:07:21 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:07:35 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:07:35 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:07:35 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:07:57 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:07:57,181][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:07:57,456][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:07:57,457][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:07:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:26, 14.41s/it, est. speed input: 35.04 toks/s, output: 13.88 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, est. speed input: 1121.26 toks/s, output: 444.06 toks/s]
[2025-01-06 20:08:12,311][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:08:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:02,  4.23s/it, est. speed input: 176.29 toks/s, output: 6.86 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:58,  2.08s/it, est. speed input: 310.09 toks/s, output: 14.15 toks/s]Processed prompts:  10%|█         | 3/30 [00:05<00:38,  1.44s/it, est. speed input: 407.67 toks/s, output: 21.71 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:07<00:41,  1.60s/it, est. speed input: 406.53 toks/s, output: 27.83 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:07<00:27,  1.08s/it, est. speed input: 497.17 toks/s, output: 38.98 toks/s]Processed prompts:  20%|██        | 6/30 [00:08<00:22,  1.06it/s, est. speed input: 547.55 toks/s, output: 48.15 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:08<00:18,  1.25it/s, est. speed input: 601.66 toks/s, output: 58.16 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:09<00:16,  1.31it/s, est. speed input: 637.43 toks/s, output: 67.28 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:10<00:11,  1.67it/s, est. speed input: 760.27 toks/s, output: 96.31 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:10<00:07,  2.37it/s, est. speed input: 881.31 toks/s, output: 123.50 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:11<00:07,  2.17it/s, est. speed input: 898.86 toks/s, output: 132.21 toks/s]Processed prompts:  50%|█████     | 15/30 [00:12<00:08,  1.80it/s, est. speed input: 894.96 toks/s, output: 138.91 toks/s]Processed prompts: 100%|██████████| 30/30 [00:12<00:00,  2.40it/s, est. speed input: 1790.11 toks/s, output: 379.22 toks/s]
[2025-01-06 20:08:25,288][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:08:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it, est. speed input: 174.34 toks/s, output: 49.88 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it, est. speed input: 348.66 toks/s, output: 99.76 toks/s]
[2025-01-06 20:08:29,705][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:08:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:34, 14.66s/it, est. speed input: 34.79 toks/s, output: 13.64 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1406.01 toks/s, output: 436.61 toks/s]
[2025-01-06 20:08:44,835][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:08:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:14<07:08, 14.78s/it, est. speed input: 50.74 toks/s, output: 13.53 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.03it/s, est. speed input: 1768.38 toks/s, output: 405.93 toks/s]
[2025-01-06 20:09:12,155][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 20:09:19,984][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:09:19,985][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:09:20,364][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:09:37,426][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:09:37,427][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:09:37,832][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:09:38,036][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:09:45,797][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:10:05,556][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:10:05,557][root][INFO] - Iteration 115 took 2m 50s. Generation: 68.51%, Training: 31.49%. Estimated time remaining: 1h 26m 26s. Estimated total time for complete run: 6h 2m 45s.
[2025-01-06 20:10:05,922][root][INFO] - Loading VLLM model.
WARNING 01-06 20:10:06 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:10:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:10:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:10:07 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 20:10:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:10:26 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:10:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:10:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:10:48 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:10:48,073][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:10:48,316][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:10:48,317][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:10:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:45, 15.02s/it, est. speed input: 33.61 toks/s, output: 13.31 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1075.48 toks/s, output: 425.93 toks/s]
[2025-01-06 20:11:03,815][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:11:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:18,  4.45s/it, est. speed input: 167.27 toks/s, output: 6.51 toks/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:13,  2.44s/it, est. speed input: 271.66 toks/s, output: 13.67 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<01:01,  2.11s/it, est. speed input: 310.58 toks/s, output: 20.84 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:40,  1.44s/it, est. speed input: 392.00 toks/s, output: 30.52 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:07<00:25,  1.04it/s, est. speed input: 482.69 toks/s, output: 40.96 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:22,  1.15it/s, est. speed input: 530.61 toks/s, output: 49.04 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:19,  1.25it/s, est. speed input: 603.92 toks/s, output: 64.35 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:11<00:20,  1.10it/s, est. speed input: 605.29 toks/s, output: 70.79 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:11<00:11,  1.75it/s, est. speed input: 724.19 toks/s, output: 96.07 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:12<00:12,  1.61it/s, est. speed input: 738.70 toks/s, output: 103.96 toks/s]Processed prompts:  41%|████      | 13/32 [00:12<00:10,  1.88it/s, est. speed input: 782.90 toks/s, output: 116.01 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:13<00:11,  1.52it/s, est. speed input: 779.82 toks/s, output: 122.26 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s, est. speed input: 1781.10 toks/s, output: 391.42 toks/s]
[2025-01-06 20:11:17,643][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:11:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:26, 14.39s/it, est. speed input: 35.44 toks/s, output: 13.90 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, est. speed input: 1291.51 toks/s, output: 444.68 toks/s]
[2025-01-06 20:11:32,523][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:11:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:56, 15.37s/it, est. speed input: 48.79 toks/s, output: 13.01 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, est. speed input: 1720.30 toks/s, output: 416.33 toks/s]
[2025-01-06 20:12:00,504][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:12:08,184][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:12:08,184][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:12:08,596][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:12:25,840][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:12:25,841][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:12:26,215][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:12:26,358][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:12:33,953][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:12:52,796][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:12:52,797][root][INFO] - Iteration 116 took 2m 47s. Generation: 68.65%, Training: 31.35%. Estimated time remaining: 1h 17m 40s. Estimated total time for complete run: 5h 56m 46s.
[2025-01-06 20:12:53,097][root][INFO] - Loading VLLM model.
WARNING 01-06 20:12:53 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:12:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:12:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:12:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:12:58 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:13:12 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:13:13 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:13:13 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:13:34 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:13:34,588][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:13:34,829][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:13:34,830][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:13:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:45, 15.01s/it, est. speed input: 33.64 toks/s, output: 13.32 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1076.48 toks/s, output: 426.33 toks/s]
[2025-01-06 20:13:50,271][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:13:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:02,  4.22s/it, est. speed input: 176.16 toks/s, output: 6.87 toks/s]Processed prompts:  10%|█         | 3/30 [00:04<00:34,  1.28s/it, est. speed input: 472.27 toks/s, output: 20.29 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:05<00:18,  1.34it/s, est. speed input: 714.71 toks/s, output: 34.93 toks/s]Processed prompts:  20%|██        | 6/30 [00:05<00:17,  1.35it/s, est. speed input: 753.30 toks/s, output: 40.96 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:15,  1.45it/s, est. speed input: 803.77 toks/s, output: 48.56 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:06<00:12,  1.78it/s, est. speed input: 885.34 toks/s, output: 58.24 toks/s]Processed prompts:  30%|███       | 9/30 [00:07<00:12,  1.69it/s, est. speed input: 906.31 toks/s, output: 65.30 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:07<00:09,  2.17it/s, est. speed input: 988.50 toks/s, output: 76.57 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:07<00:07,  2.40it/s, est. speed input: 1044.06 toks/s, output: 86.40 toks/s]Processed prompts:  40%|████      | 12/30 [00:08<00:08,  2.23it/s, est. speed input: 1068.02 toks/s, output: 94.52 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:09<00:10,  1.60it/s, est. speed input: 1028.43 toks/s, output: 98.67 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:09<00:08,  2.00it/s, est. speed input: 1084.00 toks/s, output: 111.44 toks/s]Processed prompts:  50%|█████     | 15/30 [00:11<00:12,  1.24it/s, est. speed input: 1002.23 toks/s, output: 112.57 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:11<00:09,  1.48it/s, est. speed input: 1034.93 toks/s, output: 125.74 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:11<00:07,  1.82it/s, est. speed input: 1076.26 toks/s, output: 140.07 toks/s]Processed prompts: 100%|██████████| 30/30 [00:11<00:00,  2.55it/s, est. speed input: 1899.27 toks/s, output: 361.04 toks/s]
[2025-01-06 20:14:02,515][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:14:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.03s/it, est. speed input: 173.44 toks/s, output: 49.62 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it, est. speed input: 346.86 toks/s, output: 99.24 toks/s]
[2025-01-06 20:14:06,942][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:14:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:34, 14.65s/it, est. speed input: 34.80 toks/s, output: 13.65 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1393.52 toks/s, output: 436.73 toks/s]
[2025-01-06 20:14:22,105][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:14:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:14<07:07, 14.74s/it, est. speed input: 50.87 toks/s, output: 13.57 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.03it/s, est. speed input: 1758.91 toks/s, output: 406.95 toks/s]
[2025-01-06 20:14:49,677][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:14:57,125][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:14:57,126][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:14:57,536][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:15:14,224][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:15:14,225][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:15:14,613][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:15:14,766][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:15:22,206][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:15:41,643][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:15:41,644][root][INFO] - Iteration 117 took 2m 48s. Generation: 69.14%, Training: 30.86%. Estimated time remaining: 1h 18m 17s. Estimated total time for complete run: 6h 0m 12s.
[2025-01-06 20:15:42,014][root][INFO] - Loading VLLM model.
WARNING 01-06 20:15:42 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:15:42 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:15:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:15:42 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 20:15:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:16:01 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:16:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:16:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:16:23 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:16:23,565][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:16:23,813][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:16:23,814][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:16:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:09<04:56,  9.58s/it, est. speed input: 52.73 toks/s, output: 12.43 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:24,  6.81s/it, est. speed input: 69.93 toks/s, output: 22.09 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, est. speed input: 1118.88 toks/s, output: 437.51 toks/s]
[2025-01-06 20:16:38,698][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:16:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:02,  4.22s/it, est. speed input: 176.43 toks/s, output: 6.87 toks/s]Processed prompts:  10%|█         | 3/30 [00:04<00:35,  1.30s/it, est. speed input: 467.01 toks/s, output: 20.27 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:04<00:23,  1.10it/s, est. speed input: 601.92 toks/s, output: 28.09 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:05<00:23,  1.06it/s, est. speed input: 625.01 toks/s, output: 33.57 toks/s]Processed prompts:  20%|██        | 6/30 [00:06<00:17,  1.35it/s, est. speed input: 712.98 toks/s, output: 42.60 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:06<00:15,  1.47it/s, est. speed input: 764.30 toks/s, output: 50.57 toks/s]Processed prompts:  27%|██▋       | 8/30 [00:07<00:14,  1.50it/s, est. speed input: 788.31 toks/s, output: 58.48 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:08<00:11,  1.69it/s, est. speed input: 869.63 toks/s, output: 75.67 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:09<00:11,  1.70it/s, est. speed input: 896.23 toks/s, output: 84.75 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:09<00:08,  2.11it/s, est. speed input: 991.49 toks/s, output: 107.00 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:09<00:06,  2.42it/s, est. speed input: 1046.33 toks/s, output: 119.55 toks/s]Processed prompts:  50%|█████     | 15/30 [00:10<00:05,  2.87it/s, est. speed input: 1104.50 toks/s, output: 132.66 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:10<00:04,  3.38it/s, est. speed input: 1161.69 toks/s, output: 145.86 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:10<00:03,  3.53it/s, est. speed input: 1205.24 toks/s, output: 157.79 toks/s]Processed prompts:  60%|██████    | 18/30 [00:10<00:03,  3.82it/s, est. speed input: 1251.99 toks/s, output: 170.45 toks/s]Processed prompts:  67%|██████▋   | 20/30 [00:11<00:03,  3.12it/s, est. speed input: 1296.12 toks/s, output: 190.21 toks/s]Processed prompts:  70%|███████   | 21/30 [00:11<00:02,  3.59it/s, est. speed input: 1343.79 toks/s, output: 204.78 toks/s]Processed prompts: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s, est. speed input: 1908.75 toks/s, output: 357.59 toks/s]
[2025-01-06 20:16:50,839][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:16:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.00s/it, est. speed input: 174.64 toks/s, output: 49.97 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it, est. speed input: 349.25 toks/s, output: 99.93 toks/s]
[2025-01-06 20:16:55,240][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:16:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:37, 14.74s/it, est. speed input: 34.59 toks/s, output: 13.56 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1449.01 toks/s, output: 434.04 toks/s]
[2025-01-06 20:17:10,487][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:17:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/29 [00:14<06:47, 14.55s/it, est. speed input: 65.57 toks/s, output: 13.75 toks/s]Processed prompts: 100%|██████████| 29/29 [00:14<00:00,  1.99it/s, est. speed input: 1789.09 toks/s, output: 398.63 toks/s]
[2025-01-06 20:17:37,710][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 20:17:45,232][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:17:45,233][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:17:45,641][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:18:02,270][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:18:02,271][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:18:02,589][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:18:02,734][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:18:10,143][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:18:29,874][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:18:29,875][root][INFO] - Iteration 118 took 2m 48s. Generation: 68.90%, Training: 31.10%. Estimated time remaining: 1h 14m 10s. Estimated total time for complete run: 5h 58m 53s.
[2025-01-06 20:18:30,197][root][INFO] - Loading VLLM model.
WARNING 01-06 20:18:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:18:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:18:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:18:31 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:18:35 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:18:49 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:18:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:18:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:19:12 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:19:12,105][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:19:12,359][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:19:12,360][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:19:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:30, 14.53s/it, est. speed input: 34.75 toks/s, output: 13.76 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1111.83 toks/s, output: 440.33 toks/s]
[2025-01-06 20:19:27,334][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:19:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:17,  4.45s/it, est. speed input: 167.53 toks/s, output: 6.52 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:57,  1.90s/it, est. speed input: 326.13 toks/s, output: 13.13 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:55,  1.92s/it, est. speed input: 343.00 toks/s, output: 19.03 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:40,  1.46s/it, est. speed input: 409.95 toks/s, output: 27.65 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:32,  1.22s/it, est. speed input: 462.24 toks/s, output: 36.23 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:23,  1.09it/s, est. speed input: 533.08 toks/s, output: 46.39 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:18,  1.39it/s, est. speed input: 599.10 toks/s, output: 56.52 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:10<00:29,  1.21s/it, est. speed input: 542.95 toks/s, output: 58.21 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:13<00:36,  1.61s/it, est. speed input: 498.45 toks/s, output: 62.07 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:13<00:25,  1.17s/it, est. speed input: 545.88 toks/s, output: 75.84 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s, est. speed input: 1746.29 toks/s, output: 398.22 toks/s]
[2025-01-06 20:19:41,468][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:19:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:24, 14.33s/it, est. speed input: 35.59 toks/s, output: 13.96 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 1257.68 toks/s, output: 446.65 toks/s]
[2025-01-06 20:19:56,272][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:19:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:53, 15.29s/it, est. speed input: 49.06 toks/s, output: 13.08 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, est. speed input: 1689.94 toks/s, output: 418.62 toks/s]
[2025-01-06 20:20:24,274][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:20:31,920][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:20:31,921][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:20:32,338][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:20:49,686][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:20:49,687][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:20:50,065][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:20:50,228][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:20:57,902][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:21:16,423][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:21:16,424][root][INFO] - Iteration 119 took 2m 46s. Generation: 68.60%, Training: 31.39%. Estimated time remaining: 1h 7m 48s. Estimated total time for complete run: 5h 55m 18s.
[2025-01-06 20:21:16,762][root][INFO] - Loading VLLM model.
WARNING 01-06 20:21:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:21:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:21:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:21:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]

INFO 01-06 20:21:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:21:36 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:21:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:21:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:21:58 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:21:58,181][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:21:58,445][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:21:58,447][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:21:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:42, 14.93s/it, est. speed input: 33.83 toks/s, output: 13.40 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s, est. speed input: 1082.34 toks/s, output: 428.65 toks/s]
[2025-01-06 20:22:13,830][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:22:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:22,  4.58s/it, est. speed input: 162.50 toks/s, output: 6.76 toks/s]Processed prompts:   9%|▉         | 3/32 [00:06<00:51,  1.78s/it, est. speed input: 361.88 toks/s, output: 19.43 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:06<00:25,  1.05it/s, est. speed input: 571.65 toks/s, output: 37.29 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:07<00:16,  1.49it/s, est. speed input: 734.27 toks/s, output: 53.93 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:12,  1.85it/s, est. speed input: 827.10 toks/s, output: 63.84 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:13,  1.57it/s, est. speed input: 849.78 toks/s, output: 73.80 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:08<00:08,  2.28it/s, est. speed input: 998.46 toks/s, output: 97.17 toks/s]Processed prompts:  41%|████      | 13/32 [00:09<00:09,  1.98it/s, est. speed input: 997.37 toks/s, output: 102.98 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:06,  2.63it/s, est. speed input: 1112.19 toks/s, output: 126.30 toks/s]Processed prompts:  50%|█████     | 16/32 [00:10<00:06,  2.38it/s, est. speed input: 1122.20 toks/s, output: 133.80 toks/s]Processed prompts:  53%|█████▎    | 17/32 [00:11<00:06,  2.33it/s, est. speed input: 1142.87 toks/s, output: 143.05 toks/s]Processed prompts:  59%|█████▉    | 19/32 [00:11<00:04,  2.63it/s, est. speed input: 1210.87 toks/s, output: 165.21 toks/s]Processed prompts:  69%|██████▉   | 22/32 [00:11<00:02,  4.35it/s, est. speed input: 1382.88 toks/s, output: 209.44 toks/s]Processed prompts:  72%|███████▏  | 23/32 [00:12<00:02,  4.22it/s, est. speed input: 1413.79 toks/s, output: 220.90 toks/s]Processed prompts:  75%|███████▌  | 24/32 [00:12<00:01,  4.60it/s, est. speed input: 1458.14 toks/s, output: 234.65 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s, est. speed input: 1944.02 toks/s, output: 365.14 toks/s]
[2025-01-06 20:22:26,557][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:22:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.69s/it, est. speed input: 34.72 toks/s, output: 13.62 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1419.83 toks/s, output: 435.70 toks/s]
[2025-01-06 20:22:41,720][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:22:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<08:06, 15.69s/it, est. speed input: 60.82 toks/s, output: 12.75 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s, est. speed input: 1841.86 toks/s, output: 407.98 toks/s]
[2025-01-06 20:23:10,359][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:23:18,102][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:23:18,103][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:23:18,483][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:23:35,348][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:23:35,349][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:23:35,691][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:23:35,835][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:23:43,490][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:24:03,350][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:24:03,351][root][INFO] - Iteration 120 took 2m 46s. Generation: 68.17%, Training: 31.83%. Estimated time remaining: 1h 5m 49s. Estimated total time for complete run: 5h 56m 6s.
[2025-01-06 20:24:03,667][root][INFO] - Loading VLLM model.
WARNING 01-06 20:24:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:24:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:24:04 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:24:04 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:24:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:24:23 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:24:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:24:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:24:45 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:24:45,167][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:24:45,409][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:24:45,411][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:24:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:45, 15.03s/it, est. speed input: 33.60 toks/s, output: 13.31 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1075.17 toks/s, output: 425.81 toks/s]
[2025-01-06 20:25:00,917][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:25:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:10,  4.34s/it, est. speed input: 171.47 toks/s, output: 6.67 toks/s]Processed prompts:   6%|▋         | 2/31 [00:04<00:53,  1.86s/it, est. speed input: 333.82 toks/s, output: 13.44 toks/s]Processed prompts:  10%|▉         | 3/31 [00:05<00:36,  1.30s/it, est. speed input: 438.15 toks/s, output: 20.00 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:05<00:26,  1.04it/s, est. speed input: 536.74 toks/s, output: 27.38 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:05<00:17,  1.52it/s, est. speed input: 657.99 toks/s, output: 36.03 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:06<00:09,  2.41it/s, est. speed input: 865.88 toks/s, output: 52.30 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:06<00:07,  3.01it/s, est. speed input: 973.13 toks/s, output: 61.39 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:06<00:07,  2.77it/s, est. speed input: 1021.53 toks/s, output: 67.95 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:06<00:04,  4.48it/s, est. speed input: 1299.20 toks/s, output: 96.79 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:07<00:04,  3.75it/s, est. speed input: 1323.77 toks/s, output: 102.92 toks/s]Processed prompts:  45%|████▌     | 14/31 [00:07<00:04,  3.80it/s, est. speed input: 1378.20 toks/s, output: 111.79 toks/s]Processed prompts:  48%|████▊     | 15/31 [00:07<00:03,  4.19it/s, est. speed input: 1445.40 toks/s, output: 121.97 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:07<00:03,  4.23it/s, est. speed input: 1497.22 toks/s, output: 131.38 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:08<00:03,  4.50it/s, est. speed input: 1555.01 toks/s, output: 141.69 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:08<00:02,  4.76it/s, est. speed input: 1611.22 toks/s, output: 152.23 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:08<00:02,  5.04it/s, est. speed input: 1666.70 toks/s, output: 163.08 toks/s]Processed prompts:  65%|██████▍   | 20/31 [00:08<00:02,  4.81it/s, est. speed input: 1707.96 toks/s, output: 173.10 toks/s]Processed prompts:  68%|██████▊   | 21/31 [00:09<00:02,  3.76it/s, est. speed input: 1713.41 toks/s, output: 180.50 toks/s]Processed prompts:  74%|███████▍  | 23/31 [00:10<00:03,  2.52it/s, est. speed input: 1673.04 toks/s, output: 191.78 toks/s]Processed prompts:  77%|███████▋  | 24/31 [00:10<00:02,  2.54it/s, est. speed input: 1683.47 toks/s, output: 202.92 toks/s]Processed prompts:  81%|████████  | 25/31 [00:10<00:02,  2.84it/s, est. speed input: 1715.54 toks/s, output: 216.94 toks/s]Processed prompts: 100%|██████████| 31/31 [00:10<00:00,  2.86it/s, est. speed input: 2125.95 toks/s, output: 327.47 toks/s]
[2025-01-06 20:25:12,221][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:25:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.62s/it, est. speed input: 193.20 toks/s, output: 55.28 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.62s/it, est. speed input: 193.20 toks/s, output: 55.28 toks/s]
[2025-01-06 20:25:16,274][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:25:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:36, 14.72s/it, est. speed input: 34.65 toks/s, output: 13.59 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1434.17 toks/s, output: 434.78 toks/s]
[2025-01-06 20:25:31,509][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:25:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:40, 15.35s/it, est. speed input: 48.79 toks/s, output: 13.03 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.02it/s, est. speed input: 1819.92 toks/s, output: 403.83 toks/s]
[2025-01-06 20:25:59,776][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:26:07,530][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:26:07,531][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:26:07,922][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:26:24,293][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:26:24,294][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:26:24,644][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:26:24,829][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:26:33,100][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:26:53,103][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:26:53,104][root][INFO] - Iteration 121 took 2m 49s. Generation: 68.50%, Training: 31.50%. Estimated time remaining: 1h 9m 1s. Estimated total time for complete run: 6h 2m 8s.
[2025-01-06 20:26:53,440][root][INFO] - Loading VLLM model.
WARNING 01-06 20:26:53 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:26:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:26:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:26:54 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:26:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:27:13 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:27:13 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:27:13 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:27:35 model_runner.py:1335] Graph capturing finished in 21 secs.
[2025-01-06 20:27:35,122][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:27:35,391][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:27:35,392][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:27:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:30, 14.52s/it, est. speed input: 34.77 toks/s, output: 13.77 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.20it/s, est. speed input: 1112.56 toks/s, output: 440.62 toks/s]
[2025-01-06 20:27:50,368][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:27:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:04<02:02,  4.23s/it, est. speed input: 176.26 toks/s, output: 6.86 toks/s]Processed prompts:   7%|▋         | 2/30 [00:04<00:59,  2.12s/it, est. speed input: 306.36 toks/s, output: 14.19 toks/s]Processed prompts:  13%|█▎        | 4/30 [00:05<00:25,  1.01it/s, est. speed input: 540.14 toks/s, output: 29.18 toks/s]Processed prompts:  17%|█▋        | 5/30 [00:06<00:28,  1.13s/it, est. speed input: 535.83 toks/s, output: 34.52 toks/s]Processed prompts:  23%|██▎       | 7/30 [00:07<00:17,  1.29it/s, est. speed input: 676.55 toks/s, output: 53.71 toks/s]Processed prompts:  30%|███       | 9/30 [00:08<00:14,  1.48it/s, est. speed input: 765.57 toks/s, output: 71.48 toks/s]Processed prompts:  33%|███▎      | 10/30 [00:09<00:12,  1.65it/s, est. speed input: 816.32 toks/s, output: 82.17 toks/s]Processed prompts:  37%|███▋      | 11/30 [00:09<00:09,  1.90it/s, est. speed input: 872.39 toks/s, output: 93.67 toks/s]Processed prompts:  40%|████      | 12/30 [00:09<00:08,  2.04it/s, est. speed input: 913.62 toks/s, output: 104.12 toks/s]Processed prompts:  43%|████▎     | 13/30 [00:10<00:08,  1.89it/s, est. speed input: 930.00 toks/s, output: 112.63 toks/s]Processed prompts:  47%|████▋     | 14/30 [00:10<00:07,  2.22it/s, est. speed input: 978.42 toks/s, output: 125.03 toks/s]Processed prompts:  50%|█████     | 15/30 [00:10<00:05,  2.65it/s, est. speed input: 1029.72 toks/s, output: 138.02 toks/s]Processed prompts:  53%|█████▎    | 16/30 [00:11<00:04,  3.01it/s, est. speed input: 1076.49 toks/s, output: 150.71 toks/s]Processed prompts:  57%|█████▋    | 17/30 [00:11<00:03,  3.61it/s, est. speed input: 1129.17 toks/s, output: 164.39 toks/s]Processed prompts:  60%|██████    | 18/30 [00:11<00:03,  3.77it/s, est. speed input: 1170.68 toks/s, output: 176.86 toks/s]Processed prompts:  63%|██████▎   | 19/30 [00:12<00:03,  2.76it/s, est. speed input: 1174.76 toks/s, output: 184.73 toks/s]Processed prompts: 100%|██████████| 30/30 [00:12<00:00,  2.49it/s, est. speed input: 1853.46 toks/s, output: 367.29 toks/s]
[2025-01-06 20:28:02,907][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:28:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it, est. speed input: 174.39 toks/s, output: 49.90 toks/s]Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it, est. speed input: 348.77 toks/s, output: 99.79 toks/s]
[2025-01-06 20:28:07,334][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:28:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:34, 14.65s/it, est. speed input: 34.82 toks/s, output: 13.65 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1404.96 toks/s, output: 436.92 toks/s]
[2025-01-06 20:28:22,490][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:28:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/30 [00:14<07:08, 14.77s/it, est. speed input: 50.79 toks/s, output: 13.54 toks/s]Processed prompts: 100%|██████████| 30/30 [00:14<00:00,  2.03it/s, est. speed input: 1767.13 toks/s, output: 406.31 toks/s]
[2025-01-06 20:28:50,189][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:28:57,601][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:28:57,602][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:28:58,000][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:29:15,065][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:29:15,066][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:29:15,419][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:29:15,574][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:29:23,084][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:29:42,338][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:29:42,339][root][INFO] - Iteration 122 took 2m 49s. Generation: 69.10%, Training: 30.90%. Estimated time remaining: 1h 5m 6s. Estimated total time for complete run: 6h 1m 2s.
[2025-01-06 20:29:42,637][root][INFO] - Loading VLLM model.
WARNING 01-06 20:29:43 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:29:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:29:43 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:29:43 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:29:49 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:30:03 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:30:04 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:30:04 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:30:25 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:30:25,769][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:30:26,023][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:30:26,024][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:30:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:36, 14.73s/it, est. speed input: 34.28 toks/s, output: 13.58 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.17it/s, est. speed input: 1096.82 toks/s, output: 434.38 toks/s]
[2025-01-06 20:30:41,221][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:30:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:03<01:38,  3.80s/it, est. speed input: 195.86 toks/s, output: 7.36 toks/s]Processed prompts:  15%|█▍        | 4/27 [00:04<00:18,  1.23it/s, est. speed input: 716.16 toks/s, output: 29.09 toks/s]Processed prompts:  19%|█▊        | 5/27 [00:04<00:14,  1.49it/s, est. speed input: 836.45 toks/s, output: 36.39 toks/s]Processed prompts:  26%|██▌       | 7/27 [00:04<00:08,  2.25it/s, est. speed input: 1091.37 toks/s, output: 52.76 toks/s]Processed prompts:  30%|██▉       | 8/27 [00:05<00:11,  1.71it/s, est. speed input: 1027.15 toks/s, output: 55.68 toks/s]Processed prompts:  33%|███▎      | 9/27 [00:06<00:09,  1.91it/s, est. speed input: 1090.21 toks/s, output: 65.38 toks/s]Processed prompts:  37%|███▋      | 10/27 [00:06<00:08,  1.97it/s, est. speed input: 1126.95 toks/s, output: 74.44 toks/s]Processed prompts:  41%|████      | 11/27 [00:07<00:08,  1.91it/s, est. speed input: 1141.08 toks/s, output: 83.01 toks/s]Processed prompts:  48%|████▊     | 13/27 [00:07<00:05,  2.36it/s, est. speed input: 1246.35 toks/s, output: 105.80 toks/s]Processed prompts:  56%|█████▌    | 15/27 [00:08<00:04,  2.90it/s, est. speed input: 1360.91 toks/s, output: 130.94 toks/s]Processed prompts:  59%|█████▉    | 16/27 [00:08<00:04,  2.65it/s, est. speed input: 1369.38 toks/s, output: 140.52 toks/s]Processed prompts:  67%|██████▋   | 18/27 [00:09<00:03,  2.44it/s, est. speed input: 1392.96 toks/s, output: 161.13 toks/s]Processed prompts:  70%|███████   | 19/27 [00:10<00:03,  2.42it/s, est. speed input: 1408.01 toks/s, output: 173.60 toks/s]Processed prompts:  78%|███████▊  | 21/27 [00:10<00:01,  3.49it/s, est. speed input: 1531.85 toks/s, output: 209.66 toks/s]Processed prompts: 100%|██████████| 27/27 [00:10<00:00,  2.64it/s, est. speed input: 1969.44 toks/s, output: 327.16 toks/s]
[2025-01-06 20:30:51,893][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:30:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  20%|██        | 1/5 [00:05<00:20,  5.08s/it, est. speed input: 137.70 toks/s, output: 39.40 toks/s]Processed prompts: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it, est. speed input: 688.48 toks/s, output: 196.99 toks/s]
[2025-01-06 20:30:57,384][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:30:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:15<07:46, 15.04s/it, est. speed input: 33.91 toks/s, output: 13.30 toks/s]Processed prompts: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, est. speed input: 1496.14 toks/s, output: 425.49 toks/s]
[2025-01-06 20:31:12,958][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:31:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   4%|▎         | 1/27 [00:13<05:55, 13.67s/it, est. speed input: 69.69 toks/s, output: 14.63 toks/s]Processed prompts: 100%|██████████| 27/27 [00:13<00:00,  1.97it/s, est. speed input: 1789.57 toks/s, output: 394.89 toks/s]
[2025-01-06 20:31:39,634][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 20:31:47,378][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:31:47,379][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:31:47,766][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:32:03,911][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:32:03,912][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:32:04,278][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:32:04,426][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-06 20:32:12,006][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:32:31,583][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:32:31,584][root][INFO] - Iteration 123 took 2m 49s. Generation: 69.21%, Training: 30.79%. Estimated time remaining: 1h 2m 18s. Estimated total time for complete run: 6h 1m 3s.
[2025-01-06 20:32:31,942][root][INFO] - Loading VLLM model.
WARNING 01-06 20:32:32 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:32:32 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:32:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:32:32 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 20:32:37 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:32:51 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:32:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:32:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:33:13 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:33:13,784][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:33:14,039][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:33:14,040][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:33:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:28, 14.46s/it, est. speed input: 34.94 toks/s, output: 13.84 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s, est. speed input: 1117.88 toks/s, output: 442.72 toks/s]
[2025-01-06 20:33:28,940][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:33:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:14,  4.33s/it, est. speed input: 172.13 toks/s, output: 6.24 toks/s]Processed prompts:   6%|▋         | 2/32 [00:04<00:55,  1.85s/it, est. speed input: 334.85 toks/s, output: 12.58 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:05<00:24,  1.15it/s, est. speed input: 592.65 toks/s, output: 24.66 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:05<00:16,  1.53it/s, est. speed input: 764.57 toks/s, output: 37.29 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:06<00:14,  1.72it/s, est. speed input: 838.64 toks/s, output: 44.87 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:07<00:16,  1.45it/s, est. speed input: 827.99 toks/s, output: 49.87 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:07<00:14,  1.60it/s, est. speed input: 876.31 toks/s, output: 58.55 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:08<00:15,  1.44it/s, est. speed input: 873.56 toks/s, output: 65.08 toks/s]Processed prompts:  38%|███▊      | 12/32 [00:09<00:12,  1.58it/s, est. speed input: 927.81 toks/s, output: 82.31 toks/s]Processed prompts:  44%|████▍     | 14/32 [00:09<00:07,  2.33it/s, est. speed input: 1058.40 toks/s, output: 107.68 toks/s]Processed prompts:  47%|████▋     | 15/32 [00:10<00:08,  1.90it/s, est. speed input: 1040.93 toks/s, output: 113.46 toks/s]Processed prompts:  56%|█████▋    | 18/32 [00:11<00:04,  3.06it/s, est. speed input: 1211.44 toks/s, output: 153.51 toks/s]Processed prompts:  62%|██████▎   | 20/32 [00:11<00:03,  3.41it/s, est. speed input: 1294.10 toks/s, output: 177.64 toks/s]Processed prompts:  66%|██████▌   | 21/32 [00:12<00:04,  2.73it/s, est. speed input: 1281.65 toks/s, output: 183.94 toks/s]Processed prompts: 100%|██████████| 32/32 [00:12<00:00,  2.62it/s, est. speed input: 1953.02 toks/s, output: 364.17 toks/s]
[2025-01-06 20:33:41,624][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:33:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:32, 14.60s/it, est. speed input: 34.94 toks/s, output: 13.70 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1390.54 toks/s, output: 438.37 toks/s]
[2025-01-06 20:33:56,690][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:33:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:37, 15.26s/it, est. speed input: 62.20 toks/s, output: 13.11 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.03it/s, est. speed input: 1779.51 toks/s, output: 406.34 toks/s]
[2025-01-06 20:34:25,162][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:34:32,540][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:34:32,541][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:34:32,937][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:34:49,855][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:34:49,856][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:34:50,210][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:34:50,355][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:34:57,663][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:35:16,655][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:35:16,656][root][INFO] - Iteration 124 took 2m 45s. Generation: 68.72%, Training: 31.28%. Estimated time remaining: 50m 38s. Estimated total time for complete run: 5h 52m 9s.
[2025-01-06 20:35:17,017][root][INFO] - Loading VLLM model.
WARNING 01-06 20:35:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:35:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:35:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:35:17 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 20:35:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:35:36 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:35:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:35:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:35:58 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:35:58,942][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:35:59,214][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:35:59,215][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:35:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:13<06:51, 13.28s/it, est. speed input: 38.04 toks/s, output: 13.33 toks/s]Processed prompts:   6%|▋         | 2/32 [00:14<03:08,  6.28s/it, est. speed input: 68.88 toks/s, output: 25.71 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1102.04 toks/s, output: 434.88 toks/s]
[2025-01-06 20:36:14,364][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:36:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:06,  4.21s/it, est. speed input: 176.96 toks/s, output: 6.41 toks/s]Processed prompts:   6%|▋         | 2/31 [00:04<00:52,  1.80s/it, est. speed input: 344.21 toks/s, output: 12.94 toks/s]Processed prompts:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, est. speed input: 472.05 toks/s, output: 19.43 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:04<00:20,  1.30it/s, est. speed input: 607.64 toks/s, output: 26.71 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, est. speed input: 565.10 toks/s, output: 30.49 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:07<00:22,  1.10it/s, est. speed input: 627.43 toks/s, output: 39.44 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:07<00:11,  2.00it/s, est. speed input: 818.88 toks/s, output: 61.14 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:07<00:10,  2.07it/s, est. speed input: 868.84 toks/s, output: 69.59 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:08<00:06,  2.88it/s, est. speed input: 1018.79 toks/s, output: 90.75 toks/s]Processed prompts:  39%|███▊      | 12/31 [00:08<00:07,  2.63it/s, est. speed input: 1047.55 toks/s, output: 98.43 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:09<00:08,  2.18it/s, est. speed input: 1049.19 toks/s, output: 104.65 toks/s]Processed prompts:  45%|████▌     | 14/31 [00:09<00:06,  2.69it/s, est. speed input: 1114.59 toks/s, output: 117.02 toks/s]Processed prompts:  48%|████▊     | 15/31 [00:10<00:07,  2.12it/s, est. speed input: 1107.10 toks/s, output: 123.04 toks/s]Processed prompts:  52%|█████▏    | 16/31 [00:10<00:05,  2.54it/s, est. speed input: 1158.84 toks/s, output: 135.52 toks/s]Processed prompts:  55%|█████▍    | 17/31 [00:10<00:05,  2.34it/s, est. speed input: 1172.60 toks/s, output: 144.43 toks/s]Processed prompts:  58%|█████▊    | 18/31 [00:10<00:04,  2.90it/s, est. speed input: 1225.32 toks/s, output: 158.08 toks/s]Processed prompts:  61%|██████▏   | 19/31 [00:11<00:06,  1.84it/s, est. speed input: 1182.82 toks/s, output: 161.29 toks/s]Processed prompts: 100%|██████████| 31/31 [00:11<00:00,  2.59it/s, est. speed input: 1929.61 toks/s, output: 361.84 toks/s]
[2025-01-06 20:36:26,782][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:36:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.82 toks/s, output: 55.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 194.82 toks/s, output: 55.74 toks/s]
[2025-01-06 20:36:30,777][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:36:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:33, 14.64s/it, est. speed input: 34.84 toks/s, output: 13.66 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s, est. speed input: 1402.87 toks/s, output: 437.16 toks/s]
[2025-01-06 20:36:45,926][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:36:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:13<06:57, 13.91s/it, est. speed input: 68.28 toks/s, output: 12.87 toks/s]Processed prompts:   6%|▋         | 2/31 [00:15<03:07,  6.47s/it, est. speed input: 125.15 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.04it/s, est. speed input: 1774.82 toks/s, output: 407.41 toks/s]
[2025-01-06 20:37:14,217][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:37:21,578][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:37:21,579][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:37:21,973][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:37:38,850][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:37:38,852][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:37:39,230][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:37:39,384][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-06 20:37:46,801][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:38:05,827][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:38:05,828][root][INFO] - Iteration 125 took 2m 49s. Generation: 69.41%, Training: 30.59%. Estimated time remaining: 56m 34s. Estimated total time for complete run: 6h 0m 54s.
[2025-01-06 20:38:06,129][root][INFO] - Loading VLLM model.
WARNING 01-06 20:38:06 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:38:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:38:06 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:38:07 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 20:38:11 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:38:25 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:38:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:38:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:38:47 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:38:47,961][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:38:48,220][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:38:48,221][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:38:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:41, 14.88s/it, est. speed input: 33.93 toks/s, output: 13.44 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1085.63 toks/s, output: 429.95 toks/s]
[2025-01-06 20:39:03,560][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:39:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:26,  6.67s/it, est. speed input: 111.70 toks/s, output: 9.75 toks/s]Processed prompts:   6%|▋         | 2/32 [00:07<01:35,  3.18s/it, est. speed input: 201.38 toks/s, output: 19.19 toks/s]Processed prompts:   9%|▉         | 3/32 [00:07<00:52,  1.81s/it, est. speed input: 294.99 toks/s, output: 29.30 toks/s]Processed prompts:  12%|█▎        | 4/32 [00:07<00:31,  1.14s/it, est. speed input: 387.38 toks/s, output: 39.52 toks/s]Processed prompts:  16%|█▌        | 5/32 [00:08<00:28,  1.07s/it, est. speed input: 430.60 toks/s, output: 46.59 toks/s]Processed prompts:  19%|█▉        | 6/32 [00:08<00:19,  1.34it/s, est. speed input: 510.28 toks/s, output: 57.53 toks/s]Processed prompts:  22%|██▏       | 7/32 [00:08<00:14,  1.75it/s, est. speed input: 581.20 toks/s, output: 67.87 toks/s]Processed prompts:  25%|██▌       | 8/32 [00:09<00:15,  1.57it/s, est. speed input: 611.50 toks/s, output: 74.80 toks/s]Processed prompts:  28%|██▊       | 9/32 [00:10<00:12,  1.83it/s, est. speed input: 663.86 toks/s, output: 84.75 toks/s]Processed prompts:  31%|███▏      | 10/32 [00:10<00:14,  1.54it/s, est. speed input: 678.58 toks/s, output: 91.18 toks/s]Processed prompts:  34%|███▍      | 11/32 [00:13<00:26,  1.24s/it, est. speed input: 604.55 toks/s, output: 88.52 toks/s]Processed prompts: 100%|██████████| 32/32 [00:13<00:00,  2.35it/s, est. speed input: 1752.37 toks/s, output: 396.98 toks/s]
[2025-01-06 20:39:17,660][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:39:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:26, 14.39s/it, est. speed input: 35.43 toks/s, output: 13.90 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, est. speed input: 1304.55 toks/s, output: 444.65 toks/s]
[2025-01-06 20:39:32,503][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:39:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.70s/it, est. speed input: 51.31 toks/s, output: 13.61 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1427.66 toks/s, output: 435.49 toks/s]
[2025-01-06 20:40:00,435][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-06 20:40:08,030][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:40:08,030][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:40:08,420][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:40:25,789][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:40:25,790][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:40:26,161][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:40:26,312][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:40:33,732][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:40:47,427][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:40:47,428][root][INFO] - Iteration 126 took 2m 41s. Generation: 70.82%, Training: 29.18%. Estimated time remaining: 37m 43s. Estimated total time for complete run: 5h 44m 44s.
[2025-01-06 20:40:47,768][root][INFO] - Loading VLLM model.
WARNING 01-06 20:40:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:40:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:40:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:40:48 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]

INFO 01-06 20:40:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:41:07 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:41:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:41:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:41:29 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:41:29,406][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:41:29,656][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:41:29,657][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:41:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:42, 14.91s/it, est. speed input: 33.88 toks/s, output: 13.42 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s, est. speed input: 1083.94 toks/s, output: 429.28 toks/s]
[2025-01-06 20:41:45,041][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:41:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:09,  4.33s/it, est. speed input: 172.00 toks/s, output: 6.70 toks/s]Processed prompts:   6%|▋         | 2/31 [00:04<00:57,  1.99s/it, est. speed input: 318.01 toks/s, output: 13.66 toks/s]Processed prompts:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, est. speed input: 454.54 toks/s, output: 20.95 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:08<00:58,  2.16s/it, est. speed input: 347.37 toks/s, output: 24.13 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:09<00:41,  1.58s/it, est. speed input: 408.31 toks/s, output: 35.19 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:09<00:29,  1.17s/it, est. speed input: 470.73 toks/s, output: 46.55 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:10<00:17,  1.32it/s, est. speed input: 587.05 toks/s, output: 68.75 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:10<00:10,  2.03it/s, est. speed input: 716.78 toks/s, output: 93.61 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:11<00:10,  1.86it/s, est. speed input: 739.10 toks/s, output: 101.64 toks/s]Processed prompts:  42%|████▏     | 13/31 [00:12<00:12,  1.49it/s, est. speed input: 754.60 toks/s, output: 115.01 toks/s]Processed prompts:  45%|████▌     | 14/31 [00:13<00:09,  1.73it/s, est. speed input: 796.97 toks/s, output: 128.08 toks/s]Processed prompts: 100%|██████████| 31/31 [00:13<00:00,  2.37it/s, est. speed input: 1764.67 toks/s, output: 387.89 toks/s]
[2025-01-06 20:41:58,607][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:41:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.38 toks/s, output: 55.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.38 toks/s, output: 55.62 toks/s]
[2025-01-06 20:42:02,616][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:42:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:35, 14.70s/it, est. speed input: 34.68 toks/s, output: 13.60 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s, est. speed input: 1422.30 toks/s, output: 435.22 toks/s]
[2025-01-06 20:42:17,791][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:42:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:15<07:30, 15.03s/it, est. speed input: 63.13 toks/s, output: 13.30 toks/s]Processed prompts: 100%|██████████| 31/31 [00:15<00:00,  2.06it/s, est. speed input: 1703.59 toks/s, output: 412.40 toks/s]
[2025-01-06 20:42:46,135][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:42:53,585][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:42:53,585][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:42:53,985][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:43:11,167][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:43:11,168][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:43:11,503][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:43:11,658][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-06 20:43:19,101][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:43:36,520][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:43:36,521][root][INFO] - Iteration 127 took 2m 49s. Generation: 70.12%, Training: 29.88%. Estimated time remaining: 50m 53s. Estimated total time for complete run: 6h 0m 43s.
[2025-01-06 20:43:36,880][root][INFO] - Loading VLLM model.
WARNING 01-06 20:43:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 20:43:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-06 20:43:37 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-06 20:43:37 weight_utils.py:236] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]

INFO 01-06 20:43:42 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-06 20:43:56 gpu_executor.py:122] # GPU blocks: 19941, # CPU blocks: 2048
INFO 01-06 20:43:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-06 20:43:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-06 20:44:18 model_runner.py:1335] Graph capturing finished in 22 secs.
[2025-01-06 20:44:18,709][root][INFO] - Before destroying HF.: GPU memory allocated: 71.27 GB
[2025-01-06 20:44:18,989][root][INFO] - After destroying HF.: GPU memory allocated: 55.67 GB
[2025-01-06 20:44:18,990][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:44:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:26, 14.42s/it, est. speed input: 35.03 toks/s, output: 13.87 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.22it/s, est. speed input: 1120.97 toks/s, output: 443.95 toks/s]
[2025-01-06 20:44:33,881][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice)
WARNING 01-06 20:44:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:04<02:10,  4.34s/it, est. speed input: 171.83 toks/s, output: 6.69 toks/s]Processed prompts:   6%|▋         | 2/31 [00:08<02:01,  4.18s/it, est. speed input: 177.04 toks/s, output: 15.10 toks/s]Processed prompts:  10%|▉         | 3/31 [00:08<01:06,  2.38s/it, est. speed input: 258.48 toks/s, output: 26.50 toks/s]Processed prompts:  13%|█▎        | 4/31 [00:10<00:55,  2.06s/it, est. speed input: 291.40 toks/s, output: 35.12 toks/s]Processed prompts:  16%|█▌        | 5/31 [00:10<00:37,  1.46s/it, est. speed input: 351.16 toks/s, output: 46.77 toks/s]Processed prompts:  19%|█▉        | 6/31 [00:10<00:26,  1.05s/it, est. speed input: 411.09 toks/s, output: 58.69 toks/s]Processed prompts:  23%|██▎       | 7/31 [00:11<00:23,  1.02it/s, est. speed input: 445.67 toks/s, output: 68.04 toks/s]Processed prompts:  26%|██▌       | 8/31 [00:12<00:22,  1.01it/s, est. speed input: 468.80 toks/s, output: 76.63 toks/s]Processed prompts:  29%|██▉       | 9/31 [00:12<00:16,  1.32it/s, est. speed input: 517.44 toks/s, output: 89.30 toks/s]Processed prompts:  32%|███▏      | 10/31 [00:13<00:13,  1.60it/s, est. speed input: 560.47 toks/s, output: 101.35 toks/s]Processed prompts:  35%|███▌      | 11/31 [00:13<00:11,  1.73it/s, est. speed input: 595.76 toks/s, output: 112.48 toks/s]Processed prompts: 100%|██████████| 31/31 [00:13<00:00,  2.25it/s, est. speed input: 1677.87 toks/s, output: 403.29 toks/s]
[2025-01-06 20:44:48,101][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:44:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.14 toks/s, output: 55.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it, est. speed input: 194.14 toks/s, output: 55.55 toks/s]
[2025-01-06 20:44:52,136][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:44:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/32 [00:14<07:28, 14.47s/it, est. speed input: 35.25 toks/s, output: 13.83 toks/s]Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s, est. speed input: 1314.78 toks/s, output: 442.39 toks/s]
[2025-01-06 20:45:07,074][root][INFO] - Generating using VLLM (with LoRA at /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob)
WARNING 01-06 20:45:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   3%|▎         | 1/31 [00:14<07:08, 14.30s/it, est. speed input: 38.46 toks/s, output: 13.99 toks/s]Processed prompts: 100%|██████████| 31/31 [00:14<00:00,  2.17it/s, est. speed input: 1379.73 toks/s, output: 433.55 toks/s]
[2025-01-06 20:45:34,674][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-06 20:45:42,298][root][INFO] - Adapter 'ad_alice' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice.
[2025-01-06 20:45:42,300][root][INFO] - Before destroying VLLM: GPU memory allocated: 71.26 GB
[2025-01-06 20:45:42,746][root][INFO] - After destroying VLLM.: GPU memory allocated: 15.89 GB
[2025-01-06 20:46:00,132][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_alice
[2025-01-06 20:46:00,133][root][INFO] - Before destroying HF.: GPU memory allocated: 15.89 GB
[2025-01-06 20:46:00,498][root][INFO] - After destroying HF.: GPU memory allocated: 0.30 GB
[2025-01-06 20:46:00,642][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-06 20:46:08,020][root][INFO] - Adapter 'ad_bob' loaded to HF from /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob.
[2025-01-06 20:46:20,865][root][INFO] - LoRA weights saved to /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-06/15-33-46/ad_bob
[2025-01-06 20:46:20,866][root][INFO] - Iteration 128 took 2m 44s. Generation: 71.80%, Training: 28.20%. Estimated time remaining: 38m 1s. Estimated total time for complete run: 5h 50m 36s.
[2025-01-06 20:46:20,866][root][INFO] - Total time taken for the entire run: 5h 12m 34s
