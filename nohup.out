/home/mila/d/dereck.piche/llm_negotiation/src/run.py:14: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="default")
WARNING 01-10 22:12:38 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-10 22:12:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-10 22:12:39 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-10 22:12:40 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-10 22:12:40,255][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-10 22:12:41,664][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.41s/it]
[2025-01-10 22:12:42,145][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
[2025-01-10 22:12:43,438][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.06s/it]
[2025-01-10 22:12:44,781][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
[2025-01-10 22:12:44,781][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
INFO 01-10 22:12:45 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-10 22:12:59 gpu_executor.py:122] # GPU blocks: 19859, # CPU blocks: 2048
INFO 01-10 22:13:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-10 22:13:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-10 22:13:19 model_runner.py:1335] Graph capturing finished in 19 secs.
[2025-01-10 22:13:19,731][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:21,104][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:01<00:20,  1.37s/it, est. speed input: 368.04 toks/s, output: 40.81 toks/s]
[2025-01-10 22:13:21,245][root][ERROR] - Processed prompts:  25%|##5       | 4/16 [00:01<00:03,  3.36it/s, est. speed input: 1334.90 toks/s, output: 159.92 toks/s]
[2025-01-10 22:13:21,519][root][ERROR] - Processed prompts:  38%|###7      | 6/16 [00:01<00:02,  4.39it/s, est. speed input: 1695.28 toks/s, output: 223.24 toks/s]
[2025-01-10 22:13:21,665][root][ERROR] - Processed prompts:  56%|#####6    | 9/16 [00:01<00:00,  7.08it/s, est. speed input: 2350.28 toks/s, output: 342.84 toks/s]
[2025-01-10 22:13:21,860][root][ERROR] - Processed prompts:  81%|########1 | 13/16 [00:02<00:00, 10.31it/s, est. speed input: 3083.47 toks/s, output: 495.98 toks/s]
[2025-01-10 22:13:22,460][root][ERROR] - Processed prompts:  94%|#########3| 15/16 [00:02<00:00,  6.72it/s, est. speed input: 2775.91 toks/s, output: 486.29 toks/s]
[2025-01-10 22:13:23,084][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.77it/s, est. speed input: 2410.40 toks/s, output: 455.53 toks/s]
[2025-01-10 22:13:23,088][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:23,561][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  2.11it/s, est. speed input: 1280.73 toks/s, output: 61.29 toks/s]
[2025-01-10 22:13:23,561][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  4.22it/s, est. speed input: 2853.84 toks/s, output: 122.52 toks/s]
[2025-01-10 22:13:23,576][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:24,500][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:00<00:12,  1.08it/s, est. speed input: 757.26 toks/s, output: 24.92 toks/s]
[2025-01-10 22:13:24,991][root][ERROR] - Processed prompts:  29%|##8       | 4/14 [00:01<00:03,  3.25it/s, est. speed input: 1977.26 toks/s, output: 94.76 toks/s]
[2025-01-10 22:13:25,270][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:01<00:01,  5.20it/s, est. speed input: 2889.13 toks/s, output: 191.90 toks/s]
[2025-01-10 22:13:25,387][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:01<00:00,  6.80it/s, est. speed input: 3474.05 toks/s, output: 266.17 toks/s]
[2025-01-10 22:13:25,488][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:01<00:00,  8.65it/s, est. speed input: 4021.85 toks/s, output: 343.65 toks/s]
[2025-01-10 22:13:26,284][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:02<00:00,  5.65it/s, est. speed input: 3614.74 toks/s, output: 364.20 toks/s]
[2025-01-10 22:13:26,284][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:02<00:00,  5.17it/s, est. speed input: 3614.74 toks/s, output: 364.20 toks/s]
[2025-01-10 22:13:26,294][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:27,604][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.31s/it, est. speed input: 496.17 toks/s, output: 51.91 toks/s]
[2025-01-10 22:13:27,763][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:01,  2.51it/s, est. speed input: 1350.77 toks/s, output: 146.38 toks/s]
[2025-01-10 22:13:28,239][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  3.98it/s, est. speed input: 2077.03 toks/s, output: 254.49 toks/s]
[2025-01-10 22:13:28,517][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.89it/s, est. speed input: 2149.25 toks/s, output: 283.00 toks/s]
[2025-01-10 22:13:28,570][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.52it/s, est. speed input: 2392.96 toks/s, output: 337.08 toks/s]
[2025-01-10 22:13:28,580][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:29,376][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.26it/s, est. speed input: 1053.89 toks/s, output: 36.43 toks/s]
[2025-01-10 22:13:29,930][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  5.13it/s, est. speed input: 3640.03 toks/s, output: 158.49 toks/s]
[2025-01-10 22:13:30,102][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  5.25it/s, est. speed input: 3688.35 toks/s, output: 194.47 toks/s]
[2025-01-10 22:13:30,858][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.16it/s, est. speed input: 2771.50 toks/s, output: 191.41 toks/s]
[2025-01-10 22:13:30,858][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.51it/s, est. speed input: 2771.50 toks/s, output: 191.41 toks/s]
[2025-01-10 22:13:30,868][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:31,627][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.32it/s, est. speed input: 1043.10 toks/s, output: 38.24 toks/s]
[2025-01-10 22:13:32,058][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:01,  2.81it/s, est. speed input: 1953.41 toks/s, output: 98.38 toks/s]
[2025-01-10 22:13:32,268][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.40it/s, est. speed input: 2607.86 toks/s, output: 180.10 toks/s]
[2025-01-10 22:13:32,655][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  4.70it/s, est. speed input: 2799.35 toks/s, output: 243.54 toks/s]
[2025-01-10 22:13:32,941][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.35it/s, est. speed input: 2739.58 toks/s, output: 270.14 toks/s]
[2025-01-10 22:13:32,942][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.86it/s, est. speed input: 2739.58 toks/s, output: 270.14 toks/s]
[2025-01-10 22:13:32,954][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:34,280][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.33s/it, est. speed input: 673.64 toks/s, output: 43.00 toks/s]
[2025-01-10 22:13:34,668][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.29it/s, est. speed input: 1135.64 toks/s, output: 81.70 toks/s]
[2025-01-10 22:13:34,787][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.99it/s, est. speed input: 3322.30 toks/s, output: 269.67 toks/s]
[2025-01-10 22:13:35,906][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.05it/s, est. speed input: 2771.91 toks/s, output: 258.86 toks/s]
[2025-01-10 22:13:35,906][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.71it/s, est. speed input: 2771.91 toks/s, output: 258.86 toks/s]
[2025-01-10 22:13:35,918][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:37,000][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.08s/it, est. speed input: 660.84 toks/s, output: 43.44 toks/s]
[2025-01-10 22:13:37,384][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.49it/s, est. speed input: 1036.31 toks/s, output: 81.87 toks/s]
[2025-01-10 22:13:37,575][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.22it/s, est. speed input: 1907.18 toks/s, output: 171.46 toks/s]
[2025-01-10 22:13:38,160][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.53it/s, est. speed input: 1804.39 toks/s, output: 183.34 toks/s]
[2025-01-10 22:13:38,260][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.28it/s, est. speed input: 2110.33 toks/s, output: 232.72 toks/s]
[2025-01-10 22:13:38,432][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.93it/s, est. speed input: 2683.27 toks/s, output: 330.19 toks/s]
[2025-01-10 22:13:38,432][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.18it/s, est. speed input: 2683.27 toks/s, output: 330.19 toks/s]
[2025-01-10 22:13:38,446][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:39,401][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:06,  1.05it/s, est. speed input: 1129.85 toks/s, output: 30.37 toks/s]
[2025-01-10 22:13:40,121][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.38it/s, est. speed input: 3360.37 toks/s, output: 116.39 toks/s]
[2025-01-10 22:13:40,260][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  4.76it/s, est. speed input: 4329.52 toks/s, output: 202.88 toks/s]
[2025-01-10 22:13:40,746][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.72it/s, est. speed input: 3872.82 toks/s, output: 214.77 toks/s]
[2025-01-10 22:13:40,746][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.48it/s, est. speed input: 3872.82 toks/s, output: 214.77 toks/s]
[2025-01-10 22:13:40,758][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:41,578][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.22it/s, est. speed input: 1175.78 toks/s, output: 35.41 toks/s]
[2025-01-10 22:13:42,076][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.59it/s, est. speed input: 1355.26 toks/s, output: 69.81 toks/s]
[2025-01-10 22:13:42,282][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.29it/s, est. speed input: 1750.58 toks/s, output: 110.93 toks/s]
[2025-01-10 22:13:42,553][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.76it/s, est. speed input: 2453.53 toks/s, output: 192.25 toks/s]
[2025-01-10 22:13:42,780][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.92it/s, est. speed input: 2583.64 toks/s, output: 226.10 toks/s]
[2025-01-10 22:13:43,003][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.26it/s, est. speed input: 3078.14 toks/s, output: 313.20 toks/s]
[2025-01-10 22:13:43,003][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.56it/s, est. speed input: 3078.14 toks/s, output: 313.20 toks/s]
[2025-01-10 22:13:43,019][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:44,523][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.50s/it, est. speed input: 908.76 toks/s, output: 36.56 toks/s]
[2025-01-10 22:13:45,108][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:03,  1.65it/s, est. speed input: 1910.43 toks/s, output: 98.13 toks/s]
[2025-01-10 22:13:45,360][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.78it/s, est. speed input: 2910.83 toks/s, output: 175.55 toks/s]
[2025-01-10 22:13:45,741][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.74it/s, est. speed input: 3059.96 toks/s, output: 200.98 toks/s]
[2025-01-10 22:13:46,256][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:03<00:00,  2.46it/s, est. speed input: 3010.29 toks/s, output: 223.03 toks/s]
[2025-01-10 22:13:46,389][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  3.04it/s, est. speed input: 3312.18 toks/s, output: 269.16 toks/s]
[2025-01-10 22:13:46,389][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.37it/s, est. speed input: 3312.18 toks/s, output: 269.16 toks/s]
[2025-01-10 22:13:46,403][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:47,588][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.18s/it, est. speed input: 715.67 toks/s, output: 39.66 toks/s]
[2025-01-10 22:13:47,947][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.43it/s, est. speed input: 1238.51 toks/s, output: 76.43 toks/s]
[2025-01-10 22:13:48,110][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.21it/s, est. speed input: 2947.44 toks/s, output: 204.50 toks/s]
[2025-01-10 22:13:48,311][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.38it/s, est. speed input: 3162.45 toks/s, output: 233.19 toks/s]
[2025-01-10 22:13:48,523][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.66it/s, est. speed input: 3887.01 toks/s, output: 309.02 toks/s]
[2025-01-10 22:13:48,523][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.77it/s, est. speed input: 3887.01 toks/s, output: 309.02 toks/s]
[2025-01-10 22:13:48,539][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:49,668][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.13s/it, est. speed input: 1225.08 toks/s, output: 25.71 toks/s]
[2025-01-10 22:13:50,220][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.01it/s, est. speed input: 2555.13 toks/s, output: 73.17 toks/s]
[2025-01-10 22:13:50,399][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.48it/s, est. speed input: 3899.58 toks/s, output: 143.59 toks/s]
[2025-01-10 22:13:50,604][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.77it/s, est. speed input: 4201.34 toks/s, output: 173.40 toks/s]
[2025-01-10 22:13:50,908][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.62it/s, est. speed input: 4303.22 toks/s, output: 199.26 toks/s]
[2025-01-10 22:13:51,106][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.94it/s, est. speed input: 4529.08 toks/s, output: 234.17 toks/s]
[2025-01-10 22:13:51,106][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.12it/s, est. speed input: 4529.08 toks/s, output: 234.17 toks/s]
[2025-01-10 22:13:51,120][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:52,035][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:06,  1.09it/s, est. speed input: 1178.88 toks/s, output: 31.71 toks/s]
[2025-01-10 22:13:52,259][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.97it/s, est. speed input: 1801.30 toks/s, output: 64.14 toks/s]
[2025-01-10 22:13:52,808][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  1.90it/s, est. speed input: 1833.06 toks/s, output: 91.24 toks/s]
[2025-01-10 22:13:53,101][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.21it/s, est. speed input: 2611.08 toks/s, output: 171.21 toks/s]
[2025-01-10 22:13:53,258][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.74it/s, est. speed input: 2932.46 toks/s, output: 210.96 toks/s]
[2025-01-10 22:13:53,905][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  2.65it/s, est. speed input: 2614.84 toks/s, output: 219.82 toks/s]
[2025-01-10 22:13:54,076][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.15it/s, est. speed input: 2800.26 toks/s, output: 265.95 toks/s]
[2025-01-10 22:13:54,076][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.71it/s, est. speed input: 2800.26 toks/s, output: 265.95 toks/s]
[2025-01-10 22:13:54,097][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:55,919][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.82s/it, est. speed input: 930.91 toks/s, output: 34.03 toks/s]
[2025-01-10 22:13:56,047][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  1.92it/s, est. speed input: 2602.86 toks/s, output: 101.53 toks/s]
[2025-01-10 22:13:56,185][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.47it/s, est. speed input: 4251.83 toks/s, output: 168.54 toks/s]
[2025-01-10 22:13:56,429][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.59it/s, est. speed input: 5366.39 toks/s, output: 231.51 toks/s]
[2025-01-10 22:13:56,693][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.08it/s, est. speed input: 5501.73 toks/s, output: 252.67 toks/s]
[2025-01-10 22:13:56,709][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:13:57,831][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.12s/it, est. speed input: 873.98 toks/s, output: 32.96 toks/s]
[2025-01-10 22:13:58,271][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.39it/s, est. speed input: 1405.42 toks/s, output: 65.92 toks/s]
[2025-01-10 22:13:58,765][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:01,  2.36it/s, est. speed input: 2249.46 toks/s, output: 132.29 toks/s]
[2025-01-10 22:13:58,897][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.92it/s, est. speed input: 3246.96 toks/s, output: 219.78 toks/s]
[2025-01-10 22:13:59,621][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.36it/s, est. speed input: 3284.41 toks/s, output: 258.93 toks/s]
[2025-01-10 22:13:59,621][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.75it/s, est. speed input: 3284.41 toks/s, output: 258.93 toks/s]
[2025-01-10 22:13:59,641][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:00,951][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.31s/it, est. speed input: 1285.99 toks/s, output: 22.13 toks/s]
[2025-01-10 22:14:01,532][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  2.47it/s, est. speed input: 3722.77 toks/s, output: 80.91 toks/s]
[2025-01-10 22:14:01,665][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.04it/s, est. speed input: 4420.46 toks/s, output: 112.65 toks/s]
[2025-01-10 22:14:02,110][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.55it/s, est. speed input: 5115.04 toks/s, output: 168.49 toks/s]
[2025-01-10 22:14:02,505][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.24it/s, est. speed input: 5041.90 toks/s, output: 193.42 toks/s]
[2025-01-10 22:14:02,506][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.79it/s, est. speed input: 5041.90 toks/s, output: 193.42 toks/s]
[2025-01-10 22:14:02,521][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:03,535][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.01s/it, est. speed input: 1177.75 toks/s, output: 28.63 toks/s]
[2025-01-10 22:14:04,126][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.31it/s, est. speed input: 1531.02 toks/s, output: 60.47 toks/s]
[2025-01-10 22:14:04,262][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.09it/s, est. speed input: 2040.74 toks/s, output: 99.97 toks/s]
[2025-01-10 22:14:04,383][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.14it/s, est. speed input: 3210.08 toks/s, output: 183.77 toks/s]
[2025-01-10 22:14:04,617][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  4.18it/s, est. speed input: 3429.43 toks/s, output: 211.44 toks/s]
[2025-01-10 22:14:05,104][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.14it/s, est. speed input: 3767.99 toks/s, output: 264.83 toks/s]
[2025-01-10 22:14:05,105][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.10it/s, est. speed input: 3767.99 toks/s, output: 264.83 toks/s]
[2025-01-10 22:14:05,128][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:07,028][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.90s/it, est. speed input: 1069.92 toks/s, output: 28.43 toks/s]
[2025-01-10 22:14:07,339][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:03,  1.65it/s, est. speed input: 2793.65 toks/s, output: 82.79 toks/s]
[2025-01-10 22:14:07,477][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.02it/s, est. speed input: 4467.43 toks/s, output: 145.21 toks/s]
[2025-01-10 22:14:07,594][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.66it/s, est. speed input: 5162.10 toks/s, output: 174.79 toks/s]
[2025-01-10 22:14:08,044][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.96it/s, est. speed input: 5886.55 toks/s, output: 222.95 toks/s]
[2025-01-10 22:14:08,044][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.74it/s, est. speed input: 5886.55 toks/s, output: 222.95 toks/s]
[2025-01-10 22:14:08,062][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:09,424][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.36s/it, est. speed input: 810.49 toks/s, output: 34.50 toks/s]
[2025-01-10 22:14:09,747][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.33it/s, est. speed input: 1415.72 toks/s, output: 68.23 toks/s]
[2025-01-10 22:14:09,885][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.09it/s, est. speed input: 2879.16 toks/s, output: 144.29 toks/s]
[2025-01-10 22:14:10,193][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  4.11it/s, est. speed input: 3775.80 toks/s, output: 206.51 toks/s]
[2025-01-10 22:14:10,299][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.79it/s, est. speed input: 4249.83 toks/s, output: 244.10 toks/s]
[2025-01-10 22:14:10,537][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.62it/s, est. speed input: 4440.44 toks/s, output: 270.76 toks/s]
[2025-01-10 22:14:10,537][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.23it/s, est. speed input: 4440.44 toks/s, output: 270.76 toks/s]
[2025-01-10 22:14:10,560][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:11,889][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.33s/it, est. speed input: 1503.70 toks/s, output: 14.30 toks/s]
[2025-01-10 22:14:12,053][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.55it/s, est. speed input: 2675.19 toks/s, output: 32.17 toks/s]
[2025-01-10 22:14:12,619][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:01,  2.40it/s, est. speed input: 4030.22 toks/s, output: 68.51 toks/s]
[2025-01-10 22:14:12,810][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.77it/s, est. speed input: 5672.04 toks/s, output: 128.01 toks/s]
[2025-01-10 22:14:13,129][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.59it/s, est. speed input: 5849.48 toks/s, output: 151.46 toks/s]
[2025-01-10 22:14:13,526][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.22it/s, est. speed input: 5806.65 toks/s, output: 175.38 toks/s]
[2025-01-10 22:14:13,526][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.70it/s, est. speed input: 5806.65 toks/s, output: 175.38 toks/s]
[2025-01-10 22:14:13,541][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:14,469][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:00<00:04,  1.08it/s, est. speed input: 1408.75 toks/s, output: 31.23 toks/s]
[2025-01-10 22:14:15,064][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.37it/s, est. speed input: 1681.46 toks/s, output: 63.69 toks/s]
[2025-01-10 22:14:15,212][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.11it/s, est. speed input: 4023.10 toks/s, output: 187.95 toks/s]
[2025-01-10 22:14:16,094][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.35it/s, est. speed input: 3210.43 toks/s, output: 180.21 toks/s]
[2025-01-10 22:14:16,127][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:18,082][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.95s/it, est. speed input: 1049.70 toks/s, output: 14.83 toks/s]
[2025-01-10 22:14:18,519][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:02<00:04,  1.50it/s, est. speed input: 2838.62 toks/s, output: 47.23 toks/s]
[2025-01-10 22:14:18,739][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:03,  1.95it/s, est. speed input: 3566.87 toks/s, output: 69.30 toks/s]
[2025-01-10 22:14:18,856][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.48it/s, est. speed input: 5338.18 toks/s, output: 120.55 toks/s]
[2025-01-10 22:14:18,959][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  5.28it/s, est. speed input: 6942.72 toks/s, output: 172.62 toks/s]
[2025-01-10 22:14:19,451][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.75it/s, est. speed input: 7358.64 toks/s, output: 217.77 toks/s]
[2025-01-10 22:14:19,452][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.01it/s, est. speed input: 7358.64 toks/s, output: 217.77 toks/s]
[2025-01-10 22:14:19,476][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:20,837][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:12,  1.36s/it, est. speed input: 909.12 toks/s, output: 21.31 toks/s]
[2025-01-10 22:14:21,418][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:07,  1.11it/s, est. speed input: 1376.96 toks/s, output: 48.94 toks/s]
[2025-01-10 22:14:21,559][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.63it/s, est. speed input: 2872.89 toks/s, output: 115.24 toks/s]
[2025-01-10 22:14:22,111][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.02it/s, est. speed input: 3414.38 toks/s, output: 162.84 toks/s]
[2025-01-10 22:14:22,316][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.18it/s, est. speed input: 4340.56 toks/s, output: 234.84 toks/s]
[2025-01-10 22:14:22,621][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  3.94it/s, est. speed input: 4369.43 toks/s, output: 259.19 toks/s]
[2025-01-10 22:14:22,687][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.11it/s, est. speed input: 4738.25 toks/s, output: 301.49 toks/s]
[2025-01-10 22:14:22,708][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:24,088][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.38s/it, est. speed input: 1668.75 toks/s, output: 21.01 toks/s]
[2025-01-10 22:14:24,706][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  1.70it/s, est. speed input: 3648.93 toks/s, output: 64.57 toks/s]
[2025-01-10 22:14:24,911][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.20it/s, est. speed input: 4482.26 toks/s, output: 97.12 toks/s]
[2025-01-10 22:14:25,099][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.73it/s, est. speed input: 5196.06 toks/s, output: 130.88 toks/s]
[2025-01-10 22:14:25,126][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.48it/s, est. speed input: 6228.60 toks/s, output: 171.19 toks/s]
[2025-01-10 22:14:25,142][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:26,128][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:00<00:04,  1.01it/s, est. speed input: 1443.18 toks/s, output: 29.41 toks/s]
[2025-01-10 22:14:26,639][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.42it/s, est. speed input: 2008.99 toks/s, output: 60.80 toks/s]
[2025-01-10 22:14:27,005][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.64it/s, est. speed input: 3198.63 toks/s, output: 132.00 toks/s]
[2025-01-10 22:14:27,257][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.95it/s, est. speed input: 3526.67 toks/s, output: 166.43 toks/s]
[2025-01-10 22:14:27,718][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.65it/s, est. speed input: 3554.88 toks/s, output: 191.35 toks/s]
[2025-01-10 22:14:27,718][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.33it/s, est. speed input: 3554.88 toks/s, output: 191.35 toks/s]
[2025-01-10 22:14:27,755][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:29,942][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:19,  2.19s/it, est. speed input: 1076.81 toks/s, output: 13.26 toks/s]
[2025-01-10 22:14:30,409][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:09,  1.17s/it, est. speed input: 1883.29 toks/s, output: 32.41 toks/s]
[2025-01-10 22:14:30,512][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.13it/s, est. speed input: 3857.67 toks/s, output: 76.55 toks/s]
[2025-01-10 22:14:30,714][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.38it/s, est. speed input: 5575.43 toks/s, output: 118.62 toks/s]
[2025-01-10 22:14:30,847][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  4.91it/s, est. speed input: 7222.11 toks/s, output: 166.87 toks/s]
[2025-01-10 22:14:31,100][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  5.71it/s, est. speed input: 8360.29 toks/s, output: 210.51 toks/s]
[2025-01-10 22:14:31,100][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.99it/s, est. speed input: 8360.29 toks/s, output: 210.51 toks/s]
[2025-01-10 22:14:31,126][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:32,861][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.74s/it, est. speed input: 779.19 toks/s, output: 25.93 toks/s]
[2025-01-10 22:14:33,243][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:07,  1.07it/s, est. speed input: 1389.12 toks/s, output: 53.86 toks/s]
[2025-01-10 22:14:33,355][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.60it/s, est. speed input: 2946.80 toks/s, output: 118.46 toks/s]
[2025-01-10 22:14:33,590][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.95it/s, est. speed input: 3329.90 toks/s, output: 144.09 toks/s]
[2025-01-10 22:14:33,885][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.96it/s, est. speed input: 4884.63 toks/s, output: 236.33 toks/s]
[2025-01-10 22:14:34,031][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  6.39it/s, est. speed input: 5860.57 toks/s, output: 305.02 toks/s]
[2025-01-10 22:14:34,031][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.44it/s, est. speed input: 5860.57 toks/s, output: 305.02 toks/s]
[2025-01-10 22:14:34,054][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:35,564][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.51s/it, est. speed input: 1729.59 toks/s, output: 19.20 toks/s]
[2025-01-10 22:14:35,993][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  1.82it/s, est. speed input: 4252.19 toks/s, output: 59.83 toks/s]
[2025-01-10 22:14:36,111][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.50it/s, est. speed input: 5442.56 toks/s, output: 88.48 toks/s]
[2025-01-10 22:14:36,680][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.19it/s, est. speed input: 5382.36 toks/s, output: 110.44 toks/s]
[2025-01-10 22:14:37,091][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  2.26it/s, est. speed input: 5645.25 toks/s, output: 141.25 toks/s]
[2025-01-10 22:14:37,091][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.98it/s, est. speed input: 5645.25 toks/s, output: 141.25 toks/s]
[2025-01-10 22:14:37,109][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:38,162][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.05s/it, est. speed input: 1455.26 toks/s, output: 27.55 toks/s]
[2025-01-10 22:14:38,634][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.41it/s, est. speed input: 2140.94 toks/s, output: 57.72 toks/s]
[2025-01-10 22:14:38,869][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.95it/s, est. speed input: 3715.06 toks/s, output: 128.97 toks/s]
[2025-01-10 22:14:39,068][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.38it/s, est. speed input: 4200.36 toks/s, output: 161.81 toks/s]
[2025-01-10 22:14:39,833][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.27it/s, est. speed input: 3725.56 toks/s, output: 170.68 toks/s]
[2025-01-10 22:14:39,834][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.20it/s, est. speed input: 3725.56 toks/s, output: 170.68 toks/s]
[2025-01-10 22:14:39,874][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:42,279][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:21,  2.40s/it, est. speed input: 1105.50 toks/s, output: 12.06 toks/s]
[2025-01-10 22:14:42,854][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:10,  1.33s/it, est. speed input: 1909.47 toks/s, output: 30.88 toks/s]
[2025-01-10 22:14:42,957][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:03<00:03,  1.89it/s, est. speed input: 3913.19 toks/s, output: 72.68 toks/s]
[2025-01-10 22:14:43,059][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:01,  2.53it/s, est. speed input: 4801.70 toks/s, output: 93.90 toks/s]
[2025-01-10 22:14:43,177][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:03<00:01,  3.22it/s, est. speed input: 5609.18 toks/s, output: 115.37 toks/s]
[2025-01-10 22:14:43,340][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.76it/s, est. speed input: 6304.34 toks/s, output: 136.77 toks/s]
[2025-01-10 22:14:43,473][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  4.43it/s, est. speed input: 6986.13 toks/s, output: 160.08 toks/s]
[2025-01-10 22:14:43,593][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  6.72it/s, est. speed input: 8456.59 toks/s, output: 212.46 toks/s]
[2025-01-10 22:14:43,593][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.69it/s, est. speed input: 8456.59 toks/s, output: 212.46 toks/s]
[2025-01-10 22:14:43,622][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:45,643][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.02s/it, est. speed input: 730.92 toks/s, output: 27.22 toks/s]
[2025-01-10 22:14:45,836][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:07,  1.06it/s, est. speed input: 1454.70 toks/s, output: 55.12 toks/s]
[2025-01-10 22:14:45,965][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  3.34it/s, est. speed input: 3877.63 toks/s, output: 145.12 toks/s]
[2025-01-10 22:14:46,438][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.65it/s, est. speed input: 4612.01 toks/s, output: 190.70 toks/s]
[2025-01-10 22:14:47,023][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.22it/s, est. speed input: 5512.40 toks/s, output: 266.43 toks/s]
[2025-01-10 22:14:47,023][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.94it/s, est. speed input: 5512.40 toks/s, output: 266.43 toks/s]
[2025-01-10 22:14:47,048][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:48,702][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:08,  1.65s/it, est. speed input: 1766.45 toks/s, output: 17.54 toks/s]
[2025-01-10 22:14:49,175][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.66it/s, est. speed input: 4322.55 toks/s, output: 55.95 toks/s]
[2025-01-10 22:14:49,646][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:01,  1.79it/s, est. speed input: 4817.16 toks/s, output: 81.61 toks/s]
[2025-01-10 22:14:49,756][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.44it/s, est. speed input: 5886.76 toks/s, output: 115.60 toks/s]
[2025-01-10 22:14:51,067][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.43it/s, est. speed input: 4786.74 toks/s, output: 127.66 toks/s]
[2025-01-10 22:14:51,067][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.49it/s, est. speed input: 4786.74 toks/s, output: 127.66 toks/s]
[2025-01-10 22:14:51,086][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:52,206][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.12s/it, est. speed input: 1465.71 toks/s, output: 25.90 toks/s]
[2025-01-10 22:14:52,590][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.46it/s, est. speed input: 2224.05 toks/s, output: 54.52 toks/s]
[2025-01-10 22:14:53,309][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.08it/s, est. speed input: 3188.03 toks/s, output: 108.86 toks/s]
[2025-01-10 22:14:53,443][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.67it/s, est. speed input: 3801.24 toks/s, output: 150.22 toks/s]
[2025-01-10 22:14:53,733][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.87it/s, est. speed input: 4195.89 toks/s, output: 184.35 toks/s]
[2025-01-10 22:14:53,734][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.27it/s, est. speed input: 4195.89 toks/s, output: 184.35 toks/s]
[2025-01-10 22:14:53,780][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:56,417][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:23,  2.64s/it, est. speed input: 1123.08 toks/s, output: 11.00 toks/s]
[2025-01-10 22:14:56,778][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:02<00:05,  1.22it/s, est. speed input: 3242.62 toks/s, output: 36.03 toks/s]
[2025-01-10 22:14:57,168][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:02,  2.01it/s, est. speed input: 4945.86 toks/s, output: 68.18 toks/s]
[2025-01-10 22:14:57,401][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:01,  2.97it/s, est. speed input: 6610.64 toks/s, output: 109.64 toks/s]
[2025-01-10 22:14:57,652][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  3.16it/s, est. speed input: 7141.65 toks/s, output: 129.67 toks/s]
[2025-01-10 22:14:57,816][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:04<00:00,  3.59it/s, est. speed input: 7766.22 toks/s, output: 153.37 toks/s]
[2025-01-10 22:14:57,830][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.47it/s, est. speed input: 8645.09 toks/s, output: 181.98 toks/s]
[2025-01-10 22:14:57,862][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:14:59,775][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.91s/it, est. speed input: 842.64 toks/s, output: 21.95 toks/s]
[2025-01-10 22:15:00,132][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:07,  1.00it/s, est. speed input: 1545.61 toks/s, output: 46.70 toks/s]
[2025-01-10 22:15:00,585][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.56it/s, est. speed input: 3582.37 toks/s, output: 121.58 toks/s]
[2025-01-10 22:15:00,696][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.92it/s, est. speed input: 4933.36 toks/s, output: 186.31 toks/s]
[2025-01-10 22:15:01,006][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  4.58it/s, est. speed input: 5832.65 toks/s, output: 239.21 toks/s]
[2025-01-10 22:15:01,138][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.99it/s, est. speed input: 6250.03 toks/s, output: 269.84 toks/s]
[2025-01-10 22:15:01,138][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.05it/s, est. speed input: 6250.03 toks/s, output: 269.84 toks/s]
[2025-01-10 22:15:01,168][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:02,952][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:08,  1.78s/it, est. speed input: 1806.68 toks/s, output: 16.25 toks/s]
[2025-01-10 22:15:03,501][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.50it/s, est. speed input: 4419.84 toks/s, output: 53.15 toks/s]
[2025-01-10 22:15:04,037][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.72it/s, est. speed input: 7459.69 toks/s, output: 129.29 toks/s]
[2025-01-10 22:15:04,038][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.09it/s, est. speed input: 7459.69 toks/s, output: 129.29 toks/s]
[2025-01-10 22:15:04,057][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:05,252][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.19s/it, est. speed input: 1470.04 toks/s, output: 24.28 toks/s]
[2025-01-10 22:15:05,706][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:03,  1.32it/s, est. speed input: 2179.53 toks/s, output: 52.17 toks/s]
[2025-01-10 22:15:06,104][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.45it/s, est. speed input: 3746.79 toks/s, output: 112.40 toks/s]
[2025-01-10 22:15:06,356][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.78it/s, est. speed input: 4366.29 toks/s, output: 144.88 toks/s]
[2025-01-10 22:15:06,488][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.45it/s, est. speed input: 4977.58 toks/s, output: 183.49 toks/s]
[2025-01-10 22:15:06,488][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.47it/s, est. speed input: 4977.58 toks/s, output: 183.49 toks/s]
[2025-01-10 22:15:06,540][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:09,406][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:25,  2.87s/it, est. speed input: 1139.12 toks/s, output: 10.12 toks/s]
[2025-01-10 22:15:09,983][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:03<00:12,  1.52s/it, est. speed input: 2019.57 toks/s, output: 26.44 toks/s]
[2025-01-10 22:15:10,087][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:03<00:03,  1.67it/s, est. speed input: 4111.16 toks/s, output: 63.73 toks/s]
[2025-01-10 22:15:10,206][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:02,  2.22it/s, est. speed input: 4987.47 toks/s, output: 82.11 toks/s]
[2025-01-10 22:15:10,469][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.32it/s, est. speed input: 6686.63 toks/s, output: 120.39 toks/s]
[2025-01-10 22:15:10,573][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:04<00:00,  3.98it/s, est. speed input: 7530.69 toks/s, output: 141.85 toks/s]
[2025-01-10 22:15:11,147][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  3.75it/s, est. speed input: 8360.07 toks/s, output: 178.00 toks/s]
[2025-01-10 22:15:11,147][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.17it/s, est. speed input: 8360.07 toks/s, output: 178.00 toks/s]
[2025-01-10 22:15:11,182][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:13,127][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.94s/it, est. speed input: 894.92 toks/s, output: 19.03 toks/s]
[2025-01-10 22:15:13,585][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:08,  1.07s/it, est. speed input: 1650.29 toks/s, output: 42.46 toks/s]
[2025-01-10 22:15:13,831][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.76it/s, est. speed input: 4012.51 toks/s, output: 120.06 toks/s]
[2025-01-10 22:15:13,946][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.34it/s, est. speed input: 4627.14 toks/s, output: 146.58 toks/s]
[2025-01-10 22:15:14,064][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.98it/s, est. speed input: 5296.13 toks/s, output: 173.54 toks/s]
[2025-01-10 22:15:14,181][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.66it/s, est. speed input: 5817.69 toks/s, output: 201.08 toks/s]
[2025-01-10 22:15:14,461][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  4.30it/s, est. speed input: 6055.34 toks/s, output: 221.77 toks/s]
[2025-01-10 22:15:14,620][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.73it/s, est. speed input: 6458.90 toks/s, output: 251.07 toks/s]
[2025-01-10 22:15:14,620][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.91it/s, est. speed input: 6458.90 toks/s, output: 251.07 toks/s]
[2025-01-10 22:15:14,652][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:16,577][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:09,  1.92s/it, est. speed input: 1832.33 toks/s, output: 15.07 toks/s]
[2025-01-10 22:15:16,918][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.60it/s, est. speed input: 4997.80 toks/s, output: 48.54 toks/s]
[2025-01-10 22:15:17,125][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.08it/s, est. speed input: 6248.07 toks/s, output: 71.18 toks/s]
[2025-01-10 22:15:17,358][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.51it/s, est. speed input: 7201.39 toks/s, output: 95.71 toks/s]
[2025-01-10 22:15:18,088][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.98it/s, est. speed input: 6822.46 toks/s, output: 115.53 toks/s]
[2025-01-10 22:15:18,089][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.75it/s, est. speed input: 6822.46 toks/s, output: 115.53 toks/s]
[2025-01-10 22:15:18,106][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:19,195][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.09s/it, est. speed input: 1719.70 toks/s, output: 26.65 toks/s]
[2025-01-10 22:15:19,636][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.41it/s, est. speed input: 2639.11 toks/s, output: 57.53 toks/s]
[2025-01-10 22:15:19,798][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.18it/s, est. speed input: 3553.77 toks/s, output: 93.41 toks/s]
[2025-01-10 22:15:19,984][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.85it/s, est. speed input: 4405.58 toks/s, output: 128.87 toks/s]
[2025-01-10 22:15:20,064][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.55it/s, est. speed input: 5359.19 toks/s, output: 169.60 toks/s]
[2025-01-10 22:15:20,126][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:23,500][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:33,  3.37s/it, est. speed input: 1057.20 toks/s, output: 8.60 toks/s]
[2025-01-10 22:15:23,831][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:03<00:14,  1.58s/it, est. speed input: 2050.46 toks/s, output: 20.51 toks/s]
[2025-01-10 22:15:24,092][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:04,  1.49it/s, est. speed input: 3954.28 toks/s, output: 47.40 toks/s]
[2025-01-10 22:15:24,213][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:04<00:01,  2.61it/s, est. speed input: 5948.65 toks/s, output: 78.78 toks/s]
[2025-01-10 22:15:24,450][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:04<00:01,  2.89it/s, est. speed input: 6614.94 toks/s, output: 93.67 toks/s]
[2025-01-10 22:15:24,584][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:04<00:00,  3.45it/s, est. speed input: 7412.59 toks/s, output: 111.48 toks/s]
[2025-01-10 22:15:24,878][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:04<00:00,  4.38it/s, est. speed input: 8823.19 toks/s, output: 148.77 toks/s]
[2025-01-10 22:15:24,959][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.28it/s, est. speed input: 9602.55 toks/s, output: 170.92 toks/s]
[2025-01-10 22:15:24,998][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:27,312][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:23,  2.31s/it, est. speed input: 805.43 toks/s, output: 18.16 toks/s]
[2025-01-10 22:15:27,680][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:10,  1.17s/it, est. speed input: 1628.17 toks/s, output: 39.17 toks/s]
[2025-01-10 22:15:27,781][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:02<00:03,  2.14it/s, est. speed input: 3212.72 toks/s, output: 85.54 toks/s]
[2025-01-10 22:15:28,117][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:03<00:02,  2.34it/s, est. speed input: 3612.10 toks/s, output: 104.87 toks/s]
[2025-01-10 22:15:28,272][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  4.69it/s, est. speed input: 5775.28 toks/s, output: 186.02 toks/s]
[2025-01-10 22:15:28,572][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  5.24it/s, est. speed input: 6688.16 toks/s, output: 232.85 toks/s]
[2025-01-10 22:15:28,783][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  5.13it/s, est. speed input: 6992.27 toks/s, output: 256.03 toks/s]
[2025-01-10 22:15:28,784][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  2.91it/s, est. speed input: 6992.27 toks/s, output: 256.03 toks/s]
[2025-01-10 22:15:28,812][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:30,590][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:07,  1.78s/it, est. speed input: 2157.53 toks/s, output: 16.31 toks/s]
[2025-01-10 22:15:30,871][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.77it/s, est. speed input: 5887.04 toks/s, output: 51.49 toks/s]
[2025-01-10 22:15:31,118][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  2.20it/s, est. speed input: 7192.14 toks/s, output: 74.57 toks/s]
[2025-01-10 22:15:31,638][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.10it/s, est. speed input: 7428.75 toks/s, output: 98.03 toks/s]
[2025-01-10 22:15:31,638][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.77it/s, est. speed input: 7428.75 toks/s, output: 98.03 toks/s]
[2025-01-10 22:15:31,657][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:32,797][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.14s/it, est. speed input: 1736.41 toks/s, output: 25.43 toks/s]
[2025-01-10 22:15:33,210][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.40it/s, est. speed input: 2763.53 toks/s, output: 55.39 toks/s]
[2025-01-10 22:15:33,365][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.18it/s, est. speed input: 5180.23 toks/s, output: 127.06 toks/s]
[2025-01-10 22:15:33,774][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.91it/s, est. speed input: 5309.06 toks/s, output: 149.24 toks/s]
[2025-01-10 22:15:33,774][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.36it/s, est. speed input: 5309.06 toks/s, output: 149.24 toks/s]
[2025-01-10 22:15:33,839][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:37,484][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:36,  3.64s/it, est. speed input: 1061.81 toks/s, output: 7.96 toks/s]
[2025-01-10 22:15:37,818][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:03<00:15,  1.70s/it, est. speed input: 2067.68 toks/s, output: 19.10 toks/s]
[2025-01-10 22:15:38,132][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:04<00:05,  1.37it/s, est. speed input: 4019.02 toks/s, output: 44.26 toks/s]
[2025-01-10 22:15:38,338][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:04<00:02,  2.30it/s, est. speed input: 5952.99 toks/s, output: 74.47 toks/s]
[2025-01-10 22:15:38,793][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:04<00:01,  2.85it/s, est. speed input: 7314.82 toks/s, output: 105.56 toks/s]
[2025-01-10 22:15:38,942][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:05<00:00,  3.27it/s, est. speed input: 8045.13 toks/s, output: 125.43 toks/s]
[2025-01-10 22:15:39,397][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  3.63it/s, est. speed input: 9049.45 toks/s, output: 164.64 toks/s]
[2025-01-10 22:15:39,397][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  1.98it/s, est. speed input: 9049.45 toks/s, output: 164.64 toks/s]
[2025-01-10 22:15:39,440][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:41,965][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:25,  2.53s/it, est. speed input: 788.52 toks/s, output: 18.61 toks/s]
[2025-01-10 22:15:42,301][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:11,  1.24s/it, est. speed input: 1622.39 toks/s, output: 39.49 toks/s]
[2025-01-10 22:15:42,437][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:02<00:03,  1.98it/s, est. speed input: 3223.07 toks/s, output: 85.08 toks/s]
[2025-01-10 22:15:42,589][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:03<00:01,  3.31it/s, est. speed input: 4795.02 toks/s, output: 132.41 toks/s]
[2025-01-10 22:15:42,879][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  4.19it/s, est. speed input: 5818.96 toks/s, output: 176.49 toks/s]
[2025-01-10 22:15:43,306][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  3.56it/s, est. speed input: 5886.33 toks/s, output: 190.90 toks/s]
[2025-01-10 22:15:43,612][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  4.33it/s, est. speed input: 6798.89 toks/s, output: 246.15 toks/s]
[2025-01-10 22:15:43,612][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.64it/s, est. speed input: 6798.89 toks/s, output: 246.15 toks/s]
[2025-01-10 22:15:43,643][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:45,537][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:07,  1.89s/it, est. speed input: 2188.64 toks/s, output: 15.31 toks/s]
[2025-01-10 22:15:46,070][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.45it/s, est. speed input: 5430.67 toks/s, output: 50.68 toks/s]
[2025-01-10 22:15:46,359][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  1.82it/s, est. speed input: 6612.47 toks/s, output: 76.95 toks/s]
[2025-01-10 22:15:46,492][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.42it/s, est. speed input: 7950.92 toks/s, output: 107.03 toks/s]
[2025-01-10 22:15:46,493][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.75it/s, est. speed input: 7950.92 toks/s, output: 107.03 toks/s]
[2025-01-10 22:15:46,512][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:47,692][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.18s/it, est. speed input: 1775.27 toks/s, output: 24.57 toks/s]
[2025-01-10 22:15:48,016][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.48it/s, est. speed input: 3024.44 toks/s, output: 53.19 toks/s]
[2025-01-10 22:15:48,149][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.34it/s, est. speed input: 4164.41 toks/s, output: 85.54 toks/s]
[2025-01-10 22:15:48,509][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.49it/s, est. speed input: 4701.96 toks/s, output: 113.68 toks/s]
[2025-01-10 22:15:49,156][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.04it/s, est. speed input: 4526.63 toks/s, output: 137.28 toks/s]
[2025-01-10 22:15:49,157][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.89it/s, est. speed input: 4526.63 toks/s, output: 137.28 toks/s]
[2025-01-10 22:15:49,225][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:53,135][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:39,  3.91s/it, est. speed input: 1068.79 toks/s, output: 7.42 toks/s]
[2025-01-10 22:15:53,416][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:04<00:15,  1.77s/it, est. speed input: 2123.27 toks/s, output: 17.42 toks/s]
[2025-01-10 22:15:53,697][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:04<00:05,  1.33it/s, est. speed input: 4111.10 toks/s, output: 40.26 toks/s]
[2025-01-10 22:15:53,851][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:04<00:01,  2.86it/s, est. speed input: 7256.24 toks/s, output: 81.29 toks/s]
[2025-01-10 22:15:54,151][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:04<00:00,  3.58it/s, est. speed input: 8948.20 toks/s, output: 112.27 toks/s]
[2025-01-10 22:15:54,326][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  4.65it/s, est. speed input: 10640.50 toks/s, output: 146.25 toks/s]
[2025-01-10 22:15:54,327][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  2.16it/s, est. speed input: 10640.50 toks/s, output: 146.25 toks/s]
[2025-01-10 22:15:54,372][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:15:57,014][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:26,  2.64s/it, est. speed input: 804.02 toks/s, output: 17.41 toks/s]
[2025-01-10 22:15:57,264][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:11,  1.23s/it, est. speed input: 1703.86 toks/s, output: 36.66 toks/s]
[2025-01-10 22:15:57,382][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:03,  2.01it/s, est. speed input: 3365.03 toks/s, output: 79.08 toks/s]
[2025-01-10 22:15:57,585][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:03<00:01,  3.22it/s, est. speed input: 4841.83 toks/s, output: 119.51 toks/s]
[2025-01-10 22:15:58,157][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:03<00:01,  2.66it/s, est. speed input: 4902.25 toks/s, output: 131.30 toks/s]
[2025-01-10 22:15:58,290][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  4.05it/s, est. speed input: 6214.04 toks/s, output: 187.31 toks/s]
[2025-01-10 22:15:59,068][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:04<00:00,  2.71it/s, est. speed input: 5829.69 toks/s, output: 194.44 toks/s]
[2025-01-10 22:15:59,346][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.89it/s, est. speed input: 6097.65 toks/s, output: 223.78 toks/s]
[2025-01-10 22:15:59,346][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.21it/s, est. speed input: 6097.65 toks/s, output: 223.78 toks/s]
[2025-01-10 22:15:59,378][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:01,391][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:08,  2.01s/it, est. speed input: 2209.70 toks/s, output: 14.41 toks/s]
[2025-01-10 22:16:01,733][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.54it/s, est. speed input: 5961.69 toks/s, output: 46.72 toks/s]
[2025-01-10 22:16:01,927][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  2.04it/s, est. speed input: 7533.40 toks/s, output: 69.07 toks/s]
[2025-01-10 22:16:02,460][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.98it/s, est. speed input: 7899.18 toks/s, output: 91.50 toks/s]
[2025-01-10 22:16:02,460][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.62it/s, est. speed input: 7899.18 toks/s, output: 91.50 toks/s]
[2025-01-10 22:16:02,486][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:03,932][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.45s/it, est. speed input: 1524.16 toks/s, output: 20.05 toks/s]
[2025-01-10 22:16:04,603][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:02<00:03,  1.01it/s, est. speed input: 2262.16 toks/s, output: 46.29 toks/s]
[2025-01-10 22:16:04,751][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.65it/s, est. speed input: 3177.03 toks/s, output: 78.14 toks/s]
[2025-01-10 22:16:04,936][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  3.18it/s, est. speed input: 5364.49 toks/s, output: 144.53 toks/s]
[2025-01-10 22:16:05,174][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.41it/s, est. speed input: 5928.41 toks/s, output: 172.62 toks/s]
[2025-01-10 22:16:05,175][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.23it/s, est. speed input: 5928.41 toks/s, output: 172.62 toks/s]
[2025-01-10 22:16:05,242][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:09,088][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:03<00:34,  3.85s/it, est. speed input: 1165.46 toks/s, output: 7.54 toks/s]
[2025-01-10 22:16:09,424][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:04<00:14,  1.78s/it, est. speed input: 2277.55 toks/s, output: 18.17 toks/s]
[2025-01-10 22:16:09,784][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:04<00:02,  1.69it/s, est. speed input: 5598.56 toks/s, output: 52.40 toks/s]
[2025-01-10 22:16:09,937][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:04<00:01,  2.08it/s, est. speed input: 6608.89 toks/s, output: 67.09 toks/s]
[2025-01-10 22:16:10,209][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:04<00:00,  3.61it/s, est. speed input: 9590.19 toks/s, output: 115.98 toks/s]
[2025-01-10 22:16:10,902][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:05<00:00,  2.78it/s, est. speed input: 9323.83 toks/s, output: 127.92 toks/s]
[2025-01-10 22:16:10,902][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:05<00:00,  1.77it/s, est. speed input: 9323.83 toks/s, output: 127.92 toks/s]
[2025-01-10 22:16:10,947][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:13,491][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:22,  2.54s/it, est. speed input: 886.76 toks/s, output: 17.69 toks/s]
[2025-01-10 22:16:13,794][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:09,  1.23s/it, est. speed input: 1731.21 toks/s, output: 37.93 toks/s]
[2025-01-10 22:16:13,983][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:03<00:05,  1.33it/s, est. speed input: 2594.92 toks/s, output: 59.95 toks/s]
[2025-01-10 22:16:14,493][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.33it/s, est. speed input: 5473.92 toks/s, output: 146.08 toks/s]
[2025-01-10 22:16:14,611][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  3.80it/s, est. speed input: 6170.23 toks/s, output: 173.03 toks/s]
[2025-01-10 22:16:15,394][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  3.22it/s, est. speed input: 6552.29 toks/s, output: 209.33 toks/s]
[2025-01-10 22:16:15,394][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.25it/s, est. speed input: 6552.29 toks/s, output: 209.33 toks/s]
[2025-01-10 22:16:15,435][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:17,895][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:02<00:12,  2.46s/it, est. speed input: 1931.88 toks/s, output: 11.79 toks/s]
[2025-01-10 22:16:18,375][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:02,  1.23it/s, est. speed input: 5094.29 toks/s, output: 40.48 toks/s]
[2025-01-10 22:16:18,613][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:03<00:01,  1.62it/s, est. speed input: 6452.50 toks/s, output: 61.69 toks/s]
[2025-01-10 22:16:19,002][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  2.46it/s, est. speed input: 8782.77 toks/s, output: 106.83 toks/s]
[2025-01-10 22:16:19,002][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.68it/s, est. speed input: 8782.77 toks/s, output: 106.83 toks/s]
[2025-01-10 22:16:19,025][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:20,313][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.29s/it, est. speed input: 1795.61 toks/s, output: 22.51 toks/s]
[2025-01-10 22:16:20,977][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.09it/s, est. speed input: 2718.35 toks/s, output: 52.78 toks/s]
[2025-01-10 22:16:21,074][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.44it/s, est. speed input: 6600.85 toks/s, output: 165.48 toks/s]
[2025-01-10 22:16:21,152][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:25,809][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:04<00:46,  4.66s/it, est. speed input: 1027.61 toks/s, output: 6.23 toks/s]
[2025-01-10 22:16:26,149][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:04<00:10,  1.33s/it, est. speed input: 3201.10 toks/s, output: 21.02 toks/s]
[2025-01-10 22:16:26,306][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:05<00:04,  1.45it/s, est. speed input: 5225.80 toks/s, output: 40.75 toks/s]
[2025-01-10 22:16:26,444][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:05<00:02,  1.85it/s, est. speed input: 6165.97 toks/s, output: 51.78 toks/s]
[2025-01-10 22:16:26,718][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:05<00:01,  2.74it/s, est. speed input: 7902.98 toks/s, output: 75.64 toks/s]
[2025-01-10 22:16:26,943][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:05<00:00,  3.73it/s, est. speed input: 9634.36 toks/s, output: 103.97 toks/s]
[2025-01-10 22:16:27,291][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  3.50it/s, est. speed input: 10058.35 toks/s, output: 118.27 toks/s]
[2025-01-10 22:16:27,291][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  1.79it/s, est. speed input: 10058.35 toks/s, output: 118.27 toks/s]
[2025-01-10 22:16:27,343][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:30,215][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:28,  2.87s/it, est. speed input: 831.01 toks/s, output: 15.67 toks/s]
[2025-01-10 22:16:30,756][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:03<00:13,  1.50s/it, est. speed input: 1526.24 toks/s, output: 35.16 toks/s]
[2025-01-10 22:16:31,030][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:04,  1.55it/s, est. speed input: 3033.07 toks/s, output: 77.56 toks/s]
[2025-01-10 22:16:31,474][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:04<00:03,  1.71it/s, est. speed input: 3530.17 toks/s, output: 97.55 toks/s]
[2025-01-10 22:16:31,654][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:04<00:00,  3.51it/s, est. speed input: 5616.26 toks/s, output: 177.89 toks/s]
[2025-01-10 22:16:31,890][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:04<00:00,  3.64it/s, est. speed input: 6085.26 toks/s, output: 200.55 toks/s]
[2025-01-10 22:16:32,078][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  4.85it/s, est. speed input: 7243.97 toks/s, output: 257.44 toks/s]
[2025-01-10 22:16:32,078][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.32it/s, est. speed input: 7243.97 toks/s, output: 257.44 toks/s]
[2025-01-10 22:16:32,114][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:34,360][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:08,  2.25s/it, est. speed input: 2253.08 toks/s, output: 12.91 toks/s]
[2025-01-10 22:16:34,852][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.31it/s, est. speed input: 5818.48 toks/s, output: 43.84 toks/s]
[2025-01-10 22:16:35,422][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:03<00:00,  1.43it/s, est. speed input: 6591.90 toks/s, output: 67.42 toks/s]
[2025-01-10 22:16:35,450][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.50it/s, est. speed input: 8301.05 toks/s, output: 98.34 toks/s]
[2025-01-10 22:16:35,473][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:36,814][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.34s/it, est. speed input: 1806.76 toks/s, output: 21.63 toks/s]
[2025-01-10 22:16:37,286][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.20it/s, est. speed input: 2839.03 toks/s, output: 49.64 toks/s]
[2025-01-10 22:16:37,565][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.73it/s, est. speed input: 3843.64 toks/s, output: 81.25 toks/s]
[2025-01-10 22:16:38,124][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.44it/s, est. speed input: 5384.69 toks/s, output: 142.61 toks/s]
[2025-01-10 22:16:38,124][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.89it/s, est. speed input: 5384.69 toks/s, output: 142.61 toks/s]
[2025-01-10 22:16:38,206][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:42,927][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:04<00:47,  4.72s/it, est. speed input: 1079.12 toks/s, output: 6.14 toks/s]
[2025-01-10 22:16:43,276][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:05<00:19,  2.15s/it, est. speed input: 2118.47 toks/s, output: 14.99 toks/s]
[2025-01-10 22:16:43,467][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:05<00:10,  1.26s/it, est. speed input: 3124.68 toks/s, output: 25.28 toks/s]
[2025-01-10 22:16:43,610][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:05<00:03,  1.71it/s, est. speed input: 5208.23 toks/s, output: 48.12 toks/s]
[2025-01-10 22:16:43,835][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:05<00:01,  2.68it/s, est. speed input: 7174.63 toks/s, output: 72.14 toks/s]
[2025-01-10 22:16:44,008][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:05<00:00,  4.56it/s, est. speed input: 10194.19 toks/s, output: 113.59 toks/s]
[2025-01-10 22:16:44,357][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  1.79it/s, est. speed input: 10656.45 toks/s, output: 126.01 toks/s]
[2025-01-10 22:16:44,435][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:45,895][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:04,  1.46s/it, est. speed input: 1725.26 toks/s, output: 30.15 toks/s]
[2025-01-10 22:16:46,557][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:02<00:01,  1.01it/s, est. speed input: 2765.68 toks/s, output: 62.68 toks/s]
[2025-01-10 22:16:47,103][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.85it/s, est. speed input: 4960.50 toks/s, output: 133.42 toks/s]
[2025-01-10 22:16:47,104][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.50it/s, est. speed input: 4960.50 toks/s, output: 133.42 toks/s]
[2025-01-10 22:16:47,142][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:49,502][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:09,  2.36s/it, est. speed input: 2275.20 toks/s, output: 12.29 toks/s]
[2025-01-10 22:16:49,980][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.27it/s, est. speed input: 5948.33 toks/s, output: 41.93 toks/s]
[2025-01-10 22:16:50,244][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:03<00:00,  1.65it/s, est. speed input: 7459.14 toks/s, output: 64.14 toks/s]
[2025-01-10 22:16:50,339][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.56it/s, est. speed input: 9196.06 toks/s, output: 89.47 toks/s]
[2025-01-10 22:16:50,409][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:52,352][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it, est. speed input: 2780.12 toks/s, output: 14.92 toks/s]
[2025-01-10 22:16:52,622][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:02<00:01,  1.04it/s, est. speed input: 5141.87 toks/s, output: 34.35 toks/s]
[2025-01-10 22:16:52,969][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:02<00:00,  1.47it/s, est. speed input: 6842.98 toks/s, output: 57.80 toks/s]
[2025-01-10 22:16:53,866][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.31it/s, est. speed input: 6874.40 toks/s, output: 83.01 toks/s]
[2025-01-10 22:16:53,867][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.16it/s, est. speed input: 6874.40 toks/s, output: 83.01 toks/s]
[2025-01-10 22:16:53,899][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:55,325][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 2641.72 toks/s, output: 62.38 toks/s]
[2025-01-10 22:16:55,326][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 2641.72 toks/s, output: 62.38 toks/s]
[2025-01-10 22:16:55,335][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:16:57,812][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 2622.82 toks/s, output: 61.77 toks/s]
[2025-01-10 22:16:57,812][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 2622.82 toks/s, output: 61.77 toks/s]
[2025-01-10 22:16:58,105][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-10 22:16:58,159][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-10 22:17:00,481][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.32s/it]
[2025-01-10 22:17:02,297][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.02s/it]
[2025-01-10 22:17:02,473][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.18s/it]
[2025-01-10 22:17:02,644][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:04<00:00,  1.28it/s]
[2025-01-10 22:17:02,644][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:04<00:00,  1.12s/it]
[2025-01-10 22:17:02,929][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
[2025-01-10 22:17:09,855][root][ERROR] - Error executing job with overrides: ['matches.dond_game_args.rounds_per_game=16', 'matches.run_matches_args.log_func_args.training_data_func=set_discounted_advalign_returns']
[2025-01-10 22:17:09,860][root][ERROR] - Traceback (most recent call last):
[2025-01-10 22:17:09,860][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/run.py", line 42, in main
    globals()[cfg.experiment.method](cfg)
[2025-01-10 22:17:09,860][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/experiments/dond_run_train.py", line 139, in dond_run_train
    train_main(
[2025-01-10 22:17:09,860][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/train_main.py", line 20, in train_main
    globals()[train_func](hf_model, paths, train_func_args, output_path=output_path)
[2025-01-10 22:17:09,860][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/train_main.py", line 40, in train_ppo_main
    ppo_train(model=hf_model.hf_model,
[2025-01-10 22:17:09,861][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/ppo_train.py", line 132, in ppo_train
    model_accelerator.backward(loss)
[2025-01-10 22:17:09,861][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
[2025-01-10 22:17:09,861][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
[2025-01-10 22:17:09,861][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
[2025-01-10 22:17:09,861][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-01-10 22:17:09,861][root][ERROR] - RuntimeError: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0
[2025-01-10 22:17:09,861][root][ERROR] - Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/mila/d/dereck.piche/llm_negotiation/src/run.py:14: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="default")
WARNING 01-10 22:33:04 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-10 22:33:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-10 22:33:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-10 22:33:06 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-10 22:33:06,510][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-10 22:33:14,278][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:23,  7.77s/it]
[2025-01-10 22:33:15,019][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:07,  3.63s/it]
[2025-01-10 22:33:22,574][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:16<00:05,  5.42s/it]
[2025-01-10 22:33:30,067][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:23<00:00,  6.24s/it]
[2025-01-10 22:33:30,067][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:23<00:00,  5.89s/it]
INFO 01-10 22:33:30 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-10 22:33:45 gpu_executor.py:122] # GPU blocks: 19859, # CPU blocks: 2048
INFO 01-10 22:33:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-10 22:33:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-10 22:34:05 model_runner.py:1335] Graph capturing finished in 19 secs.
[2025-01-10 22:34:05,680][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:07,039][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:01<00:20,  1.36s/it, est. speed input: 371.75 toks/s, output: 41.22 toks/s]
[2025-01-10 22:34:07,180][root][ERROR] - Processed prompts:  25%|##5       | 4/16 [00:01<00:03,  3.39it/s, est. speed input: 1347.14 toks/s, output: 161.39 toks/s]
[2025-01-10 22:34:07,454][root][ERROR] - Processed prompts:  38%|###7      | 6/16 [00:01<00:02,  4.42it/s, est. speed input: 1708.92 toks/s, output: 225.03 toks/s]
[2025-01-10 22:34:07,601][root][ERROR] - Processed prompts:  56%|#####6    | 9/16 [00:01<00:00,  7.11it/s, est. speed input: 2367.23 toks/s, output: 345.32 toks/s]
[2025-01-10 22:34:07,796][root][ERROR] - Processed prompts:  81%|########1 | 13/16 [00:02<00:00, 10.35it/s, est. speed input: 3103.31 toks/s, output: 499.17 toks/s]
[2025-01-10 22:34:08,396][root][ERROR] - Processed prompts:  94%|#########3| 15/16 [00:02<00:00,  6.73it/s, est. speed input: 2789.40 toks/s, output: 488.65 toks/s]
[2025-01-10 22:34:09,030][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.78it/s, est. speed input: 2412.79 toks/s, output: 455.98 toks/s]
[2025-01-10 22:34:09,033][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:09,508][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  2.11it/s, est. speed input: 1277.83 toks/s, output: 61.15 toks/s]
[2025-01-10 22:34:09,508][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  4.21it/s, est. speed input: 2847.44 toks/s, output: 122.24 toks/s]
[2025-01-10 22:34:09,523][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:10,457][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:00<00:12,  1.07it/s, est. speed input: 748.19 toks/s, output: 24.62 toks/s]
[2025-01-10 22:34:10,949][root][ERROR] - Processed prompts:  29%|##8       | 4/14 [00:01<00:03,  3.23it/s, est. speed input: 1961.12 toks/s, output: 93.99 toks/s]
[2025-01-10 22:34:11,228][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:01<00:01,  5.17it/s, est. speed input: 2870.05 toks/s, output: 190.63 toks/s]
[2025-01-10 22:34:11,345][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:01<00:00,  6.77it/s, est. speed input: 3453.43 toks/s, output: 264.59 toks/s]
[2025-01-10 22:34:11,446][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:01<00:00,  8.62it/s, est. speed input: 3999.56 toks/s, output: 341.75 toks/s]
[2025-01-10 22:34:12,242][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:02<00:00,  5.63it/s, est. speed input: 3599.02 toks/s, output: 362.62 toks/s]
[2025-01-10 22:34:12,242][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:02<00:00,  5.15it/s, est. speed input: 3599.02 toks/s, output: 362.62 toks/s]
[2025-01-10 22:34:12,252][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:13,567][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.31s/it, est. speed input: 494.46 toks/s, output: 51.73 toks/s]
[2025-01-10 22:34:13,725][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:01,  2.50it/s, est. speed input: 1347.05 toks/s, output: 145.97 toks/s]
[2025-01-10 22:34:14,202][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  3.97it/s, est. speed input: 2072.31 toks/s, output: 253.91 toks/s]
[2025-01-10 22:34:14,479][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.88it/s, est. speed input: 2145.08 toks/s, output: 282.45 toks/s]
[2025-01-10 22:34:14,532][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.51it/s, est. speed input: 2388.62 toks/s, output: 336.47 toks/s]
[2025-01-10 22:34:14,542][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:15,344][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.25it/s, est. speed input: 1045.59 toks/s, output: 36.14 toks/s]
[2025-01-10 22:34:15,899][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  5.11it/s, est. speed input: 3622.79 toks/s, output: 157.74 toks/s]
[2025-01-10 22:34:16,071][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  5.24it/s, est. speed input: 3672.51 toks/s, output: 193.63 toks/s]
[2025-01-10 22:34:16,828][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.15it/s, est. speed input: 2761.56 toks/s, output: 190.72 toks/s]
[2025-01-10 22:34:16,828][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.50it/s, est. speed input: 2761.56 toks/s, output: 190.72 toks/s]
[2025-01-10 22:34:16,838][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:17,603][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.31it/s, est. speed input: 1034.59 toks/s, output: 37.93 toks/s]
[2025-01-10 22:34:18,034][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:01,  2.79it/s, est. speed input: 1942.92 toks/s, output: 97.86 toks/s]
[2025-01-10 22:34:18,244][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.39it/s, est. speed input: 2596.19 toks/s, output: 179.29 toks/s]
[2025-01-10 22:34:18,631][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  4.69it/s, est. speed input: 2788.27 toks/s, output: 242.58 toks/s]
[2025-01-10 22:34:18,919][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.34it/s, est. speed input: 2729.49 toks/s, output: 269.15 toks/s]
[2025-01-10 22:34:18,919][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.84it/s, est. speed input: 2729.49 toks/s, output: 269.15 toks/s]
[2025-01-10 22:34:18,932][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:20,267][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.34s/it, est. speed input: 668.81 toks/s, output: 42.69 toks/s]
[2025-01-10 22:34:20,655][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.29it/s, est. speed input: 1129.42 toks/s, output: 81.25 toks/s]
[2025-01-10 22:34:20,773][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.97it/s, est. speed input: 3305.43 toks/s, output: 268.30 toks/s]
[2025-01-10 22:34:21,894][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.04it/s, est. speed input: 2761.51 toks/s, output: 257.89 toks/s]
[2025-01-10 22:34:21,895][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.70it/s, est. speed input: 2761.51 toks/s, output: 257.89 toks/s]
[2025-01-10 22:34:21,906][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:22,995][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.09s/it, est. speed input: 656.75 toks/s, output: 43.17 toks/s]
[2025-01-10 22:34:23,378][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.48it/s, est. speed input: 1032.10 toks/s, output: 81.53 toks/s]
[2025-01-10 22:34:23,569][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.22it/s, est. speed input: 1900.39 toks/s, output: 170.85 toks/s]
[2025-01-10 22:34:24,156][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.52it/s, est. speed input: 1798.66 toks/s, output: 182.75 toks/s]
[2025-01-10 22:34:24,256][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.27it/s, est. speed input: 2103.78 toks/s, output: 232.00 toks/s]
[2025-01-10 22:34:24,428][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.93it/s, est. speed input: 2675.62 toks/s, output: 329.24 toks/s]
[2025-01-10 22:34:24,428][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.17it/s, est. speed input: 2675.62 toks/s, output: 329.24 toks/s]
[2025-01-10 22:34:24,441][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:25,405][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:06,  1.04it/s, est. speed input: 1119.46 toks/s, output: 30.09 toks/s]
[2025-01-10 22:34:26,127][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.36it/s, est. speed input: 3338.91 toks/s, output: 115.64 toks/s]
[2025-01-10 22:34:26,266][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  4.74it/s, est. speed input: 4303.47 toks/s, output: 201.66 toks/s]
[2025-01-10 22:34:26,753][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.71it/s, est. speed input: 3852.75 toks/s, output: 213.66 toks/s]
[2025-01-10 22:34:26,753][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.46it/s, est. speed input: 3852.75 toks/s, output: 213.66 toks/s]
[2025-01-10 22:34:26,765][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:27,592][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:05,  1.21it/s, est. speed input: 1164.70 toks/s, output: 35.07 toks/s]
[2025-01-10 22:34:28,090][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.58it/s, est. speed input: 1347.53 toks/s, output: 69.41 toks/s]
[2025-01-10 22:34:28,296][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.28it/s, est. speed input: 1741.64 toks/s, output: 110.36 toks/s]
[2025-01-10 22:34:28,567][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.75it/s, est. speed input: 2442.85 toks/s, output: 191.41 toks/s]
[2025-01-10 22:34:28,794][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.92it/s, est. speed input: 2573.97 toks/s, output: 225.26 toks/s]
[2025-01-10 22:34:29,017][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.25it/s, est. speed input: 3067.40 toks/s, output: 312.11 toks/s]
[2025-01-10 22:34:29,018][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.55it/s, est. speed input: 3067.40 toks/s, output: 312.11 toks/s]
[2025-01-10 22:34:29,033][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:30,550][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.52s/it, est. speed input: 901.26 toks/s, output: 36.26 toks/s]
[2025-01-10 22:34:31,134][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:03,  1.64it/s, est. speed input: 1899.89 toks/s, output: 97.59 toks/s]
[2025-01-10 22:34:31,386][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.77it/s, est. speed input: 2896.48 toks/s, output: 174.68 toks/s]
[2025-01-10 22:34:31,767][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.73it/s, est. speed input: 3046.40 toks/s, output: 200.09 toks/s]
[2025-01-10 22:34:32,283][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:03<00:00,  2.45it/s, est. speed input: 2998.73 toks/s, output: 222.17 toks/s]
[2025-01-10 22:34:32,415][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  3.03it/s, est. speed input: 3300.28 toks/s, output: 268.20 toks/s]
[2025-01-10 22:34:32,415][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.37it/s, est. speed input: 3300.28 toks/s, output: 268.20 toks/s]
[2025-01-10 22:34:32,429][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:33,623][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.19s/it, est. speed input: 710.60 toks/s, output: 39.38 toks/s]
[2025-01-10 22:34:33,981][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.42it/s, est. speed input: 1232.42 toks/s, output: 76.06 toks/s]
[2025-01-10 22:34:34,143][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.20it/s, est. speed input: 2934.69 toks/s, output: 203.62 toks/s]
[2025-01-10 22:34:34,345][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.37it/s, est. speed input: 3150.37 toks/s, output: 232.30 toks/s]
[2025-01-10 22:34:34,556][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.65it/s, est. speed input: 3874.10 toks/s, output: 307.99 toks/s]
[2025-01-10 22:34:34,556][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.76it/s, est. speed input: 3874.10 toks/s, output: 307.99 toks/s]
[2025-01-10 22:34:34,572][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:35,712][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.14s/it, est. speed input: 1213.30 toks/s, output: 25.46 toks/s]
[2025-01-10 22:34:36,264][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.00it/s, est. speed input: 2539.47 toks/s, output: 72.72 toks/s]
[2025-01-10 22:34:36,442][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.46it/s, est. speed input: 3878.37 toks/s, output: 142.81 toks/s]
[2025-01-10 22:34:36,647][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.76it/s, est. speed input: 4180.54 toks/s, output: 172.54 toks/s]
[2025-01-10 22:34:36,952][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.61it/s, est. speed input: 4284.51 toks/s, output: 198.40 toks/s]
[2025-01-10 22:34:37,150][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.93it/s, est. speed input: 4510.47 toks/s, output: 233.21 toks/s]
[2025-01-10 22:34:37,150][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.10it/s, est. speed input: 4510.47 toks/s, output: 233.21 toks/s]
[2025-01-10 22:34:37,164][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:38,088][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:06,  1.08it/s, est. speed input: 1165.97 toks/s, output: 31.37 toks/s]
[2025-01-10 22:34:38,312][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.95it/s, est. speed input: 1785.69 toks/s, output: 63.59 toks/s]
[2025-01-10 22:34:38,860][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  1.89it/s, est. speed input: 1824.05 toks/s, output: 90.79 toks/s]
[2025-01-10 22:34:39,152][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.21it/s, est. speed input: 2600.36 toks/s, output: 170.51 toks/s]
[2025-01-10 22:34:39,310][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.74it/s, est. speed input: 2921.44 toks/s, output: 210.17 toks/s]
[2025-01-10 22:34:39,957][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  2.64it/s, est. speed input: 2606.72 toks/s, output: 219.13 toks/s]
[2025-01-10 22:34:40,128][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.15it/s, est. speed input: 2791.85 toks/s, output: 265.15 toks/s]
[2025-01-10 22:34:40,128][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.70it/s, est. speed input: 2791.85 toks/s, output: 265.15 toks/s]
[2025-01-10 22:34:40,149][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:41,984][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.84s/it, est. speed input: 923.89 toks/s, output: 33.77 toks/s]
[2025-01-10 22:34:42,113][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  1.91it/s, est. speed input: 2584.73 toks/s, output: 100.82 toks/s]
[2025-01-10 22:34:42,251][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.45it/s, est. speed input: 4223.16 toks/s, output: 167.40 toks/s]
[2025-01-10 22:34:42,496][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.57it/s, est. speed input: 5333.60 toks/s, output: 230.10 toks/s]
[2025-01-10 22:34:42,760][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.06it/s, est. speed input: 5469.35 toks/s, output: 251.18 toks/s]
[2025-01-10 22:34:42,776][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:43,909][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.13s/it, est. speed input: 866.19 toks/s, output: 32.67 toks/s]
[2025-01-10 22:34:44,348][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.38it/s, est. speed input: 1397.04 toks/s, output: 65.53 toks/s]
[2025-01-10 22:34:44,841][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:01,  2.35it/s, est. speed input: 2239.54 toks/s, output: 131.71 toks/s]
[2025-01-10 22:34:44,974][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.91it/s, est. speed input: 3233.26 toks/s, output: 218.86 toks/s]
[2025-01-10 22:34:45,698][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.35it/s, est. speed input: 3272.80 toks/s, output: 258.02 toks/s]
[2025-01-10 22:34:45,699][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.74it/s, est. speed input: 3272.80 toks/s, output: 258.02 toks/s]
[2025-01-10 22:34:45,719][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:47,043][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.32s/it, est. speed input: 1272.30 toks/s, output: 21.90 toks/s]
[2025-01-10 22:34:47,624][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  2.45it/s, est. speed input: 3695.72 toks/s, output: 80.32 toks/s]
[2025-01-10 22:34:47,757][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.02it/s, est. speed input: 4390.12 toks/s, output: 111.87 toks/s]
[2025-01-10 22:34:48,201][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.54it/s, est. speed input: 5087.01 toks/s, output: 167.57 toks/s]
[2025-01-10 22:34:48,597][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.23it/s, est. speed input: 5016.66 toks/s, output: 192.45 toks/s]
[2025-01-10 22:34:48,598][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.78it/s, est. speed input: 5016.66 toks/s, output: 192.45 toks/s]
[2025-01-10 22:34:48,613][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:49,637][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:07,  1.02s/it, est. speed input: 1165.82 toks/s, output: 28.34 toks/s]
[2025-01-10 22:34:50,228][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.30it/s, est. speed input: 1520.98 toks/s, output: 60.07 toks/s]
[2025-01-10 22:34:50,365][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.08it/s, est. speed input: 2028.34 toks/s, output: 99.36 toks/s]
[2025-01-10 22:34:50,485][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  4.12it/s, est. speed input: 3191.81 toks/s, output: 182.72 toks/s]
[2025-01-10 22:34:50,720][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  4.16it/s, est. speed input: 3411.34 toks/s, output: 210.33 toks/s]
[2025-01-10 22:34:51,207][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.14it/s, est. speed input: 3752.11 toks/s, output: 263.71 toks/s]
[2025-01-10 22:34:51,207][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.08it/s, est. speed input: 3752.11 toks/s, output: 263.71 toks/s]
[2025-01-10 22:34:51,231][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:53,146][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.91s/it, est. speed input: 1061.13 toks/s, output: 28.20 toks/s]
[2025-01-10 22:34:53,456][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:03,  1.64it/s, est. speed input: 2775.09 toks/s, output: 82.24 toks/s]
[2025-01-10 22:34:53,594][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.01it/s, est. speed input: 4439.61 toks/s, output: 144.30 toks/s]
[2025-01-10 22:34:53,712][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.64it/s, est. speed input: 5130.94 toks/s, output: 173.73 toks/s]
[2025-01-10 22:34:54,162][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.95it/s, est. speed input: 5855.12 toks/s, output: 221.76 toks/s]
[2025-01-10 22:34:54,162][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.73it/s, est. speed input: 5855.12 toks/s, output: 221.76 toks/s]
[2025-01-10 22:34:54,180][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:55,552][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.37s/it, est. speed input: 804.79 toks/s, output: 34.26 toks/s]
[2025-01-10 22:34:55,874][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.33it/s, est. speed input: 1408.34 toks/s, output: 67.88 toks/s]
[2025-01-10 22:34:56,011][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.08it/s, est. speed input: 2865.79 toks/s, output: 143.62 toks/s]
[2025-01-10 22:34:56,319][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  4.10it/s, est. speed input: 3760.49 toks/s, output: 205.67 toks/s]
[2025-01-10 22:34:56,426][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.78it/s, est. speed input: 4233.17 toks/s, output: 243.14 toks/s]
[2025-01-10 22:34:56,663][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.61it/s, est. speed input: 4424.97 toks/s, output: 269.81 toks/s]
[2025-01-10 22:34:56,663][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.22it/s, est. speed input: 4424.97 toks/s, output: 269.81 toks/s]
[2025-01-10 22:34:56,686][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:34:58,034][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.35s/it, est. speed input: 1482.32 toks/s, output: 14.10 toks/s]
[2025-01-10 22:34:58,198][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.54it/s, est. speed input: 2641.10 toks/s, output: 31.76 toks/s]
[2025-01-10 22:34:58,763][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:01,  2.38it/s, est. speed input: 3994.88 toks/s, output: 67.91 toks/s]
[2025-01-10 22:34:58,954][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.75it/s, est. speed input: 5626.61 toks/s, output: 126.98 toks/s]
[2025-01-10 22:34:59,273][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.57it/s, est. speed input: 5807.52 toks/s, output: 150.38 toks/s]
[2025-01-10 22:34:59,670][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.22it/s, est. speed input: 5770.21 toks/s, output: 174.28 toks/s]
[2025-01-10 22:34:59,670][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.68it/s, est. speed input: 5770.21 toks/s, output: 174.28 toks/s]
[2025-01-10 22:34:59,684][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:00,623][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:00<00:04,  1.07it/s, est. speed input: 1393.61 toks/s, output: 30.90 toks/s]
[2025-01-10 22:35:01,216][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.36it/s, est. speed input: 1671.68 toks/s, output: 63.32 toks/s]
[2025-01-10 22:35:01,364][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.10it/s, est. speed input: 4001.95 toks/s, output: 186.97 toks/s]
[2025-01-10 22:35:02,247][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.34it/s, est. speed input: 3198.20 toks/s, output: 179.52 toks/s]
[2025-01-10 22:35:02,280][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:04,259][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.98s/it, est. speed input: 1036.78 toks/s, output: 14.65 toks/s]
[2025-01-10 22:35:04,696][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:02<00:04,  1.48it/s, est. speed input: 2810.47 toks/s, output: 46.76 toks/s]
[2025-01-10 22:35:04,915][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:03,  1.93it/s, est. speed input: 3534.74 toks/s, output: 68.68 toks/s]
[2025-01-10 22:35:05,032][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.46it/s, est. speed input: 5293.03 toks/s, output: 119.53 toks/s]
[2025-01-10 22:35:05,136][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  5.25it/s, est. speed input: 6885.62 toks/s, output: 171.20 toks/s]
[2025-01-10 22:35:05,628][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.73it/s, est. speed input: 7306.74 toks/s, output: 216.23 toks/s]
[2025-01-10 22:35:05,628][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.99it/s, est. speed input: 7306.74 toks/s, output: 216.23 toks/s]
[2025-01-10 22:35:05,651][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:07,029][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:12,  1.38s/it, est. speed input: 898.19 toks/s, output: 21.06 toks/s]
[2025-01-10 22:35:07,609][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:07,  1.10it/s, est. speed input: 1365.32 toks/s, output: 48.52 toks/s]
[2025-01-10 22:35:07,751][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.61it/s, est. speed input: 2850.54 toks/s, output: 114.34 toks/s]
[2025-01-10 22:35:08,301][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.01it/s, est. speed input: 3395.47 toks/s, output: 161.94 toks/s]
[2025-01-10 22:35:08,507][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.17it/s, est. speed input: 4317.75 toks/s, output: 233.61 toks/s]
[2025-01-10 22:35:08,811][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  3.93it/s, est. speed input: 4348.51 toks/s, output: 257.95 toks/s]
[2025-01-10 22:35:08,878][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.10it/s, est. speed input: 4715.82 toks/s, output: 300.07 toks/s]
[2025-01-10 22:35:08,898][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:10,294][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.40s/it, est. speed input: 1649.99 toks/s, output: 20.78 toks/s]
[2025-01-10 22:35:10,913][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.69it/s, est. speed input: 3618.61 toks/s, output: 64.03 toks/s]
[2025-01-10 22:35:11,119][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.18it/s, est. speed input: 4447.75 toks/s, output: 96.38 toks/s]
[2025-01-10 22:35:11,307][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.71it/s, est. speed input: 5159.33 toks/s, output: 129.96 toks/s]
[2025-01-10 22:35:11,334][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.46it/s, est. speed input: 6184.46 toks/s, output: 169.98 toks/s]
[2025-01-10 22:35:11,349][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:12,343][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:00<00:04,  1.01it/s, est. speed input: 1431.33 toks/s, output: 29.17 toks/s]
[2025-01-10 22:35:12,854][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.41it/s, est. speed input: 1998.94 toks/s, output: 60.49 toks/s]
[2025-01-10 22:35:13,221][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.63it/s, est. speed input: 3185.20 toks/s, output: 131.45 toks/s]
[2025-01-10 22:35:13,472][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.94it/s, est. speed input: 3513.03 toks/s, output: 165.78 toks/s]
[2025-01-10 22:35:13,934][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.65it/s, est. speed input: 3542.79 toks/s, output: 190.70 toks/s]
[2025-01-10 22:35:13,935][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.32it/s, est. speed input: 3542.79 toks/s, output: 190.70 toks/s]
[2025-01-10 22:35:13,971][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:16,184][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:19,  2.21s/it, est. speed input: 1063.95 toks/s, output: 13.10 toks/s]
[2025-01-10 22:35:16,651][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:09,  1.19s/it, est. speed input: 1864.53 toks/s, output: 32.09 toks/s]
[2025-01-10 22:35:16,754][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.11it/s, est. speed input: 3820.70 toks/s, output: 75.82 toks/s]
[2025-01-10 22:35:16,956][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.35it/s, est. speed input: 5525.95 toks/s, output: 117.57 toks/s]
[2025-01-10 22:35:17,090][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  4.88it/s, est. speed input: 7160.08 toks/s, output: 165.44 toks/s]
[2025-01-10 22:35:17,342][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  5.68it/s, est. speed input: 8293.30 toks/s, output: 208.82 toks/s]
[2025-01-10 22:35:17,342][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.97it/s, est. speed input: 8293.30 toks/s, output: 208.82 toks/s]
[2025-01-10 22:35:17,369][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:19,124][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.75s/it, est. speed input: 770.48 toks/s, output: 25.64 toks/s]
[2025-01-10 22:35:19,505][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:07,  1.06it/s, est. speed input: 1376.24 toks/s, output: 53.36 toks/s]
[2025-01-10 22:35:19,617][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.58it/s, est. speed input: 2921.09 toks/s, output: 117.43 toks/s]
[2025-01-10 22:35:19,852][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.93it/s, est. speed input: 3303.83 toks/s, output: 142.96 toks/s]
[2025-01-10 22:35:20,147][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.93it/s, est. speed input: 4850.19 toks/s, output: 234.66 toks/s]
[2025-01-10 22:35:20,294][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  6.37it/s, est. speed input: 5820.68 toks/s, output: 302.95 toks/s]
[2025-01-10 22:35:20,294][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.42it/s, est. speed input: 5820.68 toks/s, output: 302.95 toks/s]
[2025-01-10 22:35:20,316][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:21,842][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.53s/it, est. speed input: 1712.64 toks/s, output: 19.01 toks/s]
[2025-01-10 22:35:22,271][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  1.80it/s, est. speed input: 4218.25 toks/s, output: 59.35 toks/s]
[2025-01-10 22:35:22,389][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.48it/s, est. speed input: 5401.37 toks/s, output: 87.81 toks/s]
[2025-01-10 22:35:22,958][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.18it/s, est. speed input: 5349.51 toks/s, output: 109.77 toks/s]
[2025-01-10 22:35:23,369][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  2.26it/s, est. speed input: 5616.12 toks/s, output: 140.53 toks/s]
[2025-01-10 22:35:23,370][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.97it/s, est. speed input: 5616.12 toks/s, output: 140.53 toks/s]
[2025-01-10 22:35:23,386][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:24,448][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.06s/it, est. speed input: 1442.62 toks/s, output: 27.31 toks/s]
[2025-01-10 22:35:24,920][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.40it/s, est. speed input: 2128.50 toks/s, output: 57.39 toks/s]
[2025-01-10 22:35:25,155][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.94it/s, est. speed input: 3697.03 toks/s, output: 128.34 toks/s]
[2025-01-10 22:35:25,354][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.37it/s, est. speed input: 4181.48 toks/s, output: 161.08 toks/s]
[2025-01-10 22:35:26,120][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.27it/s, est. speed input: 3713.11 toks/s, output: 170.11 toks/s]
[2025-01-10 22:35:26,120][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.19it/s, est. speed input: 3713.11 toks/s, output: 170.11 toks/s]
[2025-01-10 22:35:26,160][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:28,594][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:21,  2.43s/it, est. speed input: 1091.96 toks/s, output: 11.91 toks/s]
[2025-01-10 22:35:29,170][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:03<00:10,  1.34s/it, est. speed input: 1890.31 toks/s, output: 30.57 toks/s]
[2025-01-10 22:35:29,272][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:03<00:03,  1.88it/s, est. speed input: 3875.40 toks/s, output: 71.97 toks/s]
[2025-01-10 22:35:29,374][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:01,  2.50it/s, est. speed input: 4756.75 toks/s, output: 93.03 toks/s]
[2025-01-10 22:35:29,492][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:03<00:01,  3.19it/s, est. speed input: 5559.04 toks/s, output: 114.34 toks/s]
[2025-01-10 22:35:29,656][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.74it/s, est. speed input: 6250.53 toks/s, output: 135.61 toks/s]
[2025-01-10 22:35:29,789][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  4.41it/s, est. speed input: 6928.20 toks/s, output: 158.75 toks/s]
[2025-01-10 22:35:29,909][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  6.70it/s, est. speed input: 8388.71 toks/s, output: 210.75 toks/s]
[2025-01-10 22:35:29,909][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.67it/s, est. speed input: 8388.71 toks/s, output: 210.75 toks/s]
[2025-01-10 22:35:29,938][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:31,977][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.04s/it, est. speed input: 724.17 toks/s, output: 26.97 toks/s]
[2025-01-10 22:35:32,170][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:07,  1.05it/s, est. speed input: 1442.79 toks/s, output: 54.66 toks/s]
[2025-01-10 22:35:32,299][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  3.32it/s, est. speed input: 3848.04 toks/s, output: 144.01 toks/s]
[2025-01-10 22:35:32,772][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.63it/s, est. speed input: 4582.31 toks/s, output: 189.47 toks/s]
[2025-01-10 22:35:33,357][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.20it/s, est. speed input: 5482.11 toks/s, output: 264.97 toks/s]
[2025-01-10 22:35:33,357][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.92it/s, est. speed input: 5482.11 toks/s, output: 264.97 toks/s]
[2025-01-10 22:35:33,382][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:35,056][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:08,  1.67s/it, est. speed input: 1744.77 toks/s, output: 17.32 toks/s]
[2025-01-10 22:35:35,530][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.64it/s, est. speed input: 4280.22 toks/s, output: 55.40 toks/s]
[2025-01-10 22:35:36,001][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:01,  1.78it/s, est. speed input: 4777.70 toks/s, output: 80.94 toks/s]
[2025-01-10 22:35:36,111][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.43it/s, est. speed input: 5840.24 toks/s, output: 114.69 toks/s]
[2025-01-10 22:35:37,424][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.43it/s, est. speed input: 4759.55 toks/s, output: 126.93 toks/s]
[2025-01-10 22:35:37,424][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.48it/s, est. speed input: 4759.55 toks/s, output: 126.93 toks/s]
[2025-01-10 22:35:37,443][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:38,572][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.13s/it, est. speed input: 1453.57 toks/s, output: 25.69 toks/s]
[2025-01-10 22:35:38,956][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.45it/s, est. speed input: 2211.69 toks/s, output: 54.22 toks/s]
[2025-01-10 22:35:39,675][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.07it/s, est. speed input: 3175.70 toks/s, output: 108.44 toks/s]
[2025-01-10 22:35:39,808][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.67it/s, est. speed input: 3788.18 toks/s, output: 149.70 toks/s]
[2025-01-10 22:35:40,099][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.87it/s, est. speed input: 4182.74 toks/s, output: 183.77 toks/s]
[2025-01-10 22:35:40,099][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.26it/s, est. speed input: 4182.74 toks/s, output: 183.77 toks/s]
[2025-01-10 22:35:40,144][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:42,816][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:24,  2.67s/it, est. speed input: 1108.55 toks/s, output: 10.86 toks/s]
[2025-01-10 22:35:43,177][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:03<00:05,  1.21it/s, est. speed input: 3205.26 toks/s, output: 35.62 toks/s]
[2025-01-10 22:35:43,567][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:02,  1.99it/s, est. speed input: 4896.07 toks/s, output: 67.49 toks/s]
[2025-01-10 22:35:43,800][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:01,  2.95it/s, est. speed input: 6548.31 toks/s, output: 108.60 toks/s]
[2025-01-10 22:35:44,050][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  3.14it/s, est. speed input: 7078.48 toks/s, output: 128.52 toks/s]
[2025-01-10 22:35:44,215][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:04<00:00,  3.57it/s, est. speed input: 7699.97 toks/s, output: 152.06 toks/s]
[2025-01-10 22:35:44,229][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.45it/s, est. speed input: 8571.30 toks/s, output: 180.43 toks/s]
[2025-01-10 22:35:44,260][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:46,192][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.93s/it, est. speed input: 834.69 toks/s, output: 21.75 toks/s]
[2025-01-10 22:35:46,548][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:08,  1.00s/it, est. speed input: 1533.45 toks/s, output: 46.34 toks/s]
[2025-01-10 22:35:47,001][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.54it/s, est. speed input: 3559.54 toks/s, output: 120.80 toks/s]
[2025-01-10 22:35:47,112][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.90it/s, est. speed input: 4903.19 toks/s, output: 185.17 toks/s]
[2025-01-10 22:35:47,422][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  4.56it/s, est. speed input: 5799.93 toks/s, output: 237.87 toks/s]
[2025-01-10 22:35:47,555][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.97it/s, est. speed input: 6216.06 toks/s, output: 268.37 toks/s]
[2025-01-10 22:35:47,555][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.04it/s, est. speed input: 6216.06 toks/s, output: 268.37 toks/s]
[2025-01-10 22:35:47,583][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:49,392][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:09,  1.81s/it, est. speed input: 1782.66 toks/s, output: 16.03 toks/s]
[2025-01-10 22:35:49,940][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:02,  1.49it/s, est. speed input: 4375.37 toks/s, output: 52.62 toks/s]
[2025-01-10 22:35:50,478][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.71it/s, est. speed input: 7396.96 toks/s, output: 128.20 toks/s]
[2025-01-10 22:35:50,478][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.07it/s, est. speed input: 7396.96 toks/s, output: 128.20 toks/s]
[2025-01-10 22:35:50,497][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:51,706][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.21s/it, est. speed input: 1453.33 toks/s, output: 24.00 toks/s]
[2025-01-10 22:35:52,159][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:03,  1.31it/s, est. speed input: 2162.68 toks/s, output: 51.76 toks/s]
[2025-01-10 22:35:52,556][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.44it/s, est. speed input: 3725.21 toks/s, output: 111.75 toks/s]
[2025-01-10 22:35:52,808][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.77it/s, est. speed input: 4343.28 toks/s, output: 144.11 toks/s]
[2025-01-10 22:35:52,941][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.44it/s, est. speed input: 4951.86 toks/s, output: 182.54 toks/s]
[2025-01-10 22:35:52,941][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.46it/s, est. speed input: 4951.86 toks/s, output: 182.54 toks/s]
[2025-01-10 22:35:52,993][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:55,899][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:26,  2.91s/it, est. speed input: 1123.25 toks/s, output: 9.98 toks/s]
[2025-01-10 22:35:56,476][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:03<00:12,  1.54s/it, est. speed input: 1996.07 toks/s, output: 26.13 toks/s]
[2025-01-10 22:35:56,580][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:03<00:03,  1.65it/s, est. speed input: 4064.82 toks/s, output: 63.01 toks/s]
[2025-01-10 22:35:56,699][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:03<00:02,  2.20it/s, est. speed input: 4933.20 toks/s, output: 81.21 toks/s]
[2025-01-10 22:35:56,962][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.30it/s, est. speed input: 6618.65 toks/s, output: 119.17 toks/s]
[2025-01-10 22:35:57,065][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:04<00:00,  3.95it/s, est. speed input: 7456.18 toks/s, output: 140.45 toks/s]
[2025-01-10 22:35:57,641][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  3.74it/s, est. speed input: 8284.89 toks/s, output: 176.40 toks/s]
[2025-01-10 22:35:57,641][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.15it/s, est. speed input: 8284.89 toks/s, output: 176.40 toks/s]
[2025-01-10 22:35:57,676][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:35:59,645][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.97s/it, est. speed input: 883.58 toks/s, output: 18.79 toks/s]
[2025-01-10 22:36:00,103][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:08,  1.08s/it, est. speed input: 1633.34 toks/s, output: 42.03 toks/s]
[2025-01-10 22:36:00,349][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.74it/s, est. speed input: 3975.35 toks/s, output: 118.95 toks/s]
[2025-01-10 22:36:00,464][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.31it/s, est. speed input: 4586.23 toks/s, output: 145.28 toks/s]
[2025-01-10 22:36:00,582][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.96it/s, est. speed input: 5250.81 toks/s, output: 172.06 toks/s]
[2025-01-10 22:36:00,700][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  4.64it/s, est. speed input: 5769.69 toks/s, output: 199.42 toks/s]
[2025-01-10 22:36:00,980][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  4.28it/s, est. speed input: 6008.41 toks/s, output: 220.05 toks/s]
[2025-01-10 22:36:01,139][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  4.71it/s, est. speed input: 6411.04 toks/s, output: 249.21 toks/s]
[2025-01-10 22:36:01,139][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.89it/s, est. speed input: 6411.04 toks/s, output: 249.21 toks/s]
[2025-01-10 22:36:01,170][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:03,120][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:09,  1.95s/it, est. speed input: 1809.63 toks/s, output: 14.88 toks/s]
[2025-01-10 22:36:03,462][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.58it/s, est. speed input: 4943.06 toks/s, output: 48.01 toks/s]
[2025-01-10 22:36:03,668][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.06it/s, est. speed input: 6185.02 toks/s, output: 70.46 toks/s]
[2025-01-10 22:36:03,902][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.49it/s, est. speed input: 7134.52 toks/s, output: 94.82 toks/s]
[2025-01-10 22:36:04,634][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.97it/s, est. speed input: 6769.13 toks/s, output: 114.62 toks/s]
[2025-01-10 22:36:04,634][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.73it/s, est. speed input: 6769.13 toks/s, output: 114.62 toks/s]
[2025-01-10 22:36:04,652][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:05,751][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.10s/it, est. speed input: 1702.64 toks/s, output: 26.39 toks/s]
[2025-01-10 22:36:06,194][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.40it/s, est. speed input: 2618.78 toks/s, output: 57.08 toks/s]
[2025-01-10 22:36:06,356][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.17it/s, est. speed input: 3528.65 toks/s, output: 92.75 toks/s]
[2025-01-10 22:36:06,542][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.83it/s, est. speed input: 4377.74 toks/s, output: 128.06 toks/s]
[2025-01-10 22:36:06,622][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.54it/s, est. speed input: 5325.51 toks/s, output: 168.53 toks/s]
[2025-01-10 22:36:06,683][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:10,104][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:34,  3.42s/it, est. speed input: 1042.86 toks/s, output: 8.48 toks/s]
[2025-01-10 22:36:10,435][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:03<00:14,  1.60s/it, est. speed input: 2024.98 toks/s, output: 20.26 toks/s]
[2025-01-10 22:36:10,696][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:04<00:04,  1.47it/s, est. speed input: 3908.42 toks/s, output: 46.85 toks/s]
[2025-01-10 22:36:10,816][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:04<00:01,  2.58it/s, est. speed input: 5882.50 toks/s, output: 77.91 toks/s]
[2025-01-10 22:36:11,053][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:04<00:01,  2.86it/s, est. speed input: 6545.41 toks/s, output: 92.68 toks/s]
[2025-01-10 22:36:11,188][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:04<00:00,  3.42it/s, est. speed input: 7336.43 toks/s, output: 110.34 toks/s]
[2025-01-10 22:36:11,483][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:04<00:00,  4.35it/s, est. speed input: 8736.43 toks/s, output: 147.30 toks/s]
[2025-01-10 22:36:11,563][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.25it/s, est. speed input: 9509.62 toks/s, output: 169.27 toks/s]
[2025-01-10 22:36:11,603][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:13,943][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:23,  2.34s/it, est. speed input: 796.21 toks/s, output: 17.95 toks/s]
[2025-01-10 22:36:14,311][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:10,  1.18s/it, est. speed input: 1612.00 toks/s, output: 38.78 toks/s]
[2025-01-10 22:36:14,412][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:02<00:03,  2.12it/s, est. speed input: 3182.07 toks/s, output: 84.72 toks/s]
[2025-01-10 22:36:14,747][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:03<00:02,  2.33it/s, est. speed input: 3582.29 toks/s, output: 104.00 toks/s]
[2025-01-10 22:36:14,903][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  4.67it/s, est. speed input: 5729.94 toks/s, output: 184.56 toks/s]
[2025-01-10 22:36:15,203][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  5.22it/s, est. speed input: 6638.65 toks/s, output: 231.13 toks/s]
[2025-01-10 22:36:15,415][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  5.11it/s, est. speed input: 6943.34 toks/s, output: 254.23 toks/s]
[2025-01-10 22:36:15,415][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  2.89it/s, est. speed input: 6943.34 toks/s, output: 254.23 toks/s]
[2025-01-10 22:36:15,443][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:17,240][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:07,  1.80s/it, est. speed input: 2135.01 toks/s, output: 16.14 toks/s]
[2025-01-10 22:36:17,521][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.75it/s, est. speed input: 5832.17 toks/s, output: 51.01 toks/s]
[2025-01-10 22:36:17,769][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  2.18it/s, est. speed input: 7132.03 toks/s, output: 73.94 toks/s]
[2025-01-10 22:36:18,288][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.09it/s, est. speed input: 7380.00 toks/s, output: 97.38 toks/s]
[2025-01-10 22:36:18,288][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.76it/s, est. speed input: 7380.00 toks/s, output: 97.38 toks/s]
[2025-01-10 22:36:18,306][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:19,457][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.15s/it, est. speed input: 1719.91 toks/s, output: 25.19 toks/s]
[2025-01-10 22:36:19,870][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.40it/s, est. speed input: 2744.23 toks/s, output: 55.00 toks/s]
[2025-01-10 22:36:20,025][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.17it/s, est. speed input: 5147.94 toks/s, output: 126.27 toks/s]
[2025-01-10 22:36:20,435][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.90it/s, est. speed input: 5281.20 toks/s, output: 148.46 toks/s]
[2025-01-10 22:36:20,435][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.35it/s, est. speed input: 5281.20 toks/s, output: 148.46 toks/s]
[2025-01-10 22:36:20,499][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:24,194][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:36,  3.69s/it, est. speed input: 1047.48 toks/s, output: 7.85 toks/s]
[2025-01-10 22:36:24,528][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:04<00:15,  1.72s/it, est. speed input: 2042.13 toks/s, output: 18.86 toks/s]
[2025-01-10 22:36:24,842][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:04<00:05,  1.35it/s, est. speed input: 3973.08 toks/s, output: 43.75 toks/s]
[2025-01-10 22:36:25,048][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:04<00:02,  2.28it/s, est. speed input: 5888.13 toks/s, output: 73.66 toks/s]
[2025-01-10 22:36:25,503][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:05<00:01,  2.83it/s, est. speed input: 7242.75 toks/s, output: 104.52 toks/s]
[2025-01-10 22:36:25,651][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:05<00:00,  3.25it/s, est. speed input: 7967.88 toks/s, output: 124.22 toks/s]
[2025-01-10 22:36:26,107][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  3.62it/s, est. speed input: 8969.35 toks/s, output: 163.18 toks/s]
[2025-01-10 22:36:26,107][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  1.96it/s, est. speed input: 8969.35 toks/s, output: 163.18 toks/s]
[2025-01-10 22:36:26,148][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:28,697][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:25,  2.55s/it, est. speed input: 781.23 toks/s, output: 18.44 toks/s]
[2025-01-10 22:36:29,033][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:11,  1.25s/it, est. speed input: 1609.51 toks/s, output: 39.18 toks/s]
[2025-01-10 22:36:29,168][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:03,  1.97it/s, est. speed input: 3199.04 toks/s, output: 84.45 toks/s]
[2025-01-10 22:36:29,320][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:03<00:01,  3.29it/s, est. speed input: 4761.51 toks/s, output: 131.48 toks/s]
[2025-01-10 22:36:29,610][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  4.17it/s, est. speed input: 5781.16 toks/s, output: 175.34 toks/s]
[2025-01-10 22:36:30,037][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  3.55it/s, est. speed input: 5851.85 toks/s, output: 189.78 toks/s]
[2025-01-10 22:36:30,343][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  4.32it/s, est. speed input: 6762.04 toks/s, output: 244.81 toks/s]
[2025-01-10 22:36:30,344][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.62it/s, est. speed input: 6762.04 toks/s, output: 244.81 toks/s]
[2025-01-10 22:36:30,374][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:32,292][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:07,  1.92s/it, est. speed input: 2162.08 toks/s, output: 15.13 toks/s]
[2025-01-10 22:36:32,825][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.44it/s, est. speed input: 5377.98 toks/s, output: 50.19 toks/s]
[2025-01-10 22:36:33,114][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  1.80it/s, est. speed input: 6554.56 toks/s, output: 76.27 toks/s]
[2025-01-10 22:36:33,248][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.41it/s, est. speed input: 7884.86 toks/s, output: 106.14 toks/s]
[2025-01-10 22:36:33,248][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.74it/s, est. speed input: 7884.86 toks/s, output: 106.14 toks/s]
[2025-01-10 22:36:33,267][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:34,461][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.19s/it, est. speed input: 1755.54 toks/s, output: 24.30 toks/s]
[2025-01-10 22:36:34,785][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.47it/s, est. speed input: 2997.68 toks/s, output: 52.72 toks/s]
[2025-01-10 22:36:34,918][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.32it/s, est. speed input: 4130.31 toks/s, output: 84.84 toks/s]
[2025-01-10 22:36:35,278][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  2.48it/s, est. speed input: 4669.57 toks/s, output: 112.90 toks/s]
[2025-01-10 22:36:35,926][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.04it/s, est. speed input: 4502.60 toks/s, output: 136.56 toks/s]
[2025-01-10 22:36:35,926][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.88it/s, est. speed input: 4502.60 toks/s, output: 136.56 toks/s]
[2025-01-10 22:36:35,995][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:39,965][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:03<00:39,  3.97s/it, est. speed input: 1052.66 toks/s, output: 7.30 toks/s]
[2025-01-10 22:36:40,246][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:04<00:16,  1.80s/it, est. speed input: 2093.21 toks/s, output: 17.17 toks/s]
[2025-01-10 22:36:40,526][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:04<00:05,  1.32it/s, est. speed input: 4056.58 toks/s, output: 39.72 toks/s]
[2025-01-10 22:36:40,681][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:04<00:01,  2.83it/s, est. speed input: 7163.26 toks/s, output: 80.24 toks/s]
[2025-01-10 22:36:40,981][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:04<00:00,  3.55it/s, est. speed input: 8839.88 toks/s, output: 110.91 toks/s]
[2025-01-10 22:36:41,156][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  4.62it/s, est. speed input: 10516.76 toks/s, output: 144.55 toks/s]
[2025-01-10 22:36:41,156][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  2.13it/s, est. speed input: 10516.76 toks/s, output: 144.55 toks/s]
[2025-01-10 22:36:41,200][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:43,868][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:26,  2.67s/it, est. speed input: 796.39 toks/s, output: 17.25 toks/s]
[2025-01-10 22:36:44,117][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:11,  1.24s/it, est. speed input: 1689.31 toks/s, output: 36.34 toks/s]
[2025-01-10 22:36:44,235][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:03,  1.99it/s, est. speed input: 3337.49 toks/s, output: 78.43 toks/s]
[2025-01-10 22:36:44,438][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:03<00:01,  3.20it/s, est. speed input: 4805.11 toks/s, output: 118.61 toks/s]
[2025-01-10 22:36:45,009][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:03<00:01,  2.65it/s, est. speed input: 4872.48 toks/s, output: 130.50 toks/s]
[2025-01-10 22:36:45,142][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  4.04it/s, est. speed input: 6177.84 toks/s, output: 186.22 toks/s]
[2025-01-10 22:36:45,919][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:04<00:00,  2.71it/s, est. speed input: 5801.97 toks/s, output: 193.51 toks/s]
[2025-01-10 22:36:46,197][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.88it/s, est. speed input: 6069.77 toks/s, output: 222.75 toks/s]
[2025-01-10 22:36:46,197][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.20it/s, est. speed input: 6069.77 toks/s, output: 222.75 toks/s]
[2025-01-10 22:36:46,228][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:48,265][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:08,  2.04s/it, est. speed input: 2184.69 toks/s, output: 14.24 toks/s]
[2025-01-10 22:36:48,606][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.53it/s, est. speed input: 5904.57 toks/s, output: 46.27 toks/s]
[2025-01-10 22:36:48,800][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:02<00:00,  2.02it/s, est. speed input: 7466.83 toks/s, output: 68.46 toks/s]
[2025-01-10 22:36:49,334][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.97it/s, est. speed input: 7840.35 toks/s, output: 90.82 toks/s]
[2025-01-10 22:36:49,334][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.61it/s, est. speed input: 7840.35 toks/s, output: 90.82 toks/s]
[2025-01-10 22:36:49,359][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:50,822][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.46s/it, est. speed input: 1507.38 toks/s, output: 19.83 toks/s]
[2025-01-10 22:36:51,490][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:02<00:03,  1.00it/s, est. speed input: 2247.53 toks/s, output: 45.99 toks/s]
[2025-01-10 22:36:51,638][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:01,  1.64it/s, est. speed input: 3157.89 toks/s, output: 77.67 toks/s]
[2025-01-10 22:36:51,822][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  3.16it/s, est. speed input: 5334.64 toks/s, output: 143.73 toks/s]
[2025-01-10 22:36:52,061][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.40it/s, est. speed input: 5899.10 toks/s, output: 171.77 toks/s]
[2025-01-10 22:36:52,061][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.22it/s, est. speed input: 5899.10 toks/s, output: 171.77 toks/s]
[2025-01-10 22:36:52,128][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:36:56,029][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:03<00:35,  3.90s/it, est. speed input: 1148.80 toks/s, output: 7.43 toks/s]
[2025-01-10 22:36:56,365][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:04<00:14,  1.80s/it, est. speed input: 2247.74 toks/s, output: 17.94 toks/s]
[2025-01-10 22:36:56,724][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:04<00:02,  1.67it/s, est. speed input: 5531.73 toks/s, output: 51.78 toks/s]
[2025-01-10 22:36:56,877][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:04<00:01,  2.06it/s, est. speed input: 6532.91 toks/s, output: 66.32 toks/s]
[2025-01-10 22:36:57,148][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:05<00:00,  3.58it/s, est. speed input: 9486.81 toks/s, output: 114.73 toks/s]
[2025-01-10 22:36:57,843][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:05<00:00,  2.76it/s, est. speed input: 9234.15 toks/s, output: 126.69 toks/s]
[2025-01-10 22:36:57,843][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:05<00:00,  1.75it/s, est. speed input: 9234.15 toks/s, output: 126.69 toks/s]
[2025-01-10 22:36:57,886][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:00,458][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:23,  2.57s/it, est. speed input: 877.09 toks/s, output: 17.50 toks/s]
[2025-01-10 22:37:00,761][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:09,  1.24s/it, est. speed input: 1714.45 toks/s, output: 37.57 toks/s]
[2025-01-10 22:37:00,949][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:03<00:05,  1.32it/s, est. speed input: 2572.03 toks/s, output: 59.42 toks/s]
[2025-01-10 22:37:01,459][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:03<00:00,  3.31it/s, est. speed input: 5432.10 toks/s, output: 144.97 toks/s]
[2025-01-10 22:37:01,577][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  3.78it/s, est. speed input: 6124.54 toks/s, output: 171.75 toks/s]
[2025-01-10 22:37:02,362][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  3.21it/s, est. speed input: 6510.66 toks/s, output: 208.00 toks/s]
[2025-01-10 22:37:02,362][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.23it/s, est. speed input: 6510.66 toks/s, output: 208.00 toks/s]
[2025-01-10 22:37:02,402][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:04,890][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:02<00:12,  2.49s/it, est. speed input: 1909.29 toks/s, output: 11.65 toks/s]
[2025-01-10 22:37:05,370][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:02<00:02,  1.22it/s, est. speed input: 5044.70 toks/s, output: 40.09 toks/s]
[2025-01-10 22:37:05,608][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:03<00:01,  1.61it/s, est. speed input: 6394.33 toks/s, output: 61.13 toks/s]
[2025-01-10 22:37:05,997][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  2.45it/s, est. speed input: 8713.19 toks/s, output: 105.98 toks/s]
[2025-01-10 22:37:05,997][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.67it/s, est. speed input: 8713.19 toks/s, output: 105.98 toks/s]
[2025-01-10 22:37:06,018][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:07,323][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.30s/it, est. speed input: 1772.57 toks/s, output: 22.22 toks/s]
[2025-01-10 22:37:07,987][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.08it/s, est. speed input: 2694.54 toks/s, output: 52.32 toks/s]
[2025-01-10 22:37:08,085][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.42it/s, est. speed input: 6545.04 toks/s, output: 164.08 toks/s]
[2025-01-10 22:37:08,162][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:12,883][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:04<00:47,  4.72s/it, est. speed input: 1013.64 toks/s, output: 6.14 toks/s]
[2025-01-10 22:37:13,221][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:05<00:10,  1.35s/it, est. speed input: 3161.03 toks/s, output: 20.75 toks/s]
[2025-01-10 22:37:13,378][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:05<00:04,  1.44it/s, est. speed input: 5162.76 toks/s, output: 40.26 toks/s]
[2025-01-10 22:37:13,516][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:05<00:02,  1.83it/s, est. speed input: 6093.82 toks/s, output: 51.17 toks/s]
[2025-01-10 22:37:13,791][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:05<00:01,  2.71it/s, est. speed input: 7814.94 toks/s, output: 74.80 toks/s]
[2025-01-10 22:37:14,018][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:05<00:00,  3.69it/s, est. speed input: 9526.04 toks/s, output: 102.80 toks/s]
[2025-01-10 22:37:14,366][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  3.48it/s, est. speed input: 9951.62 toks/s, output: 117.02 toks/s]
[2025-01-10 22:37:14,367][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  1.77it/s, est. speed input: 9951.62 toks/s, output: 117.02 toks/s]
[2025-01-10 22:37:14,418][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:17,322][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:29,  2.90s/it, est. speed input: 821.83 toks/s, output: 15.49 toks/s]
[2025-01-10 22:37:17,862][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:03<00:13,  1.51s/it, est. speed input: 1512.53 toks/s, output: 34.84 toks/s]
[2025-01-10 22:37:18,135][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:03<00:04,  1.54it/s, est. speed input: 3008.53 toks/s, output: 76.93 toks/s]
[2025-01-10 22:37:18,577][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:04<00:03,  1.71it/s, est. speed input: 3506.02 toks/s, output: 96.88 toks/s]
[2025-01-10 22:37:18,758][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:04<00:00,  3.49it/s, est. speed input: 5579.69 toks/s, output: 176.73 toks/s]
[2025-01-10 22:37:18,993][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:04<00:00,  3.63it/s, est. speed input: 6047.97 toks/s, output: 199.32 toks/s]
[2025-01-10 22:37:19,181][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  4.84it/s, est. speed input: 7201.53 toks/s, output: 255.93 toks/s]
[2025-01-10 22:37:19,181][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.31it/s, est. speed input: 7201.53 toks/s, output: 255.93 toks/s]
[2025-01-10 22:37:19,216][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:21,487][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:09,  2.27s/it, est. speed input: 2228.81 toks/s, output: 12.77 toks/s]
[2025-01-10 22:37:21,978][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.30it/s, est. speed input: 5767.39 toks/s, output: 43.45 toks/s]
[2025-01-10 22:37:22,548][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:03<00:00,  1.43it/s, est. speed input: 6545.90 toks/s, output: 66.94 toks/s]
[2025-01-10 22:37:22,575][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.49it/s, est. speed input: 8243.98 toks/s, output: 97.67 toks/s]
[2025-01-10 22:37:22,597][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:23,951][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.35s/it, est. speed input: 1789.49 toks/s, output: 21.43 toks/s]
[2025-01-10 22:37:24,424][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.20it/s, est. speed input: 2818.43 toks/s, output: 49.28 toks/s]
[2025-01-10 22:37:24,704][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.72it/s, est. speed input: 3818.57 toks/s, output: 80.72 toks/s]
[2025-01-10 22:37:25,262][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.44it/s, est. speed input: 5357.13 toks/s, output: 141.88 toks/s]
[2025-01-10 22:37:25,262][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.88it/s, est. speed input: 5357.13 toks/s, output: 141.88 toks/s]
[2025-01-10 22:37:25,345][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:30,134][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:04<00:47,  4.79s/it, est. speed input: 1063.66 toks/s, output: 6.06 toks/s]
[2025-01-10 22:37:30,484][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:05<00:19,  2.18s/it, est. speed input: 2090.22 toks/s, output: 14.79 toks/s]
[2025-01-10 22:37:30,674][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:05<00:10,  1.27s/it, est. speed input: 3084.59 toks/s, output: 24.96 toks/s]
[2025-01-10 22:37:30,816][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:05<00:03,  1.69it/s, est. speed input: 5143.74 toks/s, output: 47.52 toks/s]
[2025-01-10 22:37:31,040][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:05<00:01,  2.65it/s, est. speed input: 7090.30 toks/s, output: 71.29 toks/s]
[2025-01-10 22:37:31,213][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:05<00:00,  4.53it/s, est. speed input: 10078.99 toks/s, output: 112.31 toks/s]
[2025-01-10 22:37:31,561][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:06<00:00,  1.77it/s, est. speed input: 10543.60 toks/s, output: 124.67 toks/s]
[2025-01-10 22:37:31,636][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:33,111][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:04,  1.47s/it, est. speed input: 1707.45 toks/s, output: 29.84 toks/s]
[2025-01-10 22:37:33,772][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:02<00:01,  1.00it/s, est. speed input: 2746.44 toks/s, output: 62.25 toks/s]
[2025-01-10 22:37:34,318][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.84it/s, est. speed input: 4934.78 toks/s, output: 132.73 toks/s]
[2025-01-10 22:37:34,318][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.49it/s, est. speed input: 4934.78 toks/s, output: 132.73 toks/s]
[2025-01-10 22:37:34,355][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:36,741][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:02<00:09,  2.39s/it, est. speed input: 2251.13 toks/s, output: 12.16 toks/s]
[2025-01-10 22:37:37,219][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:02<00:01,  1.26it/s, est. speed input: 5895.89 toks/s, output: 41.56 toks/s]
[2025-01-10 22:37:37,483][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:03<00:00,  1.63it/s, est. speed input: 7398.96 toks/s, output: 63.63 toks/s]
[2025-01-10 22:37:37,578][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:03<00:00,  1.55it/s, est. speed input: 9123.73 toks/s, output: 88.77 toks/s]
[2025-01-10 22:37:37,646][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:39,613][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:05,  1.97s/it, est. speed input: 2747.79 toks/s, output: 14.75 toks/s]
[2025-01-10 22:37:39,881][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:02<00:01,  1.03it/s, est. speed input: 5090.58 toks/s, output: 34.01 toks/s]
[2025-01-10 22:37:40,229][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:02<00:00,  1.46it/s, est. speed input: 6784.97 toks/s, output: 57.31 toks/s]
[2025-01-10 22:37:41,125][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.30it/s, est. speed input: 6832.58 toks/s, output: 82.50 toks/s]
[2025-01-10 22:37:41,125][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.15it/s, est. speed input: 6832.58 toks/s, output: 82.50 toks/s]
[2025-01-10 22:37:41,155][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:42,584][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 2636.85 toks/s, output: 62.27 toks/s]
[2025-01-10 22:37:42,585][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 2636.85 toks/s, output: 62.27 toks/s]
[2025-01-10 22:37:42,594][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-10 22:37:45,080][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 2613.72 toks/s, output: 61.56 toks/s]
[2025-01-10 22:37:45,080][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 2613.72 toks/s, output: 61.56 toks/s]
[2025-01-10 22:37:45,493][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-10 22:37:45,547][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-10 22:37:49,194][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:10,  3.65s/it]
[2025-01-10 22:37:51,610][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:05,  2.92s/it]
[2025-01-10 22:37:51,827][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:01,  1.69s/it]
[2025-01-10 22:37:52,001][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.09s/it]
[2025-01-10 22:37:52,001][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.61s/it]
[2025-01-10 22:37:52,095][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
[2025-01-10 22:38:01,385][root][ERROR] - Error executing job with overrides: ['matches.dond_game_args.rounds_per_game=16', 'matches.run_matches_args.log_func_args.training_data_func=set_discounted_advalign_returns']
[2025-01-10 22:38:01,393][root][ERROR] - Traceback (most recent call last):
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/run.py", line 42, in main
    globals()[cfg.experiment.method](cfg)
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/experiments/dond_run_train.py", line 139, in dond_run_train
    train_main(
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/train_main.py", line 20, in train_main
    globals()[train_func](hf_model, paths, train_func_args, output_path=output_path)
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/train_main.py", line 40, in train_ppo_main
    ppo_train(model=hf_model.hf_model,
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/training/ppo_train.py", line 132, in ppo_train
    model_accelerator.backward(loss)
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
[2025-01-10 22:38:01,393][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
[2025-01-10 22:38:01,394][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-01-10 22:38:01,394][root][ERROR] - RuntimeError: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0
[2025-01-10 22:38:01,394][root][ERROR] - Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/mila/d/dereck.piche/llm_negotiation/src/run.py:14: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../conf", config_name="default")
WARNING 01-11 00:17:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:17:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:17:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:17:31 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:17:31,694][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:17:33,035][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 00:17:33,447][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 00:17:34,748][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 00:17:36,090][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 00:17:36,090][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 00:17:36 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-11 00:17:50 gpu_executor.py:122] # GPU blocks: 19859, # CPU blocks: 2048
INFO 01-11 00:17:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:17:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:18:11 model_runner.py:1335] Graph capturing finished in 20 secs.
[2025-01-11 00:18:11,565][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:13,003][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:01<00:44,  1.44s/it, est. speed input: 441.86 toks/s, output: 9.05 toks/s]
[2025-01-11 00:18:13,103][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:01<00:06,  4.25it/s, est. speed input: 2065.12 toks/s, output: 52.03 toks/s]
[2025-01-11 00:18:13,216][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:01<00:02,  8.22it/s, est. speed input: 3463.27 toks/s, output: 102.41 toks/s]
[2025-01-11 00:18:13,322][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:01<00:00, 16.64it/s, est. speed input: 5784.26 toks/s, output: 199.83 toks/s]
[2025-01-11 00:18:13,432][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:01<00:00, 23.36it/s, est. speed input: 7482.81 toks/s, output: 293.53 toks/s]
[2025-01-11 00:18:13,625][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:02<00:00, 24.14it/s, est. speed input: 8322.71 toks/s, output: 373.78 toks/s]
[2025-01-11 00:18:13,975][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 19.79it/s, est. speed input: 8434.01 toks/s, output: 446.60 toks/s]
[2025-01-11 00:18:13,975][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:02<00:00, 13.28it/s, est. speed input: 8434.01 toks/s, output: 446.60 toks/s]
[2025-01-11 00:18:13,981][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:14,566][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:01,  1.71it/s, est. speed input: 1189.50 toks/s, output: 49.63 toks/s]
[2025-01-11 00:18:14,705][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:00<00:00,  3.10it/s, est. speed input: 1925.52 toks/s, output: 93.93 toks/s]
[2025-01-11 00:18:15,112][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  4.03it/s, est. speed input: 2517.72 toks/s, output: 160.95 toks/s]
[2025-01-11 00:18:15,112][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.54it/s, est. speed input: 2517.72 toks/s, output: 160.95 toks/s]
[2025-01-11 00:18:15,141][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:16,604][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:01<00:39,  1.46s/it, est. speed input: 443.15 toks/s, output: 13.68 toks/s]
[2025-01-11 00:18:16,731][root][ERROR] - Processed prompts:  25%|##5       | 7/28 [00:01<00:03,  5.82it/s, est. speed input: 2888.19 toks/s, output: 101.89 toks/s]
[2025-01-11 00:18:16,837][root][ERROR] - Processed prompts:  39%|###9      | 11/28 [00:01<00:01,  9.54it/s, est. speed input: 4261.61 toks/s, output: 169.28 toks/s]
[2025-01-11 00:18:17,025][root][ERROR] - Processed prompts:  61%|######    | 17/28 [00:01<00:00, 14.62it/s, est. speed input: 5942.79 toks/s, output: 271.74 toks/s]
[2025-01-11 00:18:17,161][root][ERROR] - Processed prompts:  75%|#######5  | 21/28 [00:02<00:00, 17.44it/s, est. speed input: 6882.46 toks/s, output: 352.98 toks/s]
[2025-01-11 00:18:17,375][root][ERROR] - Processed prompts:  89%|########9 | 25/28 [00:02<00:00, 17.82it/s, est. speed input: 7413.51 toks/s, output: 428.45 toks/s]
[2025-01-11 00:18:17,922][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:02<00:00, 11.38it/s, est. speed input: 6697.53 toks/s, output: 442.02 toks/s]
[2025-01-11 00:18:17,922][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:02<00:00, 10.07it/s, est. speed input: 6697.53 toks/s, output: 442.02 toks/s]
[2025-01-11 00:18:17,952][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:19,507][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:01<00:34,  1.56s/it, est. speed input: 486.80 toks/s, output: 18.65 toks/s]
[2025-01-11 00:18:19,632][root][ERROR] - Processed prompts:  26%|##6       | 6/23 [00:01<00:03,  4.70it/s, est. speed input: 2797.99 toks/s, output: 112.54 toks/s]
[2025-01-11 00:18:19,734][root][ERROR] - Processed prompts:  39%|###9      | 9/23 [00:01<00:01,  7.35it/s, est. speed input: 3902.03 toks/s, output: 175.71 toks/s]
[2025-01-11 00:18:19,843][root][ERROR] - Processed prompts:  52%|#####2    | 12/23 [00:01<00:01, 10.18it/s, est. speed input: 4896.74 toks/s, output: 236.88 toks/s]
[2025-01-11 00:18:19,950][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:01<00:00, 17.75it/s, est. speed input: 6956.20 toks/s, output: 383.92 toks/s]
[2025-01-11 00:18:20,179][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:02<00:00, 17.66it/s, est. speed input: 7662.33 toks/s, output: 469.38 toks/s]
[2025-01-11 00:18:20,205][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:02<00:00, 10.21it/s, est. speed input: 7917.95 toks/s, output: 496.67 toks/s]
[2025-01-11 00:18:20,216][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:20,964][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:00<00:05,  1.34it/s, est. speed input: 959.64 toks/s, output: 32.08 toks/s]
[2025-01-11 00:18:21,139][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:00<00:02,  2.43it/s, est. speed input: 1596.79 toks/s, output: 65.00 toks/s]
[2025-01-11 00:18:21,312][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:01<00:00,  6.33it/s, est. speed input: 3362.18 toks/s, output: 167.83 toks/s]
[2025-01-11 00:18:21,480][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:01<00:00,  7.81it/s, est. speed input: 3983.66 toks/s, output: 235.68 toks/s]
[2025-01-11 00:18:21,769][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:01<00:00,  7.47it/s, est. speed input: 4206.71 toks/s, output: 284.05 toks/s]
[2025-01-11 00:18:21,769][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:01<00:00,  5.80it/s, est. speed input: 4206.71 toks/s, output: 284.05 toks/s]
[2025-01-11 00:18:21,794][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:22,778][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:00<00:09,  1.02it/s, est. speed input: 768.27 toks/s, output: 28.45 toks/s]
[2025-01-11 00:18:23,000][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:01<00:00,  6.26it/s, est. speed input: 4140.47 toks/s, output: 157.49 toks/s]
[2025-01-11 00:18:23,336][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:01<00:00,  6.16it/s, est. speed input: 4337.47 toks/s, output: 207.56 toks/s]
[2025-01-11 00:18:23,674][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:01<00:00,  6.08it/s, est. speed input: 4457.10 toks/s, output: 264.43 toks/s]
[2025-01-11 00:18:23,779][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:01<00:00,  6.51it/s, est. speed input: 4689.14 toks/s, output: 299.71 toks/s]
[2025-01-11 00:18:23,780][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:01<00:00,  5.54it/s, est. speed input: 4689.14 toks/s, output: 299.71 toks/s]
[2025-01-11 00:18:23,804][root][ERROR] - Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:24,862][root][ERROR] - Processed prompts:   5%|5         | 1/20 [00:01<00:20,  1.06s/it, est. speed input: 704.11 toks/s, output: 8.51 toks/s]
[2025-01-11 00:18:25,179][root][ERROR] - Processed prompts:  10%|#         | 2/20 [00:01<00:11,  1.61it/s, est. speed input: 1076.67 toks/s, output: 26.19 toks/s]
[2025-01-11 00:18:25,388][root][ERROR] - Processed prompts:  30%|###       | 6/20 [00:01<00:02,  5.47it/s, est. speed input: 2899.34 toks/s, output: 102.28 toks/s]
[2025-01-11 00:18:25,560][root][ERROR] - Processed prompts:  45%|####5     | 9/20 [00:01<00:01,  7.92it/s, est. speed input: 3907.19 toks/s, output: 170.22 toks/s]
[2025-01-11 00:18:25,789][root][ERROR] - Processed prompts:  60%|######    | 12/20 [00:01<00:00,  9.38it/s, est. speed input: 4648.69 toks/s, output: 239.23 toks/s]
[2025-01-11 00:18:25,981][root][ERROR] - Processed prompts:  75%|#######5  | 15/20 [00:02<00:00, 10.96it/s, est. speed input: 5289.93 toks/s, output: 317.90 toks/s]
[2025-01-11 00:18:26,110][root][ERROR] - Processed prompts:  85%|########5 | 17/20 [00:02<00:00, 11.84it/s, est. speed input: 5660.26 toks/s, output: 374.63 toks/s]
[2025-01-11 00:18:26,303][root][ERROR] - Processed prompts:  95%|#########5| 19/20 [00:02<00:00, 11.40it/s, est. speed input: 5846.47 toks/s, output: 425.29 toks/s]
[2025-01-11 00:18:26,369][root][ERROR] - Processed prompts: 100%|##########| 20/20 [00:02<00:00,  7.80it/s, est. speed input: 6024.47 toks/s, output: 456.07 toks/s]
[2025-01-11 00:18:26,410][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:27,835][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:01<00:24,  1.42s/it, est. speed input: 581.15 toks/s, output: 18.25 toks/s]
[2025-01-11 00:18:28,031][root][ERROR] - Processed prompts:  39%|###8      | 7/18 [00:01<00:01,  5.62it/s, est. speed input: 3918.97 toks/s, output: 129.01 toks/s]
[2025-01-11 00:18:28,297][root][ERROR] - Processed prompts:  50%|#####     | 9/18 [00:01<00:01,  6.08it/s, est. speed input: 4314.55 toks/s, output: 164.88 toks/s]
[2025-01-11 00:18:28,495][root][ERROR] - Processed prompts:  61%|######1   | 11/18 [00:02<00:01,  6.90it/s, est. speed input: 4733.51 toks/s, output: 213.52 toks/s]
[2025-01-11 00:18:28,629][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:02<00:00,  9.45it/s, est. speed input: 5722.10 toks/s, output: 301.56 toks/s]
[2025-01-11 00:18:28,758][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:02<00:00, 10.56it/s, est. speed input: 6207.33 toks/s, output: 356.92 toks/s]
[2025-01-11 00:18:30,261][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:03<00:00,  3.60it/s, est. speed input: 4263.33 toks/s, output: 297.88 toks/s]
[2025-01-11 00:18:30,261][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:03<00:00,  4.67it/s, est. speed input: 4263.33 toks/s, output: 297.88 toks/s]
[2025-01-11 00:18:30,276][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:31,276][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:00<00:09,  1.00it/s, est. speed input: 900.37 toks/s, output: 29.01 toks/s]
[2025-01-11 00:18:31,598][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:01<00:00,  5.58it/s, est. speed input: 3960.71 toks/s, output: 152.92 toks/s]
[2025-01-11 00:18:31,726][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:01<00:00,  7.10it/s, est. speed input: 4685.76 toks/s, output: 217.35 toks/s]
[2025-01-11 00:18:32,255][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:01<00:00,  5.54it/s, est. speed input: 4271.15 toks/s, output: 240.15 toks/s]
[2025-01-11 00:18:32,308][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  5.42it/s, est. speed input: 4570.81 toks/s, output: 285.06 toks/s]
[2025-01-11 00:18:32,359][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:33,462][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:01<00:12,  1.10s/it, est. speed input: 757.96 toks/s, output: 21.76 toks/s]
[2025-01-11 00:18:33,632][root][ERROR] - Processed prompts:  42%|####1     | 5/12 [00:01<00:01,  4.99it/s, est. speed input: 3839.47 toks/s, output: 114.66 toks/s]
[2025-01-11 00:18:33,783][root][ERROR] - Processed prompts:  58%|#####8    | 7/12 [00:01<00:00,  6.53it/s, est. speed input: 4892.64 toks/s, output: 165.73 toks/s]
[2025-01-11 00:18:34,168][root][ERROR] - Processed prompts:  75%|#######5  | 9/12 [00:01<00:00,  5.98it/s, est. speed input: 5050.25 toks/s, output: 208.33 toks/s]
[2025-01-11 00:18:34,826][root][ERROR] - Processed prompts:  92%|#########1| 11/12 [00:02<00:00,  4.52it/s, est. speed input: 4531.13 toks/s, output: 244.37 toks/s]
[2025-01-11 00:18:34,879][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:02<00:00,  4.76it/s, est. speed input: 4776.73 toks/s, output: 287.66 toks/s]
[2025-01-11 00:18:34,897][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:36,001][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:01<00:12,  1.10s/it, est. speed input: 991.25 toks/s, output: 25.37 toks/s]
[2025-01-11 00:18:36,223][root][ERROR] - Processed prompts:  58%|#####8    | 7/12 [00:01<00:00,  6.74it/s, est. speed input: 5100.32 toks/s, output: 165.18 toks/s]
[2025-01-11 00:18:36,511][root][ERROR] - Processed prompts:  75%|#######5  | 9/12 [00:01<00:00,  6.80it/s, est. speed input: 5322.26 toks/s, output: 206.94 toks/s]
[2025-01-11 00:18:36,606][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:01<00:00,  7.02it/s, est. speed input: 6585.48 toks/s, output: 315.43 toks/s]
[2025-01-11 00:18:36,640][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:37,351][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.41it/s, est. speed input: 1693.52 toks/s, output: 40.79 toks/s]
[2025-01-11 00:18:37,352][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:00<00:00,  5.62it/s, est. speed input: 6516.86 toks/s, output: 163.10 toks/s]
[2025-01-11 00:18:37,376][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:38,791][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:01<00:21,  1.41s/it, est. speed input: 819.44 toks/s, output: 20.50 toks/s]
[2025-01-11 00:18:38,903][root][ERROR] - Processed prompts:  38%|###7      | 6/16 [00:01<00:01,  5.17it/s, est. speed input: 3911.61 toks/s, output: 121.19 toks/s]
[2025-01-11 00:18:39,028][root][ERROR] - Processed prompts:  56%|#####6    | 9/16 [00:01<00:00,  7.80it/s, est. speed input: 5450.88 toks/s, output: 189.53 toks/s]
[2025-01-11 00:18:39,386][root][ERROR] - Processed prompts:  75%|#######5  | 12/16 [00:02<00:00,  8.01it/s, est. speed input: 5985.34 toks/s, output: 248.35 toks/s]
[2025-01-11 00:18:39,660][root][ERROR] - Processed prompts:  88%|########7 | 14/16 [00:02<00:00,  7.81it/s, est. speed input: 6174.75 toks/s, output: 294.70 toks/s]
[2025-01-11 00:18:40,845][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  3.91it/s, est. speed input: 4659.90 toks/s, output: 277.94 toks/s]
[2025-01-11 00:18:40,845][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.61it/s, est. speed input: 4659.90 toks/s, output: 277.94 toks/s]
[2025-01-11 00:18:40,911][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:41,841][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:00<00:06,  1.08it/s, est. speed input: 1265.55 toks/s, output: 31.18 toks/s]
[2025-01-11 00:18:42,014][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  6.91it/s, est. speed input: 5859.12 toks/s, output: 168.57 toks/s]
[2025-01-11 00:18:42,265][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  7.24it/s, est. speed input: 6231.89 toks/s, output: 219.35 toks/s]
[2025-01-11 00:18:42,265][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  5.91it/s, est. speed input: 6231.89 toks/s, output: 219.35 toks/s]
[2025-01-11 00:18:42,273][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:42,973][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.43it/s, est. speed input: 1366.43 toks/s, output: 41.41 toks/s]
[2025-01-11 00:18:43,162][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:00<00:00,  2.50it/s, est. speed input: 2432.64 toks/s, output: 79.89 toks/s]
[2025-01-11 00:18:43,307][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  5.14it/s, est. speed input: 4395.30 toks/s, output: 164.33 toks/s]
[2025-01-11 00:18:43,308][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.87it/s, est. speed input: 4395.30 toks/s, output: 164.33 toks/s]
[2025-01-11 00:18:43,356][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:44,137][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:00<00:03,  1.28it/s, est. speed input: 1385.56 toks/s, output: 37.17 toks/s]
[2025-01-11 00:18:44,329][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:00<00:00,  5.01it/s, est. speed input: 4641.85 toks/s, output: 138.82 toks/s]
[2025-01-11 00:18:44,632][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.38it/s, est. speed input: 4488.14 toks/s, output: 157.60 toks/s]
[2025-01-11 00:18:44,632][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.92it/s, est. speed input: 4488.14 toks/s, output: 157.60 toks/s]
[2025-01-11 00:18:44,635][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:45,722][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 1018.19 toks/s, output: 70.82 toks/s]
[2025-01-11 00:18:45,722][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 1018.19 toks/s, output: 70.82 toks/s]
[2025-01-11 00:18:45,747][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:46,311][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.77it/s, est. speed input: 2061.07 toks/s, output: 54.98 toks/s]
[2025-01-11 00:18:46,469][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  3.07it/s, est. speed input: 3472.19 toks/s, output: 102.49 toks/s]
[2025-01-11 00:18:46,469][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.77it/s, est. speed input: 3472.19 toks/s, output: 102.49 toks/s]
[2025-01-11 00:18:46,472][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:47,437][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 1247.10 toks/s, output: 69.46 toks/s]
[2025-01-11 00:18:47,437][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 1247.10 toks/s, output: 69.46 toks/s]
[2025-01-11 00:18:47,453][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:18:47,920][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 2721.48 toks/s, output: 62.19 toks/s]
[2025-01-11 00:18:47,920][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 2721.48 toks/s, output: 62.19 toks/s]
[2025-01-11 00:18:48,188][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:18:48,240][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:18:50,011][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.77s/it]
[2025-01-11 00:18:51,715][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 00:18:53,339][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 00:18:53,863][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 00:18:53,863][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-11 00:19:15,007][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:19:15,059][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:19:17,090][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.03s/it]
[2025-01-11 00:19:18,661][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 00:19:20,206][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 00:19:20,703][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 00:19:20,703][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 00:19:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:19:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:19:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:19:41 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:19:41,424][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:19:42,778][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 00:19:43,159][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 00:19:44,469][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 00:19:45,803][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 00:19:45,803][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 00:19:46 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:19:59 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:20:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:20:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:20:21 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:20:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:22,032][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:26,464][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:17,  4.43s/it, est. speed input: 143.30 toks/s, output: 3.38 toks/s]
[2025-01-11 00:20:26,585][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:56,  1.90s/it, est. speed input: 278.96 toks/s, output: 7.03 toks/s]
[2025-01-11 00:20:26,751][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:12,  2.14it/s, est. speed input: 807.35 toks/s, output: 22.25 toks/s]
[2025-01-11 00:20:26,960][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.96it/s, est. speed input: 1030.82 toks/s, output: 30.64 toks/s]
[2025-01-11 00:20:27,106][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:05<00:04,  4.69it/s, est. speed input: 1376.72 toks/s, output: 44.94 toks/s]
[2025-01-11 00:20:27,233][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:05<00:01,  8.48it/s, est. speed input: 1953.42 toks/s, output: 71.14 toks/s]
[2025-01-11 00:20:27,338][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:05<00:00, 12.96it/s, est. speed input: 2513.35 toks/s, output: 99.33 toks/s]
[2025-01-11 00:20:27,513][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:05<00:00, 15.08it/s, est. speed input: 2896.57 toks/s, output: 122.98 toks/s]
[2025-01-11 00:20:27,703][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:05<00:00, 15.24it/s, est. speed input: 3135.11 toks/s, output: 141.94 toks/s]
[2025-01-11 00:20:28,017][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 13.14it/s, est. speed input: 3288.97 toks/s, output: 164.57 toks/s]
[2025-01-11 00:20:28,101][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  5.27it/s, est. speed input: 3348.31 toks/s, output: 173.35 toks/s]
WARNING 01-11 00:20:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:28,301][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:29,642][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.34s/it, est. speed input: 518.55 toks/s, output: 21.64 toks/s]
[2025-01-11 00:20:29,868][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.46it/s, est. speed input: 894.45 toks/s, output: 43.41 toks/s]
[2025-01-11 00:20:30,030][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  3.26it/s, est. speed input: 1644.32 toks/s, output: 90.23 toks/s]
[2025-01-11 00:20:30,214][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.72it/s, est. speed input: 1870.84 toks/s, output: 111.34 toks/s]
[2025-01-11 00:20:30,532][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.53it/s, est. speed input: 1920.22 toks/s, output: 129.57 toks/s]
[2025-01-11 00:20:30,532][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.69it/s, est. speed input: 1920.22 toks/s, output: 129.57 toks/s]
WARNING 01-11 00:20:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:30,753][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:34,225][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:26,  3.47s/it, est. speed input: 191.51 toks/s, output: 6.34 toks/s]
[2025-01-11 00:20:34,328][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:35,  1.49s/it, est. speed input: 368.33 toks/s, output: 12.86 toks/s]
[2025-01-11 00:20:34,529][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:20,  1.11it/s, est. speed input: 521.62 toks/s, output: 19.59 toks/s]
[2025-01-11 00:20:34,716][root][ERROR] - Processed prompts:  27%|##6       | 7/26 [00:03<00:05,  3.53it/s, est. speed input: 1169.36 toks/s, output: 49.46 toks/s]
[2025-01-11 00:20:34,884][root][ERROR] - Processed prompts:  35%|###4      | 9/26 [00:04<00:03,  4.64it/s, est. speed input: 1436.94 toks/s, output: 64.14 toks/s]
[2025-01-11 00:20:35,045][root][ERROR] - Processed prompts:  42%|####2     | 11/26 [00:04<00:02,  5.84it/s, est. speed input: 1693.82 toks/s, output: 80.15 toks/s]
[2025-01-11 00:20:35,145][root][ERROR] - Processed prompts:  62%|######1   | 16/26 [00:04<00:00, 11.14it/s, est. speed input: 2408.59 toks/s, output: 125.45 toks/s]
[2025-01-11 00:20:35,282][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:04<00:00, 14.57it/s, est. speed input: 2926.45 toks/s, output: 162.05 toks/s]
[2025-01-11 00:20:35,590][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:04<00:00, 12.75it/s, est. speed input: 3150.21 toks/s, output: 185.45 toks/s]
[2025-01-11 00:20:36,249][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  8.34it/s, est. speed input: 3135.65 toks/s, output: 212.15 toks/s]
[2025-01-11 00:20:36,249][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  4.73it/s, est. speed input: 3135.65 toks/s, output: 212.15 toks/s]
WARNING 01-11 00:20:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:36,491][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:39,669][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:06,  3.18s/it, est. speed input: 236.01 toks/s, output: 8.18 toks/s]
[2025-01-11 00:20:39,805][root][ERROR] - Processed prompts:  14%|#3        | 3/22 [00:03<00:16,  1.14it/s, est. speed input: 677.42 toks/s, output: 24.74 toks/s]
[2025-01-11 00:20:39,971][root][ERROR] - Processed prompts:  27%|##7       | 6/22 [00:03<00:05,  2.69it/s, est. speed input: 1358.32 toks/s, output: 50.29 toks/s]
[2025-01-11 00:20:40,119][root][ERROR] - Processed prompts:  41%|####      | 9/22 [00:03<00:02,  4.52it/s, est. speed input: 1926.21 toks/s, output: 77.73 toks/s]
[2025-01-11 00:20:40,249][root][ERROR] - Processed prompts:  55%|#####4    | 12/22 [00:03<00:01,  6.66it/s, est. speed input: 2476.22 toks/s, output: 106.43 toks/s]
[2025-01-11 00:20:40,601][root][ERROR] - Processed prompts:  68%|######8   | 15/22 [00:04<00:00,  7.24it/s, est. speed input: 2822.85 toks/s, output: 130.89 toks/s]
[2025-01-11 00:20:40,784][root][ERROR] - Processed prompts:  77%|#######7  | 17/22 [00:04<00:00,  7.93it/s, est. speed input: 3068.40 toks/s, output: 153.75 toks/s]
[2025-01-11 00:20:40,941][root][ERROR] - Processed prompts:  86%|########6 | 19/22 [00:04<00:00,  8.82it/s, est. speed input: 3309.80 toks/s, output: 178.42 toks/s]
[2025-01-11 00:20:41,107][root][ERROR] - Processed prompts:  95%|#########5| 21/22 [00:04<00:00,  9.53it/s, est. speed input: 3523.15 toks/s, output: 203.42 toks/s]
[2025-01-11 00:20:41,459][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  4.43it/s, est. speed input: 3438.75 toks/s, output: 208.72 toks/s]
WARNING 01-11 00:20:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:41,662][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:43,236][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:14,  1.57s/it, est. speed input: 420.64 toks/s, output: 13.34 toks/s]
[2025-01-11 00:20:43,434][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:06,  1.31it/s, est. speed input: 841.19 toks/s, output: 27.64 toks/s]
[2025-01-11 00:20:43,540][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:01<00:01,  3.14it/s, est. speed input: 1628.26 toks/s, output: 58.57 toks/s]
[2025-01-11 00:20:43,813][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:00,  4.30it/s, est. speed input: 2050.83 toks/s, output: 86.94 toks/s]
[2025-01-11 00:20:43,929][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  7.41it/s, est. speed input: 2890.04 toks/s, output: 144.70 toks/s]
[2025-01-11 00:20:44,580][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.43it/s, est. speed input: 2523.20 toks/s, output: 142.92 toks/s]
WARNING 01-11 00:20:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:44,800][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:46,433][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:08,  1.63s/it, est. speed input: 524.89 toks/s, output: 23.89 toks/s]
[2025-01-11 00:20:46,697][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  1.92it/s, est. speed input: 1255.82 toks/s, output: 69.59 toks/s]
[2025-01-11 00:20:46,913][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  4.07it/s, est. speed input: 2262.49 toks/s, output: 144.81 toks/s]
[2025-01-11 00:20:46,914][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.84it/s, est. speed input: 2262.49 toks/s, output: 144.81 toks/s]
WARNING 01-11 00:20:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:47,138][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:50,327][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:03<01:13,  3.19s/it, est. speed input: 225.21 toks/s, output: 6.59 toks/s]
[2025-01-11 00:20:50,522][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:31,  1.43s/it, est. speed input: 422.92 toks/s, output: 13.59 toks/s]
[2025-01-11 00:20:50,713][root][ERROR] - Processed prompts:  12%|#2        | 3/24 [00:03<00:18,  1.16it/s, est. speed input: 656.04 toks/s, output: 20.98 toks/s]
[2025-01-11 00:20:50,919][root][ERROR] - Processed prompts:  33%|###3      | 8/24 [00:03<00:03,  4.27it/s, est. speed input: 1640.24 toks/s, output: 59.78 toks/s]
[2025-01-11 00:20:51,489][root][ERROR] - Processed prompts:  42%|####1     | 10/24 [00:04<00:03,  4.01it/s, est. speed input: 1816.08 toks/s, output: 74.02 toks/s]
[2025-01-11 00:20:51,631][root][ERROR] - Processed prompts:  46%|####5     | 11/24 [00:04<00:02,  4.34it/s, est. speed input: 1940.90 toks/s, output: 83.47 toks/s]
[2025-01-11 00:20:51,733][root][ERROR] - Processed prompts:  54%|#####4    | 13/24 [00:04<00:01,  5.89it/s, est. speed input: 2226.41 toks/s, output: 105.78 toks/s]
[2025-01-11 00:20:51,850][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:04<00:00,  8.70it/s, est. speed input: 2661.89 toks/s, output: 140.08 toks/s]
[2025-01-11 00:20:52,039][root][ERROR] - Processed prompts:  75%|#######5  | 18/24 [00:04<00:00,  9.18it/s, est. speed input: 2864.02 toks/s, output: 161.83 toks/s]
[2025-01-11 00:20:52,302][root][ERROR] - Processed prompts:  88%|########7 | 21/24 [00:05<00:00,  9.90it/s, est. speed input: 3156.98 toks/s, output: 195.20 toks/s]
[2025-01-11 00:20:52,487][root][ERROR] - Processed prompts:  96%|#########5| 23/24 [00:05<00:00, 10.14it/s, est. speed input: 3343.13 toks/s, output: 219.88 toks/s]
[2025-01-11 00:20:54,029][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:06<00:00,  3.48it/s, est. speed input: 2720.97 toks/s, output: 196.95 toks/s]
WARNING 01-11 00:20:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:54,267][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:20:57,066][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:41,  2.80s/it, est. speed input: 369.79 toks/s, output: 10.36 toks/s]
[2025-01-11 00:20:57,411][root][ERROR] - Processed prompts:  38%|###7      | 6/16 [00:03<00:04,  2.47it/s, est. speed input: 1801.99 toks/s, output: 58.84 toks/s]
[2025-01-11 00:20:57,605][root][ERROR] - Processed prompts:  56%|#####6    | 9/16 [00:03<00:01,  3.87it/s, est. speed input: 2462.54 toks/s, output: 94.37 toks/s]
[2025-01-11 00:20:57,866][root][ERROR] - Processed prompts:  69%|######8   | 11/16 [00:03<00:01,  4.52it/s, est. speed input: 2778.02 toks/s, output: 119.18 toks/s]
[2025-01-11 00:20:58,177][root][ERROR] - Processed prompts:  81%|########1 | 13/16 [00:03<00:00,  4.95it/s, est. speed input: 3002.59 toks/s, output: 143.73 toks/s]
[2025-01-11 00:20:58,488][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:04<00:00,  6.09it/s, est. speed input: 3439.81 toks/s, output: 189.05 toks/s]
[2025-01-11 00:20:58,488][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:04<00:00,  3.79it/s, est. speed input: 3439.81 toks/s, output: 189.05 toks/s]
WARNING 01-11 00:20:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:20:58,695][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:00,348][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:16,  1.65s/it, est. speed input: 520.28 toks/s, output: 9.07 toks/s]
[2025-01-11 00:21:00,762][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:08,  1.08it/s, est. speed input: 890.09 toks/s, output: 21.28 toks/s]
[2025-01-11 00:21:00,877][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:02<00:04,  1.80it/s, est. speed input: 1256.03 toks/s, output: 35.30 toks/s]
[2025-01-11 00:21:00,983][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:01,  3.70it/s, est. speed input: 1894.56 toks/s, output: 64.70 toks/s]
[2025-01-11 00:21:01,415][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:00,  4.06it/s, est. speed input: 2213.37 toks/s, output: 88.59 toks/s]
[2025-01-11 00:21:01,700][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  3.91it/s, est. speed input: 2267.96 toks/s, output: 103.18 toks/s]
[2025-01-11 00:21:02,102][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  3.41it/s, est. speed input: 2252.43 toks/s, output: 117.13 toks/s]
[2025-01-11 00:21:02,799][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  3.16it/s, est. speed input: 2368.18 toks/s, output: 151.82 toks/s]
[2025-01-11 00:21:02,799][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.68it/s, est. speed input: 2368.18 toks/s, output: 151.82 toks/s]
WARNING 01-11 00:21:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:03,061][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:05,335][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:25,  2.27s/it, est. speed input: 380.00 toks/s, output: 12.75 toks/s]
[2025-01-11 00:21:05,564][root][ERROR] - Processed prompts:  33%|###3      | 4/12 [00:02<00:03,  2.03it/s, est. speed input: 1410.51 toks/s, output: 49.56 toks/s]
[2025-01-11 00:21:05,748][root][ERROR] - Processed prompts:  50%|#####     | 6/12 [00:02<00:01,  3.15it/s, est. speed input: 1966.18 toks/s, output: 76.32 toks/s]
[2025-01-11 00:21:05,914][root][ERROR] - Processed prompts:  67%|######6   | 8/12 [00:02<00:00,  4.39it/s, est. speed input: 2467.27 toks/s, output: 106.57 toks/s]
[2025-01-11 00:21:06,098][root][ERROR] - Processed prompts:  83%|########3 | 10/12 [00:03<00:00,  5.57it/s, est. speed input: 2920.85 toks/s, output: 137.02 toks/s]
[2025-01-11 00:21:08,427][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:05<00:00,  1.96it/s, est. speed input: 2037.69 toks/s, output: 132.14 toks/s]
[2025-01-11 00:21:08,427][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:05<00:00,  2.24it/s, est. speed input: 2037.69 toks/s, output: 132.14 toks/s]
WARNING 01-11 00:21:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:08,644][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:10,561][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:19,  1.92s/it, est. speed input: 426.38 toks/s, output: 11.48 toks/s]
[2025-01-11 00:21:10,769][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:08,  1.10it/s, est. speed input: 880.72 toks/s, output: 24.01 toks/s]
[2025-01-11 00:21:11,085][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.99it/s, est. speed input: 1848.79 toks/s, output: 62.28 toks/s]
[2025-01-11 00:21:11,275][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:00,  4.22it/s, est. speed input: 2395.64 toks/s, output: 93.53 toks/s]
[2025-01-11 00:21:11,669][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  3.67it/s, est. speed input: 2406.66 toks/s, output: 103.47 toks/s]
[2025-01-11 00:21:11,811][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  4.15it/s, est. speed input: 2601.49 toks/s, output: 122.20 toks/s]
[2025-01-11 00:21:11,922][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  4.82it/s, est. speed input: 2784.16 toks/s, output: 142.47 toks/s]
[2025-01-11 00:21:12,326][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.84it/s, est. speed input: 2774.78 toks/s, output: 155.09 toks/s]
[2025-01-11 00:21:12,326][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  2.99it/s, est. speed input: 2774.78 toks/s, output: 155.09 toks/s]
WARNING 01-11 00:21:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:12,541][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:14,106][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.57s/it, est. speed input: 575.51 toks/s, output: 18.52 toks/s]
[2025-01-11 00:21:14,781][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:02<00:00,  2.09it/s, est. speed input: 2031.26 toks/s, output: 66.52 toks/s]
[2025-01-11 00:21:14,947][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.55it/s, est. speed input: 2329.46 toks/s, output: 91.43 toks/s]
[2025-01-11 00:21:16,290][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.51it/s, est. speed input: 1743.83 toks/s, output: 98.95 toks/s]
[2025-01-11 00:21:16,290][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.60it/s, est. speed input: 1743.83 toks/s, output: 98.95 toks/s]
WARNING 01-11 00:21:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:16,507][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:19,108][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:36,  2.60s/it, est. speed input: 292.29 toks/s, output: 9.23 toks/s]
[2025-01-11 00:21:19,287][root][ERROR] - Processed prompts:  13%|#3        | 2/15 [00:02<00:15,  1.18s/it, est. speed input: 582.11 toks/s, output: 19.07 toks/s]
[2025-01-11 00:21:19,657][root][ERROR] - Processed prompts:  60%|######    | 9/15 [00:03<00:01,  4.63it/s, est. speed input: 2809.58 toks/s, output: 86.37 toks/s]
[2025-01-11 00:21:19,964][root][ERROR] - Processed prompts:  87%|########6 | 13/15 [00:03<00:00,  6.29it/s, est. speed input: 3730.56 toks/s, output: 133.64 toks/s]
[2025-01-11 00:21:20,643][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  5.01it/s, est. speed input: 3565.19 toks/s, output: 152.59 toks/s]
[2025-01-11 00:21:20,643][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  3.63it/s, est. speed input: 3565.19 toks/s, output: 152.59 toks/s]
WARNING 01-11 00:21:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:20,915][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:22,508][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.59s/it, est. speed input: 599.69 toks/s, output: 18.21 toks/s]
[2025-01-11 00:21:22,803][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.33it/s, est. speed input: 2986.74 toks/s, output: 85.30 toks/s]
[2025-01-11 00:21:23,610][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.39it/s, est. speed input: 2514.87 toks/s, output: 94.27 toks/s]
[2025-01-11 00:21:23,610][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.23it/s, est. speed input: 2514.87 toks/s, output: 94.27 toks/s]
WARNING 01-11 00:21:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:23,821][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:25,673][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.85s/it, est. speed input: 580.76 toks/s, output: 15.67 toks/s]
[2025-01-11 00:21:25,940][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.65it/s, est. speed input: 2985.95 toks/s, output: 89.22 toks/s]
[2025-01-11 00:21:26,382][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.90it/s, est. speed input: 3324.23 toks/s, output: 117.95 toks/s]
[2025-01-11 00:21:26,382][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.12it/s, est. speed input: 3324.23 toks/s, output: 117.95 toks/s]
WARNING 01-11 00:21:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:26,632][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:28,192][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.56s/it, est. speed input: 775.37 toks/s, output: 18.60 toks/s]
[2025-01-11 00:21:28,432][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.78it/s, est. speed input: 2431.89 toks/s, output: 71.12 toks/s]
[2025-01-11 00:21:28,727][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.93it/s, est. speed input: 2624.46 toks/s, output: 88.34 toks/s]
[2025-01-11 00:21:29,296][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.46it/s, est. speed input: 2391.81 toks/s, output: 103.63 toks/s]
[2025-01-11 00:21:29,296][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.25it/s, est. speed input: 2391.81 toks/s, output: 103.63 toks/s]
WARNING 01-11 00:21:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:29,488][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:30,440][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 1086.39 toks/s, output: 30.50 toks/s]
[2025-01-11 00:21:30,894][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.52it/s, est. speed input: 1660.67 toks/s, output: 60.48 toks/s]
[2025-01-11 00:21:30,894][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.42it/s, est. speed input: 1660.67 toks/s, output: 60.48 toks/s]
WARNING 01-11 00:21:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:31,121][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:32,022][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.11it/s, est. speed input: 1327.35 toks/s, output: 29.99 toks/s]
[2025-01-11 00:21:32,459][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.59it/s, est. speed input: 1651.90 toks/s, output: 59.80 toks/s]
[2025-01-11 00:21:32,459][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.49it/s, est. speed input: 1651.90 toks/s, output: 59.80 toks/s]
WARNING 01-11 00:21:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:32,649][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:33,928][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 1091.33 toks/s, output: 44.56 toks/s]
[2025-01-11 00:21:33,929][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 1091.33 toks/s, output: 44.56 toks/s]
WARNING 01-11 00:21:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:34,133][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:37,413][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:03<00:00,  3.28s/it, est. speed input: 382.02 toks/s, output: 53.66 toks/s]
[2025-01-11 00:21:37,413][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:03<00:00,  3.28s/it, est. speed input: 382.02 toks/s, output: 53.66 toks/s]
WARNING 01-11 00:21:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:21:37,601][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:21:39,068][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 1001.32 toks/s, output: 46.38 toks/s]
[2025-01-11 00:21:39,068][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 1001.32 toks/s, output: 46.38 toks/s]
[2025-01-11 00:21:41,154][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:21:41,207][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:21:42,804][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 00:21:44,520][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 00:21:46,143][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 00:21:46,724][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 00:21:46,724][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 00:22:07,745][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:22:07,797][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:22:09,781][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 00:22:11,404][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 00:22:12,963][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 00:22:13,484][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 00:22:13,484][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 00:22:33 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:22:33 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:22:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:22:34 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:22:34,614][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:22:35,950][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 00:22:36,458][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.18it/s]
[2025-01-11 00:22:38,138][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.23s/it]
[2025-01-11 00:22:39,994][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.48s/it]
[2025-01-11 00:22:39,994][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.34s/it]
INFO 01-11 00:22:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:22:53 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:22:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:22:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:23:15 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:23:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:15,737][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:19,725][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.99s/it, est. speed input: 159.23 toks/s, output: 3.26 toks/s]
[2025-01-11 00:23:19,846][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.71s/it, est. speed input: 309.08 toks/s, output: 6.81 toks/s]
[2025-01-11 00:23:19,964][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.02it/s, est. speed input: 450.71 toks/s, output: 10.65 toks/s]
[2025-01-11 00:23:20,076][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.79it/s, est. speed input: 878.09 toks/s, output: 23.05 toks/s]
[2025-01-11 00:23:20,235][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:06,  4.00it/s, est. speed input: 1129.39 toks/s, output: 32.01 toks/s]
[2025-01-11 00:23:20,427][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:02,  6.94it/s, est. speed input: 1624.89 toks/s, output: 51.39 toks/s]
[2025-01-11 00:23:20,583][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01, 11.18it/s, est. speed input: 2227.60 toks/s, output: 77.80 toks/s]
[2025-01-11 00:23:20,767][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00, 17.12it/s, est. speed input: 3029.68 toks/s, output: 119.87 toks/s]
[2025-01-11 00:23:21,026][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 15.43it/s, est. speed input: 3241.93 toks/s, output: 138.60 toks/s]
[2025-01-11 00:23:21,555][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 10.80it/s, est. speed input: 3274.15 toks/s, output: 155.72 toks/s]
[2025-01-11 00:23:22,031][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  8.31it/s, est. speed input: 3228.61 toks/s, output: 172.08 toks/s]
[2025-01-11 00:23:22,031][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  5.08it/s, est. speed input: 3228.61 toks/s, output: 172.08 toks/s]
WARNING 01-11 00:23:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:22,275][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:23,380][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.10s/it, est. speed input: 619.29 toks/s, output: 16.30 toks/s]
[2025-01-11 00:23:23,629][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.66it/s, est. speed input: 1073.64 toks/s, output: 34.73 toks/s]
[2025-01-11 00:23:23,749][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  3.82it/s, est. speed input: 1946.18 toks/s, output: 75.35 toks/s]
[2025-01-11 00:23:24,135][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.36it/s, est. speed input: 2316.71 toks/s, output: 110.24 toks/s]
[2025-01-11 00:23:24,135][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.23it/s, est. speed input: 2316.71 toks/s, output: 110.24 toks/s]
WARNING 01-11 00:23:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:24,363][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:27,017][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:02<01:06,  2.65s/it, est. speed input: 247.86 toks/s, output: 4.90 toks/s]
[2025-01-11 00:23:27,320][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:02<00:18,  1.25it/s, est. speed input: 665.92 toks/s, output: 15.56 toks/s]
[2025-01-11 00:23:27,637][root][ERROR] - Processed prompts:  27%|##6       | 7/26 [00:03<00:05,  3.22it/s, est. speed input: 1402.29 toks/s, output: 40.92 toks/s]
[2025-01-11 00:23:27,803][root][ERROR] - Processed prompts:  38%|###8      | 10/26 [00:03<00:03,  4.92it/s, est. speed input: 1911.69 toks/s, output: 63.37 toks/s]
[2025-01-11 00:23:28,026][root][ERROR] - Processed prompts:  46%|####6     | 12/26 [00:03<00:02,  5.65it/s, est. speed input: 2158.98 toks/s, output: 78.34 toks/s]
[2025-01-11 00:23:28,166][root][ERROR] - Processed prompts:  54%|#####3    | 14/26 [00:03<00:01,  6.85it/s, est. speed input: 2429.27 toks/s, output: 96.22 toks/s]
[2025-01-11 00:23:28,326][root][ERROR] - Processed prompts:  62%|######1   | 16/26 [00:03<00:01,  7.90it/s, est. speed input: 2666.14 toks/s, output: 114.31 toks/s]
[2025-01-11 00:23:28,597][root][ERROR] - Processed prompts:  73%|#######3  | 19/26 [00:04<00:00,  8.88it/s, est. speed input: 2961.70 toks/s, output: 141.92 toks/s]
[2025-01-11 00:23:28,720][root][ERROR] - Processed prompts:  81%|########  | 21/26 [00:04<00:00, 10.12it/s, est. speed input: 3192.53 toks/s, output: 164.55 toks/s]
[2025-01-11 00:23:28,850][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:04<00:00, 11.19it/s, est. speed input: 3402.21 toks/s, output: 187.88 toks/s]
[2025-01-11 00:23:29,081][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:04<00:00, 10.33it/s, est. speed input: 3514.76 toks/s, output: 210.69 toks/s]
[2025-01-11 00:23:29,630][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  4.94it/s, est. speed input: 3272.42 toks/s, output: 209.78 toks/s]
WARNING 01-11 00:23:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:29,866][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:33,148][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:22,  3.28s/it, est. speed input: 226.40 toks/s, output: 5.79 toks/s]
[2025-01-11 00:23:33,664][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:39,  1.65s/it, est. speed input: 398.68 toks/s, output: 12.64 toks/s]
[2025-01-11 00:23:33,859][root][ERROR] - Processed prompts:  19%|#9        | 5/26 [00:03<00:10,  1.94it/s, est. speed input: 932.99 toks/s, output: 35.57 toks/s]
[2025-01-11 00:23:33,995][root][ERROR] - Processed prompts:  27%|##6       | 7/26 [00:04<00:06,  2.99it/s, est. speed input: 1267.36 toks/s, output: 51.35 toks/s]
[2025-01-11 00:23:34,203][root][ERROR] - Processed prompts:  38%|###8      | 10/26 [00:04<00:03,  4.73it/s, est. speed input: 1733.05 toks/s, output: 75.63 toks/s]
[2025-01-11 00:23:34,382][root][ERROR] - Processed prompts:  50%|#####     | 13/26 [00:04<00:01,  6.63it/s, est. speed input: 2168.46 toks/s, output: 101.19 toks/s]
[2025-01-11 00:23:34,718][root][ERROR] - Processed prompts:  58%|#####7    | 15/26 [00:04<00:01,  6.43it/s, est. speed input: 2333.83 toks/s, output: 116.46 toks/s]
[2025-01-11 00:23:34,842][root][ERROR] - Processed prompts:  65%|######5   | 17/26 [00:04<00:01,  7.75it/s, est. speed input: 2603.07 toks/s, output: 137.48 toks/s]
[2025-01-11 00:23:34,975][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:05<00:00, 10.29it/s, est. speed input: 2990.56 toks/s, output: 170.50 toks/s]
[2025-01-11 00:23:35,298][root][ERROR] - Processed prompts:  85%|########4 | 22/26 [00:05<00:00,  8.76it/s, est. speed input: 3102.24 toks/s, output: 187.42 toks/s]
[2025-01-11 00:23:35,503][root][ERROR] - Processed prompts:  92%|#########2| 24/26 [00:05<00:00,  9.01it/s, est. speed input: 3275.44 toks/s, output: 210.93 toks/s]
[2025-01-11 00:23:35,855][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  7.74it/s, est. speed input: 3352.31 toks/s, output: 231.77 toks/s]
[2025-01-11 00:23:35,855][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  4.34it/s, est. speed input: 3352.31 toks/s, output: 231.77 toks/s]
WARNING 01-11 00:23:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:36,061][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:37,401][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.34s/it, est. speed input: 494.09 toks/s, output: 21.64 toks/s]
[2025-01-11 00:23:37,745][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.89it/s, est. speed input: 1776.10 toks/s, output: 81.35 toks/s]
[2025-01-11 00:23:38,014][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.94it/s, est. speed input: 2207.73 toks/s, output: 125.98 toks/s]
[2025-01-11 00:23:38,014][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.07it/s, est. speed input: 2207.73 toks/s, output: 125.98 toks/s]
WARNING 01-11 00:23:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:38,253][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:39,646][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.39s/it, est. speed input: 585.33 toks/s, output: 20.83 toks/s]
[2025-01-11 00:23:39,755][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.49it/s, est. speed input: 1680.55 toks/s, output: 61.28 toks/s]
[2025-01-11 00:23:39,912][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.25it/s, est. speed input: 2512.19 toks/s, output: 104.93 toks/s]
[2025-01-11 00:23:40,147][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.17it/s, est. speed input: 2652.84 toks/s, output: 121.47 toks/s]
WARNING 01-11 00:23:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:40,390][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:43,342][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:02<01:10,  2.95s/it, est. speed input: 241.51 toks/s, output: 4.74 toks/s]
[2025-01-11 00:23:43,595][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:31,  1.36s/it, est. speed input: 446.84 toks/s, output: 10.30 toks/s]
[2025-01-11 00:23:44,072][root][ERROR] - Processed prompts:  16%|#6        | 4/25 [00:03<00:13,  1.52it/s, est. speed input: 781.84 toks/s, output: 22.27 toks/s]
[2025-01-11 00:23:44,231][root][ERROR] - Processed prompts:  40%|####      | 10/25 [00:03<00:02,  5.05it/s, est. speed input: 2080.60 toks/s, output: 68.22 toks/s]
[2025-01-11 00:23:44,448][root][ERROR] - Processed prompts:  48%|####8     | 12/25 [00:04<00:02,  5.72it/s, est. speed input: 2344.49 toks/s, output: 83.28 toks/s]
[2025-01-11 00:23:44,651][root][ERROR] - Processed prompts:  56%|#####6    | 14/25 [00:04<00:01,  6.44it/s, est. speed input: 2594.95 toks/s, output: 100.20 toks/s]
[2025-01-11 00:23:44,925][root][ERROR] - Processed prompts:  64%|######4   | 16/25 [00:04<00:01,  6.66it/s, est. speed input: 2776.74 toks/s, output: 116.87 toks/s]
[2025-01-11 00:23:45,036][root][ERROR] - Processed prompts:  72%|#######2  | 18/25 [00:04<00:00,  8.11it/s, est. speed input: 3025.70 toks/s, output: 138.61 toks/s]
[2025-01-11 00:23:45,182][root][ERROR] - Processed prompts:  80%|########  | 20/25 [00:04<00:00,  9.20it/s, est. speed input: 3251.38 toks/s, output: 159.85 toks/s]
[2025-01-11 00:23:45,560][root][ERROR] - Processed prompts:  88%|########8 | 22/25 [00:05<00:00,  7.57it/s, est. speed input: 3330.19 toks/s, output: 177.56 toks/s]
[2025-01-11 00:23:45,992][root][ERROR] - Processed prompts:  96%|#########6| 24/25 [00:05<00:00,  6.38it/s, est. speed input: 3376.14 toks/s, output: 199.57 toks/s]
[2025-01-11 00:23:46,160][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:05<00:00,  6.30it/s, est. speed input: 3423.73 toks/s, output: 213.34 toks/s]
[2025-01-11 00:23:46,160][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:05<00:00,  4.33it/s, est. speed input: 3423.73 toks/s, output: 213.34 toks/s]
WARNING 01-11 00:23:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:46,434][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:49,472][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:03<00:54,  3.04s/it, est. speed input: 284.05 toks/s, output: 7.90 toks/s]
[2025-01-11 00:23:49,683][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:03<00:23,  1.38s/it, est. speed input: 564.77 toks/s, output: 16.31 toks/s]
[2025-01-11 00:23:49,882][root][ERROR] - Processed prompts:  42%|####2     | 8/19 [00:03<00:02,  3.92it/s, est. speed input: 2194.53 toks/s, output: 67.58 toks/s]
[2025-01-11 00:23:50,225][root][ERROR] - Processed prompts:  58%|#####7    | 11/19 [00:03<00:01,  4.93it/s, est. speed input: 2665.76 toks/s, output: 92.59 toks/s]
[2025-01-11 00:23:50,414][root][ERROR] - Processed prompts:  68%|######8   | 13/19 [00:03<00:01,  5.74it/s, est. speed input: 2950.65 toks/s, output: 114.84 toks/s]
[2025-01-11 00:23:51,138][root][ERROR] - Processed prompts:  79%|#######8  | 15/19 [00:04<00:00,  4.43it/s, est. speed input: 2885.77 toks/s, output: 128.40 toks/s]
[2025-01-11 00:23:51,282][root][ERROR] - Processed prompts:  89%|########9 | 17/19 [00:04<00:00,  5.48it/s, est. speed input: 3176.41 toks/s, output: 161.31 toks/s]
[2025-01-11 00:23:51,620][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  5.60it/s, est. speed input: 3342.88 toks/s, output: 190.31 toks/s]
[2025-01-11 00:23:51,621][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  3.66it/s, est. speed input: 3342.88 toks/s, output: 190.31 toks/s]
WARNING 01-11 00:23:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:51,832][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:53,589][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.76s/it, est. speed input: 516.90 toks/s, output: 15.94 toks/s]
[2025-01-11 00:23:53,762][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:01<00:01,  2.64it/s, est. speed input: 1764.05 toks/s, output: 62.71 toks/s]
[2025-01-11 00:23:54,101][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:02<00:00,  3.47it/s, est. speed input: 2179.14 toks/s, output: 96.95 toks/s]
[2025-01-11 00:23:54,482][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  3.23it/s, est. speed input: 2194.44 toks/s, output: 109.06 toks/s]
[2025-01-11 00:23:54,648][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  3.66it/s, est. speed input: 2330.29 toks/s, output: 130.33 toks/s]
[2025-01-11 00:23:56,702][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  1.34it/s, est. speed input: 1552.04 toks/s, output: 116.43 toks/s]
[2025-01-11 00:23:56,702][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  1.85it/s, est. speed input: 1552.04 toks/s, output: 116.43 toks/s]
WARNING 01-11 00:23:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:23:56,950][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:23:58,879][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.93s/it, est. speed input: 532.70 toks/s, output: 15.04 toks/s]
[2025-01-11 00:23:59,141][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:02<00:03,  1.67it/s, est. speed input: 1394.65 toks/s, output: 44.28 toks/s]
[2025-01-11 00:23:59,630][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:02<00:02,  1.79it/s, est. speed input: 1487.36 toks/s, output: 58.23 toks/s]
[2025-01-11 00:23:59,813][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  2.30it/s, est. speed input: 1720.44 toks/s, output: 77.90 toks/s]
[2025-01-11 00:23:59,967][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:03<00:01,  2.90it/s, est. speed input: 1956.32 toks/s, output: 98.46 toks/s]
[2025-01-11 00:24:00,210][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:03<00:00,  3.20it/s, est. speed input: 2135.80 toks/s, output: 117.51 toks/s]
[2025-01-11 00:24:00,486][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:03<00:00,  3.32it/s, est. speed input: 2231.97 toks/s, output: 136.90 toks/s]
[2025-01-11 00:24:00,840][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  3.15it/s, est. speed input: 2270.27 toks/s, output: 155.83 toks/s]
[2025-01-11 00:24:00,840][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.31it/s, est. speed input: 2270.27 toks/s, output: 155.83 toks/s]
WARNING 01-11 00:24:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:01,070][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:03,228][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:28,  2.16s/it, est. speed input: 367.54 toks/s, output: 8.34 toks/s]
[2025-01-11 00:24:03,501][root][ERROR] - Processed prompts:  14%|#4        | 2/14 [00:02<00:12,  1.05s/it, est. speed input: 729.68 toks/s, output: 18.10 toks/s]
[2025-01-11 00:24:03,655][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:02<00:01,  4.41it/s, est. speed input: 2474.26 toks/s, output: 73.88 toks/s]
[2025-01-11 00:24:04,139][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:03<00:01,  4.32it/s, est. speed input: 2722.97 toks/s, output: 91.88 toks/s]
[2025-01-11 00:24:04,612][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:03<00:00,  4.29it/s, est. speed input: 2859.86 toks/s, output: 118.28 toks/s]
[2025-01-11 00:24:04,840][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  5.88it/s, est. speed input: 3409.60 toks/s, output: 172.43 toks/s]
[2025-01-11 00:24:04,840][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  3.71it/s, est. speed input: 3409.60 toks/s, output: 172.43 toks/s]
WARNING 01-11 00:24:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:05,127][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:06,944][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.82s/it, est. speed input: 729.21 toks/s, output: 15.41 toks/s]
[2025-01-11 00:24:08,054][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  2.39it/s, est. speed input: 2391.84 toks/s, output: 79.25 toks/s]
[2025-01-11 00:24:08,156][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.82it/s, est. speed input: 2662.04 toks/s, output: 107.63 toks/s]
[2025-01-11 00:24:08,156][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.31it/s, est. speed input: 2662.04 toks/s, output: 107.63 toks/s]
WARNING 01-11 00:24:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:08,375][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:10,560][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.19s/it, est. speed input: 484.65 toks/s, output: 13.27 toks/s]
[2025-01-11 00:24:10,736][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.34it/s, est. speed input: 2393.40 toks/s, output: 77.92 toks/s]
[2025-01-11 00:24:11,050][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  3.97it/s, est. speed input: 2914.48 toks/s, output: 103.91 toks/s]
[2025-01-11 00:24:11,415][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  4.36it/s, est. speed input: 3154.87 toks/s, output: 133.52 toks/s]
[2025-01-11 00:24:11,903][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.60it/s, est. speed input: 2985.45 toks/s, output: 142.84 toks/s]
[2025-01-11 00:24:11,903][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.12it/s, est. speed input: 2985.45 toks/s, output: 142.84 toks/s]
WARNING 01-11 00:24:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:12,165][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:14,246][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:02<00:16,  2.08s/it, est. speed input: 605.67 toks/s, output: 13.94 toks/s]
[2025-01-11 00:24:14,379][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.22it/s, est. speed input: 3533.40 toks/s, output: 97.13 toks/s]
[2025-01-11 00:24:15,096][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.07it/s, est. speed input: 3507.84 toks/s, output: 113.62 toks/s]
WARNING 01-11 00:24:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:15,294][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:16,492][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.20s/it, est. speed input: 801.40 toks/s, output: 28.38 toks/s]
[2025-01-11 00:24:16,787][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.50it/s, est. speed input: 1577.36 toks/s, output: 56.26 toks/s]
[2025-01-11 00:24:16,804][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.99it/s, est. speed input: 2194.29 toks/s, output: 89.39 toks/s]
WARNING 01-11 00:24:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:17,051][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:18,166][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 1139.75 toks/s, output: 26.03 toks/s]
[2025-01-11 00:24:18,268][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  3.06it/s, est. speed input: 2832.90 toks/s, output: 76.48 toks/s]
[2025-01-11 00:24:18,268][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.47it/s, est. speed input: 2832.90 toks/s, output: 76.48 toks/s]
WARNING 01-11 00:24:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:18,467][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:19,371][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.11it/s, est. speed input: 1018.27 toks/s, output: 32.10 toks/s]
[2025-01-11 00:24:19,556][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  2.08it/s, est. speed input: 1774.89 toks/s, output: 63.36 toks/s]
[2025-01-11 00:24:19,557][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.84it/s, est. speed input: 1774.89 toks/s, output: 63.36 toks/s]
WARNING 01-11 00:24:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:19,777][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:20,736][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 1360.42 toks/s, output: 30.25 toks/s]
[2025-01-11 00:24:21,023][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.78it/s, est. speed input: 1991.83 toks/s, output: 60.24 toks/s]
[2025-01-11 00:24:21,023][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.61it/s, est. speed input: 1991.83 toks/s, output: 60.24 toks/s]
WARNING 01-11 00:24:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:24:21,228][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:24:22,625][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 888.80 toks/s, output: 46.55 toks/s]
[2025-01-11 00:24:22,625][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 888.80 toks/s, output: 46.55 toks/s]
[2025-01-11 00:24:24,949][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:24:25,001][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:24:26,629][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 00:24:28,263][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 00:24:29,807][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.59s/it]
[2025-01-11 00:24:30,321][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.17s/it]
[2025-01-11 00:24:30,321][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-11 00:24:51,876][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:24:51,928][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:24:53,984][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.05s/it]
[2025-01-11 00:24:55,555][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 00:24:57,190][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 00:24:57,711][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 00:24:57,711][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 00:25:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:25:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:25:18 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:25:18 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:25:18,226][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:25:19,554][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 00:25:19,934][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 00:25:21,242][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 00:25:22,575][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 00:25:22,576][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 00:25:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:25:36 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:25:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:25:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:25:58 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:25:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:25:58,804][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:02,424][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:52,  3.62s/it, est. speed input: 175.43 toks/s, output: 3.59 toks/s]
[2025-01-11 00:26:02,545][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:46,  1.56s/it, est. speed input: 339.50 toks/s, output: 7.49 toks/s]
[2025-01-11 00:26:02,663][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:26,  1.11it/s, est. speed input: 493.70 toks/s, output: 11.66 toks/s]
[2025-01-11 00:26:02,992][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.65it/s, est. speed input: 909.88 toks/s, output: 24.84 toks/s]
[2025-01-11 00:26:03,146][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:05,  4.56it/s, est. speed input: 1316.37 toks/s, output: 41.00 toks/s]
[2025-01-11 00:26:03,325][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  7.39it/s, est. speed input: 1825.97 toks/s, output: 63.48 toks/s]
[2025-01-11 00:26:03,478][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 12.56it/s, est. speed input: 2581.26 toks/s, output: 102.27 toks/s]
[2025-01-11 00:26:03,731][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 15.66it/s, est. speed input: 3222.16 toks/s, output: 141.88 toks/s]
[2025-01-11 00:26:03,847][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:05<00:00, 18.55it/s, est. speed input: 3651.72 toks/s, output: 174.90 toks/s]
[2025-01-11 00:26:04,186][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 14.75it/s, est. speed input: 3775.50 toks/s, output: 196.58 toks/s]
[2025-01-11 00:26:04,187][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.95it/s, est. speed input: 3775.50 toks/s, output: 196.58 toks/s]
WARNING 01-11 00:26:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:04,399][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:05,246][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.18it/s, est. speed input: 826.56 toks/s, output: 25.98 toks/s]
[2025-01-11 00:26:05,375][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:00<00:00,  2.35it/s, est. speed input: 1429.16 toks/s, output: 52.25 toks/s]
[2025-01-11 00:26:05,675][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.72it/s, est. speed input: 1655.12 toks/s, output: 76.80 toks/s]
[2025-01-11 00:26:05,676][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.35it/s, est. speed input: 1655.12 toks/s, output: 76.80 toks/s]
WARNING 01-11 00:26:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:05,912][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:09,259][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:33,  3.35s/it, est. speed input: 198.38 toks/s, output: 5.68 toks/s]
[2025-01-11 00:26:09,372][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:39,  1.44s/it, est. speed input: 391.34 toks/s, output: 11.56 toks/s]
[2025-01-11 00:26:09,529][root][ERROR] - Processed prompts:  17%|#7        | 5/29 [00:03<00:10,  2.23it/s, est. speed input: 914.12 toks/s, output: 29.59 toks/s]
[2025-01-11 00:26:09,728][root][ERROR] - Processed prompts:  24%|##4       | 7/29 [00:03<00:06,  3.26it/s, est. speed input: 1215.88 toks/s, output: 42.45 toks/s]
[2025-01-11 00:26:09,901][root][ERROR] - Processed prompts:  45%|####4     | 13/29 [00:03<00:02,  7.67it/s, est. speed input: 2158.65 toks/s, output: 85.48 toks/s]
[2025-01-11 00:26:10,088][root][ERROR] - Processed prompts:  52%|#####1    | 15/29 [00:04<00:01,  8.21it/s, est. speed input: 2380.56 toks/s, output: 98.89 toks/s]
[2025-01-11 00:26:10,251][root][ERROR] - Processed prompts:  66%|######5   | 19/29 [00:04<00:00, 11.18it/s, est. speed input: 2904.58 toks/s, output: 131.37 toks/s]
[2025-01-11 00:26:10,368][root][ERROR] - Processed prompts:  72%|#######2  | 21/29 [00:04<00:00, 12.11it/s, est. speed input: 3125.95 toks/s, output: 148.33 toks/s]
[2025-01-11 00:26:10,554][root][ERROR] - Processed prompts:  79%|#######9  | 23/29 [00:04<00:00, 11.76it/s, est. speed input: 3285.63 toks/s, output: 164.81 toks/s]
[2025-01-11 00:26:10,798][root][ERROR] - Processed prompts:  97%|#########6| 28/29 [00:04<00:00, 14.63it/s, est. speed input: 3811.60 toks/s, output: 214.06 toks/s]
[2025-01-11 00:26:13,037][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:07<00:00,  4.07it/s, est. speed input: 2709.61 toks/s, output: 174.88 toks/s]
WARNING 01-11 00:26:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:13,268][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:16,254][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:02<01:02,  2.99s/it, est. speed input: 259.20 toks/s, output: 7.37 toks/s]
[2025-01-11 00:26:16,577][root][ERROR] - Processed prompts:   9%|9         | 2/22 [00:03<00:28,  1.42s/it, est. speed input: 460.58 toks/s, output: 15.41 toks/s]
[2025-01-11 00:26:16,709][root][ERROR] - Processed prompts:  18%|#8        | 4/22 [00:03<00:10,  1.75it/s, est. speed input: 885.83 toks/s, output: 32.84 toks/s]
[2025-01-11 00:26:17,037][root][ERROR] - Processed prompts:  27%|##7       | 6/22 [00:03<00:06,  2.64it/s, est. speed input: 1216.63 toks/s, output: 49.36 toks/s]
[2025-01-11 00:26:17,187][root][ERROR] - Processed prompts:  36%|###6      | 8/22 [00:03<00:03,  3.90it/s, est. speed input: 1561.28 toks/s, output: 69.41 toks/s]
[2025-01-11 00:26:17,327][root][ERROR] - Processed prompts:  45%|####5     | 10/22 [00:04<00:02,  5.32it/s, est. speed input: 1887.25 toks/s, output: 90.19 toks/s]
[2025-01-11 00:26:17,620][root][ERROR] - Processed prompts:  55%|#####4    | 12/22 [00:04<00:01,  5.75it/s, est. speed input: 2107.75 toks/s, output: 109.85 toks/s]
[2025-01-11 00:26:17,746][root][ERROR] - Processed prompts:  86%|########6 | 19/22 [00:04<00:00, 13.26it/s, est. speed input: 3247.42 toks/s, output: 199.67 toks/s]
[2025-01-11 00:26:18,537][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:05<00:00,  7.81it/s, est. speed input: 3215.30 toks/s, output: 219.58 toks/s]
[2025-01-11 00:26:18,538][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:05<00:00,  4.17it/s, est. speed input: 3215.30 toks/s, output: 219.58 toks/s]
WARNING 01-11 00:26:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:18,768][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:20,506][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.74s/it, est. speed input: 380.99 toks/s, output: 14.96 toks/s]
[2025-01-11 00:26:20,756][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:01<00:01,  3.21it/s, est. speed input: 1750.09 toks/s, output: 72.46 toks/s]
[2025-01-11 00:26:20,937][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.55it/s, est. speed input: 1944.78 toks/s, output: 86.68 toks/s]
[2025-01-11 00:26:21,063][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  5.15it/s, est. speed input: 2555.09 toks/s, output: 124.20 toks/s]
[2025-01-11 00:26:21,419][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  5.31it/s, est. speed input: 2753.55 toks/s, output: 154.34 toks/s]
[2025-01-11 00:26:21,419][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.77it/s, est. speed input: 2753.55 toks/s, output: 154.34 toks/s]
WARNING 01-11 00:26:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:21,651][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:24,019][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:30,  2.37s/it, est. speed input: 358.13 toks/s, output: 11.83 toks/s]
[2025-01-11 00:26:24,211][root][ERROR] - Processed prompts:  29%|##8       | 4/14 [00:02<00:04,  2.00it/s, est. speed input: 1308.50 toks/s, output: 46.89 toks/s]
[2025-01-11 00:26:24,327][root][ERROR] - Processed prompts:  43%|####2     | 6/14 [00:02<00:02,  3.25it/s, est. speed input: 1810.17 toks/s, output: 72.50 toks/s]
[2025-01-11 00:26:24,538][root][ERROR] - Processed prompts:  57%|#####7    | 8/14 [00:02<00:01,  4.35it/s, est. speed input: 2273.15 toks/s, output: 96.66 toks/s]
[2025-01-11 00:26:24,769][root][ERROR] - Processed prompts:  71%|#######1  | 10/14 [00:03<00:00,  5.28it/s, est. speed input: 2647.99 toks/s, output: 122.86 toks/s]
[2025-01-11 00:26:25,097][root][ERROR] - Processed prompts:  86%|########5 | 12/14 [00:03<00:00,  5.53it/s, est. speed input: 2879.40 toks/s, output: 149.74 toks/s]
[2025-01-11 00:26:25,571][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  5.02it/s, est. speed input: 2935.51 toks/s, output: 176.29 toks/s]
[2025-01-11 00:26:25,571][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  3.57it/s, est. speed input: 2935.51 toks/s, output: 176.29 toks/s]
WARNING 01-11 00:26:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:25,806][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:28,526][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:46,  2.72s/it, est. speed input: 273.52 toks/s, output: 9.56 toks/s]
[2025-01-11 00:26:28,649][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:02<00:19,  1.19s/it, est. speed input: 575.78 toks/s, output: 19.35 toks/s]
[2025-01-11 00:26:29,160][root][ERROR] - Processed prompts:  22%|##2       | 4/18 [00:03<00:08,  1.65it/s, est. speed input: 924.42 toks/s, output: 37.86 toks/s]
[2025-01-11 00:26:29,516][root][ERROR] - Processed prompts:  28%|##7       | 5/18 [00:03<00:06,  1.89it/s, est. speed input: 1044.33 toks/s, output: 48.51 toks/s]
[2025-01-11 00:26:29,686][root][ERROR] - Processed prompts:  33%|###3      | 6/18 [00:03<00:05,  2.39it/s, est. speed input: 1198.10 toks/s, output: 61.34 toks/s]
[2025-01-11 00:26:29,812][root][ERROR] - Processed prompts:  44%|####4     | 8/18 [00:04<00:02,  3.96it/s, est. speed input: 1549.21 toks/s, output: 89.37 toks/s]
[2025-01-11 00:26:29,949][root][ERROR] - Processed prompts:  67%|######6   | 12/18 [00:04<00:00,  7.83it/s, est. speed input: 2234.14 toks/s, output: 148.68 toks/s]
[2025-01-11 00:26:30,193][root][ERROR] - Processed prompts:  83%|########3 | 15/18 [00:04<00:00,  9.08it/s, est. speed input: 2658.42 toks/s, output: 189.63 toks/s]
[2025-01-11 00:26:30,655][root][ERROR] - Processed prompts:  94%|#########4| 17/18 [00:04<00:00,  7.03it/s, est. speed input: 2734.84 toks/s, output: 209.09 toks/s]
[2025-01-11 00:26:32,046][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:06<00:00,  2.88it/s, est. speed input: 2244.25 toks/s, output: 192.31 toks/s]
WARNING 01-11 00:26:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:32,287][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:34,875][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:33,  2.59s/it, est. speed input: 350.54 toks/s, output: 11.21 toks/s]
[2025-01-11 00:26:35,138][root][ERROR] - Processed prompts:  29%|##8       | 4/14 [00:02<00:05,  1.78it/s, est. speed input: 1317.01 toks/s, output: 43.50 toks/s]
[2025-01-11 00:26:35,881][root][ERROR] - Processed prompts:  36%|###5      | 5/14 [00:03<00:05,  1.64it/s, est. speed input: 1294.80 toks/s, output: 51.48 toks/s]
[2025-01-11 00:26:36,169][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:03<00:01,  3.57it/s, est. speed input: 2098.03 toks/s, output: 114.12 toks/s]
[2025-01-11 00:26:36,663][root][ERROR] - Processed prompts:  93%|#########2| 13/14 [00:04<00:00,  4.83it/s, est. speed input: 2701.77 toks/s, output: 174.13 toks/s]
[2025-01-11 00:26:36,997][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  4.44it/s, est. speed input: 2723.04 toks/s, output: 186.43 toks/s]
[2025-01-11 00:26:36,997][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  2.97it/s, est. speed input: 2723.04 toks/s, output: 186.43 toks/s]
WARNING 01-11 00:26:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:37,400][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:39,984][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:38,  2.58s/it, est. speed input: 286.38 toks/s, output: 9.29 toks/s]
[2025-01-11 00:26:40,175][root][ERROR] - Processed prompts:  12%|#2        | 2/16 [00:02<00:16,  1.18s/it, est. speed input: 540.14 toks/s, output: 19.10 toks/s]
[2025-01-11 00:26:40,286][root][ERROR] - Processed prompts:  25%|##5       | 4/16 [00:02<00:05,  2.11it/s, est. speed input: 1035.95 toks/s, output: 40.19 toks/s]
[2025-01-11 00:26:40,627][root][ERROR] - Processed prompts:  31%|###1      | 5/16 [00:03<00:04,  2.31it/s, est. speed input: 1178.03 toks/s, output: 48.95 toks/s]
[2025-01-11 00:26:40,759][root][ERROR] - Processed prompts:  38%|###7      | 6/16 [00:03<00:03,  2.94it/s, est. speed input: 1418.18 toks/s, output: 60.74 toks/s]
[2025-01-11 00:26:41,215][root][ERROR] - Processed prompts:  62%|######2   | 10/16 [00:03<00:01,  5.09it/s, est. speed input: 2079.55 toks/s, output: 107.46 toks/s]
[2025-01-11 00:26:41,386][root][ERROR] - Processed prompts:  69%|######8   | 11/16 [00:03<00:00,  5.22it/s, est. speed input: 2227.55 toks/s, output: 120.42 toks/s]
[2025-01-11 00:26:41,521][root][ERROR] - Processed prompts:  81%|########1 | 13/16 [00:04<00:00,  6.72it/s, est. speed input: 2567.38 toks/s, output: 152.62 toks/s]
[2025-01-11 00:26:41,681][root][ERROR] - Processed prompts:  94%|#########3| 15/16 [00:04<00:00,  7.95it/s, est. speed input: 2874.71 toks/s, output: 185.03 toks/s]
[2025-01-11 00:26:41,717][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:04<00:00,  3.71it/s, est. speed input: 3047.76 toks/s, output: 203.38 toks/s]
WARNING 01-11 00:26:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:42,196][root][ERROR] - Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:45,190][root][ERROR] - Processed prompts:   6%|5         | 1/17 [00:02<00:47,  2.99s/it, est. speed input: 279.98 toks/s, output: 8.02 toks/s]
[2025-01-11 00:26:45,391][root][ERROR] - Processed prompts:  12%|#1        | 2/17 [00:03<00:20,  1.35s/it, est. speed input: 561.75 toks/s, output: 16.59 toks/s]
[2025-01-11 00:26:45,623][root][ERROR] - Processed prompts:  41%|####1     | 7/17 [00:03<00:02,  3.36it/s, est. speed input: 2005.33 toks/s, output: 60.40 toks/s]
[2025-01-11 00:26:46,018][root][ERROR] - Processed prompts:  53%|#####2    | 9/17 [00:03<00:02,  3.76it/s, est. speed input: 2269.05 toks/s, output: 77.98 toks/s]
[2025-01-11 00:26:46,378][root][ERROR] - Processed prompts:  71%|#######   | 12/17 [00:04<00:01,  4.83it/s, est. speed input: 2692.65 toks/s, output: 110.23 toks/s]
[2025-01-11 00:26:46,972][root][ERROR] - Processed prompts:  94%|#########4| 16/17 [00:04<00:00,  5.51it/s, est. speed input: 3186.98 toks/s, output: 157.05 toks/s]
[2025-01-11 00:26:47,253][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:05<00:00,  5.13it/s, est. speed input: 3190.74 toks/s, output: 169.87 toks/s]
[2025-01-11 00:26:47,253][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:05<00:00,  3.36it/s, est. speed input: 3190.74 toks/s, output: 169.87 toks/s]
WARNING 01-11 00:26:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:47,682][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:49,660][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:19,  1.98s/it, est. speed input: 381.36 toks/s, output: 9.61 toks/s]
[2025-01-11 00:26:49,957][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:08,  1.01it/s, est. speed input: 787.43 toks/s, output: 21.10 toks/s]
[2025-01-11 00:26:50,177][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.72it/s, est. speed input: 2322.35 toks/s, output: 69.35 toks/s]
[2025-01-11 00:26:50,681][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.13it/s, est. speed input: 2281.98 toks/s, output: 77.72 toks/s]
[2025-01-11 00:26:50,791][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  3.70it/s, est. speed input: 2484.23 toks/s, output: 95.88 toks/s]
[2025-01-11 00:26:50,933][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  4.20it/s, est. speed input: 2654.72 toks/s, output: 113.84 toks/s]
[2025-01-11 00:26:51,338][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.54it/s, est. speed input: 2651.68 toks/s, output: 126.95 toks/s]
[2025-01-11 00:26:51,406][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  2.95it/s, est. speed input: 2868.69 toks/s, output: 150.95 toks/s]
WARNING 01-11 00:26:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:51,664][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:53,115][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.45s/it, est. speed input: 610.59 toks/s, output: 11.03 toks/s]
[2025-01-11 00:26:53,458][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.25it/s, est. speed input: 1168.54 toks/s, output: 25.09 toks/s]
[2025-01-11 00:26:53,826][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:03,  1.66it/s, est. speed input: 1409.27 toks/s, output: 41.18 toks/s]
[2025-01-11 00:26:54,239][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:02,  1.90it/s, est. speed input: 1580.35 toks/s, output: 58.65 toks/s]
[2025-01-11 00:26:54,767][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:03<00:00,  3.26it/s, est. speed input: 2299.06 toks/s, output: 119.88 toks/s]
[2025-01-11 00:26:55,170][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  3.04it/s, est. speed input: 2281.88 toks/s, output: 138.62 toks/s]
[2025-01-11 00:26:55,170][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.28it/s, est. speed input: 2281.88 toks/s, output: 138.62 toks/s]
WARNING 01-11 00:26:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:55,392][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:26:57,863][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:29,  2.47s/it, est. speed input: 403.93 toks/s, output: 11.33 toks/s]
[2025-01-11 00:26:58,306][root][ERROR] - Processed prompts:  46%|####6     | 6/13 [00:02<00:02,  2.62it/s, est. speed input: 2058.12 toks/s, output: 65.20 toks/s]
[2025-01-11 00:26:58,410][root][ERROR] - Processed prompts:  62%|######1   | 8/13 [00:03<00:01,  3.69it/s, est. speed input: 2597.36 toks/s, output: 94.42 toks/s]
[2025-01-11 00:26:58,567][root][ERROR] - Processed prompts:  77%|#######6  | 10/13 [00:03<00:00,  4.79it/s, est. speed input: 3131.79 toks/s, output: 122.84 toks/s]
[2025-01-11 00:26:59,068][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:03<00:00,  4.50it/s, est. speed input: 3206.34 toks/s, output: 149.08 toks/s]
[2025-01-11 00:26:59,186][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  4.93it/s, est. speed input: 3385.28 toks/s, output: 167.35 toks/s]
[2025-01-11 00:26:59,187][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.43it/s, est. speed input: 3385.28 toks/s, output: 167.35 toks/s]
WARNING 01-11 00:26:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:26:59,415][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:00,461][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.05s/it, est. speed input: 938.70 toks/s, output: 11.48 toks/s]
[2025-01-11 00:27:00,837][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.53it/s, est. speed input: 1480.13 toks/s, output: 28.84 toks/s]
[2025-01-11 00:27:01,949][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.22it/s, est. speed input: 2230.57 toks/s, output: 76.56 toks/s]
[2025-01-11 00:27:01,950][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.97it/s, est. speed input: 2230.57 toks/s, output: 76.56 toks/s]
WARNING 01-11 00:27:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:27:02,168][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:04,530][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:28,  2.36s/it, est. speed input: 431.14 toks/s, output: 10.16 toks/s]
[2025-01-11 00:27:04,697][root][ERROR] - Processed prompts:  15%|#5        | 2/13 [00:02<00:11,  1.07s/it, est. speed input: 887.28 toks/s, output: 20.97 toks/s]
[2025-01-11 00:27:04,814][root][ERROR] - Processed prompts:  38%|###8      | 5/13 [00:02<00:02,  3.01it/s, est. speed input: 1863.28 toks/s, output: 55.18 toks/s]
[2025-01-11 00:27:05,136][root][ERROR] - Processed prompts:  54%|#####3    | 7/13 [00:02<00:01,  3.81it/s, est. speed input: 2281.15 toks/s, output: 77.18 toks/s]
[2025-01-11 00:27:05,376][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:03<00:00,  4.75it/s, est. speed input: 2746.58 toks/s, output: 104.44 toks/s]
[2025-01-11 00:27:05,484][root][ERROR] - Processed prompts:  85%|########4 | 11/13 [00:03<00:00,  6.39it/s, est. speed input: 3261.47 toks/s, output: 136.64 toks/s]
[2025-01-11 00:27:05,726][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  6.91it/s, est. speed input: 3669.40 toks/s, output: 165.87 toks/s]
[2025-01-11 00:27:05,726][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.65it/s, est. speed input: 3669.40 toks/s, output: 165.87 toks/s]
WARNING 01-11 00:27:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:27:06,001][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:08,253][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:20,  2.25s/it, est. speed input: 634.64 toks/s, output: 12.44 toks/s]
[2025-01-11 00:27:08,593][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.50it/s, est. speed input: 3136.66 toks/s, output: 82.95 toks/s]
[2025-01-11 00:27:09,493][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  2.98it/s, est. speed input: 3058.82 toks/s, output: 102.23 toks/s]
[2025-01-11 00:27:10,168][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.53it/s, est. speed input: 2823.64 toks/s, output: 117.10 toks/s]
[2025-01-11 00:27:10,168][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:04<00:00,  2.40it/s, est. speed input: 2823.64 toks/s, output: 117.10 toks/s]
WARNING 01-11 00:27:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:27:10,382][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:11,665][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.28s/it, est. speed input: 833.61 toks/s, output: 22.61 toks/s]
[2025-01-11 00:27:12,060][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.86it/s, est. speed input: 2474.92 toks/s, output: 85.24 toks/s]
[2025-01-11 00:27:12,060][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.38it/s, est. speed input: 2474.92 toks/s, output: 85.24 toks/s]
WARNING 01-11 00:27:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:27:12,315][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:14,304][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.99s/it, est. speed input: 605.44 toks/s, output: 14.08 toks/s]
[2025-01-11 00:27:14,826][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.98it/s, est. speed input: 2801.92 toks/s, output: 78.47 toks/s]
[2025-01-11 00:27:14,992][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.31it/s, est. speed input: 3192.34 toks/s, output: 96.76 toks/s]
[2025-01-11 00:27:15,195][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.57it/s, est. speed input: 3445.70 toks/s, output: 115.64 toks/s]
[2025-01-11 00:27:15,195][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.78it/s, est. speed input: 3445.70 toks/s, output: 115.64 toks/s]
WARNING 01-11 00:27:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:27:15,438][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:27:16,416][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.02it/s, est. speed input: 1301.82 toks/s, output: 29.66 toks/s]
[2025-01-11 00:27:16,906][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.45it/s, est. speed input: 1743.15 toks/s, output: 59.26 toks/s]
[2025-01-11 00:27:16,906][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.36it/s, est. speed input: 1743.15 toks/s, output: 59.26 toks/s]
[2025-01-11 00:27:19,101][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:27:19,153][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:27:21,191][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.04s/it]
[2025-01-11 00:27:22,803][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 00:27:24,492][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.74s/it]
[2025-01-11 00:27:25,128][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.31s/it]
[2025-01-11 00:27:25,128][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.49s/it]
[2025-01-11 00:27:47,654][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:27:47,706][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:27:49,803][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.10s/it]
[2025-01-11 00:27:51,390][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 00:27:52,964][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 00:27:53,463][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 00:27:53,463][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 00:28:13 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:28:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:28:14 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:28:14 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:28:15,168][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:28:16,523][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 00:28:16,925][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 00:28:18,232][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 00:28:19,589][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 00:28:19,589][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 00:28:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:28:33 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:28:34 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:28:34 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:28:55 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:28:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:28:56,039][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:28:59,867][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.83s/it, est. speed input: 165.89 toks/s, output: 3.40 toks/s]
[2025-01-11 00:28:59,988][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:49,  1.65s/it, est. speed input: 321.62 toks/s, output: 7.09 toks/s]
[2025-01-11 00:29:00,105][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:27,  1.05it/s, est. speed input: 468.47 toks/s, output: 11.07 toks/s]
[2025-01-11 00:29:00,326][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.71it/s, est. speed input: 888.76 toks/s, output: 23.79 toks/s]
[2025-01-11 00:29:00,432][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:07,  3.26it/s, est. speed input: 1011.78 toks/s, output: 28.45 toks/s]
[2025-01-11 00:29:00,580][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:03,  6.64it/s, est. speed input: 1538.23 toks/s, output: 49.11 toks/s]
[2025-01-11 00:29:00,708][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:01, 10.31it/s, est. speed input: 2039.98 toks/s, output: 71.10 toks/s]
[2025-01-11 00:29:00,825][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 12.81it/s, est. speed input: 2388.31 toks/s, output: 88.80 toks/s]
[2025-01-11 00:29:00,929][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 15.57it/s, est. speed input: 2727.20 toks/s, output: 107.98 toks/s]
[2025-01-11 00:29:01,048][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00, 17.66it/s, est. speed input: 3042.36 toks/s, output: 127.96 toks/s]
[2025-01-11 00:29:01,179][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 19.01it/s, est. speed input: 3335.54 toks/s, output: 149.61 toks/s]
[2025-01-11 00:29:01,627][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 12.17it/s, est. speed input: 3409.19 toks/s, output: 168.40 toks/s]
[2025-01-11 00:29:02,201][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  7.79it/s, est. speed input: 3297.61 toks/s, output: 179.97 toks/s]
[2025-01-11 00:29:02,201][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  5.19it/s, est. speed input: 3297.61 toks/s, output: 179.97 toks/s]
WARNING 01-11 00:29:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:02,412][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:03,359][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 743.32 toks/s, output: 34.84 toks/s]
[2025-01-11 00:29:04,028][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.28it/s, est. speed input: 890.12 toks/s, output: 65.61 toks/s]
[2025-01-11 00:29:04,028][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.24it/s, est. speed input: 890.12 toks/s, output: 65.61 toks/s]
WARNING 01-11 00:29:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:04,260][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:07,587][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:36,  3.33s/it, est. speed input: 197.80 toks/s, output: 5.41 toks/s]
[2025-01-11 00:29:07,703][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:40,  1.44s/it, est. speed input: 379.41 toks/s, output: 11.04 toks/s]
[2025-01-11 00:29:07,927][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:23,  1.13it/s, est. speed input: 555.83 toks/s, output: 16.91 toks/s]
[2025-01-11 00:29:08,036][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:15,  1.73it/s, est. speed input: 713.86 toks/s, output: 23.31 toks/s]
[2025-01-11 00:29:08,195][root][ERROR] - Processed prompts:  17%|#6        | 5/30 [00:03<00:10,  2.34it/s, est. speed input: 851.21 toks/s, output: 29.74 toks/s]
[2025-01-11 00:29:08,340][root][ERROR] - Processed prompts:  30%|###       | 9/30 [00:04<00:03,  6.10it/s, est. speed input: 1506.43 toks/s, output: 58.34 toks/s]
[2025-01-11 00:29:08,515][root][ERROR] - Processed prompts:  43%|####3     | 13/30 [00:04<00:01,  9.53it/s, est. speed input: 2068.49 toks/s, output: 88.15 toks/s]
[2025-01-11 00:29:08,626][root][ERROR] - Processed prompts:  57%|#####6    | 17/30 [00:04<00:00, 13.68it/s, est. speed input: 2627.20 toks/s, output: 120.27 toks/s]
[2025-01-11 00:29:08,849][root][ERROR] - Processed prompts:  67%|######6   | 20/30 [00:04<00:00, 13.60it/s, est. speed input: 2936.66 toks/s, output: 142.10 toks/s]
[2025-01-11 00:29:08,967][root][ERROR] - Processed prompts:  73%|#######3  | 22/30 [00:04<00:00, 14.24it/s, est. speed input: 3147.31 toks/s, output: 159.79 toks/s]
[2025-01-11 00:29:09,141][root][ERROR] - Processed prompts:  83%|########3 | 25/30 [00:04<00:00, 15.09it/s, est. speed input: 3444.73 toks/s, output: 186.87 toks/s]
[2025-01-11 00:29:09,251][root][ERROR] - Processed prompts:  93%|#########3| 28/30 [00:04<00:00, 17.60it/s, est. speed input: 3765.63 toks/s, output: 219.43 toks/s]
[2025-01-11 00:29:09,782][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00,  5.43it/s, est. speed input: 3645.82 toks/s, output: 227.83 toks/s]
WARNING 01-11 00:29:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:10,046][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:12,957][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:02<01:12,  2.91s/it, est. speed input: 256.36 toks/s, output: 4.12 toks/s]
[2025-01-11 00:29:13,575][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:37,  1.56s/it, est. speed input: 423.95 toks/s, output: 10.20 toks/s]
[2025-01-11 00:29:13,676][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:20,  1.12it/s, est. speed input: 618.78 toks/s, output: 17.08 toks/s]
[2025-01-11 00:29:14,010][root][ERROR] - Processed prompts:  19%|#9        | 5/26 [00:03<00:10,  2.07it/s, est. speed input: 952.46 toks/s, output: 30.53 toks/s]
[2025-01-11 00:29:14,195][root][ERROR] - Processed prompts:  23%|##3       | 6/26 [00:04<00:07,  2.52it/s, est. speed input: 1092.52 toks/s, output: 38.09 toks/s]
[2025-01-11 00:29:14,372][root][ERROR] - Processed prompts:  31%|###       | 8/26 [00:04<00:04,  3.88it/s, est. speed input: 1412.22 toks/s, output: 55.03 toks/s]
[2025-01-11 00:29:14,630][root][ERROR] - Processed prompts:  46%|####6     | 12/26 [00:04<00:02,  6.71it/s, est. speed input: 1993.05 toks/s, output: 89.45 toks/s]
[2025-01-11 00:29:14,971][root][ERROR] - Processed prompts:  54%|#####3    | 14/26 [00:04<00:01,  6.45it/s, est. speed input: 2175.43 toks/s, output: 104.98 toks/s]
[2025-01-11 00:29:15,173][root][ERROR] - Processed prompts:  69%|######9   | 18/26 [00:05<00:00,  9.26it/s, est. speed input: 2687.05 toks/s, output: 147.86 toks/s]
[2025-01-11 00:29:15,411][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:05<00:00,  9.03it/s, est. speed input: 2858.39 toks/s, output: 168.13 toks/s]
[2025-01-11 00:29:15,649][root][ERROR] - Processed prompts:  85%|########4 | 22/26 [00:05<00:00,  8.86it/s, est. speed input: 3012.14 toks/s, output: 190.43 toks/s]
[2025-01-11 00:29:15,778][root][ERROR] - Processed prompts:  92%|#########2| 24/26 [00:05<00:00, 10.02it/s, est. speed input: 3220.31 toks/s, output: 217.39 toks/s]
[2025-01-11 00:29:16,022][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  9.44it/s, est. speed input: 3353.90 toks/s, output: 241.82 toks/s]
[2025-01-11 00:29:16,022][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  4.35it/s, est. speed input: 3353.90 toks/s, output: 241.82 toks/s]
WARNING 01-11 00:29:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:16,257][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:17,665][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.41s/it, est. speed input: 502.02 toks/s, output: 26.27 toks/s]
[2025-01-11 00:29:18,032][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.26it/s, est. speed input: 773.39 toks/s, output: 51.26 toks/s]
[2025-01-11 00:29:18,163][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.96it/s, est. speed input: 1499.40 toks/s, output: 108.60 toks/s]
[2025-01-11 00:29:19,083][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.93it/s, est. speed input: 1290.50 toks/s, output: 114.29 toks/s]
[2025-01-11 00:29:19,083][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  1.77it/s, est. speed input: 1290.50 toks/s, output: 114.29 toks/s]
WARNING 01-11 00:29:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:19,297][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:20,981][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.68s/it, est. speed input: 557.60 toks/s, output: 17.22 toks/s]
[2025-01-11 00:29:21,419][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:02<00:02,  1.67it/s, est. speed input: 1157.39 toks/s, output: 49.48 toks/s]
[2025-01-11 00:29:21,553][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.07it/s, est. speed input: 1895.90 toks/s, output: 91.78 toks/s]
[2025-01-11 00:29:21,933][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.94it/s, est. speed input: 1968.37 toks/s, output: 105.83 toks/s]
[2025-01-11 00:29:22,246][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.01it/s, est. speed input: 2061.04 toks/s, output: 124.79 toks/s]
[2025-01-11 00:29:22,314][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.65it/s, est. speed input: 2298.56 toks/s, output: 152.82 toks/s]
WARNING 01-11 00:29:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:22,546][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:25,926][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:14,  3.38s/it, est. speed input: 213.04 toks/s, output: 7.99 toks/s]
[2025-01-11 00:29:26,027][root][ERROR] - Processed prompts:   9%|8         | 2/23 [00:03<00:30,  1.45s/it, est. speed input: 414.63 toks/s, output: 16.09 toks/s]
[2025-01-11 00:29:26,244][root][ERROR] - Processed prompts:  26%|##6       | 6/23 [00:03<00:06,  2.67it/s, est. speed input: 1263.86 toks/s, output: 47.87 toks/s]
[2025-01-11 00:29:26,936][root][ERROR] - Processed prompts:  35%|###4      | 8/23 [00:04<00:05,  2.74it/s, est. speed input: 1408.91 toks/s, output: 61.29 toks/s]
[2025-01-11 00:29:27,047][root][ERROR] - Processed prompts:  48%|####7     | 11/23 [00:04<00:02,  4.49it/s, est. speed input: 1862.66 toks/s, output: 94.88 toks/s]
[2025-01-11 00:29:27,215][root][ERROR] - Processed prompts:  57%|#####6    | 13/23 [00:04<00:01,  5.48it/s, est. speed input: 2127.34 toks/s, output: 115.89 toks/s]
[2025-01-11 00:29:27,337][root][ERROR] - Processed prompts:  70%|######9   | 16/23 [00:04<00:00,  7.82it/s, est. speed input: 2546.12 toks/s, output: 151.77 toks/s]
[2025-01-11 00:29:27,496][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:04<00:00,  8.70it/s, est. speed input: 2789.30 toks/s, output: 174.15 toks/s]
[2025-01-11 00:29:27,658][root][ERROR] - Processed prompts:  87%|########6 | 20/23 [00:05<00:00,  9.47it/s, est. speed input: 3006.50 toks/s, output: 197.39 toks/s]
[2025-01-11 00:29:27,970][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:05<00:00,  8.35it/s, est. speed input: 3129.75 toks/s, output: 217.22 toks/s]
[2025-01-11 00:29:28,180][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.08it/s, est. speed input: 3161.25 toks/s, output: 227.56 toks/s]
WARNING 01-11 00:29:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:28,610][root][ERROR] - Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:31,322][root][ERROR] - Processed prompts:   6%|5         | 1/17 [00:02<00:43,  2.71s/it, est. speed input: 317.09 toks/s, output: 7.37 toks/s]
[2025-01-11 00:29:31,591][root][ERROR] - Processed prompts:  12%|#1        | 2/17 [00:02<00:19,  1.27s/it, est. speed input: 580.43 toks/s, output: 15.77 toks/s]
[2025-01-11 00:29:31,900][root][ERROR] - Processed prompts:  65%|######4   | 11/17 [00:03<00:01,  5.58it/s, est. speed input: 3099.30 toks/s, output: 96.35 toks/s]
[2025-01-11 00:29:32,108][root][ERROR] - Processed prompts:  76%|#######6  | 13/17 [00:03<00:00,  6.15it/s, est. speed input: 3426.07 toks/s, output: 115.22 toks/s]
[2025-01-11 00:29:32,940][root][ERROR] - Processed prompts:  88%|########8 | 15/17 [00:04<00:00,  4.48it/s, est. speed input: 3167.22 toks/s, output: 128.18 toks/s]
[2025-01-11 00:29:33,261][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:04<00:00,  4.82it/s, est. speed input: 3337.67 toks/s, output: 161.02 toks/s]
[2025-01-11 00:29:33,262][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:04<00:00,  3.65it/s, est. speed input: 3337.67 toks/s, output: 161.02 toks/s]
WARNING 01-11 00:29:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:33,486][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:35,690][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.20s/it, est. speed input: 375.64 toks/s, output: 12.70 toks/s]
[2025-01-11 00:29:35,844][root][ERROR] - Processed prompts:  58%|#####8    | 7/12 [00:02<00:01,  3.95it/s, est. speed input: 2612.54 toks/s, output: 87.78 toks/s]
[2025-01-11 00:29:36,725][root][ERROR] - Processed prompts:  83%|########3 | 10/12 [00:03<00:00,  3.72it/s, est. speed input: 2715.49 toks/s, output: 122.57 toks/s]
[2025-01-11 00:29:38,125][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:04<00:00,  2.59it/s, est. speed input: 2283.13 toks/s, output: 135.58 toks/s]
[2025-01-11 00:29:38,125][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:04<00:00,  2.59it/s, est. speed input: 2283.13 toks/s, output: 135.58 toks/s]
WARNING 01-11 00:29:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:38,422][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:40,634][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:22,  2.21s/it, est. speed input: 476.18 toks/s, output: 12.66 toks/s]
[2025-01-11 00:29:40,884][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.17it/s, est. speed input: 2329.78 toks/s, output: 73.54 toks/s]
[2025-01-11 00:29:41,365][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  3.44it/s, est. speed input: 2660.93 toks/s, output: 100.93 toks/s]
[2025-01-11 00:29:41,588][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  3.60it/s, est. speed input: 2763.81 toks/s, output: 115.95 toks/s]
[2025-01-11 00:29:41,863][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  4.39it/s, est. speed input: 3144.85 toks/s, output: 152.90 toks/s]
[2025-01-11 00:29:41,863][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.20it/s, est. speed input: 3144.85 toks/s, output: 152.90 toks/s]
WARNING 01-11 00:29:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:42,103][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:44,252][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.15s/it, est. speed input: 411.56 toks/s, output: 13.04 toks/s]
[2025-01-11 00:29:44,369][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  4.75it/s, est. speed input: 3414.01 toks/s, output: 103.72 toks/s]
[2025-01-11 00:29:44,640][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.34it/s, est. speed input: 4134.94 toks/s, output: 141.96 toks/s]
WARNING 01-11 00:29:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:44,897][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:46,000][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.10s/it, est. speed input: 1003.66 toks/s, output: 26.29 toks/s]
[2025-01-11 00:29:46,571][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.27it/s, est. speed input: 1328.95 toks/s, output: 53.18 toks/s]
[2025-01-11 00:29:46,622][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.74it/s, est. speed input: 1942.45 toks/s, output: 88.13 toks/s]
WARNING 01-11 00:29:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:46,849][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:49,150][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:25,  2.30s/it, est. speed input: 412.91 toks/s, output: 11.30 toks/s]
[2025-01-11 00:29:49,355][root][ERROR] - Processed prompts:  75%|#######5  | 9/12 [00:02<00:00,  4.79it/s, est. speed input: 3692.71 toks/s, output: 104.97 toks/s]
[2025-01-11 00:29:50,131][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  4.44it/s, est. speed input: 3675.51 toks/s, output: 134.09 toks/s]
[2025-01-11 00:29:50,131][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.66it/s, est. speed input: 3675.51 toks/s, output: 134.09 toks/s]
WARNING 01-11 00:29:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:50,392][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:51,822][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.43s/it, est. speed input: 807.84 toks/s, output: 20.28 toks/s]
[2025-01-11 00:29:52,264][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.57it/s, est. speed input: 2436.14 toks/s, output: 74.81 toks/s]
[2025-01-11 00:29:52,818][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.30it/s, est. speed input: 2251.33 toks/s, output: 93.19 toks/s]
[2025-01-11 00:29:52,818][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.06it/s, est. speed input: 2251.33 toks/s, output: 93.19 toks/s]
WARNING 01-11 00:29:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:53,024][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:54,087][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 894.90 toks/s, output: 26.35 toks/s]
[2025-01-11 00:29:54,174][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.61it/s, est. speed input: 2607.19 toks/s, output: 78.29 toks/s]
WARNING 01-11 00:29:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:54,402][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:55,812][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.41s/it, est. speed input: 927.24 toks/s, output: 20.57 toks/s]
[2025-01-11 00:29:55,960][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.26it/s, est. speed input: 2691.09 toks/s, output: 79.62 toks/s]
[2025-01-11 00:29:56,028][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.08it/s, est. speed input: 3199.63 toks/s, output: 101.53 toks/s]
WARNING 01-11 00:29:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:29:56,256][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:29:57,013][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1206.29 toks/s, output: 38.27 toks/s]
[2025-01-11 00:29:57,014][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1206.29 toks/s, output: 38.27 toks/s]
[2025-01-11 00:29:59,329][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:29:59,382][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:30:00,987][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 00:30:02,684][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 00:30:04,297][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 00:30:04,821][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 00:30:04,821][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 00:30:25,615][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:30:25,667][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:30:27,785][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.12s/it]
[2025-01-11 00:30:29,457][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.86s/it]
[2025-01-11 00:30:31,020][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 00:30:31,519][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 00:30:31,519][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 00:30:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:30:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:30:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:30:52 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:30:52,674][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:30:53,996][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 00:30:54,373][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 00:30:55,668][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 00:30:57,020][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 00:30:57,020][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 00:30:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:31:11 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:31:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:31:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:31:32 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:31:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:33,251][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:36,889][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:52,  3.64s/it, est. speed input: 174.57 toks/s, output: 3.57 toks/s]
[2025-01-11 00:31:37,010][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:47,  1.57s/it, est. speed input: 337.90 toks/s, output: 7.45 toks/s]
[2025-01-11 00:31:37,128][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:26,  1.10it/s, est. speed input: 491.46 toks/s, output: 11.61 toks/s]
[2025-01-11 00:31:37,348][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.82it/s, est. speed input: 930.02 toks/s, output: 24.90 toks/s]
[2025-01-11 00:31:37,453][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:05,  4.20it/s, est. speed input: 1209.06 toks/s, output: 34.99 toks/s]
[2025-01-11 00:31:37,591][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  8.62it/s, est. speed input: 1902.16 toks/s, output: 61.75 toks/s]
[2025-01-11 00:31:37,796][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  9.99it/s, est. speed input: 2235.82 toks/s, output: 77.46 toks/s]
[2025-01-11 00:31:37,905][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 12.55it/s, est. speed input: 2592.48 toks/s, output: 96.69 toks/s]
[2025-01-11 00:31:38,068][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 21.28it/s, est. speed input: 3559.61 toks/s, output: 153.64 toks/s]
[2025-01-11 00:31:38,495][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 15.67it/s, est. speed input: 3754.48 toks/s, output: 178.52 toks/s]
[2025-01-11 00:31:38,529][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.06it/s, est. speed input: 3850.69 toks/s, output: 189.50 toks/s]
WARNING 01-11 00:31:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:38,758][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:39,965][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.21s/it, est. speed input: 589.88 toks/s, output: 33.14 toks/s]
[2025-01-11 00:31:40,241][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.52it/s, est. speed input: 973.19 toks/s, output: 64.07 toks/s]
[2025-01-11 00:31:40,375][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.38it/s, est. speed input: 1343.28 toks/s, output: 97.71 toks/s]
[2025-01-11 00:31:40,375][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.86it/s, est. speed input: 1343.28 toks/s, output: 97.71 toks/s]
WARNING 01-11 00:31:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:40,609][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:43,818][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:29,  3.21s/it, est. speed input: 204.78 toks/s, output: 5.61 toks/s]
[2025-01-11 00:31:44,089][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:24,  1.07it/s, est. speed input: 567.06 toks/s, output: 16.96 toks/s]
[2025-01-11 00:31:44,340][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:09,  2.43it/s, est. speed input: 1062.43 toks/s, output: 35.66 toks/s]
[2025-01-11 00:31:44,519][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:03<00:03,  5.30it/s, est. speed input: 1858.95 toks/s, output: 71.88 toks/s]
[2025-01-11 00:31:44,633][root][ERROR] - Processed prompts:  55%|#####5    | 16/29 [00:04<00:01,  8.89it/s, est. speed input: 2623.46 toks/s, output: 111.59 toks/s]
[2025-01-11 00:31:44,792][root][ERROR] - Processed prompts:  69%|######8   | 20/29 [00:04<00:00, 11.44it/s, est. speed input: 3156.49 toks/s, output: 143.70 toks/s]
[2025-01-11 00:31:44,925][root][ERROR] - Processed prompts:  79%|#######9  | 23/29 [00:04<00:00, 13.18it/s, est. speed input: 3516.05 toks/s, output: 168.91 toks/s]
[2025-01-11 00:31:45,150][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:04<00:00, 14.46it/s, est. speed input: 3930.46 toks/s, output: 203.52 toks/s]
[2025-01-11 00:31:45,427][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.02it/s, est. speed input: 3984.18 toks/s, output: 219.61 toks/s]
WARNING 01-11 00:31:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:45,668][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:49,228][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:32,  3.56s/it, est. speed input: 212.90 toks/s, output: 6.18 toks/s]
[2025-01-11 00:31:49,442][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:39,  1.59s/it, est. speed input: 400.14 toks/s, output: 12.72 toks/s]
[2025-01-11 00:31:49,545][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:21,  1.10it/s, est. speed input: 625.15 toks/s, output: 19.60 toks/s]
[2025-01-11 00:31:49,689][root][ERROR] - Processed prompts:  26%|##5       | 7/27 [00:04<00:05,  3.59it/s, est. speed input: 1395.01 toks/s, output: 48.25 toks/s]
[2025-01-11 00:31:49,955][root][ERROR] - Processed prompts:  33%|###3      | 9/27 [00:04<00:04,  4.36it/s, est. speed input: 1666.45 toks/s, output: 62.27 toks/s]
[2025-01-11 00:31:50,122][root][ERROR] - Processed prompts:  41%|####      | 11/27 [00:04<00:02,  5.51it/s, est. speed input: 1940.25 toks/s, output: 78.12 toks/s]
[2025-01-11 00:31:50,233][root][ERROR] - Processed prompts:  48%|####8     | 13/27 [00:04<00:01,  7.08it/s, est. speed input: 2230.57 toks/s, output: 94.86 toks/s]
[2025-01-11 00:31:50,368][root][ERROR] - Processed prompts:  59%|#####9    | 16/27 [00:04<00:01,  9.79it/s, est. speed input: 2656.12 toks/s, output: 121.50 toks/s]
[2025-01-11 00:31:50,526][root][ERROR] - Processed prompts:  81%|########1 | 22/27 [00:04<00:00, 16.27it/s, est. speed input: 3512.88 toks/s, output: 179.71 toks/s]
[2025-01-11 00:31:50,797][root][ERROR] - Processed prompts:  93%|#########2| 25/27 [00:05<00:00, 14.39it/s, est. speed input: 3777.87 toks/s, output: 205.70 toks/s]
[2025-01-11 00:31:51,177][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00, 10.56it/s, est. speed input: 3805.44 toks/s, output: 221.08 toks/s]
[2025-01-11 00:31:51,177][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  4.90it/s, est. speed input: 3805.44 toks/s, output: 221.08 toks/s]
WARNING 01-11 00:31:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:51,386][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:52,401][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.01s/it, est. speed input: 631.12 toks/s, output: 19.72 toks/s]
[2025-01-11 00:31:52,658][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.76it/s, est. speed input: 1032.63 toks/s, output: 40.90 toks/s]
[2025-01-11 00:31:52,813][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.83it/s, est. speed input: 1946.01 toks/s, output: 90.43 toks/s]
[2025-01-11 00:31:53,063][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.88it/s, est. speed input: 2070.84 toks/s, output: 109.74 toks/s]
[2025-01-11 00:31:53,063][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.98it/s, est. speed input: 2070.84 toks/s, output: 109.74 toks/s]
WARNING 01-11 00:31:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:53,281][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:54,493][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.21s/it, est. speed input: 717.95 toks/s, output: 16.50 toks/s]
[2025-01-11 00:31:54,972][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:03,  1.28it/s, est. speed input: 1098.66 toks/s, output: 36.09 toks/s]
[2025-01-11 00:31:55,146][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  1.99it/s, est. speed input: 1451.46 toks/s, output: 59.00 toks/s]
[2025-01-11 00:31:55,275][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.92it/s, est. speed input: 2127.05 toks/s, output: 107.86 toks/s]
[2025-01-11 00:31:55,812][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.97it/s, est. speed input: 2035.85 toks/s, output: 119.73 toks/s]
[2025-01-11 00:31:55,812][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.37it/s, est. speed input: 2035.85 toks/s, output: 119.73 toks/s]
WARNING 01-11 00:31:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:31:56,075][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:31:59,755][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:28,  3.68s/it, est. speed input: 238.07 toks/s, output: 7.61 toks/s]
[2025-01-11 00:31:59,895][root][ERROR] - Processed prompts:  24%|##4       | 6/25 [00:03<00:09,  2.10it/s, est. speed input: 1323.62 toks/s, output: 45.81 toks/s]
[2025-01-11 00:32:00,318][root][ERROR] - Processed prompts:  36%|###6      | 9/25 [00:04<00:05,  3.01it/s, est. speed input: 1715.81 toks/s, output: 68.35 toks/s]
[2025-01-11 00:32:00,654][root][ERROR] - Processed prompts:  44%|####4     | 11/25 [00:04<00:03,  3.51it/s, est. speed input: 1920.67 toks/s, output: 84.09 toks/s]
[2025-01-11 00:32:00,790][root][ERROR] - Processed prompts:  56%|#####6    | 14/25 [00:04<00:02,  5.18it/s, est. speed input: 2339.39 toks/s, output: 114.95 toks/s]
[2025-01-11 00:32:00,906][root][ERROR] - Processed prompts:  68%|######8   | 17/25 [00:04<00:01,  7.23it/s, est. speed input: 2761.73 toks/s, output: 146.78 toks/s]
[2025-01-11 00:32:01,147][root][ERROR] - Processed prompts:  76%|#######6  | 19/25 [00:05<00:00,  7.47it/s, est. speed input: 2943.56 toks/s, output: 166.03 toks/s]
[2025-01-11 00:32:01,327][root][ERROR] - Processed prompts:  88%|########8 | 22/25 [00:05<00:00,  9.27it/s, est. speed input: 3272.16 toks/s, output: 200.92 toks/s]
[2025-01-11 00:32:01,571][root][ERROR] - Processed prompts:  96%|#########6| 24/25 [00:05<00:00,  8.97it/s, est. speed input: 3417.78 toks/s, output: 222.19 toks/s]
[2025-01-11 00:32:01,957][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:05<00:00,  4.25it/s, est. speed input: 3334.67 toks/s, output: 226.49 toks/s]
WARNING 01-11 00:32:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:02,225][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:05,134][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:02<00:52,  2.91s/it, est. speed input: 316.29 toks/s, output: 7.56 toks/s]
[2025-01-11 00:32:05,428][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:03<00:23,  1.37s/it, est. speed input: 576.29 toks/s, output: 15.92 toks/s]
[2025-01-11 00:32:05,576][root][ERROR] - Processed prompts:  26%|##6       | 5/19 [00:03<00:05,  2.36it/s, est. speed input: 1357.02 toks/s, output: 42.38 toks/s]
[2025-01-11 00:32:05,854][root][ERROR] - Processed prompts:  37%|###6      | 7/19 [00:03<00:03,  3.24it/s, est. speed input: 1747.76 toks/s, output: 60.35 toks/s]
[2025-01-11 00:32:06,019][root][ERROR] - Processed prompts:  42%|####2     | 8/19 [00:03<00:03,  3.62it/s, est. speed input: 1889.19 toks/s, output: 69.84 toks/s]
[2025-01-11 00:32:06,174][root][ERROR] - Processed prompts:  53%|#####2    | 10/19 [00:03<00:01,  5.03it/s, est. speed input: 2253.34 toks/s, output: 92.43 toks/s]
[2025-01-11 00:32:06,304][root][ERROR] - Processed prompts:  74%|#######3  | 14/19 [00:04<00:00,  9.00it/s, est. speed input: 3070.75 toks/s, output: 142.18 toks/s]
[2025-01-11 00:32:06,616][root][ERROR] - Processed prompts:  84%|########4 | 16/19 [00:04<00:00,  8.11it/s, est. speed input: 3282.16 toks/s, output: 161.91 toks/s]
[2025-01-11 00:32:07,010][root][ERROR] - Processed prompts:  95%|#########4| 18/19 [00:04<00:00,  6.95it/s, est. speed input: 3393.94 toks/s, output: 183.08 toks/s]
[2025-01-11 00:32:07,127][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:04<00:00,  3.88it/s, est. speed input: 3489.34 toks/s, output: 198.68 toks/s]
WARNING 01-11 00:32:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:07,350][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:08,996][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.65s/it, est. speed input: 515.37 toks/s, output: 17.62 toks/s]
[2025-01-11 00:32:09,127][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.33it/s, est. speed input: 939.02 toks/s, output: 35.47 toks/s]
[2025-01-11 00:32:09,288][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.01it/s, est. speed input: 1782.99 toks/s, output: 71.73 toks/s]
[2025-01-11 00:32:09,971][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.97it/s, est. speed input: 1962.74 toks/s, output: 98.06 toks/s]
[2025-01-11 00:32:10,058][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.96it/s, est. speed input: 2501.22 toks/s, output: 152.56 toks/s]
WARNING 01-11 00:32:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:10,288][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:12,430][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.14s/it, est. speed input: 436.94 toks/s, output: 12.60 toks/s]
[2025-01-11 00:32:12,613][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.39it/s, est. speed input: 2504.89 toks/s, output: 76.14 toks/s]
[2025-01-11 00:32:13,184][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  3.43it/s, est. speed input: 2610.10 toks/s, output: 93.92 toks/s]
[2025-01-11 00:32:13,784][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.40it/s, est. speed input: 2728.28 toks/s, output: 123.01 toks/s]
[2025-01-11 00:32:13,986][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.61it/s, est. speed input: 2873.66 toks/s, output: 144.41 toks/s]
[2025-01-11 00:32:13,986][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  2.97it/s, est. speed input: 2873.66 toks/s, output: 144.41 toks/s]
WARNING 01-11 00:32:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:14,204][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:16,728][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:32,  2.52s/it, est. speed input: 398.71 toks/s, output: 11.49 toks/s]
[2025-01-11 00:32:16,847][root][ERROR] - Processed prompts:  36%|###5      | 5/14 [00:02<00:03,  2.49it/s, est. speed input: 1819.44 toks/s, output: 56.40 toks/s]
[2025-01-11 00:32:17,016][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:02<00:01,  3.55it/s, est. speed input: 2242.65 toks/s, output: 80.37 toks/s]
[2025-01-11 00:32:17,912][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:03<00:01,  2.94it/s, est. speed input: 2203.66 toks/s, output: 97.38 toks/s]
[2025-01-11 00:32:18,088][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:03<00:00,  3.91it/s, est. speed input: 2562.09 toks/s, output: 133.40 toks/s]
[2025-01-11 00:32:18,402][root][ERROR] - Processed prompts:  93%|#########2| 13/14 [00:04<00:00,  4.46it/s, est. speed input: 2800.75 toks/s, output: 166.75 toks/s]
[2025-01-11 00:32:18,470][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  3.28it/s, est. speed input: 2969.81 toks/s, output: 188.25 toks/s]
WARNING 01-11 00:32:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:18,750][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:20,510][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.76s/it, est. speed input: 667.01 toks/s, output: 16.48 toks/s]
[2025-01-11 00:32:20,676][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.08it/s, est. speed input: 3442.68 toks/s, output: 94.99 toks/s]
[2025-01-11 00:32:21,637][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.42it/s, est. speed input: 2681.44 toks/s, output: 96.28 toks/s]
WARNING 01-11 00:32:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:21,870][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:24,425][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:33,  2.55s/it, est. speed input: 412.57 toks/s, output: 10.57 toks/s]
[2025-01-11 00:32:24,602][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:02<00:02,  3.41it/s, est. speed input: 2491.06 toks/s, output: 74.69 toks/s]
[2025-01-11 00:32:25,249][root][ERROR] - Processed prompts:  71%|#######1  | 10/14 [00:03<00:01,  3.80it/s, est. speed input: 2844.90 toks/s, output: 102.71 toks/s]
[2025-01-11 00:32:25,969][root][ERROR] - Processed prompts:  86%|########5 | 12/14 [00:04<00:00,  3.46it/s, est. speed input: 2853.74 toks/s, output: 126.86 toks/s]
[2025-01-11 00:32:26,194][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  4.17it/s, est. speed input: 3146.46 toks/s, output: 167.66 toks/s]
[2025-01-11 00:32:26,195][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  3.24it/s, est. speed input: 3146.46 toks/s, output: 167.66 toks/s]
WARNING 01-11 00:32:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:26,444][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:28,425][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.98s/it, est. speed input: 536.83 toks/s, output: 13.13 toks/s]
[2025-01-11 00:32:28,596][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:02<00:00,  3.66it/s, est. speed input: 3175.94 toks/s, output: 80.90 toks/s]
[2025-01-11 00:32:29,194][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  3.55it/s, est. speed input: 3316.43 toks/s, output: 100.74 toks/s]
[2025-01-11 00:32:29,565][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.88it/s, est. speed input: 3257.11 toks/s, output: 116.65 toks/s]
WARNING 01-11 00:32:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:29,784][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:31,654][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.87s/it, est. speed input: 539.33 toks/s, output: 15.52 toks/s]
[2025-01-11 00:32:32,138][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.62it/s, est. speed input: 2187.75 toks/s, output: 70.54 toks/s]
[2025-01-11 00:32:32,317][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.69it/s, est. speed input: 2762.47 toks/s, output: 109.39 toks/s]
[2025-01-11 00:32:32,721][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.34it/s, est. speed input: 2782.43 toks/s, output: 122.59 toks/s]
[2025-01-11 00:32:32,721][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.72it/s, est. speed input: 2782.43 toks/s, output: 122.59 toks/s]
WARNING 01-11 00:32:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:32,980][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:34,899][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.92s/it, est. speed input: 610.94 toks/s, output: 14.60 toks/s]
[2025-01-11 00:32:35,135][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.24it/s, est. speed input: 3916.37 toks/s, output: 100.28 toks/s]
[2025-01-11 00:32:35,977][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.67it/s, est. speed input: 3160.85 toks/s, output: 102.12 toks/s]
WARNING 01-11 00:32:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:36,179][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:37,386][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.21s/it, est. speed input: 854.33 toks/s, output: 38.12 toks/s]
[2025-01-11 00:32:37,504][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.77it/s, est. speed input: 1337.36 toks/s, output: 74.76 toks/s]
[2025-01-11 00:32:37,504][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.51it/s, est. speed input: 1337.36 toks/s, output: 74.76 toks/s]
WARNING 01-11 00:32:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:37,764][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:38,931][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.17s/it, est. speed input: 1213.08 toks/s, output: 24.86 toks/s]
[2025-01-11 00:32:39,207][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.56it/s, est. speed input: 1849.77 toks/s, output: 50.59 toks/s]
[2025-01-11 00:32:39,595][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.90it/s, est. speed input: 2093.94 toks/s, output: 76.48 toks/s]
[2025-01-11 00:32:39,595][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.64it/s, est. speed input: 2093.94 toks/s, output: 76.48 toks/s]
WARNING 01-11 00:32:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:39,803][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:40,588][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1053.79 toks/s, output: 39.50 toks/s]
[2025-01-11 00:32:40,588][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1053.79 toks/s, output: 39.50 toks/s]
WARNING 01-11 00:32:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:40,803][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:41,602][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1391.22 toks/s, output: 37.60 toks/s]
[2025-01-11 00:32:41,602][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1391.22 toks/s, output: 37.60 toks/s]
WARNING 01-11 00:32:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:32:41,817][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:32:43,342][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 773.15 toks/s, output: 47.87 toks/s]
[2025-01-11 00:32:43,342][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.53s/it, est. speed input: 773.15 toks/s, output: 47.87 toks/s]
[2025-01-11 00:32:45,381][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:32:45,433][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:32:47,127][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 00:32:48,799][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 00:32:50,403][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 00:32:50,922][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 00:32:50,923][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 00:33:12,717][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:33:12,769][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:33:14,795][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.03s/it]
[2025-01-11 00:33:16,397][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 00:33:17,991][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 00:33:18,489][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 00:33:18,489][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 00:33:38 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:33:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:33:39 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:33:39 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:33:39,464][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:33:40,801][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 00:33:41,176][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 00:33:42,497][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 00:33:43,851][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 00:33:43,851][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 00:33:44 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:33:57 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:33:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:33:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:34:19 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:34:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:19,989][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:23,934][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:02,  3.95s/it, est. speed input: 160.94 toks/s, output: 3.04 toks/s]
[2025-01-11 00:34:24,116][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.73s/it, est. speed input: 307.74 toks/s, output: 6.54 toks/s]
[2025-01-11 00:34:24,234][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.01it/s, est. speed input: 448.77 toks/s, output: 10.37 toks/s]
[2025-01-11 00:34:24,451][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:07,  3.18it/s, est. speed input: 996.22 toks/s, output: 26.67 toks/s]
[2025-01-11 00:34:24,554][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:05,  4.44it/s, est. speed input: 1251.99 toks/s, output: 35.93 toks/s]
[2025-01-11 00:34:24,695][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:03,  6.67it/s, est. speed input: 1619.15 toks/s, output: 50.57 toks/s]
[2025-01-11 00:34:24,903][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  9.41it/s, est. speed input: 2067.68 toks/s, output: 71.23 toks/s]
[2025-01-11 00:34:25,038][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:05<00:00, 14.03it/s, est. speed input: 2640.76 toks/s, output: 101.39 toks/s]
[2025-01-11 00:34:25,171][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 19.99it/s, est. speed input: 3308.52 toks/s, output: 141.83 toks/s]
[2025-01-11 00:34:25,568][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 15.54it/s, est. speed input: 3528.34 toks/s, output: 167.59 toks/s]
[2025-01-11 00:34:25,602][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.70it/s, est. speed input: 3620.20 toks/s, output: 177.62 toks/s]
WARNING 01-11 00:34:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:25,828][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:26,768][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.06it/s, est. speed input: 741.00 toks/s, output: 27.68 toks/s]
[2025-01-11 00:34:27,154][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.63it/s, est. speed input: 1056.64 toks/s, output: 55.06 toks/s]
[2025-01-11 00:34:27,305][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.48it/s, est. speed input: 1438.89 toks/s, output: 87.35 toks/s]
[2025-01-11 00:34:27,305][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.03it/s, est. speed input: 1438.89 toks/s, output: 87.35 toks/s]
WARNING 01-11 00:34:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:27,566][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:30,615][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:25,  3.05s/it, est. speed input: 215.16 toks/s, output: 4.92 toks/s]
[2025-01-11 00:34:31,007][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:40,  1.49s/it, est. speed input: 380.65 toks/s, output: 10.75 toks/s]
[2025-01-11 00:34:31,110][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:08,  2.77it/s, est. speed input: 1108.91 toks/s, output: 36.12 toks/s]
[2025-01-11 00:34:31,345][root][ERROR] - Processed prompts:  31%|###1      | 9/29 [00:03<00:04,  4.29it/s, est. speed input: 1565.56 toks/s, output: 55.04 toks/s]
[2025-01-11 00:34:31,521][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:03<00:03,  5.28it/s, est. speed input: 1826.53 toks/s, output: 69.02 toks/s]
[2025-01-11 00:34:31,717][root][ERROR] - Processed prompts:  48%|####8     | 14/29 [00:04<00:02,  7.11it/s, est. speed input: 2219.80 toks/s, output: 91.30 toks/s]
[2025-01-11 00:34:31,847][root][ERROR] - Processed prompts:  66%|######5   | 19/29 [00:04<00:00, 11.73it/s, est. speed input: 2925.48 toks/s, output: 134.55 toks/s]
[2025-01-11 00:34:32,195][root][ERROR] - Processed prompts:  83%|########2 | 24/29 [00:04<00:00, 12.68it/s, est. speed input: 3435.85 toks/s, output: 174.34 toks/s]
[2025-01-11 00:34:32,305][root][ERROR] - Processed prompts:  90%|########9 | 26/29 [00:04<00:00, 13.44it/s, est. speed input: 3634.65 toks/s, output: 195.39 toks/s]
[2025-01-11 00:34:33,548][root][ERROR] - Processed prompts:  97%|#########6| 28/29 [00:05<00:00,  5.24it/s, est. speed input: 3103.17 toks/s, output: 189.40 toks/s]
[2025-01-11 00:34:33,799][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:06<00:00,  4.65it/s, est. speed input: 3085.98 toks/s, output: 204.54 toks/s]
WARNING 01-11 00:34:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:34,055][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:37,416][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:27,  3.36s/it, est. speed input: 221.09 toks/s, output: 5.36 toks/s]
[2025-01-11 00:34:37,629][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:37,  1.51s/it, est. speed input: 417.75 toks/s, output: 11.19 toks/s]
[2025-01-11 00:34:37,784][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:21,  1.12it/s, est. speed input: 603.06 toks/s, output: 17.43 toks/s]
[2025-01-11 00:34:37,885][root][ERROR] - Processed prompts:  15%|#4        | 4/27 [00:03<00:13,  1.73it/s, est. speed input: 780.59 toks/s, output: 24.02 toks/s]
[2025-01-11 00:34:38,205][root][ERROR] - Processed prompts:  30%|##9       | 8/27 [00:04<00:04,  4.29it/s, est. speed input: 1484.87 toks/s, output: 51.08 toks/s]
[2025-01-11 00:34:38,332][root][ERROR] - Processed prompts:  37%|###7      | 10/27 [00:04<00:03,  5.65it/s, est. speed input: 1795.32 toks/s, output: 66.17 toks/s]
[2025-01-11 00:34:38,555][root][ERROR] - Processed prompts:  48%|####8     | 13/27 [00:04<00:01,  7.44it/s, est. speed input: 2208.41 toks/s, output: 89.33 toks/s]
[2025-01-11 00:34:38,658][root][ERROR] - Processed prompts:  59%|#####9    | 16/27 [00:04<00:01, 10.28it/s, est. speed input: 2661.82 toks/s, output: 116.65 toks/s]
[2025-01-11 00:34:38,844][root][ERROR] - Processed prompts:  67%|######6   | 18/27 [00:04<00:00, 10.41it/s, est. speed input: 2881.37 toks/s, output: 133.43 toks/s]
[2025-01-11 00:34:39,153][root][ERROR] - Processed prompts:  74%|#######4  | 20/27 [00:05<00:00,  8.93it/s, est. speed input: 3004.16 toks/s, output: 149.46 toks/s]
[2025-01-11 00:34:39,350][root][ERROR] - Processed prompts:  81%|########1 | 22/27 [00:05<00:00,  9.24it/s, est. speed input: 3186.81 toks/s, output: 169.39 toks/s]
[2025-01-11 00:34:39,731][root][ERROR] - Processed prompts:  89%|########8 | 24/27 [00:05<00:00,  7.59it/s, est. speed input: 3251.76 toks/s, output: 187.81 toks/s]
[2025-01-11 00:34:40,106][root][ERROR] - Processed prompts:  96%|#########6| 26/27 [00:06<00:00,  6.76it/s, est. speed input: 3327.60 toks/s, output: 209.21 toks/s]
[2025-01-11 00:34:40,224][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:06<00:00,  7.01it/s, est. speed input: 3406.95 toks/s, output: 223.87 toks/s]
[2025-01-11 00:34:40,224][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:06<00:00,  4.38it/s, est. speed input: 3406.95 toks/s, output: 223.87 toks/s]
WARNING 01-11 00:34:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:40,463][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:41,535][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.07s/it, est. speed input: 616.62 toks/s, output: 20.52 toks/s]
[2025-01-11 00:34:41,837][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.57it/s, est. speed input: 1479.63 toks/s, output: 60.41 toks/s]
[2025-01-11 00:34:42,021][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.14it/s, est. speed input: 1785.29 toks/s, output: 83.42 toks/s]
[2025-01-11 00:34:42,121][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.06it/s, est. speed input: 2092.65 toks/s, output: 110.33 toks/s]
[2025-01-11 00:34:42,122][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.01it/s, est. speed input: 2092.65 toks/s, output: 110.33 toks/s]
WARNING 01-11 00:34:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:42,353][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:43,931][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.58s/it, est. speed input: 512.67 toks/s, output: 19.01 toks/s]
[2025-01-11 00:34:44,178][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:01<00:01,  2.74it/s, est. speed input: 1790.05 toks/s, output: 74.54 toks/s]
[2025-01-11 00:34:44,579][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.67it/s, est. speed input: 1816.39 toks/s, output: 88.53 toks/s]
[2025-01-11 00:34:45,112][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  2.38it/s, est. speed input: 1790.39 toks/s, output: 104.02 toks/s]
[2025-01-11 00:34:45,433][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.56it/s, est. speed input: 1907.18 toks/s, output: 128.59 toks/s]
[2025-01-11 00:34:45,433][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.27it/s, est. speed input: 1907.18 toks/s, output: 128.59 toks/s]
WARNING 01-11 00:34:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:45,682][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:49,269][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:03<01:22,  3.59s/it, est. speed input: 245.64 toks/s, output: 8.09 toks/s]
[2025-01-11 00:34:49,400][root][ERROR] - Processed prompts:  25%|##5       | 6/24 [00:03<00:08,  2.16it/s, est. speed input: 1351.97 toks/s, output: 47.61 toks/s]
[2025-01-11 00:34:49,948][root][ERROR] - Processed prompts:  38%|###7      | 9/24 [00:04<00:05,  2.92it/s, est. speed input: 1682.12 toks/s, output: 68.46 toks/s]
[2025-01-11 00:34:50,088][root][ERROR] - Processed prompts:  50%|#####     | 12/24 [00:04<00:02,  4.34it/s, est. speed input: 2147.70 toks/s, output: 98.97 toks/s]
[2025-01-11 00:34:50,217][root][ERROR] - Processed prompts:  58%|#####8    | 14/24 [00:04<00:01,  5.37it/s, est. speed input: 2434.28 toks/s, output: 119.53 toks/s]
[2025-01-11 00:34:50,453][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:04<00:01,  5.97it/s, est. speed input: 2630.25 toks/s, output: 139.39 toks/s]
[2025-01-11 00:34:50,578][root][ERROR] - Processed prompts:  79%|#######9  | 19/24 [00:04<00:00,  8.28it/s, est. speed input: 3028.35 toks/s, output: 175.27 toks/s]
[2025-01-11 00:34:51,019][root][ERROR] - Processed prompts:  88%|########7 | 21/24 [00:05<00:00,  6.81it/s, est. speed input: 3107.71 toks/s, output: 190.75 toks/s]
[2025-01-11 00:34:51,512][root][ERROR] - Processed prompts:  96%|#########5| 23/24 [00:05<00:00,  5.75it/s, est. speed input: 3099.11 toks/s, output: 210.48 toks/s]
[2025-01-11 00:34:52,131][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:06<00:00,  3.72it/s, est. speed input: 2920.43 toks/s, output: 213.53 toks/s]
WARNING 01-11 00:34:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:52,386][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:55,222][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:48,  2.84s/it, est. speed input: 319.09 toks/s, output: 7.40 toks/s]
[2025-01-11 00:34:55,536][root][ERROR] - Processed prompts:  17%|#6        | 3/18 [00:03<00:12,  1.17it/s, est. speed input: 874.98 toks/s, output: 23.18 toks/s]
[2025-01-11 00:34:55,673][root][ERROR] - Processed prompts:  61%|######1   | 11/18 [00:03<00:01,  5.66it/s, est. speed input: 3224.28 toks/s, output: 94.62 toks/s]
[2025-01-11 00:34:56,265][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:03<00:00,  5.47it/s, est. speed input: 3413.78 toks/s, output: 119.38 toks/s]
[2025-01-11 00:34:56,740][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:04<00:00,  5.13it/s, est. speed input: 3436.72 toks/s, output: 139.88 toks/s]
[2025-01-11 00:34:56,971][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  5.69it/s, est. speed input: 3667.41 toks/s, output: 172.73 toks/s]
[2025-01-11 00:34:56,972][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  3.93it/s, est. speed input: 3667.41 toks/s, output: 172.73 toks/s]
WARNING 01-11 00:34:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:34:57,198][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:34:59,287][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.09s/it, est. speed input: 389.62 toks/s, output: 17.23 toks/s]
[2025-01-11 00:34:59,397][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.35it/s, est. speed input: 1387.05 toks/s, output: 69.12 toks/s]
[2025-01-11 00:34:59,905][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  2.85it/s, est. speed input: 1754.86 toks/s, output: 100.47 toks/s]
[2025-01-11 00:35:00,248][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:03<00:00,  3.55it/s, est. speed input: 2120.30 toks/s, output: 138.34 toks/s]
[2025-01-11 00:35:00,433][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  3.83it/s, est. speed input: 2235.95 toks/s, output: 157.35 toks/s]
[2025-01-11 00:35:00,787][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.54it/s, est. speed input: 2280.57 toks/s, output: 171.93 toks/s]
[2025-01-11 00:35:00,787][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.79it/s, est. speed input: 2280.57 toks/s, output: 171.93 toks/s]
WARNING 01-11 00:35:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:01,059][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:03,208][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.15s/it, est. speed input: 363.96 toks/s, output: 13.50 toks/s]
[2025-01-11 00:35:03,411][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:02<00:05,  1.58it/s, est. speed input: 1140.47 toks/s, output: 40.82 toks/s]
[2025-01-11 00:35:03,887][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.33it/s, est. speed input: 1557.51 toks/s, output: 66.84 toks/s]
[2025-01-11 00:35:04,206][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:03<00:01,  2.50it/s, est. speed input: 1669.42 toks/s, output: 81.36 toks/s]
[2025-01-11 00:35:04,321][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:03<00:01,  3.12it/s, est. speed input: 1884.46 toks/s, output: 100.57 toks/s]
[2025-01-11 00:35:04,709][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  3.78it/s, est. speed input: 2227.00 toks/s, output: 134.82 toks/s]
[2025-01-11 00:35:04,967][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.80it/s, est. speed input: 2340.71 toks/s, output: 152.79 toks/s]
[2025-01-11 00:35:05,237][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  3.78it/s, est. speed input: 2447.93 toks/s, output: 171.89 toks/s]
[2025-01-11 00:35:05,237][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.63it/s, est. speed input: 2447.93 toks/s, output: 171.89 toks/s]
WARNING 01-11 00:35:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:05,490][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:07,602][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.11s/it, est. speed input: 427.69 toks/s, output: 13.74 toks/s]
[2025-01-11 00:35:07,972][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.08it/s, est. speed input: 2183.22 toks/s, output: 76.96 toks/s]
[2025-01-11 00:35:08,110][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.49it/s, est. speed input: 2371.43 toks/s, output: 92.00 toks/s]
[2025-01-11 00:35:08,570][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:03<00:00,  3.07it/s, est. speed input: 2339.26 toks/s, output: 101.31 toks/s]
[2025-01-11 00:35:09,413][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  2.22it/s, est. speed input: 2048.25 toks/s, output: 108.35 toks/s]
[2025-01-11 00:35:09,616][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:04<00:00,  2.59it/s, est. speed input: 2175.44 toks/s, output: 133.09 toks/s]
[2025-01-11 00:35:09,616][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.67it/s, est. speed input: 2387.18 toks/s, output: 163.14 toks/s]
WARNING 01-11 00:35:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:09,869][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:12,168][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:22,  2.30s/it, est. speed input: 493.37 toks/s, output: 12.18 toks/s]
[2025-01-11 00:35:12,462][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  4.05it/s, est. speed input: 3217.81 toks/s, output: 93.35 toks/s]
[2025-01-11 00:35:13,366][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.31it/s, est. speed input: 3017.78 toks/s, output: 109.26 toks/s]
[2025-01-11 00:35:13,468][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.70it/s, est. speed input: 3274.86 toks/s, output: 132.57 toks/s]
[2025-01-11 00:35:13,468][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.06it/s, est. speed input: 3274.86 toks/s, output: 132.57 toks/s]
WARNING 01-11 00:35:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:13,689][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:15,579][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.89s/it, est. speed input: 546.66 toks/s, output: 14.82 toks/s]
[2025-01-11 00:35:15,744][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  3.16it/s, est. speed input: 2440.94 toks/s, output: 73.02 toks/s]
[2025-01-11 00:35:16,120][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  3.73it/s, est. speed input: 2800.12 toks/s, output: 101.24 toks/s]
[2025-01-11 00:35:17,061][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.95it/s, est. speed input: 2604.91 toks/s, output: 123.69 toks/s]
[2025-01-11 00:35:17,061][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.67it/s, est. speed input: 2604.91 toks/s, output: 123.69 toks/s]
WARNING 01-11 00:35:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:17,308][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:19,056][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.75s/it, est. speed input: 524.20 toks/s, output: 16.02 toks/s]
[2025-01-11 00:35:19,466][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.49it/s, est. speed input: 3181.90 toks/s, output: 89.43 toks/s]
[2025-01-11 00:35:19,920][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.12it/s, est. speed input: 2974.14 toks/s, output: 103.39 toks/s]
[2025-01-11 00:35:19,920][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.68it/s, est. speed input: 2974.14 toks/s, output: 103.39 toks/s]
WARNING 01-11 00:35:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:20,163][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:21,963][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.80s/it, est. speed input: 541.74 toks/s, output: 15.56 toks/s]
[2025-01-11 00:35:22,681][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:02,  1.87it/s, est. speed input: 1652.98 toks/s, output: 57.59 toks/s]
[2025-01-11 00:35:23,375][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:03<00:00,  2.21it/s, est. speed input: 1909.21 toks/s, output: 93.39 toks/s]
[2025-01-11 00:35:23,949][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:03<00:00,  2.08it/s, est. speed input: 1904.99 toks/s, output: 111.98 toks/s]
[2025-01-11 00:35:24,607][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:04<00:00,  1.90it/s, est. speed input: 1857.33 toks/s, output: 132.09 toks/s]
[2025-01-11 00:35:24,607][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:04<00:00,  1.80it/s, est. speed input: 1857.33 toks/s, output: 132.09 toks/s]
WARNING 01-11 00:35:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:24,888][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:25,963][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.07s/it, est. speed input: 1177.82 toks/s, output: 25.12 toks/s]
[2025-01-11 00:35:26,404][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.26it/s, est. speed input: 2208.86 toks/s, output: 71.91 toks/s]
[2025-01-11 00:35:26,404][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.98it/s, est. speed input: 2208.86 toks/s, output: 71.91 toks/s]
WARNING 01-11 00:35:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:26,633][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:27,913][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.28s/it, est. speed input: 769.85 toks/s, output: 22.67 toks/s]
[2025-01-11 00:35:28,249][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.19it/s, est. speed input: 2030.49 toks/s, output: 66.24 toks/s]
[2025-01-11 00:35:28,300][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.40it/s, est. speed input: 2630.85 toks/s, output: 94.19 toks/s]
WARNING 01-11 00:35:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:28,543][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:29,963][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.42s/it, est. speed input: 858.11 toks/s, output: 28.89 toks/s]
[2025-01-11 00:35:30,516][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.10it/s, est. speed input: 1123.79 toks/s, output: 56.77 toks/s]
[2025-01-11 00:35:30,635][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.82it/s, est. speed input: 1807.37 toks/s, output: 90.85 toks/s]
[2025-01-11 00:35:30,635][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.43it/s, est. speed input: 1807.37 toks/s, output: 90.85 toks/s]
WARNING 01-11 00:35:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:30,841][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:32,554][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.71s/it, est. speed input: 685.39 toks/s, output: 49.04 toks/s]
[2025-01-11 00:35:32,554][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.71s/it, est. speed input: 685.39 toks/s, output: 49.04 toks/s]
WARNING 01-11 00:35:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:32,772][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:33,764][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.01it/s, est. speed input: 1689.79 toks/s, output: 29.24 toks/s]
[2025-01-11 00:35:33,781][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.98it/s, est. speed input: 2616.88 toks/s, output: 58.46 toks/s]
WARNING 01-11 00:35:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:35:33,996][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:35:34,819][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1233.80 toks/s, output: 38.94 toks/s]
[2025-01-11 00:35:34,819][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1233.80 toks/s, output: 38.94 toks/s]
[2025-01-11 00:35:37,276][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:35:37,329][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:35:38,956][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 00:35:40,639][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 00:35:42,254][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 00:35:42,808][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 00:35:42,809][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 00:36:04,801][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:36:04,852][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:36:06,865][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.01s/it]
[2025-01-11 00:36:08,727][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.92s/it]
[2025-01-11 00:36:10,411][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.81s/it]
[2025-01-11 00:36:11,039][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.35s/it]
[2025-01-11 00:36:11,039][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.55s/it]
WARNING 01-11 00:36:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:36:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:36:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:36:32 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:36:32,318][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:36:33,637][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 00:36:34,011][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 00:36:35,297][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-01-11 00:36:36,626][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 00:36:36,626][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 00:36:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:36:50 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:36:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:36:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:37:12 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:37:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:13,023][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:16,991][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.97s/it, est. speed input: 160.02 toks/s, output: 3.78 toks/s]
[2025-01-11 00:37:17,112][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.71s/it, est. speed input: 310.58 toks/s, output: 7.83 toks/s]
[2025-01-11 00:37:17,282][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:14,  1.91it/s, est. speed input: 745.42 toks/s, output: 20.43 toks/s]
[2025-01-11 00:37:17,443][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.90it/s, est. speed input: 1005.67 toks/s, output: 29.64 toks/s]
[2025-01-11 00:37:17,591][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:03,  5.57it/s, est. speed input: 1529.21 toks/s, output: 50.13 toks/s]
[2025-01-11 00:37:17,751][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01, 10.23it/s, est. speed input: 2283.33 toks/s, output: 82.91 toks/s]
[2025-01-11 00:37:17,886][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 13.12it/s, est. speed input: 2741.88 toks/s, output: 106.71 toks/s]
[2025-01-11 00:37:18,031][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00, 14.54it/s, est. speed input: 3043.03 toks/s, output: 125.39 toks/s]
[2025-01-11 00:37:18,406][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 11.87it/s, est. speed input: 3184.78 toks/s, output: 142.66 toks/s]
[2025-01-11 00:37:18,569][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 13.19it/s, est. speed input: 3434.81 toks/s, output: 169.49 toks/s]
[2025-01-11 00:37:19,212][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  7.82it/s, est. speed input: 3283.32 toks/s, output: 178.87 toks/s]
[2025-01-11 00:37:19,212][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  5.17it/s, est. speed input: 3283.32 toks/s, output: 178.87 toks/s]
WARNING 01-11 00:37:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:19,426][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:20,318][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.12it/s, est. speed input: 806.77 toks/s, output: 25.81 toks/s]
[2025-01-11 00:37:20,621][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.91it/s, est. speed input: 1778.53 toks/s, output: 74.49 toks/s]
[2025-01-11 00:37:20,621][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.51it/s, est. speed input: 1778.53 toks/s, output: 74.49 toks/s]
WARNING 01-11 00:37:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:20,866][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:23,943][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:26,  3.08s/it, est. speed input: 213.89 toks/s, output: 4.88 toks/s]
[2025-01-11 00:37:24,391][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:41,  1.53s/it, est. speed input: 376.73 toks/s, output: 10.78 toks/s]
[2025-01-11 00:37:24,501][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:22,  1.13it/s, est. speed input: 544.82 toks/s, output: 17.34 toks/s]
[2025-01-11 00:37:24,708][root][ERROR] - Processed prompts:  17%|#7        | 5/29 [00:03<00:10,  2.27it/s, est. speed input: 858.41 toks/s, output: 30.71 toks/s]
[2025-01-11 00:37:24,901][root][ERROR] - Processed prompts:  28%|##7       | 8/29 [00:04<00:04,  4.26it/s, est. speed input: 1347.65 toks/s, output: 52.30 toks/s]
[2025-01-11 00:37:25,033][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:04<00:02,  6.62it/s, est. speed input: 1777.03 toks/s, output: 75.59 toks/s]
[2025-01-11 00:37:25,156][root][ERROR] - Processed prompts:  45%|####4     | 13/29 [00:04<00:01,  8.03it/s, est. speed input: 2033.67 toks/s, output: 90.92 toks/s]
[2025-01-11 00:37:25,379][root][ERROR] - Processed prompts:  52%|#####1    | 15/29 [00:04<00:01,  8.28it/s, est. speed input: 2230.90 toks/s, output: 105.69 toks/s]
[2025-01-11 00:37:25,481][root][ERROR] - Processed prompts:  59%|#####8    | 17/29 [00:04<00:01, 10.01it/s, est. speed input: 2473.86 toks/s, output: 123.52 toks/s]
[2025-01-11 00:37:25,600][root][ERROR] - Processed prompts:  76%|#######5  | 22/29 [00:04<00:00, 16.48it/s, est. speed input: 3109.68 toks/s, output: 173.45 toks/s]
[2025-01-11 00:37:25,884][root][ERROR] - Processed prompts:  86%|########6 | 25/29 [00:05<00:00, 14.06it/s, est. speed input: 3333.76 toks/s, output: 198.69 toks/s]
[2025-01-11 00:37:25,985][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:05<00:00, 15.03it/s, est. speed input: 3532.08 toks/s, output: 220.94 toks/s]
[2025-01-11 00:37:26,399][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  9.93it/s, est. speed input: 3520.61 toks/s, output: 234.98 toks/s]
[2025-01-11 00:37:26,399][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.24it/s, est. speed input: 3520.61 toks/s, output: 234.98 toks/s]
WARNING 01-11 00:37:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:26,696][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:30,392][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:36,  3.70s/it, est. speed input: 202.69 toks/s, output: 6.49 toks/s]
[2025-01-11 00:37:30,652][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:25,  1.05s/it, est. speed input: 570.33 toks/s, output: 19.72 toks/s]
[2025-01-11 00:37:30,799][root][ERROR] - Processed prompts:  19%|#8        | 5/27 [00:04<00:12,  1.82it/s, est. speed input: 934.59 toks/s, output: 33.88 toks/s]
[2025-01-11 00:37:30,987][root][ERROR] - Processed prompts:  26%|##5       | 7/27 [00:04<00:07,  2.80it/s, est. speed input: 1244.25 toks/s, output: 48.71 toks/s]
[2025-01-11 00:37:31,121][root][ERROR] - Processed prompts:  33%|###3      | 9/27 [00:04<00:04,  4.04it/s, est. speed input: 1550.72 toks/s, output: 64.64 toks/s]
[2025-01-11 00:37:31,245][root][ERROR] - Processed prompts:  41%|####      | 11/27 [00:04<00:02,  5.47it/s, est. speed input: 1846.65 toks/s, output: 80.90 toks/s]
[2025-01-11 00:37:31,421][root][ERROR] - Processed prompts:  56%|#####5    | 15/27 [00:04<00:01,  8.82it/s, est. speed input: 2421.55 toks/s, output: 115.35 toks/s]
[2025-01-11 00:37:31,550][root][ERROR] - Processed prompts:  63%|######2   | 17/27 [00:04<00:01,  9.95it/s, est. speed input: 2680.75 toks/s, output: 132.88 toks/s]
[2025-01-11 00:37:31,815][root][ERROR] - Processed prompts:  78%|#######7  | 21/27 [00:05<00:00, 11.66it/s, est. speed input: 3156.02 toks/s, output: 168.41 toks/s]
[2025-01-11 00:37:31,984][root][ERROR] - Processed prompts:  93%|#########2| 25/27 [00:05<00:00, 14.44it/s, est. speed input: 3648.31 toks/s, output: 211.80 toks/s]
[2025-01-11 00:37:32,634][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  8.07it/s, est. speed input: 3537.39 toks/s, output: 222.64 toks/s]
[2025-01-11 00:37:32,634][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  4.55it/s, est. speed input: 3537.39 toks/s, output: 222.64 toks/s]
WARNING 01-11 00:37:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:32,860][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:33,979][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.12s/it, est. speed input: 588.41 toks/s, output: 25.93 toks/s]
[2025-01-11 00:37:34,408][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.23it/s, est. speed input: 1439.17 toks/s, output: 73.67 toks/s]
[2025-01-11 00:37:34,841][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.26it/s, est. speed input: 1456.82 toks/s, output: 96.95 toks/s]
[2025-01-11 00:37:34,841][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.02it/s, est. speed input: 1456.82 toks/s, output: 96.95 toks/s]
WARNING 01-11 00:37:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:35,050][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:36,337][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.29s/it, est. speed input: 715.67 toks/s, output: 21.76 toks/s]
[2025-01-11 00:37:36,490][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.61it/s, est. speed input: 1236.33 toks/s, output: 43.76 toks/s]
[2025-01-11 00:37:36,952][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:01,  1.83it/s, est. speed input: 1384.44 toks/s, output: 63.62 toks/s]
[2025-01-11 00:37:37,211][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  3.22it/s, est. speed input: 2101.69 toks/s, output: 118.48 toks/s]
[2025-01-11 00:37:37,211][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.31it/s, est. speed input: 2101.69 toks/s, output: 118.48 toks/s]
WARNING 01-11 00:37:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:37,468][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:41,146][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:31,  3.68s/it, est. speed input: 195.23 toks/s, output: 7.07 toks/s]
[2025-01-11 00:37:41,250][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:37,  1.58s/it, est. speed input: 387.94 toks/s, output: 14.28 toks/s]
[2025-01-11 00:37:41,388][root][ERROR] - Processed prompts:  31%|###       | 8/26 [00:03<00:05,  3.57it/s, est. speed input: 1618.24 toks/s, output: 58.68 toks/s]
[2025-01-11 00:37:41,837][root][ERROR] - Processed prompts:  42%|####2     | 11/26 [00:04<00:03,  4.32it/s, est. speed input: 1983.92 toks/s, output: 79.20 toks/s]
[2025-01-11 00:37:41,946][root][ERROR] - Processed prompts:  50%|#####     | 13/26 [00:04<00:02,  5.41it/s, est. speed input: 2270.66 toks/s, output: 96.94 toks/s]
[2025-01-11 00:37:42,073][root][ERROR] - Processed prompts:  62%|######1   | 16/26 [00:04<00:01,  7.48it/s, est. speed input: 2692.90 toks/s, output: 124.65 toks/s]
[2025-01-11 00:37:42,502][root][ERROR] - Processed prompts:  69%|######9   | 18/26 [00:05<00:01,  6.49it/s, est. speed input: 2774.46 toks/s, output: 136.70 toks/s]
[2025-01-11 00:37:43,084][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:05<00:01,  5.24it/s, est. speed input: 2756.97 toks/s, output: 151.01 toks/s]
[2025-01-11 00:37:43,442][root][ERROR] - Processed prompts:  85%|########4 | 22/26 [00:05<00:00,  5.33it/s, est. speed input: 2861.48 toks/s, output: 175.10 toks/s]
[2025-01-11 00:37:43,661][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:06<00:00,  5.19it/s, est. speed input: 2889.56 toks/s, output: 186.86 toks/s]
[2025-01-11 00:37:43,821][root][ERROR] - Processed prompts:  92%|#########2| 24/26 [00:06<00:00,  5.36it/s, est. speed input: 2937.05 toks/s, output: 200.88 toks/s]
[2025-01-11 00:37:44,097][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:06<00:00,  4.86it/s, est. speed input: 2955.96 toks/s, output: 212.72 toks/s]
[2025-01-11 00:37:44,131][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:06<00:00,  3.90it/s, est. speed input: 3059.71 toks/s, output: 232.05 toks/s]
WARNING 01-11 00:37:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:44,394][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:48,454][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:04<01:37,  4.06s/it, est. speed input: 244.84 toks/s, output: 6.65 toks/s]
[2025-01-11 00:37:48,628][root][ERROR] - Processed prompts:  44%|####4     | 11/25 [00:04<00:03,  3.55it/s, est. speed input: 2505.91 toks/s, output: 74.88 toks/s]
[2025-01-11 00:37:49,137][root][ERROR] - Processed prompts:  60%|######    | 15/25 [00:04<00:02,  4.39it/s, est. speed input: 3019.97 toks/s, output: 101.43 toks/s]
[2025-01-11 00:37:49,589][root][ERROR] - Processed prompts:  72%|#######2  | 18/25 [00:05<00:01,  4.83it/s, est. speed input: 3292.49 toks/s, output: 124.75 toks/s]
[2025-01-11 00:37:49,895][root][ERROR] - Processed prompts:  84%|########4 | 21/25 [00:05<00:00,  5.63it/s, est. speed input: 3584.50 toks/s, output: 154.89 toks/s]
[2025-01-11 00:37:50,212][root][ERROR] - Processed prompts:  92%|#########2| 23/25 [00:05<00:00,  5.76it/s, est. speed input: 3701.56 toks/s, output: 176.18 toks/s]
[2025-01-11 00:37:50,871][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:06<00:00,  4.77it/s, est. speed input: 3651.34 toks/s, output: 192.22 toks/s]
[2025-01-11 00:37:50,871][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:06<00:00,  3.86it/s, est. speed input: 3651.34 toks/s, output: 192.22 toks/s]
WARNING 01-11 00:37:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:51,117][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:52,124][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.01s/it, est. speed input: 755.20 toks/s, output: 21.86 toks/s]
[2025-01-11 00:37:52,264][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:00,  2.01it/s, est. speed input: 1347.23 toks/s, output: 44.47 toks/s]
[2025-01-11 00:37:52,885][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.63it/s, est. speed input: 1939.43 toks/s, output: 82.58 toks/s]
[2025-01-11 00:37:52,886][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.26it/s, est. speed input: 1939.43 toks/s, output: 82.58 toks/s]
WARNING 01-11 00:37:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:53,161][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:54,786][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.62s/it, est. speed input: 627.26 toks/s, output: 17.24 toks/s]
[2025-01-11 00:37:54,971][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.04it/s, est. speed input: 1729.25 toks/s, output: 51.38 toks/s]
[2025-01-11 00:37:55,271][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.99it/s, est. speed input: 2858.61 toks/s, output: 104.24 toks/s]
[2025-01-11 00:37:55,943][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.89it/s, est. speed input: 2479.88 toks/s, output: 112.15 toks/s]
[2025-01-11 00:37:55,943][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.52it/s, est. speed input: 2479.88 toks/s, output: 112.15 toks/s]
WARNING 01-11 00:37:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:37:56,180][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:37:58,787][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:33,  2.61s/it, est. speed input: 376.27 toks/s, output: 10.74 toks/s]
[2025-01-11 00:37:59,119][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:02<00:01,  4.03it/s, est. speed input: 3147.01 toks/s, output: 93.22 toks/s]
[2025-01-11 00:37:59,591][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:03<00:00,  4.08it/s, est. speed input: 3241.41 toks/s, output: 114.32 toks/s]
[2025-01-11 00:37:59,772][root][ERROR] - Processed prompts:  86%|########5 | 12/14 [00:03<00:00,  4.25it/s, est. speed input: 3304.09 toks/s, output: 128.32 toks/s]
[2025-01-11 00:38:01,102][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  2.74it/s, est. speed input: 2796.20 toks/s, output: 139.16 toks/s]
[2025-01-11 00:38:01,103][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  2.84it/s, est. speed input: 2796.20 toks/s, output: 139.16 toks/s]
WARNING 01-11 00:38:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:01,367][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:02,675][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.31s/it, est. speed input: 893.90 toks/s, output: 22.18 toks/s]
[2025-01-11 00:38:02,896][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.37it/s, est. speed input: 2312.18 toks/s, output: 64.74 toks/s]
[2025-01-11 00:38:03,756][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.75it/s, est. speed input: 1925.04 toks/s, output: 79.95 toks/s]
[2025-01-11 00:38:03,756][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.67it/s, est. speed input: 1925.04 toks/s, output: 79.95 toks/s]
WARNING 01-11 00:38:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:03,972][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:05,776][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.80s/it, est. speed input: 549.91 toks/s, output: 16.08 toks/s]
[2025-01-11 00:38:06,142][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:02<00:01,  2.27it/s, est. speed input: 1904.21 toks/s, output: 60.85 toks/s]
[2025-01-11 00:38:06,580][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.91it/s, est. speed input: 2279.24 toks/s, output: 93.56 toks/s]
[2025-01-11 00:38:06,764][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.27it/s, est. speed input: 2435.36 toks/s, output: 114.96 toks/s]
[2025-01-11 00:38:07,456][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.48it/s, est. speed input: 2306.45 toks/s, output: 126.00 toks/s]
[2025-01-11 00:38:07,457][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.30it/s, est. speed input: 2306.45 toks/s, output: 126.00 toks/s]
WARNING 01-11 00:38:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:07,701][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:09,005][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.30s/it, est. speed input: 964.36 toks/s, output: 21.48 toks/s]
[2025-01-11 00:38:09,284][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.27it/s, est. speed input: 2253.03 toks/s, output: 63.20 toks/s]
[2025-01-11 00:38:09,840][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  2.08it/s, est. speed input: 2223.53 toks/s, output: 82.32 toks/s]
[2025-01-11 00:38:09,840][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.87it/s, est. speed input: 2223.53 toks/s, output: 82.32 toks/s]
WARNING 01-11 00:38:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:10,050][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:11,171][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 772.58 toks/s, output: 25.87 toks/s]
[2025-01-11 00:38:11,687][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.31it/s, est. speed input: 1375.57 toks/s, output: 52.53 toks/s]
[2025-01-11 00:38:11,941][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.88it/s, est. speed input: 1813.25 toks/s, output: 83.57 toks/s]
[2025-01-11 00:38:11,941][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.59it/s, est. speed input: 1813.25 toks/s, output: 83.57 toks/s]
WARNING 01-11 00:38:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:12,198][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:13,181][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.02it/s, est. speed input: 1158.45 toks/s, output: 29.52 toks/s]
[2025-01-11 00:38:13,721][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.38it/s, est. speed input: 1627.10 toks/s, output: 59.12 toks/s]
[2025-01-11 00:38:13,721][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.31it/s, est. speed input: 1627.10 toks/s, output: 59.12 toks/s]
WARNING 01-11 00:38:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:13,932][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:15,521][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 568.97 toks/s, output: 49.09 toks/s]
[2025-01-11 00:38:15,521][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 568.97 toks/s, output: 49.09 toks/s]
WARNING 01-11 00:38:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:38:15,755][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:38:16,589][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1768.92 toks/s, output: 34.78 toks/s]
[2025-01-11 00:38:16,589][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1768.92 toks/s, output: 34.78 toks/s]
[2025-01-11 00:38:18,901][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:38:18,954][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:38:20,552][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 00:38:22,216][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 00:38:23,807][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 00:38:24,337][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 00:38:24,337][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 00:38:45,750][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:38:45,802][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:38:47,864][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.06s/it]
[2025-01-11 00:38:49,499][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 00:38:51,137][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.73s/it]
[2025-01-11 00:38:51,639][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 00:38:51,640][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 00:39:11 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:39:11 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:39:12 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:39:12 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:39:12,512][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:39:13,846][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
[2025-01-11 00:39:14,258][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 00:39:15,570][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 00:39:16,908][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 00:39:16,908][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 00:39:17 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:39:30 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:39:31 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:39:31 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:39:53 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:39:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:39:53,414][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:39:57,198][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.78s/it, est. speed input: 167.84 toks/s, output: 3.44 toks/s]
[2025-01-11 00:39:57,319][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:48,  1.63s/it, est. speed input: 325.25 toks/s, output: 7.17 toks/s]
[2025-01-11 00:39:57,437][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:27,  1.06it/s, est. speed input: 473.60 toks/s, output: 11.19 toks/s]
[2025-01-11 00:39:57,653][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:07,  3.34it/s, est. speed input: 1048.81 toks/s, output: 28.08 toks/s]
[2025-01-11 00:39:57,801][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:04,  5.26it/s, est. speed input: 1447.54 toks/s, output: 42.40 toks/s]
[2025-01-11 00:39:57,923][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 12.36it/s, est. speed input: 2535.18 toks/s, output: 85.84 toks/s]
[2025-01-11 00:39:58,121][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 14.04it/s, est. speed input: 2968.33 toks/s, output: 108.15 toks/s]
[2025-01-11 00:39:58,295][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.72it/s, est. speed input: 3253.03 toks/s, output: 127.66 toks/s]
[2025-01-11 00:39:58,593][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:05<00:00, 13.13it/s, est. speed input: 3433.45 toks/s, output: 145.99 toks/s]
[2025-01-11 00:39:58,712][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 15.09it/s, est. speed input: 3715.78 toks/s, output: 174.04 toks/s]
[2025-01-11 00:39:59,112][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.62it/s, est. speed input: 3566.60 toks/s, output: 176.22 toks/s]
WARNING 01-11 00:39:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:39:59,350][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:00,993][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.64s/it, est. speed input: 454.75 toks/s, output: 49.31 toks/s]
[2025-01-11 00:40:00,993][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.64s/it, est. speed input: 454.75 toks/s, output: 49.31 toks/s]
WARNING 01-11 00:40:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:01,246][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:04,531][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:38,  3.28s/it, est. speed input: 200.97 toks/s, output: 4.87 toks/s]
[2025-01-11 00:40:04,761][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:26,  1.07it/s, est. speed input: 566.71 toks/s, output: 14.79 toks/s]
[2025-01-11 00:40:04,872][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.07it/s, est. speed input: 910.67 toks/s, output: 26.20 toks/s]
[2025-01-11 00:40:05,073][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:06,  3.77it/s, est. speed input: 1381.28 toks/s, output: 43.12 toks/s]
[2025-01-11 00:40:05,213][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:03<00:02,  6.66it/s, est. speed input: 1994.10 toks/s, output: 69.58 toks/s]
[2025-01-11 00:40:05,373][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:04<00:01,  9.59it/s, est. speed input: 2556.73 toks/s, output: 96.70 toks/s]
[2025-01-11 00:40:05,589][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:01, 10.60it/s, est. speed input: 2894.55 toks/s, output: 118.14 toks/s]
[2025-01-11 00:40:05,814][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 10.16it/s, est. speed input: 3038.76 toks/s, output: 131.81 toks/s]
[2025-01-11 00:40:05,989][root][ERROR] - Processed prompts:  74%|#######4  | 23/31 [00:04<00:00, 10.45it/s, est. speed input: 3212.92 toks/s, output: 148.46 toks/s]
[2025-01-11 00:40:06,091][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 13.42it/s, est. speed input: 3565.74 toks/s, output: 179.18 toks/s]
[2025-01-11 00:40:06,656][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:05<00:00,  7.94it/s, est. speed input: 3437.77 toks/s, output: 187.25 toks/s]
[2025-01-11 00:40:06,866][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  9.39it/s, est. speed input: 3659.08 toks/s, output: 227.08 toks/s]
[2025-01-11 00:40:06,866][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.52it/s, est. speed input: 3659.08 toks/s, output: 227.08 toks/s]
WARNING 01-11 00:40:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:07,114][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:10,574][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:29,  3.46s/it, est. speed input: 219.65 toks/s, output: 5.78 toks/s]
[2025-01-11 00:40:10,681][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:37,  1.49s/it, est. speed input: 421.93 toks/s, output: 11.77 toks/s]
[2025-01-11 00:40:10,783][root][ERROR] - Processed prompts:  15%|#4        | 4/27 [00:03<00:13,  1.70it/s, est. speed input: 812.92 toks/s, output: 23.99 toks/s]
[2025-01-11 00:40:11,023][root][ERROR] - Processed prompts:  22%|##2       | 6/27 [00:03<00:07,  2.73it/s, est. speed input: 1187.70 toks/s, output: 36.33 toks/s]
[2025-01-11 00:40:11,155][root][ERROR] - Processed prompts:  33%|###3      | 9/27 [00:04<00:03,  4.90it/s, est. speed input: 1706.63 toks/s, output: 57.66 toks/s]
[2025-01-11 00:40:11,403][root][ERROR] - Processed prompts:  41%|####      | 11/27 [00:04<00:02,  5.61it/s, est. speed input: 1967.86 toks/s, output: 71.12 toks/s]
[2025-01-11 00:40:11,514][root][ERROR] - Processed prompts:  52%|#####1    | 14/27 [00:04<00:01,  8.31it/s, est. speed input: 2438.68 toks/s, output: 96.59 toks/s]
[2025-01-11 00:40:11,643][root][ERROR] - Processed prompts:  63%|######2   | 17/27 [00:04<00:00, 10.90it/s, est. speed input: 2886.11 toks/s, output: 122.33 toks/s]
[2025-01-11 00:40:11,864][root][ERROR] - Processed prompts:  74%|#######4  | 20/27 [00:04<00:00, 11.69it/s, est. speed input: 3225.10 toks/s, output: 147.58 toks/s]
[2025-01-11 00:40:12,107][root][ERROR] - Processed prompts:  81%|########1 | 22/27 [00:04<00:00, 10.59it/s, est. speed input: 3380.90 toks/s, output: 163.65 toks/s]
[2025-01-11 00:40:12,306][root][ERROR] - Processed prompts:  96%|#########6| 26/27 [00:05<00:00, 13.18it/s, est. speed input: 3861.17 toks/s, output: 209.59 toks/s]
[2025-01-11 00:40:13,428][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:06<00:00,  4.28it/s, est. speed input: 3302.96 toks/s, output: 194.50 toks/s]
WARNING 01-11 00:40:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:13,646][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:15,322][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:06,  1.68s/it, est. speed input: 472.81 toks/s, output: 28.66 toks/s]
[2025-01-11 00:40:15,532][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.68it/s, est. speed input: 1596.80 toks/s, output: 110.31 toks/s]
[2025-01-11 00:40:15,900][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.69it/s, est. speed input: 1653.45 toks/s, output: 128.25 toks/s]
[2025-01-11 00:40:15,900][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.22it/s, est. speed input: 1653.45 toks/s, output: 128.25 toks/s]
WARNING 01-11 00:40:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:16,128][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:17,713][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.58s/it, est. speed input: 585.06 toks/s, output: 18.30 toks/s]
[2025-01-11 00:40:18,200][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:02<00:04,  1.07it/s, est. speed input: 822.60 toks/s, output: 37.65 toks/s]
[2025-01-11 00:40:18,567][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:02<00:02,  1.48it/s, est. speed input: 1102.36 toks/s, output: 58.64 toks/s]
[2025-01-11 00:40:18,764][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:02<00:01,  2.05it/s, est. speed input: 1343.86 toks/s, output: 82.33 toks/s]
[2025-01-11 00:40:19,066][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.38it/s, est. speed input: 1469.84 toks/s, output: 104.19 toks/s]
[2025-01-11 00:40:19,234][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  4.00it/s, est. speed input: 1965.71 toks/s, output: 159.09 toks/s]
[2025-01-11 00:40:19,234][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.25it/s, est. speed input: 1965.71 toks/s, output: 159.09 toks/s]
WARNING 01-11 00:40:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:19,491][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:22,109][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<01:00,  2.62s/it, est. speed input: 292.24 toks/s, output: 3.82 toks/s]
[2025-01-11 00:40:22,792][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:32,  1.48s/it, est. speed input: 449.00 toks/s, output: 10.30 toks/s]
[2025-01-11 00:40:22,983][root][ERROR] - Processed prompts:  12%|#2        | 3/24 [00:03<00:18,  1.12it/s, est. speed input: 629.27 toks/s, output: 17.76 toks/s]
[2025-01-11 00:40:23,113][root][ERROR] - Processed prompts:  29%|##9       | 7/24 [00:03<00:04,  3.70it/s, est. speed input: 1496.76 toks/s, output: 49.70 toks/s]
[2025-01-11 00:40:23,770][root][ERROR] - Processed prompts:  38%|###7      | 9/24 [00:04<00:04,  3.45it/s, est. speed input: 1607.68 toks/s, output: 61.24 toks/s]
[2025-01-11 00:40:23,977][root][ERROR] - Processed prompts:  50%|#####     | 12/24 [00:04<00:02,  5.08it/s, est. speed input: 2051.84 toks/s, output: 92.29 toks/s]
[2025-01-11 00:40:24,134][root][ERROR] - Processed prompts:  58%|#####8    | 14/24 [00:04<00:01,  6.14it/s, est. speed input: 2318.29 toks/s, output: 113.51 toks/s]
[2025-01-11 00:40:24,280][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:04<00:01,  7.31it/s, est. speed input: 2563.29 toks/s, output: 136.36 toks/s]
[2025-01-11 00:40:24,479][root][ERROR] - Processed prompts:  79%|#######9  | 19/24 [00:04<00:00,  9.11it/s, est. speed input: 2910.66 toks/s, output: 171.62 toks/s]
[2025-01-11 00:40:24,793][root][ERROR] - Processed prompts:  88%|########7 | 21/24 [00:05<00:00,  8.17it/s, est. speed input: 3021.71 toks/s, output: 193.14 toks/s]
[2025-01-11 00:40:25,003][root][ERROR] - Processed prompts:  96%|#########5| 23/24 [00:05<00:00,  8.51it/s, est. speed input: 3202.01 toks/s, output: 219.73 toks/s]
[2025-01-11 00:40:25,037][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:05<00:00,  4.33it/s, est. speed input: 3325.60 toks/s, output: 236.24 toks/s]
WARNING 01-11 00:40:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:25,324][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:28,764][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:03<01:08,  3.44s/it, est. speed input: 235.20 toks/s, output: 7.85 toks/s]
[2025-01-11 00:40:29,074][root][ERROR] - Processed prompts:  52%|#####2    | 11/21 [00:03<00:02,  3.94it/s, est. speed input: 2746.13 toks/s, output: 86.14 toks/s]
[2025-01-11 00:40:29,424][root][ERROR] - Processed prompts:  62%|######1   | 13/21 [00:04<00:01,  4.21it/s, est. speed input: 2925.43 toks/s, output: 100.74 toks/s]
[2025-01-11 00:40:29,635][root][ERROR] - Processed prompts:  71%|#######1  | 15/21 [00:04<00:01,  4.84it/s, est. speed input: 3182.06 toks/s, output: 120.17 toks/s]
[2025-01-11 00:40:29,878][root][ERROR] - Processed prompts:  81%|########  | 17/21 [00:04<00:00,  5.39it/s, est. speed input: 3416.81 toks/s, output: 142.53 toks/s]
[2025-01-11 00:40:30,223][root][ERROR] - Processed prompts:  90%|######### | 19/21 [00:04<00:00,  5.49it/s, est. speed input: 3540.14 toks/s, output: 163.33 toks/s]
[2025-01-11 00:40:30,389][root][ERROR] - Processed prompts:  95%|#########5| 20/21 [00:05<00:00,  5.57it/s, est. speed input: 3607.44 toks/s, output: 176.14 toks/s]
[2025-01-11 00:40:30,507][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  5.94it/s, est. speed input: 3695.85 toks/s, output: 191.24 toks/s]
[2025-01-11 00:40:30,507][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  4.05it/s, est. speed input: 3695.85 toks/s, output: 191.24 toks/s]
WARNING 01-11 00:40:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:30,731][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:32,239][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.51s/it, est. speed input: 464.71 toks/s, output: 17.90 toks/s]
[2025-01-11 00:40:33,028][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:02<00:01,  2.01it/s, est. speed input: 1437.91 toks/s, output: 64.43 toks/s]
[2025-01-11 00:40:33,149][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.54it/s, est. speed input: 1726.77 toks/s, output: 89.75 toks/s]
[2025-01-11 00:40:33,404][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.63it/s, est. speed input: 2246.09 toks/s, output: 139.14 toks/s]
[2025-01-11 00:40:33,405][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.62it/s, est. speed input: 2246.09 toks/s, output: 139.14 toks/s]
WARNING 01-11 00:40:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:33,664][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:35,438][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.77s/it, est. speed input: 533.90 toks/s, output: 15.22 toks/s]
[2025-01-11 00:40:35,629][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  2.58it/s, est. speed input: 2015.13 toks/s, output: 61.05 toks/s]
[2025-01-11 00:40:35,828][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.99it/s, est. speed input: 2356.96 toks/s, output: 75.76 toks/s]
[2025-01-11 00:40:36,193][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.91it/s, est. speed input: 2400.42 toks/s, output: 89.36 toks/s]
[2025-01-11 00:40:36,414][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.25it/s, est. speed input: 2554.99 toks/s, output: 109.08 toks/s]
[2025-01-11 00:40:36,583][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.74it/s, est. speed input: 2801.21 toks/s, output: 131.55 toks/s]
[2025-01-11 00:40:36,583][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.74it/s, est. speed input: 2801.21 toks/s, output: 131.55 toks/s]
WARNING 01-11 00:40:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:36,814][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:39,527][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:37,  2.71s/it, est. speed input: 350.63 toks/s, output: 10.69 toks/s]
[2025-01-11 00:40:40,028][root][ERROR] - Processed prompts:  60%|######    | 9/15 [00:03<00:01,  3.62it/s, est. speed input: 2652.78 toks/s, output: 88.05 toks/s]
[2025-01-11 00:40:40,349][root][ERROR] - Processed prompts:  73%|#######3  | 11/15 [00:03<00:00,  4.04it/s, est. speed input: 2880.06 toks/s, output: 111.47 toks/s]
[2025-01-11 00:40:40,515][root][ERROR] - Processed prompts:  87%|########6 | 13/15 [00:03<00:00,  4.89it/s, est. speed input: 3244.35 toks/s, output: 142.93 toks/s]
[2025-01-11 00:40:41,533][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  3.48it/s, est. speed input: 2941.80 toks/s, output: 155.34 toks/s]
[2025-01-11 00:40:41,533][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  3.18it/s, est. speed input: 2941.80 toks/s, output: 155.34 toks/s]
WARNING 01-11 00:40:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:41,819][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:43,130][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.31s/it, est. speed input: 765.93 toks/s, output: 12.97 toks/s]
[2025-01-11 00:40:43,408][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.42it/s, est. speed input: 1352.49 toks/s, output: 28.96 toks/s]
[2025-01-11 00:40:43,914][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.87it/s, est. speed input: 3203.04 toks/s, output: 91.69 toks/s]
[2025-01-11 00:40:43,914][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.86it/s, est. speed input: 3203.04 toks/s, output: 91.69 toks/s]
WARNING 01-11 00:40:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:44,145][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:46,050][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.91s/it, est. speed input: 508.67 toks/s, output: 15.22 toks/s]
[2025-01-11 00:40:46,720][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  3.84it/s, est. speed input: 2914.37 toks/s, output: 105.62 toks/s]
[2025-01-11 00:40:46,990][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.82it/s, est. speed input: 3013.42 toks/s, output: 124.06 toks/s]
[2025-01-11 00:40:46,990][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.16it/s, est. speed input: 3013.42 toks/s, output: 124.06 toks/s]
WARNING 01-11 00:40:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:47,248][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:49,170][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.92s/it, est. speed input: 604.76 toks/s, output: 15.09 toks/s]
[2025-01-11 00:40:49,794][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.91it/s, est. speed input: 3532.02 toks/s, output: 105.68 toks/s]
[2025-01-11 00:40:49,794][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.14it/s, est. speed input: 3532.02 toks/s, output: 105.68 toks/s]
WARNING 01-11 00:40:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:50,001][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:51,097][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.10s/it, est. speed input: 1081.29 toks/s, output: 26.48 toks/s]
[2025-01-11 00:40:51,097][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.74it/s, est. speed input: 2826.53 toks/s, output: 79.43 toks/s]
WARNING 01-11 00:40:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:40:51,357][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:40:52,530][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.17s/it, est. speed input: 1232.12 toks/s, output: 24.74 toks/s]
[2025-01-11 00:40:52,834][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.40it/s, est. speed input: 2562.97 toks/s, output: 71.14 toks/s]
[2025-01-11 00:40:52,834][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.03it/s, est. speed input: 2562.97 toks/s, output: 71.14 toks/s]
[2025-01-11 00:40:55,120][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:40:55,172][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:40:56,824][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 00:40:58,459][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 00:41:00,019][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.60s/it]
[2025-01-11 00:41:00,548][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 00:41:00,549][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 00:41:22,183][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:41:22,235][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:41:24,306][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.07s/it]
[2025-01-11 00:41:25,970][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.83s/it]
[2025-01-11 00:41:27,613][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.75s/it]
[2025-01-11 00:41:28,185][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.28s/it]
[2025-01-11 00:41:28,185][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.49s/it]
WARNING 01-11 00:41:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:41:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:41:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:41:48 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:41:48,455][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:41:49,790][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
[2025-01-11 00:41:50,180][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 00:41:51,473][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 00:41:52,802][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 00:41:52,802][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 00:41:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:42:06 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:42:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:42:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:42:28 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 00:42:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:29,142][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:33,132][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.99s/it, est. speed input: 159.16 toks/s, output: 3.26 toks/s]
[2025-01-11 00:42:33,253][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.71s/it, est. speed input: 308.94 toks/s, output: 6.81 toks/s]
[2025-01-11 00:42:33,371][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.02it/s, est. speed input: 450.52 toks/s, output: 10.64 toks/s]
[2025-01-11 00:42:33,586][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:07,  3.21it/s, est. speed input: 1000.27 toks/s, output: 26.78 toks/s]
[2025-01-11 00:42:33,730][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  7.31it/s, est. speed input: 1799.47 toks/s, output: 55.37 toks/s]
[2025-01-11 00:42:33,854][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01, 10.20it/s, est. speed input: 2291.39 toks/s, output: 75.78 toks/s]
[2025-01-11 00:42:34,032][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 11.48it/s, est. speed input: 2597.60 toks/s, output: 91.84 toks/s]
[2025-01-11 00:42:34,209][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00, 13.86it/s, est. speed input: 3008.24 toks/s, output: 116.07 toks/s]
[2025-01-11 00:42:34,408][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 14.17it/s, est. speed input: 3256.20 toks/s, output: 135.41 toks/s]
[2025-01-11 00:42:34,531][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 16.05it/s, est. speed input: 3535.22 toks/s, output: 159.41 toks/s]
[2025-01-11 00:42:35,049][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.42it/s, est. speed input: 3440.10 toks/s, output: 171.33 toks/s]
WARNING 01-11 00:42:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:35,262][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:36,232][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.03it/s, est. speed input: 709.67 toks/s, output: 34.04 toks/s]
[2025-01-11 00:42:37,017][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.16it/s, est. speed input: 817.21 toks/s, output: 64.40 toks/s]
[2025-01-11 00:42:37,017][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.14it/s, est. speed input: 817.21 toks/s, output: 64.40 toks/s]
WARNING 01-11 00:42:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:37,260][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:40,168][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:02<01:24,  2.91s/it, est. speed input: 224.86 toks/s, output: 3.78 toks/s]
[2025-01-11 00:42:40,571][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:40,  1.43s/it, est. speed input: 396.79 toks/s, output: 8.76 toks/s]
[2025-01-11 00:42:40,684][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:22,  1.20it/s, est. speed input: 574.20 toks/s, output: 14.31 toks/s]
[2025-01-11 00:42:40,792][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:14,  1.83it/s, est. speed input: 746.47 toks/s, output: 20.10 toks/s]
[2025-01-11 00:42:40,895][root][ERROR] - Processed prompts:  23%|##3       | 7/30 [00:03<00:05,  4.46it/s, est. speed input: 1268.26 toks/s, output: 38.52 toks/s]
[2025-01-11 00:42:41,029][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:03<00:01,  9.44it/s, est. speed input: 2090.53 toks/s, output: 70.57 toks/s]
[2025-01-11 00:42:41,195][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:03<00:01, 11.26it/s, est. speed input: 2501.78 toks/s, output: 90.96 toks/s]
[2025-01-11 00:42:41,325][root][ERROR] - Processed prompts:  70%|#######   | 21/30 [00:04<00:00, 17.94it/s, est. speed input: 3404.74 toks/s, output: 136.76 toks/s]
[2025-01-11 00:42:41,513][root][ERROR] - Processed prompts:  80%|########  | 24/30 [00:04<00:00, 17.40it/s, est. speed input: 3729.70 toks/s, output: 158.48 toks/s]
[2025-01-11 00:42:41,698][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 17.06it/s, est. speed input: 4036.62 toks/s, output: 184.32 toks/s]
[2025-01-11 00:42:42,054][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 13.26it/s, est. speed input: 4153.22 toks/s, output: 207.12 toks/s]
[2025-01-11 00:42:42,054][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.26it/s, est. speed input: 4153.22 toks/s, output: 207.12 toks/s]
WARNING 01-11 00:42:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:42,351][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:45,048][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:02<00:56,  2.70s/it, est. speed input: 276.60 toks/s, output: 5.93 toks/s]
[2025-01-11 00:42:45,417][root][ERROR] - Processed prompts:   9%|9         | 2/22 [00:03<00:26,  1.33s/it, est. speed input: 487.33 toks/s, output: 13.05 toks/s]
[2025-01-11 00:42:45,591][root][ERROR] - Processed prompts:  18%|#8        | 4/22 [00:03<00:09,  1.82it/s, est. speed input: 927.87 toks/s, output: 28.40 toks/s]
[2025-01-11 00:42:45,756][root][ERROR] - Processed prompts:  27%|##7       | 6/22 [00:03<00:05,  3.04it/s, est. speed input: 1323.52 toks/s, output: 44.94 toks/s]
[2025-01-11 00:42:45,904][root][ERROR] - Processed prompts:  36%|###6      | 8/22 [00:03<00:03,  4.42it/s, est. speed input: 1687.00 toks/s, output: 62.48 toks/s]
[2025-01-11 00:42:46,252][root][ERROR] - Processed prompts:  45%|####5     | 10/22 [00:03<00:02,  4.83it/s, est. speed input: 1921.68 toks/s, output: 79.20 toks/s]
[2025-01-11 00:42:46,410][root][ERROR] - Processed prompts:  59%|#####9    | 13/22 [00:04<00:01,  7.18it/s, est. speed input: 2400.74 toks/s, output: 112.60 toks/s]
[2025-01-11 00:42:46,552][root][ERROR] - Processed prompts:  82%|########1 | 18/22 [00:04<00:00, 12.10it/s, est. speed input: 3242.36 toks/s, output: 171.63 toks/s]
[2025-01-11 00:42:46,862][root][ERROR] - Processed prompts:  91%|######### | 20/22 [00:04<00:00, 10.11it/s, est. speed input: 3373.52 toks/s, output: 189.78 toks/s]
[2025-01-11 00:42:46,948][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  4.79it/s, est. speed input: 3644.15 toks/s, output: 218.85 toks/s]
WARNING 01-11 00:42:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:47,191][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:48,839][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:14,  1.65s/it, est. speed input: 421.81 toks/s, output: 13.96 toks/s]
[2025-01-11 00:42:49,005][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:01<00:03,  2.05it/s, est. speed input: 1204.74 toks/s, output: 43.01 toks/s]
[2025-01-11 00:42:49,350][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.86it/s, est. speed input: 2035.85 toks/s, output: 84.79 toks/s]
[2025-01-11 00:42:49,523][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  4.17it/s, est. speed input: 2210.42 toks/s, output: 100.80 toks/s]
[2025-01-11 00:42:49,840][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  3.87it/s, est. speed input: 2215.08 toks/s, output: 114.42 toks/s]
[2025-01-11 00:42:50,157][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.60it/s, est. speed input: 2442.20 toks/s, output: 154.47 toks/s]
[2025-01-11 00:42:50,157][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.37it/s, est. speed input: 2442.20 toks/s, output: 154.47 toks/s]
WARNING 01-11 00:42:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:50,385][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:52,600][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:26,  2.21s/it, est. speed input: 328.67 toks/s, output: 12.64 toks/s]
[2025-01-11 00:42:52,731][root][ERROR] - Processed prompts:  23%|##3       | 3/13 [00:02<00:06,  1.61it/s, est. speed input: 945.80 toks/s, output: 38.79 toks/s]
[2025-01-11 00:42:53,019][root][ERROR] - Processed prompts:  38%|###8      | 5/13 [00:02<00:03,  2.66it/s, est. speed input: 1453.27 toks/s, output: 63.42 toks/s]
[2025-01-11 00:42:53,154][root][ERROR] - Processed prompts:  46%|####6     | 6/13 [00:02<00:02,  3.21it/s, est. speed input: 1667.65 toks/s, output: 77.30 toks/s]
[2025-01-11 00:42:53,285][root][ERROR] - Processed prompts:  54%|#####3    | 7/13 [00:02<00:01,  3.82it/s, est. speed input: 1853.67 toks/s, output: 91.73 toks/s]
[2025-01-11 00:42:53,407][root][ERROR] - Processed prompts:  62%|######1   | 8/13 [00:03<00:01,  4.50it/s, est. speed input: 2057.43 toks/s, output: 106.89 toks/s]
[2025-01-11 00:42:53,817][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:03<00:01,  3.62it/s, est. speed input: 2061.67 toks/s, output: 115.98 toks/s]
[2025-01-11 00:42:53,926][root][ERROR] - Processed prompts:  77%|#######6  | 10/13 [00:03<00:00,  4.39it/s, est. speed input: 2214.49 toks/s, output: 134.98 toks/s]
[2025-01-11 00:42:54,094][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:03<00:00,  6.16it/s, est. speed input: 2582.39 toks/s, output: 174.71 toks/s]
[2025-01-11 00:42:54,178][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.43it/s, est. speed input: 2719.79 toks/s, output: 195.63 toks/s]
WARNING 01-11 00:42:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:54,418][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:42:57,002][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:02<00:46,  2.58s/it, est. speed input: 289.88 toks/s, output: 7.74 toks/s]
[2025-01-11 00:42:57,379][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:02<00:21,  1.29s/it, est. speed input: 557.93 toks/s, output: 16.55 toks/s]
[2025-01-11 00:42:57,643][root][ERROR] - Processed prompts:  42%|####2     | 8/19 [00:03<00:02,  4.01it/s, est. speed input: 1924.02 toks/s, output: 71.64 toks/s]
[2025-01-11 00:42:58,062][root][ERROR] - Processed prompts:  53%|#####2    | 10/19 [00:03<00:02,  4.20it/s, est. speed input: 2130.14 toks/s, output: 88.37 toks/s]
[2025-01-11 00:42:58,176][root][ERROR] - Processed prompts:  58%|#####7    | 11/19 [00:03<00:01,  4.60it/s, est. speed input: 2256.89 toks/s, output: 100.32 toks/s]
[2025-01-11 00:42:58,308][root][ERROR] - Processed prompts:  68%|######8   | 13/19 [00:03<00:01,  5.95it/s, est. speed input: 2561.48 toks/s, output: 126.75 toks/s]
[2025-01-11 00:42:58,421][root][ERROR] - Processed prompts:  79%|#######8  | 15/19 [00:04<00:00,  7.54it/s, est. speed input: 2869.31 toks/s, output: 154.37 toks/s]
[2025-01-11 00:42:58,691][root][ERROR] - Processed prompts:  95%|#########4| 18/19 [00:04<00:00,  8.67it/s, est. speed input: 3232.24 toks/s, output: 195.17 toks/s]
[2025-01-11 00:42:59,446][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  3.78it/s, est. speed input: 2915.04 toks/s, output: 190.54 toks/s]
WARNING 01-11 00:42:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:42:59,700][root][ERROR] - Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:02,591][root][ERROR] - Processed prompts:   6%|5         | 1/17 [00:02<00:46,  2.89s/it, est. speed input: 295.46 toks/s, output: 9.00 toks/s]
[2025-01-11 00:43:02,702][root][ERROR] - Processed prompts:  18%|#7        | 3/17 [00:03<00:11,  1.27it/s, est. speed input: 910.92 toks/s, output: 26.99 toks/s]
[2025-01-11 00:43:02,833][root][ERROR] - Processed prompts:  65%|######4   | 11/17 [00:03<00:00,  6.09it/s, est. speed input: 3302.38 toks/s, output: 101.51 toks/s]
[2025-01-11 00:43:03,238][root][ERROR] - Processed prompts:  88%|########8 | 15/17 [00:03<00:00,  7.08it/s, est. speed input: 3939.37 toks/s, output: 145.59 toks/s]
[2025-01-11 00:43:03,824][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:04<00:00,  4.12it/s, est. speed input: 3818.17 toks/s, output: 162.48 toks/s]
WARNING 01-11 00:43:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:04,052][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:06,330][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:27,  2.28s/it, est. speed input: 392.22 toks/s, output: 12.74 toks/s]
[2025-01-11 00:43:06,489][root][ERROR] - Processed prompts:  31%|###       | 4/13 [00:02<00:04,  2.11it/s, est. speed input: 1291.90 toks/s, output: 52.14 toks/s]
[2025-01-11 00:43:06,698][root][ERROR] - Processed prompts:  54%|#####3    | 7/13 [00:02<00:01,  3.89it/s, est. speed input: 2110.74 toks/s, output: 89.59 toks/s]
[2025-01-11 00:43:06,910][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:02<00:00,  4.86it/s, est. speed input: 2540.21 toks/s, output: 116.88 toks/s]
[2025-01-11 00:43:07,650][root][ERROR] - Processed prompts:  85%|########4 | 11/13 [00:03<00:00,  3.86it/s, est. speed input: 2496.69 toks/s, output: 137.05 toks/s]
[2025-01-11 00:43:07,816][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:03<00:00,  4.14it/s, est. speed input: 2613.59 toks/s, output: 156.26 toks/s]
[2025-01-11 00:43:07,850][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.42it/s, est. speed input: 2822.26 toks/s, output: 180.41 toks/s]
WARNING 01-11 00:43:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:08,141][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:10,321][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.18s/it, est. speed input: 368.34 toks/s, output: 13.30 toks/s]
[2025-01-11 00:43:10,637][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.55it/s, est. speed input: 1834.07 toks/s, output: 62.90 toks/s]
[2025-01-11 00:43:10,826][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.58it/s, est. speed input: 2400.82 toks/s, output: 93.10 toks/s]
[2025-01-11 00:43:11,174][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:03<00:00,  4.13it/s, est. speed input: 2742.75 toks/s, output: 121.01 toks/s]
[2025-01-11 00:43:11,285][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  4.63it/s, est. speed input: 2918.03 toks/s, output: 139.65 toks/s]
[2025-01-11 00:43:11,454][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  4.86it/s, est. speed input: 3126.32 toks/s, output: 157.28 toks/s]
[2025-01-11 00:43:11,454][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.32it/s, est. speed input: 3126.32 toks/s, output: 157.28 toks/s]
WARNING 01-11 00:43:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:11,694][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:13,669][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:01<00:23,  1.97s/it, est. speed input: 456.24 toks/s, output: 8.10 toks/s]
[2025-01-11 00:43:14,099][root][ERROR] - Processed prompts:  15%|#5        | 2/13 [00:02<00:11,  1.07s/it, est. speed input: 756.11 toks/s, output: 18.72 toks/s]
[2025-01-11 00:43:14,298][root][ERROR] - Processed prompts:  62%|######1   | 8/13 [00:02<00:01,  4.90it/s, est. speed input: 2825.28 toks/s, output: 87.95 toks/s]
[2025-01-11 00:43:14,788][root][ERROR] - Processed prompts:  77%|#######6  | 10/13 [00:03<00:00,  4.63it/s, est. speed input: 2974.50 toks/s, output: 107.63 toks/s]
[2025-01-11 00:43:15,720][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:04<00:00,  3.47it/s, est. speed input: 2736.03 toks/s, output: 126.19 toks/s]
[2025-01-11 00:43:15,906][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:04<00:00,  3.70it/s, est. speed input: 2852.84 toks/s, output: 149.12 toks/s]
[2025-01-11 00:43:15,906][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:04<00:00,  3.09it/s, est. speed input: 2852.84 toks/s, output: 149.12 toks/s]
WARNING 01-11 00:43:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:16,164][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:18,440][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:22,  2.28s/it, est. speed input: 518.36 toks/s, output: 12.74 toks/s]
[2025-01-11 00:43:18,795][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.47it/s, est. speed input: 3445.82 toks/s, output: 108.33 toks/s]
[2025-01-11 00:43:18,975][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  5.24it/s, est. speed input: 3972.48 toks/s, output: 140.17 toks/s]
[2025-01-11 00:43:18,975][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  3.91it/s, est. speed input: 3972.48 toks/s, output: 140.17 toks/s]
WARNING 01-11 00:43:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:19,197][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:21,145][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.95s/it, est. speed input: 480.51 toks/s, output: 14.89 toks/s]
[2025-01-11 00:43:21,556][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  2.65it/s, est. speed input: 2231.32 toks/s, output: 69.09 toks/s]
[2025-01-11 00:43:21,823][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  3.51it/s, est. speed input: 2674.99 toks/s, output: 103.97 toks/s]
[2025-01-11 00:43:21,952][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  3.97it/s, est. speed input: 2913.19 toks/s, output: 123.41 toks/s]
[2025-01-11 00:43:21,970][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.25it/s, est. speed input: 3249.19 toks/s, output: 147.17 toks/s]
WARNING 01-11 00:43:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:22,229][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:23,347][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 1041.10 toks/s, output: 25.94 toks/s]
[2025-01-11 00:43:24,035][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.84it/s, est. speed input: 1823.17 toks/s, output: 70.87 toks/s]
[2025-01-11 00:43:24,035][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.66it/s, est. speed input: 1823.17 toks/s, output: 70.87 toks/s]
WARNING 01-11 00:43:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:24,260][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:26,397][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:19,  2.14s/it, est. speed input: 516.89 toks/s, output: 13.58 toks/s]
[2025-01-11 00:43:26,582][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:02<00:02,  2.20it/s, est. speed input: 1769.50 toks/s, output: 52.98 toks/s]
[2025-01-11 00:43:27,045][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  2.80it/s, est. speed input: 2266.31 toks/s, output: 77.23 toks/s]
[2025-01-11 00:43:27,253][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  4.58it/s, est. speed input: 3133.18 toks/s, output: 132.67 toks/s]
[2025-01-11 00:43:27,456][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.13it/s, est. speed input: 3248.94 toks/s, output: 148.97 toks/s]
WARNING 01-11 00:43:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:27,723][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:28,992][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.27s/it, est. speed input: 814.88 toks/s, output: 29.95 toks/s]
[2025-01-11 00:43:29,250][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.48it/s, est. speed input: 1356.20 toks/s, output: 58.94 toks/s]
[2025-01-11 00:43:29,318][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.88it/s, est. speed input: 1893.19 toks/s, output: 91.55 toks/s]
WARNING 01-11 00:43:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:29,544][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:30,962][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.42s/it, est. speed input: 713.91 toks/s, output: 20.46 toks/s]
[2025-01-11 00:43:31,168][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.25it/s, est. speed input: 1854.71 toks/s, output: 59.75 toks/s]
[2025-01-11 00:43:31,389][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.73it/s, est. speed input: 2289.71 toks/s, output: 80.23 toks/s]
[2025-01-11 00:43:31,996][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.24it/s, est. speed input: 2247.36 toks/s, output: 95.85 toks/s]
[2025-01-11 00:43:31,996][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.04it/s, est. speed input: 2247.36 toks/s, output: 95.85 toks/s]
WARNING 01-11 00:43:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:32,239][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:33,985][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.75s/it, est. speed input: 647.84 toks/s, output: 16.61 toks/s]
[2025-01-11 00:43:34,285][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:02<00:02,  1.77it/s, est. speed input: 1590.00 toks/s, output: 48.88 toks/s]
[2025-01-11 00:43:34,538][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.95it/s, est. speed input: 2356.88 toks/s, output: 87.02 toks/s]
[2025-01-11 00:43:34,780][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  4.05it/s, est. speed input: 3020.38 toks/s, output: 128.31 toks/s]
[2025-01-11 00:43:34,780][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.75it/s, est. speed input: 3020.38 toks/s, output: 128.31 toks/s]
WARNING 01-11 00:43:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:43:35,017][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:43:36,418][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.40s/it, est. speed input: 986.98 toks/s, output: 36.40 toks/s]
[2025-01-11 00:43:36,705][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.34it/s, est. speed input: 1558.17 toks/s, output: 70.48 toks/s]
[2025-01-11 00:43:36,706][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.18it/s, est. speed input: 1558.17 toks/s, output: 70.48 toks/s]
[2025-01-11 00:43:38,791][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:43:38,844][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:43:40,526][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.68s/it]
[2025-01-11 00:43:42,181][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 00:43:43,818][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 00:43:44,345][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 00:43:44,345][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 00:44:05,656][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:44:05,708][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:44:07,709][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.00s/it]
[2025-01-11 00:44:09,291][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 00:44:10,858][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 00:44:11,367][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 00:44:11,368][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 00:44:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:44:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:44:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:44:32 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:44:32,315][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:44:33,639][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 00:44:34,017][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 00:44:35,318][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 00:44:36,645][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 00:44:36,645][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 00:44:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:44:50 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:44:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:44:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:45:13 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:45:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:13,346][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:17,280][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:01,  3.93s/it, est. speed input: 161.41 toks/s, output: 3.30 toks/s]
[2025-01-11 00:45:17,402][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:50,  1.69s/it, est. speed input: 313.15 toks/s, output: 6.90 toks/s]
[2025-01-11 00:45:17,520][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.03it/s, est. speed input: 456.47 toks/s, output: 10.78 toks/s]
[2025-01-11 00:45:17,685][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.74it/s, est. speed input: 878.18 toks/s, output: 23.05 toks/s]
[2025-01-11 00:45:17,789][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:04,  4.88it/s, est. speed input: 1286.37 toks/s, output: 36.91 toks/s]
[2025-01-11 00:45:17,916][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01, 12.18it/s, est. speed input: 2362.28 toks/s, output: 76.59 toks/s]
[2025-01-11 00:45:18,199][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 12.73it/s, est. speed input: 2747.79 toks/s, output: 97.88 toks/s]
[2025-01-11 00:45:18,348][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00, 14.05it/s, est. speed input: 3047.08 toks/s, output: 116.76 toks/s]
[2025-01-11 00:45:18,452][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 16.26it/s, est. speed input: 3357.85 toks/s, output: 138.47 toks/s]
[2025-01-11 00:45:18,661][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 15.70it/s, est. speed input: 3584.62 toks/s, output: 159.94 toks/s]
[2025-01-11 00:45:18,894][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.77it/s, est. speed input: 3662.71 toks/s, output: 175.93 toks/s]
WARNING 01-11 00:45:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:19,119][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:20,096][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 743.75 toks/s, output: 43.03 toks/s]
[2025-01-11 00:45:20,096][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 743.75 toks/s, output: 43.03 toks/s]
WARNING 01-11 00:45:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:20,346][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:23,507][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:34,  3.16s/it, est. speed input: 204.38 toks/s, output: 4.43 toks/s]
[2025-01-11 00:45:23,625][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:39,  1.37s/it, est. speed input: 398.03 toks/s, output: 9.15 toks/s]
[2025-01-11 00:45:23,798][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:23,  1.21it/s, est. speed input: 568.70 toks/s, output: 14.20 toks/s]
[2025-01-11 00:45:23,908][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:07,  3.30it/s, est. speed input: 1112.21 toks/s, output: 30.89 toks/s]
[2025-01-11 00:45:24,010][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:04,  4.87it/s, est. speed input: 1437.31 toks/s, output: 42.31 toks/s]
[2025-01-11 00:45:24,196][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:03<00:02,  8.19it/s, est. speed input: 2051.91 toks/s, output: 66.75 toks/s]
[2025-01-11 00:45:24,471][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:04<00:01, 10.08it/s, est. speed input: 2556.24 toks/s, output: 91.16 toks/s]
[2025-01-11 00:45:24,614][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:04<00:01, 10.79it/s, est. speed input: 2782.62 toks/s, output: 105.21 toks/s]
[2025-01-11 00:45:24,775][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 12.55it/s, est. speed input: 3135.52 toks/s, output: 128.93 toks/s]
[2025-01-11 00:45:24,887][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:04<00:00, 15.16it/s, est. speed input: 3499.19 toks/s, output: 155.71 toks/s]
[2025-01-11 00:45:25,138][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 12.49it/s, est. speed input: 3593.90 toks/s, output: 170.49 toks/s]
[2025-01-11 00:45:25,295][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:04<00:00, 12.55it/s, est. speed input: 3752.70 toks/s, output: 190.74 toks/s]
[2025-01-11 00:45:25,447][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:05<00:00, 12.71it/s, est. speed input: 3901.56 toks/s, output: 212.31 toks/s]
[2025-01-11 00:45:25,464][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  6.06it/s, est. speed input: 4017.08 toks/s, output: 225.86 toks/s]
WARNING 01-11 00:45:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:25,703][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:28,112][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:02<00:50,  2.41s/it, est. speed input: 307.68 toks/s, output: 4.15 toks/s]
[2025-01-11 00:45:28,435][root][ERROR] - Processed prompts:   9%|9         | 2/22 [00:02<00:23,  1.18s/it, est. speed input: 545.50 toks/s, output: 9.88 toks/s]
[2025-01-11 00:45:28,702][root][ERROR] - Processed prompts:  14%|#3        | 3/22 [00:02<00:14,  1.31it/s, est. speed input: 743.67 toks/s, output: 16.67 toks/s]
[2025-01-11 00:45:28,963][root][ERROR] - Processed prompts:  18%|#8        | 4/22 [00:03<00:10,  1.77it/s, est. speed input: 913.01 toks/s, output: 24.24 toks/s]
[2025-01-11 00:45:29,113][root][ERROR] - Processed prompts:  36%|###6      | 8/22 [00:03<00:02,  5.01it/s, est. speed input: 1763.24 toks/s, output: 58.95 toks/s]
[2025-01-11 00:45:29,282][root][ERROR] - Processed prompts:  50%|#####     | 11/22 [00:03<00:01,  7.21it/s, est. speed input: 2311.89 toks/s, output: 86.06 toks/s]
[2025-01-11 00:45:29,489][root][ERROR] - Processed prompts:  59%|#####9    | 13/22 [00:03<00:01,  7.78it/s, est. speed input: 2592.88 toks/s, output: 103.27 toks/s]
[2025-01-11 00:45:29,625][root][ERROR] - Processed prompts:  73%|#######2  | 16/22 [00:03<00:00, 10.34it/s, est. speed input: 3095.87 toks/s, output: 136.42 toks/s]
[2025-01-11 00:45:29,786][root][ERROR] - Processed prompts:  82%|########1 | 18/22 [00:04<00:00, 10.83it/s, est. speed input: 3368.33 toks/s, output: 157.49 toks/s]
[2025-01-11 00:45:29,940][root][ERROR] - Processed prompts:  95%|#########5| 21/22 [00:04<00:00, 12.90it/s, est. speed input: 3782.26 toks/s, output: 194.94 toks/s]
[2025-01-11 00:45:29,958][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  5.17it/s, est. speed input: 3955.44 toks/s, output: 209.66 toks/s]
WARNING 01-11 00:45:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:30,190][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:31,974][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:16,  1.78s/it, est. speed input: 399.26 toks/s, output: 15.70 toks/s]
[2025-01-11 00:45:32,160][root][ERROR] - Processed prompts:  40%|####      | 4/10 [00:01<00:02,  2.58it/s, est. speed input: 1477.06 toks/s, output: 62.43 toks/s]
[2025-01-11 00:45:32,378][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.94it/s, est. speed input: 1639.32 toks/s, output: 76.32 toks/s]
[2025-01-11 00:45:32,489][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  4.67it/s, est. speed input: 2215.84 toks/s, output: 113.97 toks/s]
[2025-01-11 00:45:32,679][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  5.95it/s, est. speed input: 2630.71 toks/s, output: 150.26 toks/s]
[2025-01-11 00:45:32,697][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.99it/s, est. speed input: 2920.31 toks/s, output: 173.17 toks/s]
WARNING 01-11 00:45:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:32,933][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:35,292][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:30,  2.36s/it, est. speed input: 349.29 toks/s, output: 11.45 toks/s]
[2025-01-11 00:45:35,465][root][ERROR] - Processed prompts:  57%|#####7    | 8/14 [00:02<00:01,  4.22it/s, est. speed input: 2720.50 toks/s, output: 92.39 toks/s]
[2025-01-11 00:45:35,847][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:02<00:00,  5.06it/s, est. speed input: 3170.11 toks/s, output: 123.87 toks/s]
[2025-01-11 00:45:36,056][root][ERROR] - Processed prompts:  93%|#########2| 13/14 [00:03<00:00,  5.75it/s, est. speed input: 3446.01 toks/s, output: 152.40 toks/s]
[2025-01-11 00:45:36,358][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  4.09it/s, est. speed input: 3381.88 toks/s, output: 162.04 toks/s]
WARNING 01-11 00:45:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:36,632][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:38,894][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:38,  2.26s/it, est. speed input: 319.18 toks/s, output: 6.63 toks/s]
[2025-01-11 00:45:39,343][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:02<00:19,  1.20s/it, est. speed input: 529.39 toks/s, output: 15.13 toks/s]
[2025-01-11 00:45:39,457][root][ERROR] - Processed prompts:  17%|#6        | 3/18 [00:02<00:10,  1.42it/s, est. speed input: 782.54 toks/s, output: 24.77 toks/s]
[2025-01-11 00:45:39,698][root][ERROR] - Processed prompts:  50%|#####     | 9/18 [00:03<00:01,  5.72it/s, est. speed input: 2254.32 toks/s, output: 82.84 toks/s]
[2025-01-11 00:45:39,837][root][ERROR] - Processed prompts:  61%|######1   | 11/18 [00:03<00:01,  6.85it/s, est. speed input: 2638.21 toks/s, output: 104.82 toks/s]
[2025-01-11 00:45:39,961][root][ERROR] - Processed prompts:  72%|#######2  | 13/18 [00:03<00:00,  8.18it/s, est. speed input: 3014.29 toks/s, output: 127.96 toks/s]
[2025-01-11 00:45:40,095][root][ERROR] - Processed prompts:  83%|########3 | 15/18 [00:03<00:00,  9.41it/s, est. speed input: 3342.70 toks/s, output: 152.77 toks/s]
[2025-01-11 00:45:40,761][root][ERROR] - Processed prompts:  94%|#########4| 17/18 [00:04<00:00,  5.80it/s, est. speed input: 3179.97 toks/s, output: 163.21 toks/s]
[2025-01-11 00:45:40,845][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  4.27it/s, est. speed input: 3303.59 toks/s, output: 182.27 toks/s]
WARNING 01-11 00:45:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:41,138][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:43,799][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:37,  2.66s/it, est. speed input: 301.78 toks/s, output: 10.90 toks/s]
[2025-01-11 00:45:43,988][root][ERROR] - Processed prompts:  53%|#####3    | 8/15 [00:02<00:01,  3.75it/s, est. speed input: 2623.75 toks/s, output: 83.87 toks/s]
[2025-01-11 00:45:44,348][root][ERROR] - Processed prompts:  73%|#######3  | 11/15 [00:03<00:00,  4.67it/s, est. speed input: 3117.21 toks/s, output: 116.21 toks/s]
[2025-01-11 00:45:44,553][root][ERROR] - Processed prompts:  87%|########6 | 13/15 [00:03<00:00,  5.38it/s, est. speed input: 3428.60 toks/s, output: 142.61 toks/s]
[2025-01-11 00:45:44,824][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  5.80it/s, est. speed input: 3674.39 toks/s, output: 169.85 toks/s]
[2025-01-11 00:45:44,824][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  4.07it/s, est. speed input: 3674.39 toks/s, output: 169.85 toks/s]
WARNING 01-11 00:45:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:45,047][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:46,739][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:16,  1.69s/it, est. speed input: 402.49 toks/s, output: 11.23 toks/s]
[2025-01-11 00:45:47,035][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:01<00:07,  1.15it/s, est. speed input: 805.42 toks/s, output: 24.15 toks/s]
[2025-01-11 00:45:47,285][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  4.03it/s, est. speed input: 2068.46 toks/s, output: 79.99 toks/s]
[2025-01-11 00:45:47,442][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  5.33it/s, est. speed input: 2625.46 toks/s, output: 111.92 toks/s]
[2025-01-11 00:45:48,465][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.37it/s, est. speed input: 2359.43 toks/s, output: 126.69 toks/s]
[2025-01-11 00:45:50,143][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  1.77it/s, est. speed input: 1744.49 toks/s, output: 124.21 toks/s]
[2025-01-11 00:45:50,143][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:05<00:00,  2.16it/s, est. speed input: 1744.49 toks/s, output: 124.21 toks/s]
WARNING 01-11 00:45:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:50,399][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:52,050][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.65s/it, est. speed input: 539.58 toks/s, output: 16.35 toks/s]
[2025-01-11 00:45:52,442][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.68it/s, est. speed input: 2609.71 toks/s, output: 92.03 toks/s]
[2025-01-11 00:45:52,663][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.83it/s, est. speed input: 2767.08 toks/s, output: 108.67 toks/s]
[2025-01-11 00:45:52,982][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.65it/s, est. speed input: 2758.33 toks/s, output: 125.06 toks/s]
[2025-01-11 00:45:52,982][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.10it/s, est. speed input: 2758.33 toks/s, output: 125.06 toks/s]
WARNING 01-11 00:45:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:53,210][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:55,388][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:26,  2.18s/it, est. speed input: 391.29 toks/s, output: 9.64 toks/s]
[2025-01-11 00:45:55,653][root][ERROR] - Processed prompts:  15%|#5        | 2/13 [00:02<00:11,  1.05s/it, est. speed input: 736.62 toks/s, output: 20.47 toks/s]
[2025-01-11 00:45:55,779][root][ERROR] - Processed prompts:  85%|########4 | 11/13 [00:02<00:00,  7.39it/s, est. speed input: 4101.95 toks/s, output: 124.96 toks/s]
[2025-01-11 00:45:56,287][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  4.23it/s, est. speed input: 3995.36 toks/s, output: 143.68 toks/s]
WARNING 01-11 00:45:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:56,555][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:45:57,936][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.38s/it, est. speed input: 559.98 toks/s, output: 14.49 toks/s]
[2025-01-11 00:45:58,148][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.29it/s, est. speed input: 1690.03 toks/s, output: 45.83 toks/s]
[2025-01-11 00:45:58,373][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.66it/s, est. speed input: 3095.75 toks/s, output: 95.71 toks/s]
[2025-01-11 00:45:59,045][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.14it/s, est. speed input: 2634.33 toks/s, output: 102.43 toks/s]
[2025-01-11 00:45:59,045][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.81it/s, est. speed input: 2634.33 toks/s, output: 102.43 toks/s]
WARNING 01-11 00:45:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:45:59,290][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:00,797][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.51s/it, est. speed input: 698.74 toks/s, output: 19.24 toks/s]
[2025-01-11 00:46:02,987][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.76it/s, est. speed input: 1625.47 toks/s, output: 82.23 toks/s]
[2025-01-11 00:46:02,987][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:03<00:00,  1.62it/s, est. speed input: 1625.47 toks/s, output: 82.23 toks/s]
WARNING 01-11 00:46:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:03,245][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:04,350][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 1031.60 toks/s, output: 26.24 toks/s]
[2025-01-11 00:46:04,806][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.20it/s, est. speed input: 2076.10 toks/s, output: 73.05 toks/s]
[2025-01-11 00:46:04,806][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.92it/s, est. speed input: 2076.10 toks/s, output: 73.05 toks/s]
WARNING 01-11 00:46:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:05,021][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:06,579][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.56s/it, est. speed input: 493.73 toks/s, output: 18.62 toks/s]
[2025-01-11 00:46:06,782][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.86it/s, est. speed input: 2190.28 toks/s, output: 71.55 toks/s]
[2025-01-11 00:46:07,040][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  3.09it/s, est. speed input: 2515.53 toks/s, output: 88.65 toks/s]
[2025-01-11 00:46:09,048][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.25it/s, est. speed input: 1576.71 toks/s, output: 87.18 toks/s]
[2025-01-11 00:46:09,048][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:04<00:00,  1.49it/s, est. speed input: 1576.71 toks/s, output: 87.18 toks/s]
WARNING 01-11 00:46:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:09,291][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:10,383][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.09s/it, est. speed input: 1083.49 toks/s, output: 34.83 toks/s]
[2025-01-11 00:46:10,735][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.52it/s, est. speed input: 1414.00 toks/s, output: 67.20 toks/s]
[2025-01-11 00:46:10,735][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.39it/s, est. speed input: 1414.00 toks/s, output: 67.20 toks/s]
WARNING 01-11 00:46:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:10,943][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:12,618][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:03,  1.67s/it, est. speed input: 533.94 toks/s, output: 34.64 toks/s]
[2025-01-11 00:46:13,130][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.60it/s, est. speed input: 1478.50 toks/s, output: 95.12 toks/s]
[2025-01-11 00:46:13,130][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.37it/s, est. speed input: 1478.50 toks/s, output: 95.12 toks/s]
WARNING 01-11 00:46:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:13,350][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:14,366][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.02s/it, est. speed input: 1235.51 toks/s, output: 25.60 toks/s]
[2025-01-11 00:46:14,495][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  2.02it/s, est. speed input: 1927.74 toks/s, output: 51.53 toks/s]
[2025-01-11 00:46:14,813][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.42it/s, est. speed input: 2065.56 toks/s, output: 75.89 toks/s]
[2025-01-11 00:46:14,813][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.05it/s, est. speed input: 2065.56 toks/s, output: 75.89 toks/s]
WARNING 01-11 00:46:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:15,021][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:16,272][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 788.25 toks/s, output: 46.37 toks/s]
[2025-01-11 00:46:16,273][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 788.25 toks/s, output: 46.37 toks/s]
WARNING 01-11 00:46:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:16,496][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:17,480][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.02it/s, est. speed input: 914.86 toks/s, output: 32.53 toks/s]
[2025-01-11 00:46:17,919][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.51it/s, est. speed input: 1421.80 toks/s, output: 63.28 toks/s]
[2025-01-11 00:46:17,919][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.41it/s, est. speed input: 1421.80 toks/s, output: 63.28 toks/s]
WARNING 01-11 00:46:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:46:18,133][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:46:19,014][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 1378.18 toks/s, output: 38.60 toks/s]
[2025-01-11 00:46:19,014][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 1378.18 toks/s, output: 38.60 toks/s]
[2025-01-11 00:46:21,185][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:46:21,238][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:46:22,910][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 00:46:24,562][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 00:46:26,160][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 00:46:26,692][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 00:46:26,692][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 00:46:47,048][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:46:47,099][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:46:49,138][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.04s/it]
[2025-01-11 00:46:50,759][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 00:46:52,355][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 00:46:52,860][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 00:46:52,860][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 00:47:12 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:47:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:47:13 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:47:13 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:47:13,430][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:47:14,887][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
[2025-01-11 00:47:15,393][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.11it/s]
[2025-01-11 00:47:16,877][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.17s/it]
[2025-01-11 00:47:18,737][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.44s/it]
[2025-01-11 00:47:18,738][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.33s/it]
INFO 01-11 00:47:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:47:32 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:47:33 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:47:33 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:47:54 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:47:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:47:55,321][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:47:59,228][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:01,  3.91s/it, est. speed input: 162.52 toks/s, output: 3.33 toks/s]
[2025-01-11 00:47:59,349][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:50,  1.68s/it, est. speed input: 315.27 toks/s, output: 6.95 toks/s]
[2025-01-11 00:47:59,467][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.03it/s, est. speed input: 459.48 toks/s, output: 10.85 toks/s]
[2025-01-11 00:47:59,632][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.75it/s, est. speed input: 883.77 toks/s, output: 23.20 toks/s]
[2025-01-11 00:47:59,736][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:04,  4.91it/s, est. speed input: 1294.42 toks/s, output: 37.14 toks/s]
[2025-01-11 00:47:59,875][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  8.16it/s, est. speed input: 1812.40 toks/s, output: 56.86 toks/s]
[2025-01-11 00:48:00,061][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:00, 14.26it/s, est. speed input: 2679.19 toks/s, output: 94.72 toks/s]
[2025-01-11 00:48:00,296][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 15.01it/s, est. speed input: 3062.98 toks/s, output: 117.17 toks/s]
[2025-01-11 00:48:00,473][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 15.45it/s, est. speed input: 3327.62 toks/s, output: 137.61 toks/s]
[2025-01-11 00:48:00,798][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 15.43it/s, est. speed input: 3709.91 toks/s, output: 174.72 toks/s]
[2025-01-11 00:48:00,798][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.84it/s, est. speed input: 3709.91 toks/s, output: 174.72 toks/s]
WARNING 01-11 00:48:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:01,047][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:01,689][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 1142.48 toks/s, output: 34.29 toks/s]
[2025-01-11 00:48:01,689][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 1142.48 toks/s, output: 34.29 toks/s]
WARNING 01-11 00:48:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:01,935][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:05,086][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:34,  3.15s/it, est. speed input: 205.65 toks/s, output: 4.44 toks/s]
[2025-01-11 00:48:05,615][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:46,  1.61s/it, est. speed input: 355.49 toks/s, output: 10.06 toks/s]
[2025-01-11 00:48:05,780][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.02it/s, est. speed input: 850.59 toks/s, output: 28.61 toks/s]
[2025-01-11 00:48:05,928][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:03<00:05,  4.32it/s, est. speed input: 1477.36 toks/s, output: 54.61 toks/s]
[2025-01-11 00:48:06,159][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:04<00:03,  5.06it/s, est. speed input: 1705.59 toks/s, output: 66.77 toks/s]
[2025-01-11 00:48:06,285][root][ERROR] - Processed prompts:  45%|####5     | 14/31 [00:04<00:02,  7.27it/s, est. speed input: 2111.65 toks/s, output: 88.98 toks/s]
[2025-01-11 00:48:06,403][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:04<00:01,  9.73it/s, est. speed input: 2503.39 toks/s, output: 113.04 toks/s]
[2025-01-11 00:48:06,574][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:01, 10.16it/s, est. speed input: 2700.21 toks/s, output: 127.41 toks/s]
[2025-01-11 00:48:06,705][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 11.14it/s, est. speed input: 2902.81 toks/s, output: 144.45 toks/s]
[2025-01-11 00:48:06,816][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:04<00:00, 15.88it/s, est. speed input: 3375.47 toks/s, output: 183.19 toks/s]
[2025-01-11 00:48:06,976][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:05<00:00, 16.67it/s, est. speed input: 3670.88 toks/s, output: 211.87 toks/s]
[2025-01-11 00:48:07,320][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00, 12.99it/s, est. speed input: 3813.25 toks/s, output: 240.13 toks/s]
[2025-01-11 00:48:07,320][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.76it/s, est. speed input: 3813.25 toks/s, output: 240.13 toks/s]
WARNING 01-11 00:48:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:07,563][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:10,576][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:06,  3.01s/it, est. speed input: 241.98 toks/s, output: 6.64 toks/s]
[2025-01-11 00:48:10,861][root][ERROR] - Processed prompts:   9%|8         | 2/23 [00:03<00:29,  1.41s/it, est. speed input: 454.77 toks/s, output: 13.95 toks/s]
[2025-01-11 00:48:11,000][root][ERROR] - Processed prompts:  13%|#3        | 3/23 [00:03<00:16,  1.21it/s, est. speed input: 660.17 toks/s, output: 21.82 toks/s]
[2025-01-11 00:48:11,356][root][ERROR] - Processed prompts:  17%|#7        | 4/23 [00:03<00:12,  1.56it/s, est. speed input: 795.38 toks/s, output: 29.53 toks/s]
[2025-01-11 00:48:11,475][root][ERROR] - Processed prompts:  35%|###4      | 8/23 [00:03<00:03,  4.59it/s, est. speed input: 1542.36 toks/s, output: 67.74 toks/s]
[2025-01-11 00:48:11,580][root][ERROR] - Processed prompts:  52%|#####2    | 12/23 [00:04<00:01,  8.17it/s, est. speed input: 2268.62 toks/s, output: 107.55 toks/s]
[2025-01-11 00:48:11,783][root][ERROR] - Processed prompts:  65%|######5   | 15/23 [00:04<00:00,  9.66it/s, est. speed input: 2712.43 toks/s, output: 135.30 toks/s]
[2025-01-11 00:48:12,121][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:04<00:00,  9.39it/s, est. speed input: 3022.16 toks/s, output: 163.88 toks/s]
[2025-01-11 00:48:12,364][root][ERROR] - Processed prompts:  87%|########6 | 20/23 [00:04<00:00,  9.08it/s, est. speed input: 3197.60 toks/s, output: 184.96 toks/s]
[2025-01-11 00:48:12,475][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:04<00:00, 10.40it/s, est. speed input: 3458.17 toks/s, output: 212.14 toks/s]
[2025-01-11 00:48:12,860][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.34it/s, est. speed input: 3354.09 toks/s, output: 216.17 toks/s]
WARNING 01-11 00:48:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:13,081][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:14,671][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:12,  1.59s/it, est. speed input: 441.54 toks/s, output: 15.72 toks/s]
[2025-01-11 00:48:14,776][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:01<00:02,  2.22it/s, est. speed input: 1222.84 toks/s, output: 47.19 toks/s]
[2025-01-11 00:48:15,166][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.74it/s, est. speed input: 2381.56 toks/s, output: 106.97 toks/s]
[2025-01-11 00:48:15,751][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  4.21it/s, est. speed input: 2429.83 toks/s, output: 142.69 toks/s]
[2025-01-11 00:48:15,751][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.37it/s, est. speed input: 2429.83 toks/s, output: 142.69 toks/s]
WARNING 01-11 00:48:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:15,970][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:17,577][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:12,  1.61s/it, est. speed input: 528.32 toks/s, output: 14.93 toks/s]
[2025-01-11 00:48:17,838][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:01<00:03,  1.95it/s, est. speed input: 1251.43 toks/s, output: 44.43 toks/s]
[2025-01-11 00:48:17,960][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:01<00:01,  2.65it/s, est. speed input: 1543.98 toks/s, output: 61.32 toks/s]
[2025-01-11 00:48:18,073][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  3.44it/s, est. speed input: 1821.87 toks/s, output: 78.92 toks/s]
[2025-01-11 00:48:18,291][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:02<00:00,  3.75it/s, est. speed input: 1983.93 toks/s, output: 94.80 toks/s]
[2025-01-11 00:48:19,220][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  3.43it/s, est. speed input: 2130.72 toks/s, output: 136.00 toks/s]
[2025-01-11 00:48:19,220][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.77it/s, est. speed input: 2130.72 toks/s, output: 136.00 toks/s]
WARNING 01-11 00:48:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:19,481][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:22,926][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:15,  3.44s/it, est. speed input: 220.06 toks/s, output: 8.42 toks/s]
[2025-01-11 00:48:23,231][root][ERROR] - Processed prompts:  22%|##1       | 5/23 [00:03<00:10,  1.73it/s, est. speed input: 1003.08 toks/s, output: 40.54 toks/s]
[2025-01-11 00:48:23,357][root][ERROR] - Processed prompts:  26%|##6       | 6/23 [00:03<00:08,  2.12it/s, est. speed input: 1163.46 toks/s, output: 49.28 toks/s]
[2025-01-11 00:48:23,472][root][ERROR] - Processed prompts:  35%|###4      | 8/23 [00:03<00:04,  3.26it/s, est. speed input: 1503.66 toks/s, output: 68.17 toks/s]
[2025-01-11 00:48:23,975][root][ERROR] - Processed prompts:  43%|####3     | 10/23 [00:04<00:03,  3.49it/s, est. speed input: 1671.67 toks/s, output: 83.45 toks/s]
[2025-01-11 00:48:24,077][root][ERROR] - Processed prompts:  48%|####7     | 11/23 [00:04<00:02,  4.02it/s, est. speed input: 1799.01 toks/s, output: 94.43 toks/s]
[2025-01-11 00:48:24,252][root][ERROR] - Processed prompts:  65%|######5   | 15/23 [00:04<00:01,  7.26it/s, est. speed input: 2393.54 toks/s, output: 142.32 toks/s]
[2025-01-11 00:48:24,414][root][ERROR] - Processed prompts:  74%|#######3  | 17/23 [00:04<00:00,  8.20it/s, est. speed input: 2632.89 toks/s, output: 166.25 toks/s]
[2025-01-11 00:48:24,825][root][ERROR] - Processed prompts:  83%|########2 | 19/23 [00:05<00:00,  6.86it/s, est. speed input: 2735.60 toks/s, output: 186.02 toks/s]
[2025-01-11 00:48:25,073][root][ERROR] - Processed prompts:  91%|#########1| 21/23 [00:05<00:00,  7.17it/s, est. speed input: 2903.89 toks/s, output: 212.12 toks/s]
[2025-01-11 00:48:25,262][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  7.91it/s, est. speed input: 3064.18 toks/s, output: 242.19 toks/s]
[2025-01-11 00:48:25,262][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  3.98it/s, est. speed input: 3064.18 toks/s, output: 242.19 toks/s]
WARNING 01-11 00:48:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:25,525][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:29,357][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:24,  3.83s/it, est. speed input: 220.53 toks/s, output: 7.57 toks/s]
[2025-01-11 00:48:29,468][root][ERROR] - Processed prompts:  39%|###9      | 9/23 [00:03<00:04,  3.11it/s, est. speed input: 2225.75 toks/s, output: 66.96 toks/s]
[2025-01-11 00:48:29,963][root][ERROR] - Processed prompts:  61%|######    | 14/23 [00:04<00:01,  4.53it/s, est. speed input: 2947.50 toks/s, output: 105.91 toks/s]
[2025-01-11 00:48:30,134][root][ERROR] - Processed prompts:  74%|#######3  | 17/23 [00:04<00:01,  5.71it/s, est. speed input: 3418.12 toks/s, output: 136.28 toks/s]
[2025-01-11 00:48:30,578][root][ERROR] - Processed prompts:  87%|########6 | 20/23 [00:05<00:00,  5.97it/s, est. speed input: 3642.36 toks/s, output: 164.29 toks/s]
[2025-01-11 00:48:31,043][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:05<00:00,  5.51it/s, est. speed input: 3685.13 toks/s, output: 181.60 toks/s]
[2025-01-11 00:48:31,498][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  3.85it/s, est. speed input: 3562.52 toks/s, output: 188.52 toks/s]
WARNING 01-11 00:48:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:31,720][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:33,506][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.79s/it, est. speed input: 488.99 toks/s, output: 16.24 toks/s]
[2025-01-11 00:48:34,135][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:02<00:04,  1.44it/s, est. speed input: 1045.44 toks/s, output: 46.79 toks/s]
[2025-01-11 00:48:34,258][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:02<00:02,  2.01it/s, est. speed input: 1332.48 toks/s, output: 67.39 toks/s]
[2025-01-11 00:48:34,485][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.07it/s, est. speed input: 2093.83 toks/s, output: 130.21 toks/s]
[2025-01-11 00:48:34,792][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  4.71it/s, est. speed input: 2490.01 toks/s, output: 168.65 toks/s]
[2025-01-11 00:48:34,792][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.93it/s, est. speed input: 2490.01 toks/s, output: 168.65 toks/s]
WARNING 01-11 00:48:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:35,057][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:36,817][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.76s/it, est. speed input: 562.63 toks/s, output: 16.48 toks/s]
[2025-01-11 00:48:37,024][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  2.57it/s, est. speed input: 1836.04 toks/s, output: 63.57 toks/s]
[2025-01-11 00:48:37,353][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.69it/s, est. speed input: 2051.24 toks/s, output: 77.54 toks/s]
[2025-01-11 00:48:37,523][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.08it/s, est. speed input: 2674.50 toks/s, output: 119.65 toks/s]
[2025-01-11 00:48:37,675][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.49it/s, est. speed input: 2912.80 toks/s, output: 139.81 toks/s]
[2025-01-11 00:48:37,675][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.06it/s, est. speed input: 2912.80 toks/s, output: 139.81 toks/s]
WARNING 01-11 00:48:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:37,910][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:40,815][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:49,  2.90s/it, est. speed input: 262.36 toks/s, output: 8.26 toks/s]
[2025-01-11 00:48:41,021][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:03<00:21,  1.32s/it, est. speed input: 555.96 toks/s, output: 17.04 toks/s]
[2025-01-11 00:48:41,124][root][ERROR] - Processed prompts:  33%|###3      | 6/18 [00:03<00:03,  3.11it/s, est. speed input: 1733.89 toks/s, output: 53.53 toks/s]
[2025-01-11 00:48:41,707][root][ERROR] - Processed prompts:  44%|####4     | 8/18 [00:03<00:03,  3.21it/s, est. speed input: 1920.41 toks/s, output: 69.55 toks/s]
[2025-01-11 00:48:41,939][root][ERROR] - Processed prompts:  56%|#####5    | 10/18 [00:04<00:01,  4.07it/s, est. speed input: 2283.46 toks/s, output: 93.09 toks/s]
[2025-01-11 00:48:42,048][root][ERROR] - Processed prompts:  67%|######6   | 12/18 [00:04<00:01,  5.45it/s, est. speed input: 2638.01 toks/s, output: 120.61 toks/s]
[2025-01-11 00:48:42,244][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:04<00:00,  6.40it/s, est. speed input: 2939.19 toks/s, output: 147.47 toks/s]
[2025-01-11 00:48:42,505][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:04<00:00,  6.74it/s, est. speed input: 3159.63 toks/s, output: 174.55 toks/s]
[2025-01-11 00:48:44,510][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:06<00:00,  2.43it/s, est. speed input: 2497.57 toks/s, output: 165.93 toks/s]
[2025-01-11 00:48:44,510][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:06<00:00,  2.73it/s, est. speed input: 2497.57 toks/s, output: 165.93 toks/s]
WARNING 01-11 00:48:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:44,788][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:46,737][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.95s/it, est. speed input: 597.41 toks/s, output: 14.88 toks/s]
[2025-01-11 00:48:46,918][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.34it/s, est. speed input: 3317.72 toks/s, output: 99.55 toks/s]
[2025-01-11 00:48:47,225][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  4.82it/s, est. speed input: 3648.95 toks/s, output: 130.51 toks/s]
[2025-01-11 00:48:47,225][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.69it/s, est. speed input: 3648.95 toks/s, output: 130.51 toks/s]
WARNING 01-11 00:48:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:47,466][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:49,997][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:30,  2.53s/it, est. speed input: 506.67 toks/s, output: 11.46 toks/s]
[2025-01-11 00:48:50,193][root][ERROR] - Processed prompts:  46%|####6     | 6/13 [00:02<00:02,  2.90it/s, est. speed input: 2237.47 toks/s, output: 67.49 toks/s]
[2025-01-11 00:48:50,540][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:03<00:00,  4.05it/s, est. speed input: 2875.91 toks/s, output: 100.53 toks/s]
[2025-01-11 00:48:50,668][root][ERROR] - Processed prompts:  85%|########4 | 11/13 [00:03<00:00,  5.16it/s, est. speed input: 3357.48 toks/s, output: 131.18 toks/s]
[2025-01-11 00:48:52,238][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:04<00:00,  2.73it/s, est. speed input: 2717.49 toks/s, output: 140.00 toks/s]
[2025-01-11 00:48:52,238][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:04<00:00,  2.72it/s, est. speed input: 2717.49 toks/s, output: 140.00 toks/s]
WARNING 01-11 00:48:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:52,503][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:53,896][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.39s/it, est. speed input: 656.06 toks/s, output: 15.79 toks/s]
[2025-01-11 00:48:54,058][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.50it/s, est. speed input: 1322.98 toks/s, output: 32.80 toks/s]
[2025-01-11 00:48:54,463][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.49it/s, est. speed input: 2701.72 toks/s, output: 81.62 toks/s]
[2025-01-11 00:48:54,986][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.88it/s, est. speed input: 2572.88 toks/s, output: 97.45 toks/s]
[2025-01-11 00:48:54,987][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.42it/s, est. speed input: 2572.88 toks/s, output: 97.45 toks/s]
WARNING 01-11 00:48:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:55,212][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:56,965][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.75s/it, est. speed input: 525.42 toks/s, output: 12.55 toks/s]
[2025-01-11 00:48:57,153][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:01<00:03,  1.91it/s, est. speed input: 1514.19 toks/s, output: 39.17 toks/s]
[2025-01-11 00:48:57,466][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  5.29it/s, est. speed input: 3585.94 toks/s, output: 105.60 toks/s]
[2025-01-11 00:48:57,887][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.36it/s, est. speed input: 3422.39 toks/s, output: 115.52 toks/s]
WARNING 01-11 00:48:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:48:58,139][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:48:59,918][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.78s/it, est. speed input: 676.74 toks/s, output: 16.30 toks/s]
[2025-01-11 00:49:00,322][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:02<00:01,  2.24it/s, est. speed input: 2029.12 toks/s, output: 63.65 toks/s]
[2025-01-11 00:49:00,527][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.64it/s, est. speed input: 2225.93 toks/s, output: 82.08 toks/s]
[2025-01-11 00:49:00,648][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  4.21it/s, est. speed input: 3086.22 toks/s, output: 127.11 toks/s]
[2025-01-11 00:49:00,648][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.79it/s, est. speed input: 3086.22 toks/s, output: 127.11 toks/s]
WARNING 01-11 00:49:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:49:00,865][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:49:01,958][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.09s/it, est. speed input: 817.20 toks/s, output: 26.54 toks/s]
[2025-01-11 00:49:02,179][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.72it/s, est. speed input: 1575.39 toks/s, output: 53.27 toks/s]
[2025-01-11 00:49:02,348][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.55it/s, est. speed input: 2096.82 toks/s, output: 81.58 toks/s]
[2025-01-11 00:49:02,348][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.02it/s, est. speed input: 2096.82 toks/s, output: 81.58 toks/s]
WARNING 01-11 00:49:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:49:02,599][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:49:03,792][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.19s/it, est. speed input: 819.32 toks/s, output: 17.61 toks/s]
[2025-01-11 00:49:03,969][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.68it/s, est. speed input: 1483.95 toks/s, output: 36.51 toks/s]
[2025-01-11 00:49:04,339][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.96it/s, est. speed input: 2365.75 toks/s, output: 74.16 toks/s]
[2025-01-11 00:49:04,441][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.78it/s, est. speed input: 2767.45 toks/s, output: 99.94 toks/s]
[2025-01-11 00:49:04,441][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.72it/s, est. speed input: 2767.45 toks/s, output: 99.94 toks/s]
WARNING 01-11 00:49:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:49:04,691][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:49:05,624][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 1038.27 toks/s, output: 31.10 toks/s]
[2025-01-11 00:49:05,944][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.75it/s, est. speed input: 1584.67 toks/s, output: 61.47 toks/s]
[2025-01-11 00:49:05,944][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.60it/s, est. speed input: 1584.67 toks/s, output: 61.47 toks/s]
[2025-01-11 00:49:08,116][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:49:08,169][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:49:09,908][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.74s/it]
[2025-01-11 00:49:11,588][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 00:49:13,200][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 00:49:13,734][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 00:49:13,734][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 00:49:35,078][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:49:35,130][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:49:37,902][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:08,  2.77s/it]
[2025-01-11 00:49:39,736][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.22s/it]
[2025-01-11 00:49:41,914][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.20s/it]
[2025-01-11 00:49:42,582][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.60s/it]
[2025-01-11 00:49:42,582][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.86s/it]
WARNING 01-11 00:50:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:50:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:50:03 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:50:03 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:50:04,092][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:50:05,425][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 00:50:05,802][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 00:50:07,125][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 00:50:08,455][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 00:50:08,456][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 00:50:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:50:22 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:50:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:50:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:50:45 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:50:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:50:45,463][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:50:49,464][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:04,  4.00s/it, est. speed input: 158.74 toks/s, output: 3.25 toks/s]
[2025-01-11 00:50:49,585][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.72s/it, est. speed input: 308.13 toks/s, output: 6.79 toks/s]
[2025-01-11 00:50:49,761][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:29,  1.01s/it, est. speed input: 443.22 toks/s, output: 10.70 toks/s]
[2025-01-11 00:50:49,874][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.72it/s, est. speed input: 863.83 toks/s, output: 23.58 toks/s]
[2025-01-11 00:50:50,059][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  8.26it/s, est. speed input: 1934.35 toks/s, output: 60.05 toks/s]
[2025-01-11 00:50:50,170][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:00, 13.06it/s, est. speed input: 2698.14 toks/s, output: 90.93 toks/s]
[2025-01-11 00:50:50,381][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 14.37it/s, est. speed input: 3098.87 toks/s, output: 112.24 toks/s]
[2025-01-11 00:50:50,562][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 14.84it/s, est. speed input: 3362.44 toks/s, output: 131.01 toks/s]
[2025-01-11 00:50:50,734][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 15.43it/s, est. speed input: 3614.33 toks/s, output: 153.11 toks/s]
[2025-01-11 00:50:51,116][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.66it/s, est. speed input: 3594.89 toks/s, output: 167.01 toks/s]
WARNING 01-11 00:50:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:50:51,348][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:50:51,986][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 1116.45 toks/s, output: 29.79 toks/s]
[2025-01-11 00:50:51,986][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 1116.45 toks/s, output: 29.79 toks/s]
WARNING 01-11 00:50:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:50:52,234][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:50:55,333][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:32,  3.10s/it, est. speed input: 212.66 toks/s, output: 4.20 toks/s]
[2025-01-11 00:50:55,796][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:27,  1.03it/s, est. speed input: 550.31 toks/s, output: 13.48 toks/s]
[2025-01-11 00:50:55,964][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:18,  1.44it/s, est. speed input: 701.06 toks/s, output: 19.30 toks/s]
[2025-01-11 00:50:56,073][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:13,  1.99it/s, est. speed input: 851.58 toks/s, output: 25.53 toks/s]
[2025-01-11 00:50:56,175][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:05,  4.40it/s, est. speed input: 1330.88 toks/s, output: 45.42 toks/s]
[2025-01-11 00:50:56,311][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:04<00:02,  7.98it/s, est. speed input: 1942.24 toks/s, output: 72.37 toks/s]
[2025-01-11 00:50:56,434][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:04<00:01, 11.80it/s, est. speed input: 2508.76 toks/s, output: 101.66 toks/s]
[2025-01-11 00:50:56,784][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:01, 10.55it/s, est. speed input: 2753.29 toks/s, output: 120.23 toks/s]
[2025-01-11 00:50:56,942][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 10.98it/s, est. speed input: 2943.90 toks/s, output: 136.17 toks/s]
[2025-01-11 00:50:57,153][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:04<00:00, 13.19it/s, est. speed input: 3364.43 toks/s, output: 172.42 toks/s]
[2025-01-11 00:50:57,267][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:05<00:00, 15.49it/s, est. speed input: 3680.39 toks/s, output: 204.48 toks/s]
[2025-01-11 00:50:57,429][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00, 16.27it/s, est. speed input: 3952.86 toks/s, output: 236.96 toks/s]
[2025-01-11 00:50:57,429][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.97it/s, est. speed input: 3952.86 toks/s, output: 236.96 toks/s]
WARNING 01-11 00:50:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:50:57,669][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:00,414][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:02<00:54,  2.75s/it, est. speed input: 265.56 toks/s, output: 6.92 toks/s]
[2025-01-11 00:51:00,722][root][ERROR] - Processed prompts:  14%|#4        | 3/21 [00:03<00:14,  1.21it/s, est. speed input: 735.32 toks/s, output: 21.62 toks/s]
[2025-01-11 00:51:00,959][root][ERROR] - Processed prompts:  29%|##8       | 6/21 [00:03<00:05,  2.72it/s, est. speed input: 1383.86 toks/s, output: 46.81 toks/s]
[2025-01-11 00:51:01,208][root][ERROR] - Processed prompts:  38%|###8      | 8/21 [00:03<00:03,  3.60it/s, est. speed input: 1710.42 toks/s, output: 63.57 toks/s]
[2025-01-11 00:51:01,412][root][ERROR] - Processed prompts:  43%|####2     | 9/21 [00:03<00:03,  3.81it/s, est. speed input: 1824.88 toks/s, output: 72.13 toks/s]
[2025-01-11 00:51:01,851][root][ERROR] - Processed prompts:  52%|#####2    | 11/21 [00:04<00:02,  4.06it/s, est. speed input: 2005.68 toks/s, output: 89.44 toks/s]
[2025-01-11 00:51:01,980][root][ERROR] - Processed prompts:  76%|#######6  | 16/21 [00:04<00:00,  8.30it/s, est. speed input: 2844.93 toks/s, output: 157.48 toks/s]
[2025-01-11 00:51:02,094][root][ERROR] - Processed prompts:  86%|########5 | 18/21 [00:04<00:00,  9.54it/s, est. speed input: 3111.83 toks/s, output: 184.62 toks/s]
[2025-01-11 00:51:02,366][root][ERROR] - Processed prompts:  95%|#########5| 20/21 [00:04<00:00,  8.86it/s, est. speed input: 3269.28 toks/s, output: 208.21 toks/s]
[2025-01-11 00:51:02,450][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:04<00:00,  4.39it/s, est. speed input: 3381.45 toks/s, output: 222.95 toks/s]
WARNING 01-11 00:51:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:02,673][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:04,331][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:16,  1.66s/it, est. speed input: 424.72 toks/s, output: 12.07 toks/s]
[2025-01-11 00:51:04,537][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:01<00:07,  1.24it/s, est. speed input: 761.62 toks/s, output: 25.21 toks/s]
[2025-01-11 00:51:04,675][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:02<00:02,  2.91it/s, est. speed input: 1422.14 toks/s, output: 53.95 toks/s]
[2025-01-11 00:51:04,820][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  4.64it/s, est. speed input: 1975.49 toks/s, output: 82.89 toks/s]
[2025-01-11 00:51:05,019][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  5.93it/s, est. speed input: 2414.11 toks/s, output: 113.80 toks/s]
[2025-01-11 00:51:05,170][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  7.43it/s, est. speed input: 2887.85 toks/s, output: 148.58 toks/s]
[2025-01-11 00:51:05,204][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.35it/s, est. speed input: 3142.35 toks/s, output: 169.11 toks/s]
WARNING 01-11 00:51:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:05,432][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:07,834][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:31,  2.40s/it, est. speed input: 383.01 toks/s, output: 12.07 toks/s]
[2025-01-11 00:51:07,966][root][ERROR] - Processed prompts:  21%|##1       | 3/14 [00:02<00:07,  1.49it/s, est. speed input: 1051.67 toks/s, output: 35.91 toks/s]
[2025-01-11 00:51:08,090][root][ERROR] - Processed prompts:  36%|###5      | 5/14 [00:02<00:03,  2.79it/s, est. speed input: 1563.81 toks/s, output: 61.69 toks/s]
[2025-01-11 00:51:08,314][root][ERROR] - Processed prompts:  50%|#####     | 7/14 [00:02<00:01,  3.94it/s, est. speed input: 2013.43 toks/s, output: 87.10 toks/s]
[2025-01-11 00:51:08,417][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:02<00:00,  5.63it/s, est. speed input: 2468.31 toks/s, output: 116.58 toks/s]
[2025-01-11 00:51:08,668][root][ERROR] - Processed prompts:  86%|########5 | 12/14 [00:03<00:00,  7.37it/s, est. speed input: 3045.29 toks/s, output: 159.16 toks/s]
[2025-01-11 00:51:08,787][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  8.85it/s, est. speed input: 3403.22 toks/s, output: 192.23 toks/s]
[2025-01-11 00:51:08,788][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  4.17it/s, est. speed input: 3403.22 toks/s, output: 192.23 toks/s]
WARNING 01-11 00:51:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:09,023][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:11,596][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:43,  2.57s/it, est. speed input: 270.17 toks/s, output: 8.16 toks/s]
[2025-01-11 00:51:11,923][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:02<00:20,  1.25s/it, est. speed input: 569.07 toks/s, output: 17.24 toks/s]
[2025-01-11 00:51:12,064][root][ERROR] - Processed prompts:  39%|###8      | 7/18 [00:03<00:02,  3.79it/s, est. speed input: 1762.44 toks/s, output: 67.74 toks/s]
[2025-01-11 00:51:12,425][root][ERROR] - Processed prompts:  50%|#####     | 9/18 [00:03<00:02,  4.21it/s, est. speed input: 2016.76 toks/s, output: 84.36 toks/s]
[2025-01-11 00:51:12,611][root][ERROR] - Processed prompts:  67%|######6   | 12/18 [00:03<00:00,  6.01it/s, est. speed input: 2551.00 toks/s, output: 120.40 toks/s]
[2025-01-11 00:51:12,727][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:03<00:00,  7.33it/s, est. speed input: 2901.95 toks/s, output: 146.34 toks/s]
[2025-01-11 00:51:13,138][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:04<00:00,  6.41it/s, est. speed input: 3002.01 toks/s, output: 165.98 toks/s]
[2025-01-11 00:51:13,859][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  4.65it/s, est. speed input: 2887.61 toks/s, output: 185.71 toks/s]
[2025-01-11 00:51:13,859][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  3.72it/s, est. speed input: 2887.61 toks/s, output: 185.71 toks/s]
WARNING 01-11 00:51:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:14,162][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:17,080][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:49,  2.92s/it, est. speed input: 286.91 toks/s, output: 8.57 toks/s]
[2025-01-11 00:51:17,244][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:03<00:20,  1.30s/it, est. speed input: 537.69 toks/s, output: 17.52 toks/s]
[2025-01-11 00:51:17,747][root][ERROR] - Processed prompts:  44%|####4     | 8/18 [00:03<00:02,  3.51it/s, est. speed input: 2052.41 toks/s, output: 68.07 toks/s]
[2025-01-11 00:51:17,864][root][ERROR] - Processed prompts:  56%|#####5    | 10/18 [00:03<00:01,  4.54it/s, est. speed input: 2439.29 toks/s, output: 91.85 toks/s]
[2025-01-11 00:51:17,966][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:03<00:00,  7.52it/s, est. speed input: 3274.02 toks/s, output: 143.55 toks/s]
[2025-01-11 00:51:18,459][root][ERROR] - Processed prompts:  94%|#########4| 17/18 [00:04<00:00,  6.99it/s, est. speed input: 3534.94 toks/s, output: 174.34 toks/s]
[2025-01-11 00:51:18,476][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  4.17it/s, est. speed input: 3739.10 toks/s, output: 191.95 toks/s]
WARNING 01-11 00:51:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:18,703][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:20,725][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:20,  2.02s/it, est. speed input: 427.22 toks/s, output: 14.34 toks/s]
[2025-01-11 00:51:20,911][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.94it/s, est. speed input: 1716.71 toks/s, output: 69.75 toks/s]
[2025-01-11 00:51:21,226][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.68it/s, est. speed input: 2240.76 toks/s, output: 99.47 toks/s]
[2025-01-11 00:51:21,531][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.35it/s, est. speed input: 2571.28 toks/s, output: 129.75 toks/s]
[2025-01-11 00:51:21,697][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  4.61it/s, est. speed input: 2696.53 toks/s, output: 146.93 toks/s]
[2025-01-11 00:51:21,900][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  4.68it/s, est. speed input: 2794.10 toks/s, output: 164.23 toks/s]
[2025-01-11 00:51:21,900][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.44it/s, est. speed input: 2794.10 toks/s, output: 164.23 toks/s]
WARNING 01-11 00:51:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:22,149][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:24,087][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:19,  1.94s/it, est. speed input: 464.65 toks/s, output: 10.33 toks/s]
[2025-01-11 00:51:24,355][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:02<00:08,  1.05it/s, est. speed input: 878.73 toks/s, output: 22.22 toks/s]
[2025-01-11 00:51:24,506][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  4.03it/s, est. speed input: 2556.76 toks/s, output: 73.43 toks/s]
[2025-01-11 00:51:24,934][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.99it/s, est. speed input: 3169.32 toks/s, output: 108.83 toks/s]
[2025-01-11 00:51:25,471][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  4.52it/s, est. speed input: 3212.85 toks/s, output: 139.99 toks/s]
[2025-01-11 00:51:25,472][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.31it/s, est. speed input: 3212.85 toks/s, output: 139.99 toks/s]
WARNING 01-11 00:51:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:25,706][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:28,385][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:37,  2.68s/it, est. speed input: 385.25 toks/s, output: 10.83 toks/s]
[2025-01-11 00:51:28,549][root][ERROR] - Processed prompts:  73%|#######3  | 11/15 [00:02<00:00,  5.25it/s, est. speed input: 3555.99 toks/s, output: 115.37 toks/s]
[2025-01-11 00:51:29,883][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  4.16it/s, est. speed input: 3308.20 toks/s, output: 146.04 toks/s]
[2025-01-11 00:51:29,883][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:04<00:00,  3.59it/s, est. speed input: 3308.20 toks/s, output: 146.04 toks/s]
WARNING 01-11 00:51:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:30,156][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:32,152][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  2.00s/it, est. speed input: 631.87 toks/s, output: 14.03 toks/s]
[2025-01-11 00:51:32,333][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:02<00:00,  3.61it/s, est. speed input: 3153.85 toks/s, output: 82.71 toks/s]
[2025-01-11 00:51:32,702][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  4.06it/s, est. speed input: 3391.60 toks/s, output: 111.15 toks/s]
[2025-01-11 00:51:32,720][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.51it/s, est. speed input: 3741.28 toks/s, output: 132.25 toks/s]
WARNING 01-11 00:51:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:32,967][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:34,862][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.89s/it, est. speed input: 560.52 toks/s, output: 15.31 toks/s]
[2025-01-11 00:51:34,999][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  3.21it/s, est. speed input: 2371.79 toks/s, output: 74.32 toks/s]
[2025-01-11 00:51:35,505][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  3.45it/s, est. speed input: 2543.99 toks/s, output: 98.09 toks/s]
[2025-01-11 00:51:36,319][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  3.02it/s, est. speed input: 2545.40 toks/s, output: 124.99 toks/s]
[2025-01-11 00:51:36,319][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.68it/s, est. speed input: 2545.40 toks/s, output: 124.99 toks/s]
WARNING 01-11 00:51:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:36,591][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:38,397][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.81s/it, est. speed input: 690.66 toks/s, output: 16.06 toks/s]
[2025-01-11 00:51:38,645][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  3.11it/s, est. speed input: 2805.21 toks/s, output: 77.42 toks/s]
[2025-01-11 00:51:38,830][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.45it/s, est. speed input: 3173.96 toks/s, output: 93.82 toks/s]
[2025-01-11 00:51:38,864][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.08it/s, est. speed input: 3564.02 toks/s, output: 115.73 toks/s]
WARNING 01-11 00:51:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:39,082][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:40,657][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.58s/it, est. speed input: 688.78 toks/s, output: 17.77 toks/s]
[2025-01-11 00:51:40,944][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.66it/s, est. speed input: 2227.67 toks/s, output: 68.76 toks/s]
[2025-01-11 00:51:41,553][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.90it/s, est. speed input: 2532.27 toks/s, output: 100.76 toks/s]
[2025-01-11 00:51:41,553][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.43it/s, est. speed input: 2532.27 toks/s, output: 100.76 toks/s]
WARNING 01-11 00:51:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:41,799][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:42,958][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.16s/it, est. speed input: 1020.17 toks/s, output: 25.01 toks/s]
[2025-01-11 00:51:42,959][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.59it/s, est. speed input: 3026.79 toks/s, output: 75.00 toks/s]
WARNING 01-11 00:51:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:43,175][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:44,494][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.32s/it, est. speed input: 704.42 toks/s, output: 20.47 toks/s]
[2025-01-11 00:51:44,984][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:01,  1.91it/s, est. speed input: 1677.20 toks/s, output: 59.17 toks/s]
[2025-01-11 00:51:45,552][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.50it/s, est. speed input: 2117.29 toks/s, output: 101.40 toks/s]
[2025-01-11 00:51:45,552][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.10it/s, est. speed input: 2117.29 toks/s, output: 101.40 toks/s]
WARNING 01-11 00:51:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:45,987][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:47,211][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.22s/it, est. speed input: 944.86 toks/s, output: 23.70 toks/s]
[2025-01-11 00:51:48,042][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:02<00:00,  1.01it/s, est. speed input: 1065.44 toks/s, output: 49.18 toks/s]
[2025-01-11 00:51:48,167][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.68it/s, est. speed input: 1431.09 toks/s, output: 82.61 toks/s]
[2025-01-11 00:51:48,167][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.38it/s, est. speed input: 1431.09 toks/s, output: 82.61 toks/s]
WARNING 01-11 00:51:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:48,535][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:49,667][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.13s/it, est. speed input: 782.37 toks/s, output: 36.24 toks/s]
[2025-01-11 00:51:49,668][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.13s/it, est. speed input: 782.37 toks/s, output: 36.24 toks/s]
WARNING 01-11 00:51:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:51:50,046][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:51:51,124][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.08s/it, est. speed input: 1167.34 toks/s, output: 26.91 toks/s]
[2025-01-11 00:51:51,361][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.72it/s, est. speed input: 1823.38 toks/s, output: 54.79 toks/s]
[2025-01-11 00:51:51,361][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.52it/s, est. speed input: 1823.38 toks/s, output: 54.79 toks/s]
[2025-01-11 00:51:53,678][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:51:53,731][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:51:55,364][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 00:51:57,025][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 00:51:58,587][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 00:51:59,116][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 00:51:59,116][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 00:52:21,028][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:52:21,080][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:52:22,994][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 00:52:24,682][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 00:52:26,223][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 00:52:26,751][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 00:52:26,751][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 00:52:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:52:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:52:46 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:52:46 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:52:46,948][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:52:48,297][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 00:52:48,702][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 00:52:50,015][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 00:52:51,363][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 00:52:51,364][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 00:52:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:53:05 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:53:05 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:53:05 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:53:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:53:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:28,312][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:32,305][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.99s/it, est. speed input: 159.02 toks/s, output: 3.26 toks/s]
[2025-01-11 00:53:32,426][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.72s/it, est. speed input: 308.68 toks/s, output: 6.81 toks/s]
[2025-01-11 00:53:32,602][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:29,  1.01s/it, est. speed input: 444.00 toks/s, output: 10.72 toks/s]
[2025-01-11 00:53:32,713][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.73it/s, est. speed input: 865.62 toks/s, output: 23.40 toks/s]
[2025-01-11 00:53:32,897][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  8.29it/s, est. speed input: 1938.65 toks/s, output: 59.75 toks/s]
[2025-01-11 00:53:33,011][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 12.12it/s, est. speed input: 2567.35 toks/s, output: 85.33 toks/s]
[2025-01-11 00:53:33,197][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 14.01it/s, est. speed input: 2989.62 toks/s, output: 106.44 toks/s]
[2025-01-11 00:53:33,384][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:05<00:00, 16.85it/s, est. speed input: 3505.08 toks/s, output: 137.40 toks/s]
[2025-01-11 00:53:33,520][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 17.85it/s, est. speed input: 3779.88 toks/s, output: 159.37 toks/s]
[2025-01-11 00:53:33,836][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.79it/s, est. speed input: 3678.24 toks/s, output: 162.37 toks/s]
WARNING 01-11 00:53:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:34,044][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:35,025][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 704.03 toks/s, output: 42.85 toks/s]
[2025-01-11 00:53:35,025][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 704.03 toks/s, output: 42.85 toks/s]
WARNING 01-11 00:53:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:35,261][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:38,296][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:31,  3.03s/it, est. speed input: 217.81 toks/s, output: 3.95 toks/s]
[2025-01-11 00:53:38,473][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:39,  1.35s/it, est. speed input: 409.18 toks/s, output: 8.41 toks/s]
[2025-01-11 00:53:38,641][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.79it/s, est. speed input: 773.48 toks/s, output: 17.75 toks/s]
[2025-01-11 00:53:39,105][root][ERROR] - Processed prompts:  23%|##2       | 7/31 [00:03<00:07,  3.06it/s, est. speed input: 1190.97 toks/s, output: 32.26 toks/s]
[2025-01-11 00:53:39,206][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:06,  3.59it/s, est. speed input: 1330.20 toks/s, output: 38.79 toks/s]
[2025-01-11 00:53:39,349][root][ERROR] - Processed prompts:  32%|###2      | 10/31 [00:04<00:04,  5.01it/s, est. speed input: 1608.72 toks/s, output: 52.36 toks/s]
[2025-01-11 00:53:39,517][root][ERROR] - Processed prompts:  48%|####8     | 15/31 [00:04<00:01,  9.61it/s, est. speed input: 2338.48 toks/s, output: 89.77 toks/s]
[2025-01-11 00:53:39,628][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:04<00:01, 12.21it/s, est. speed input: 2731.99 toks/s, output: 113.60 toks/s]
[2025-01-11 00:53:39,880][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 12.10it/s, est. speed input: 3013.56 toks/s, output: 134.88 toks/s]
[2025-01-11 00:53:40,017][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:04<00:00, 15.57it/s, est. speed input: 3479.11 toks/s, output: 172.65 toks/s]
[2025-01-11 00:53:40,230][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:04<00:00, 15.11it/s, est. speed input: 3727.54 toks/s, output: 201.48 toks/s]
[2025-01-11 00:53:40,481][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:05<00:00, 12.65it/s, est. speed input: 3800.30 toks/s, output: 219.35 toks/s]
[2025-01-11 00:53:40,548][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.86it/s, est. speed input: 3879.54 toks/s, output: 231.32 toks/s]
WARNING 01-11 00:53:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:40,813][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:44,245][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:32,  3.43s/it, est. speed input: 216.20 toks/s, output: 4.95 toks/s]
[2025-01-11 00:53:44,354][root][ERROR] - Processed prompts:   7%|7         | 2/28 [00:03<00:38,  1.48s/it, est. speed input: 423.28 toks/s, output: 10.17 toks/s]
[2025-01-11 00:53:44,674][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:23,  1.05it/s, est. speed input: 583.47 toks/s, output: 15.80 toks/s]
[2025-01-11 00:53:44,881][root][ERROR] - Processed prompts:  14%|#4        | 4/28 [00:04<00:15,  1.52it/s, est. speed input: 780.63 toks/s, output: 22.12 toks/s]
[2025-01-11 00:53:45,007][root][ERROR] - Processed prompts:  46%|####6     | 13/28 [00:04<00:01,  8.36it/s, est. speed input: 2503.27 toks/s, output: 85.83 toks/s]
[2025-01-11 00:53:45,249][root][ERROR] - Processed prompts:  57%|#####7    | 16/28 [00:04<00:01,  9.21it/s, est. speed input: 2875.85 toks/s, output: 104.83 toks/s]
[2025-01-11 00:53:45,377][root][ERROR] - Processed prompts:  68%|######7   | 19/28 [00:04<00:00, 11.18it/s, est. speed input: 3295.03 toks/s, output: 129.50 toks/s]
[2025-01-11 00:53:45,541][root][ERROR] - Processed prompts:  79%|#######8  | 22/28 [00:04<00:00, 12.61it/s, est. speed input: 3664.97 toks/s, output: 155.03 toks/s]
[2025-01-11 00:53:45,863][root][ERROR] - Processed prompts:  89%|########9 | 25/28 [00:05<00:00, 11.42it/s, est. speed input: 3891.39 toks/s, output: 179.99 toks/s]
[2025-01-11 00:53:46,402][root][ERROR] - Processed prompts:  96%|#########6| 27/28 [00:05<00:00,  7.85it/s, est. speed input: 3788.84 toks/s, output: 193.05 toks/s]
[2025-01-11 00:53:46,537][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00,  4.89it/s, est. speed input: 3840.07 toks/s, output: 205.80 toks/s]
WARNING 01-11 00:53:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:46,765][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:48,303][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:04,  1.54s/it, est. speed input: 479.24 toks/s, output: 31.21 toks/s]
[2025-01-11 00:53:48,561][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.27it/s, est. speed input: 831.29 toks/s, output: 60.69 toks/s]
[2025-01-11 00:53:49,179][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  2.06it/s, est. speed input: 1222.56 toks/s, output: 111.44 toks/s]
[2025-01-11 00:53:49,179][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.66it/s, est. speed input: 1222.56 toks/s, output: 111.44 toks/s]
WARNING 01-11 00:53:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:49,422][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:50,552][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.13s/it, est. speed input: 746.04 toks/s, output: 19.47 toks/s]
[2025-01-11 00:53:50,705][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.80it/s, est. speed input: 1380.28 toks/s, output: 39.77 toks/s]
[2025-01-11 00:53:50,865][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.67it/s, est. speed input: 1839.15 toks/s, output: 61.00 toks/s]
[2025-01-11 00:53:51,215][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.74it/s, est. speed input: 1911.43 toks/s, output: 80.34 toks/s]
[2025-01-11 00:53:51,299][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.66it/s, est. speed input: 2253.90 toks/s, output: 109.26 toks/s]
WARNING 01-11 00:53:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:51,530][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:54,233][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:02<00:59,  2.70s/it, est. speed input: 267.87 toks/s, output: 5.18 toks/s]
[2025-01-11 00:53:54,662][root][ERROR] - Processed prompts:   9%|8         | 2/23 [00:03<00:28,  1.37s/it, est. speed input: 470.03 toks/s, output: 11.81 toks/s]
[2025-01-11 00:53:54,939][root][ERROR] - Processed prompts:  13%|#3        | 3/23 [00:03<00:17,  1.15it/s, est. speed input: 690.76 toks/s, output: 19.36 toks/s]
[2025-01-11 00:53:55,168][root][ERROR] - Processed prompts:  35%|###4      | 8/23 [00:03<00:03,  4.19it/s, est. speed input: 1720.66 toks/s, output: 59.65 toks/s]
[2025-01-11 00:53:55,277][root][ERROR] - Processed prompts:  43%|####3     | 10/23 [00:03<00:02,  5.47it/s, est. speed input: 2065.40 toks/s, output: 77.92 toks/s]
[2025-01-11 00:53:55,378][root][ERROR] - Processed prompts:  52%|#####2    | 12/23 [00:03<00:01,  7.01it/s, est. speed input: 2389.43 toks/s, output: 96.68 toks/s]
[2025-01-11 00:53:55,574][root][ERROR] - Processed prompts:  70%|######9   | 16/23 [00:04<00:00, 10.08it/s, est. speed input: 3009.91 toks/s, output: 135.51 toks/s]
[2025-01-11 00:53:55,801][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:04<00:00,  9.73it/s, est. speed input: 3204.68 toks/s, output: 154.05 toks/s]
[2025-01-11 00:53:56,186][root][ERROR] - Processed prompts:  87%|########6 | 20/23 [00:04<00:00,  7.92it/s, est. speed input: 3264.12 toks/s, output: 172.69 toks/s]
[2025-01-11 00:53:56,556][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:05<00:00,  7.03it/s, est. speed input: 3356.92 toks/s, output: 195.59 toks/s]
[2025-01-11 00:53:56,573][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.56it/s, est. speed input: 3501.90 toks/s, output: 213.56 toks/s]
WARNING 01-11 00:53:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:53:56,824][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:53:59,471][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:44,  2.65s/it, est. speed input: 315.09 toks/s, output: 7.18 toks/s]
[2025-01-11 00:53:59,636][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:02<00:18,  1.19s/it, est. speed input: 584.37 toks/s, output: 14.94 toks/s]
[2025-01-11 00:53:59,866][root][ERROR] - Processed prompts:  17%|#6        | 3/18 [00:03<00:11,  1.33it/s, est. speed input: 871.48 toks/s, output: 23.34 toks/s]
[2025-01-11 00:54:00,354][root][ERROR] - Processed prompts:  61%|######1   | 11/18 [00:03<00:01,  5.82it/s, est. speed input: 2814.70 toks/s, output: 91.50 toks/s]
[2025-01-11 00:54:00,520][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:03<00:00,  7.37it/s, est. speed input: 3380.52 toks/s, output: 128.52 toks/s]
[2025-01-11 00:54:00,766][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:03<00:00,  7.53it/s, est. speed input: 3600.97 toks/s, output: 151.70 toks/s]
[2025-01-11 00:54:00,906][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  8.52it/s, est. speed input: 3905.57 toks/s, output: 181.55 toks/s]
[2025-01-11 00:54:00,906][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  4.41it/s, est. speed input: 3905.57 toks/s, output: 181.55 toks/s]
WARNING 01-11 00:54:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:01,122][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:02,695][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.57s/it, est. speed input: 576.89 toks/s, output: 18.44 toks/s]
[2025-01-11 00:54:02,809][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.22it/s, est. speed input: 1424.67 toks/s, output: 54.57 toks/s]
[2025-01-11 00:54:03,563][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.42it/s, est. speed input: 1733.67 toks/s, output: 87.69 toks/s]
[2025-01-11 00:54:03,711][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  2.93it/s, est. speed input: 1989.16 toks/s, output: 112.83 toks/s]
[2025-01-11 00:54:05,377][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:04<00:00,  1.41it/s, est. speed input: 1430.62 toks/s, output: 110.23 toks/s]
[2025-01-11 00:54:05,378][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:04<00:00,  1.65it/s, est. speed input: 1430.62 toks/s, output: 110.23 toks/s]
WARNING 01-11 00:54:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:05,652][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:07,452][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.80s/it, est. speed input: 597.60 toks/s, output: 16.12 toks/s]
[2025-01-11 00:54:07,955][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.23it/s, est. speed input: 2546.35 toks/s, output: 86.44 toks/s]
[2025-01-11 00:54:08,746][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:03<00:00,  2.46it/s, est. speed input: 2213.75 toks/s, output: 95.69 toks/s]
[2025-01-11 00:54:09,588][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  1.99it/s, est. speed input: 2011.47 toks/s, output: 112.57 toks/s]
[2025-01-11 00:54:09,588][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.03it/s, est. speed input: 2011.47 toks/s, output: 112.57 toks/s]
WARNING 01-11 00:54:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:09,825][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:12,096][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.27s/it, est. speed input: 432.85 toks/s, output: 12.77 toks/s]
[2025-01-11 00:54:12,380][root][ERROR] - Processed prompts:  75%|#######5  | 9/12 [00:02<00:00,  4.64it/s, est. speed input: 3251.16 toks/s, output: 107.26 toks/s]
[2025-01-11 00:54:12,701][root][ERROR] - Processed prompts:  92%|#########1| 11/12 [00:02<00:00,  4.96it/s, est. speed input: 3500.53 toks/s, output: 135.63 toks/s]
[2025-01-11 00:54:12,802][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:02<00:00,  4.03it/s, est. speed input: 3661.55 toks/s, output: 152.54 toks/s]
WARNING 01-11 00:54:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:13,062][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:14,460][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.40s/it, est. speed input: 829.66 toks/s, output: 20.74 toks/s]
[2025-01-11 00:54:14,755][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.91it/s, est. speed input: 2496.21 toks/s, output: 77.97 toks/s]
[2025-01-11 00:54:15,327][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.45it/s, est. speed input: 2321.33 toks/s, output: 93.15 toks/s]
[2025-01-11 00:54:15,327][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.21it/s, est. speed input: 2321.33 toks/s, output: 93.15 toks/s]
WARNING 01-11 00:54:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:15,539][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:17,261][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.72s/it, est. speed input: 531.26 toks/s, output: 16.84 toks/s]
[2025-01-11 00:54:18,646][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:03<00:00,  2.20it/s, est. speed input: 1978.90 toks/s, output: 80.79 toks/s]
[2025-01-11 00:54:19,825][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:04<00:00,  1.67it/s, est. speed input: 1732.59 toks/s, output: 99.16 toks/s]
[2025-01-11 00:54:19,825][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:04<00:00,  1.63it/s, est. speed input: 1732.59 toks/s, output: 99.16 toks/s]
WARNING 01-11 00:54:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:20,068][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:21,184][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 1034.64 toks/s, output: 25.98 toks/s]
[2025-01-11 00:54:21,976][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.72it/s, est. speed input: 1779.40 toks/s, output: 70.23 toks/s]
[2025-01-11 00:54:21,976][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.57it/s, est. speed input: 1779.40 toks/s, output: 70.23 toks/s]
WARNING 01-11 00:54:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:22,184][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:23,327][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.14s/it, est. speed input: 900.25 toks/s, output: 24.50 toks/s]
[2025-01-11 00:54:23,805][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.11it/s, est. speed input: 2332.43 toks/s, output: 70.98 toks/s]
[2025-01-11 00:54:23,805][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.85it/s, est. speed input: 2332.43 toks/s, output: 70.98 toks/s]
WARNING 01-11 00:54:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:24,027][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:25,483][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.46s/it, est. speed input: 996.81 toks/s, output: 37.78 toks/s]
[2025-01-11 00:54:26,208][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.03s/it, est. speed input: 1171.81 toks/s, output: 70.17 toks/s]
[2025-01-11 00:54:26,208][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.09s/it, est. speed input: 1171.81 toks/s, output: 70.17 toks/s]
WARNING 01-11 00:54:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:26,442][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:27,524][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.08s/it, est. speed input: 1013.40 toks/s, output: 35.14 toks/s]
[2025-01-11 00:54:28,328][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.09it/s, est. speed input: 1081.53 toks/s, output: 65.74 toks/s]
[2025-01-11 00:54:28,329][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.06it/s, est. speed input: 1081.53 toks/s, output: 65.74 toks/s]
WARNING 01-11 00:54:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:28,556][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:29,435][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 1168.56 toks/s, output: 39.82 toks/s]
[2025-01-11 00:54:29,435][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 1168.56 toks/s, output: 39.82 toks/s]
WARNING 01-11 00:54:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:29,651][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:30,425][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1355.62 toks/s, output: 37.48 toks/s]
[2025-01-11 00:54:30,425][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1355.62 toks/s, output: 37.48 toks/s]
WARNING 01-11 00:54:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:54:30,639][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:54:31,458][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1748.71 toks/s, output: 35.44 toks/s]
[2025-01-11 00:54:31,458][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1748.71 toks/s, output: 35.44 toks/s]
[2025-01-11 00:54:33,685][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:54:33,738][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:54:35,332][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.59s/it]
[2025-01-11 00:54:36,980][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 00:54:38,620][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 00:54:39,172][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 00:54:39,172][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 00:54:59,952][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:55:00,003][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:55:02,084][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.08s/it]
[2025-01-11 00:55:03,775][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.85s/it]
[2025-01-11 00:55:05,421][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.76s/it]
[2025-01-11 00:55:05,941][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.27s/it]
[2025-01-11 00:55:05,942][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.48s/it]
WARNING 01-11 00:55:25 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:55:25 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:55:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:55:26 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:55:26,614][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:55:28,925][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.31s/it]
[2025-01-11 00:55:29,537][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.31s/it]
[2025-01-11 00:55:32,051][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.86s/it]
[2025-01-11 00:55:33,390][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.65s/it]
[2025-01-11 00:55:33,390][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.69s/it]
INFO 01-11 00:55:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:55:47 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:55:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:55:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:56:11 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 00:56:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:11,798][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:15,724][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:01,  3.93s/it, est. speed input: 161.76 toks/s, output: 3.31 toks/s]
[2025-01-11 00:56:15,845][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:50,  1.69s/it, est. speed input: 313.84 toks/s, output: 6.92 toks/s]
[2025-01-11 00:56:16,021][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:28,  1.00it/s, est. speed input: 451.09 toks/s, output: 10.89 toks/s]
[2025-01-11 00:56:16,132][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:09,  2.76it/s, est. speed input: 879.10 toks/s, output: 23.77 toks/s]
[2025-01-11 00:56:16,317][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  7.63it/s, est. speed input: 1826.64 toks/s, output: 55.76 toks/s]
[2025-01-11 00:56:16,442][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  9.56it/s, est. speed input: 2187.66 toks/s, output: 70.84 toks/s]
[2025-01-11 00:56:16,667][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 10.45it/s, est. speed input: 2478.06 toks/s, output: 87.29 toks/s]
[2025-01-11 00:56:16,791][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 15.08it/s, est. speed input: 3052.42 toks/s, output: 120.37 toks/s]
[2025-01-11 00:56:16,895][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:05<00:00, 17.22it/s, est. speed input: 3363.59 toks/s, output: 141.45 toks/s]
[2025-01-11 00:56:16,996][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 21.22it/s, est. speed input: 3786.88 toks/s, output: 172.17 toks/s]
[2025-01-11 00:56:17,113][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.02it/s, est. speed input: 3822.98 toks/s, output: 178.36 toks/s]
WARNING 01-11 00:56:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:17,348][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:17,882][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.87it/s, est. speed input: 1292.14 toks/s, output: 18.73 toks/s]
[2025-01-11 00:56:18,382][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.95it/s, est. speed input: 1349.29 toks/s, output: 48.36 toks/s]
[2025-01-11 00:56:18,382][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.93it/s, est. speed input: 1349.29 toks/s, output: 48.36 toks/s]
WARNING 01-11 00:56:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:18,625][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:21,989][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:37,  3.36s/it, est. speed input: 194.75 toks/s, output: 5.65 toks/s]
[2025-01-11 00:56:22,210][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:18,  1.44it/s, est. speed input: 733.76 toks/s, output: 22.88 toks/s]
[2025-01-11 00:56:22,358][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:06,  3.38it/s, est. speed input: 1403.71 toks/s, output: 47.69 toks/s]
[2025-01-11 00:56:22,492][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:03<00:03,  5.74it/s, est. speed input: 2029.52 toks/s, output: 74.48 toks/s]
[2025-01-11 00:56:22,648][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:04<00:02,  7.49it/s, est. speed input: 2450.35 toks/s, output: 94.47 toks/s]
[2025-01-11 00:56:22,853][root][ERROR] - Processed prompts:  63%|######3   | 19/30 [00:04<00:01,  9.84it/s, est. speed input: 2968.74 toks/s, output: 123.96 toks/s]
[2025-01-11 00:56:22,993][root][ERROR] - Processed prompts:  77%|#######6  | 23/30 [00:04<00:00, 12.84it/s, est. speed input: 3480.30 toks/s, output: 157.76 toks/s]
[2025-01-11 00:56:23,220][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 14.14it/s, est. speed input: 3891.92 toks/s, output: 192.20 toks/s]
[2025-01-11 00:56:23,849][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00,  9.29it/s, est. speed input: 3807.51 toks/s, output: 210.22 toks/s]
[2025-01-11 00:56:23,849][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00,  5.74it/s, est. speed input: 3807.51 toks/s, output: 210.22 toks/s]
WARNING 01-11 00:56:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:24,098][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:27,558][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:33,  3.46s/it, est. speed input: 215.57 toks/s, output: 5.20 toks/s]
[2025-01-11 00:56:27,668][root][ERROR] - Processed prompts:   7%|7         | 2/28 [00:03<00:38,  1.49s/it, est. speed input: 416.27 toks/s, output: 10.64 toks/s]
[2025-01-11 00:56:27,987][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:23,  1.05it/s, est. speed input: 572.64 toks/s, output: 16.46 toks/s]
[2025-01-11 00:56:28,142][root][ERROR] - Processed prompts:  14%|#4        | 4/28 [00:04<00:15,  1.56it/s, est. speed input: 742.55 toks/s, output: 23.00 toks/s]
[2025-01-11 00:56:28,285][root][ERROR] - Processed prompts:  25%|##5       | 7/28 [00:04<00:05,  3.74it/s, est. speed input: 1336.90 toks/s, output: 43.70 toks/s]
[2025-01-11 00:56:28,685][root][ERROR] - Processed prompts:  32%|###2      | 9/28 [00:04<00:04,  4.13it/s, est. speed input: 1546.37 toks/s, output: 55.80 toks/s]
[2025-01-11 00:56:28,812][root][ERROR] - Processed prompts:  43%|####2     | 12/28 [00:04<00:02,  6.53it/s, est. speed input: 2001.23 toks/s, output: 81.66 toks/s]
[2025-01-11 00:56:29,037][root][ERROR] - Processed prompts:  50%|#####     | 14/28 [00:04<00:01,  7.09it/s, est. speed input: 2227.19 toks/s, output: 97.59 toks/s]
[2025-01-11 00:56:29,140][root][ERROR] - Processed prompts:  61%|######    | 17/28 [00:05<00:01, 10.05it/s, est. speed input: 2628.07 toks/s, output: 126.53 toks/s]
[2025-01-11 00:56:29,326][root][ERROR] - Processed prompts:  68%|######7   | 19/28 [00:05<00:00, 10.23it/s, est. speed input: 2834.47 toks/s, output: 144.40 toks/s]
[2025-01-11 00:56:29,501][root][ERROR] - Processed prompts:  82%|########2 | 23/28 [00:05<00:00, 13.47it/s, est. speed input: 3302.83 toks/s, output: 185.25 toks/s]
[2025-01-11 00:56:29,925][root][ERROR] - Processed prompts:  93%|#########2| 26/28 [00:05<00:00, 10.47it/s, est. speed input: 3466.07 toks/s, output: 209.89 toks/s]
[2025-01-11 00:56:30,066][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00, 11.15it/s, est. speed input: 3642.92 toks/s, output: 236.09 toks/s]
[2025-01-11 00:56:30,066][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00,  4.69it/s, est. speed input: 3642.92 toks/s, output: 236.09 toks/s]
WARNING 01-11 00:56:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:30,280][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:31,206][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.08it/s, est. speed input: 696.52 toks/s, output: 22.68 toks/s]
[2025-01-11 00:56:31,325][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:00,  2.22it/s, est. speed input: 1299.53 toks/s, output: 45.93 toks/s]
[2025-01-11 00:56:31,545][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  4.21it/s, est. speed input: 2139.97 toks/s, output: 92.46 toks/s]
[2025-01-11 00:56:31,546][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.16it/s, est. speed input: 2139.97 toks/s, output: 92.46 toks/s]
WARNING 01-11 00:56:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:31,801][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:33,382][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:15,  1.58s/it, est. speed input: 474.56 toks/s, output: 8.86 toks/s]
[2025-01-11 00:56:33,797][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:01<00:08,  1.12it/s, est. speed input: 791.82 toks/s, output: 21.05 toks/s]
[2025-01-11 00:56:33,929][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:01,  3.50it/s, est. speed input: 1884.41 toks/s, output: 62.03 toks/s]
[2025-01-11 00:56:34,319][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:00,  4.02it/s, est. speed input: 2286.27 toks/s, output: 91.34 toks/s]
[2025-01-11 00:56:34,640][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.62it/s, est. speed input: 2627.39 toks/s, output: 121.17 toks/s]
[2025-01-11 00:56:35,027][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:03<00:00,  3.97it/s, est. speed input: 2594.83 toks/s, output: 133.32 toks/s]
[2025-01-11 00:56:35,111][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.32it/s, est. speed input: 2796.21 toks/s, output: 157.43 toks/s]
WARNING 01-11 00:56:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:35,357][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:37,607][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:02<00:40,  2.25s/it, est. speed input: 331.56 toks/s, output: 5.33 toks/s]
[2025-01-11 00:56:38,320][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:02<00:22,  1.35s/it, est. speed input: 562.02 toks/s, output: 13.84 toks/s]
[2025-01-11 00:56:38,543][root][ERROR] - Processed prompts:  47%|####7     | 9/19 [00:03<00:02,  4.49it/s, est. speed input: 2278.92 toks/s, output: 79.43 toks/s]
[2025-01-11 00:56:38,660][root][ERROR] - Processed prompts:  58%|#####7    | 11/19 [00:03<00:01,  5.55it/s, est. speed input: 2649.08 toks/s, output: 100.52 toks/s]
[2025-01-11 00:56:38,763][root][ERROR] - Processed prompts:  74%|#######3  | 14/19 [00:03<00:00,  7.80it/s, est. speed input: 3215.85 toks/s, output: 134.78 toks/s]
[2025-01-11 00:56:38,993][root][ERROR] - Processed prompts:  89%|########9 | 17/19 [00:03<00:00,  9.03it/s, est. speed input: 3645.77 toks/s, output: 167.25 toks/s]
[2025-01-11 00:56:39,388][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:04<00:00,  7.58it/s, est. speed input: 3676.46 toks/s, output: 185.35 toks/s]
[2025-01-11 00:56:39,388][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:04<00:00,  4.71it/s, est. speed input: 3676.46 toks/s, output: 185.35 toks/s]
WARNING 01-11 00:56:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:39,651][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:42,083][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:31,  2.43s/it, est. speed input: 337.31 toks/s, output: 10.28 toks/s]
[2025-01-11 00:56:42,220][root][ERROR] - Processed prompts:  14%|#4        | 2/14 [00:02<00:12,  1.08s/it, est. speed input: 725.84 toks/s, output: 21.03 toks/s]
[2025-01-11 00:56:42,441][root][ERROR] - Processed prompts:  64%|######4   | 9/14 [00:02<00:00,  5.43it/s, est. speed input: 3039.64 toks/s, output: 95.71 toks/s]
[2025-01-11 00:56:42,758][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:03<00:00,  5.63it/s, est. speed input: 3299.80 toks/s, output: 118.48 toks/s]
[2025-01-11 00:56:42,991][root][ERROR] - Processed prompts:  93%|#########2| 13/14 [00:03<00:00,  6.21it/s, est. speed input: 3611.53 toks/s, output: 147.05 toks/s]
[2025-01-11 00:56:43,293][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:03<00:00,  3.84it/s, est. speed input: 3559.64 toks/s, output: 157.36 toks/s]
WARNING 01-11 00:56:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:43,522][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:45,700][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:26,  2.18s/it, est. speed input: 326.47 toks/s, output: 12.86 toks/s]
[2025-01-11 00:56:45,818][root][ERROR] - Processed prompts:  46%|####6     | 6/13 [00:02<00:02,  3.47it/s, est. speed input: 2145.98 toks/s, output: 77.54 toks/s]
[2025-01-11 00:56:45,992][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:02<00:00,  5.28it/s, est. speed input: 2909.79 toks/s, output: 117.43 toks/s]
[2025-01-11 00:56:46,565][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:03<00:00,  5.26it/s, est. speed input: 3116.11 toks/s, output: 151.84 toks/s]
[2025-01-11 00:56:46,582][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  4.25it/s, est. speed input: 3344.00 toks/s, output: 173.54 toks/s]
WARNING 01-11 00:56:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:46,858][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:49,258][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:31,  2.40s/it, est. speed input: 387.51 toks/s, output: 9.58 toks/s]
[2025-01-11 00:56:49,464][root][ERROR] - Processed prompts:  14%|#4        | 2/14 [00:02<00:13,  1.11s/it, est. speed input: 729.25 toks/s, output: 19.96 toks/s]
[2025-01-11 00:56:49,572][root][ERROR] - Processed prompts:  57%|#####7    | 8/14 [00:02<00:01,  5.04it/s, est. speed input: 2845.40 toks/s, output: 85.87 toks/s]
[2025-01-11 00:56:49,832][root][ERROR] - Processed prompts:  79%|#######8  | 11/14 [00:02<00:00,  6.38it/s, est. speed input: 3484.81 toks/s, output: 120.41 toks/s]
[2025-01-11 00:56:50,889][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  4.45it/s, est. speed input: 3312.63 toks/s, output: 143.40 toks/s]
[2025-01-11 00:56:50,889][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:04<00:00,  3.47it/s, est. speed input: 3312.63 toks/s, output: 143.40 toks/s]
WARNING 01-11 00:56:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:51,115][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:52,517][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.40s/it, est. speed input: 690.43 toks/s, output: 20.68 toks/s]
[2025-01-11 00:56:52,843][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.07it/s, est. speed input: 1427.14 toks/s, output: 59.03 toks/s]
[2025-01-11 00:56:53,215][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  3.02it/s, est. speed input: 1972.09 toks/s, output: 100.96 toks/s]
[2025-01-11 00:56:53,316][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.72it/s, est. speed input: 2304.97 toks/s, output: 128.13 toks/s]
[2025-01-11 00:56:53,316][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.73it/s, est. speed input: 2304.97 toks/s, output: 128.13 toks/s]
WARNING 01-11 00:56:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:53,560][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:54,962][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.40s/it, est. speed input: 752.81 toks/s, output: 20.69 toks/s]
[2025-01-11 00:56:55,138][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.47it/s, est. speed input: 1259.67 toks/s, output: 41.82 toks/s]
[2025-01-11 00:56:55,382][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.08it/s, est. speed input: 1640.69 toks/s, output: 63.12 toks/s]
[2025-01-11 00:56:55,493][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.98it/s, est. speed input: 2077.45 toks/s, output: 87.96 toks/s]
[2025-01-11 00:56:55,544][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.52it/s, est. speed input: 2608.74 toks/s, output: 114.93 toks/s]
WARNING 01-11 00:56:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:55,793][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:56:57,997][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.20s/it, est. speed input: 415.69 toks/s, output: 12.25 toks/s]
[2025-01-11 00:56:58,107][root][ERROR] - Processed prompts:  67%|######6   | 8/12 [00:02<00:00,  4.66it/s, est. speed input: 3279.39 toks/s, output: 100.29 toks/s]
[2025-01-11 00:56:59,064][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  4.44it/s, est. speed input: 3332.79 toks/s, output: 143.41 toks/s]
[2025-01-11 00:56:59,064][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.67it/s, est. speed input: 3332.79 toks/s, output: 143.41 toks/s]
WARNING 01-11 00:56:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:56:59,342][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:01,113][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.77s/it, est. speed input: 612.39 toks/s, output: 15.82 toks/s]
[2025-01-11 00:57:01,273][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.79it/s, est. speed input: 4147.40 toks/s, output: 108.79 toks/s]
[2025-01-11 00:57:01,273][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  3.63it/s, est. speed input: 4147.40 toks/s, output: 108.79 toks/s]
WARNING 01-11 00:57:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:01,491][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:02,926][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.43s/it, est. speed input: 642.72 toks/s, output: 20.22 toks/s]
[2025-01-11 00:57:03,027][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  3.35it/s, est. speed input: 2272.16 toks/s, output: 78.82 toks/s]
[2025-01-11 00:57:03,345][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.20it/s, est. speed input: 2814.38 toks/s, output: 117.06 toks/s]
[2025-01-11 00:57:03,345][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.24it/s, est. speed input: 2814.38 toks/s, output: 117.06 toks/s]
WARNING 01-11 00:57:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:03,595][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:04,698][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.10s/it, est. speed input: 1069.61 toks/s, output: 13.60 toks/s]
[2025-01-11 00:57:05,007][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.57it/s, est. speed input: 1596.19 toks/s, output: 31.16 toks/s]
[2025-01-11 00:57:05,176][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.53it/s, est. speed input: 3447.61 toks/s, output: 89.21 toks/s]
[2025-01-11 00:57:05,176][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.16it/s, est. speed input: 3447.61 toks/s, output: 89.21 toks/s]
WARNING 01-11 00:57:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:05,388][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:06,434][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.05s/it, est. speed input: 792.03 toks/s, output: 27.71 toks/s]
[2025-01-11 00:57:06,674][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.75it/s, est. speed input: 1374.49 toks/s, output: 55.20 toks/s]
[2025-01-11 00:57:06,708][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.27it/s, est. speed input: 1988.95 toks/s, output: 87.10 toks/s]
WARNING 01-11 00:57:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:06,938][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:08,247][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.31s/it, est. speed input: 986.06 toks/s, output: 21.39 toks/s]
[2025-01-11 00:57:08,673][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.01it/s, est. speed input: 2020.97 toks/s, output: 62.24 toks/s]
[2025-01-11 00:57:08,859][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.57it/s, est. speed input: 2475.98 toks/s, output: 88.50 toks/s]
[2025-01-11 00:57:08,859][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.08it/s, est. speed input: 2475.98 toks/s, output: 88.50 toks/s]
WARNING 01-11 00:57:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:09,072][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:10,067][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.01it/s, est. speed input: 1043.79 toks/s, output: 29.16 toks/s]
[2025-01-11 00:57:10,640][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.34it/s, est. speed input: 1312.98 toks/s, output: 58.69 toks/s]
[2025-01-11 00:57:10,640][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.28it/s, est. speed input: 1312.98 toks/s, output: 58.69 toks/s]
WARNING 01-11 00:57:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:10,885][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:12,073][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.19s/it, est. speed input: 1155.38 toks/s, output: 33.66 toks/s]
[2025-01-11 00:57:12,225][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.73it/s, est. speed input: 1869.35 toks/s, output: 66.39 toks/s]
[2025-01-11 00:57:12,226][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.49it/s, est. speed input: 1869.35 toks/s, output: 66.39 toks/s]
WARNING 01-11 00:57:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:57:12,437][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:57:13,238][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1500.25 toks/s, output: 34.98 toks/s]
[2025-01-11 00:57:13,238][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1500.25 toks/s, output: 34.98 toks/s]
[2025-01-11 00:57:15,544][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:57:15,597][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:57:17,289][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 00:57:19,048][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 00:57:20,660][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 00:57:21,197][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 00:57:21,198][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 00:57:43,163][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:57:43,214][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:57:45,159][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 00:57:46,809][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 00:57:48,414][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 00:57:48,949][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 00:57:48,949][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 00:58:08 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 00:58:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 00:58:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 00:58:08 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 00:58:09,093][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:58:11,000][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 00:58:11,456][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.05s/it]
[2025-01-11 00:58:12,834][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.20s/it]
[2025-01-11 00:58:14,165][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 00:58:14,165][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.27s/it]
INFO 01-11 00:58:14 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 00:58:28 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 00:58:28 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 00:58:28 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 00:58:50 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 00:58:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:58:51,038][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:58:54,645][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:51,  3.61s/it, est. speed input: 176.08 toks/s, output: 3.60 toks/s]
[2025-01-11 00:58:54,766][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:46,  1.56s/it, est. speed input: 340.68 toks/s, output: 7.51 toks/s]
[2025-01-11 00:58:54,943][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:26,  1.08it/s, est. speed input: 487.92 toks/s, output: 11.78 toks/s]
[2025-01-11 00:58:55,057][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:11,  2.30it/s, est. speed input: 790.07 toks/s, output: 21.15 toks/s]
[2025-01-11 00:58:55,203][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:02,  7.76it/s, est. speed input: 1829.60 toks/s, output: 55.94 toks/s]
[2025-01-11 00:58:55,327][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 12.92it/s, est. speed input: 2665.19 toks/s, output: 89.07 toks/s]
[2025-01-11 00:58:55,533][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 14.42it/s, est. speed input: 3108.18 toks/s, output: 112.36 toks/s]
[2025-01-11 00:58:55,641][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 19.13it/s, est. speed input: 3725.27 toks/s, output: 146.88 toks/s]
[2025-01-11 00:58:55,742][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 22.50it/s, est. speed input: 4185.08 toks/s, output: 176.46 toks/s]
[2025-01-11 00:58:55,859][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.64it/s, est. speed input: 4215.34 toks/s, output: 182.14 toks/s]
WARNING 01-11 00:58:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:58:56,123][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:58:59,549][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:46,  3.43s/it, est. speed input: 191.51 toks/s, output: 4.96 toks/s]
[2025-01-11 00:58:59,664][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:11,  2.27it/s, est. speed input: 1113.60 toks/s, output: 30.50 toks/s]
[2025-01-11 00:58:59,968][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:06,  3.39it/s, est. speed input: 1538.18 toks/s, output: 45.00 toks/s]
[2025-01-11 00:59:00,206][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:05,  4.10it/s, est. speed input: 1775.06 toks/s, output: 55.85 toks/s]
[2025-01-11 00:59:00,412][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.10it/s, est. speed input: 2458.85 toks/s, output: 89.77 toks/s]
[2025-01-11 00:59:00,616][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.13it/s, est. speed input: 3085.98 toks/s, output: 127.11 toks/s]
[2025-01-11 00:59:00,856][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 10.67it/s, est. speed input: 3345.90 toks/s, output: 150.46 toks/s]
[2025-01-11 00:59:01,014][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.02it/s, est. speed input: 3509.07 toks/s, output: 167.48 toks/s]
[2025-01-11 00:59:01,336][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:05<00:00,  9.39it/s, est. speed input: 3551.43 toks/s, output: 181.69 toks/s]
[2025-01-11 00:59:01,609][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00,  8.79it/s, est. speed input: 3615.32 toks/s, output: 201.80 toks/s]
[2025-01-11 00:59:02,151][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  6.48it/s, est. speed input: 3508.84 toks/s, output: 218.48 toks/s]
[2025-01-11 00:59:02,152][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:06<00:00,  5.31it/s, est. speed input: 3508.84 toks/s, output: 218.48 toks/s]
WARNING 01-11 00:59:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:02,403][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:05,842][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:36,  3.44s/it, est. speed input: 216.10 toks/s, output: 4.65 toks/s]
[2025-01-11 00:59:06,171][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:26,  1.01s/it, est. speed input: 594.07 toks/s, output: 14.60 toks/s]
[2025-01-11 00:59:06,490][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:04<00:19,  1.29it/s, est. speed input: 731.95 toks/s, output: 20.31 toks/s]
[2025-01-11 00:59:06,637][root][ERROR] - Processed prompts:  28%|##7       | 8/29 [00:04<00:05,  3.52it/s, est. speed input: 1432.69 toks/s, output: 47.47 toks/s]
[2025-01-11 00:59:06,775][root][ERROR] - Processed prompts:  34%|###4      | 10/29 [00:04<00:04,  4.65it/s, est. speed input: 1741.93 toks/s, output: 61.30 toks/s]
[2025-01-11 00:59:07,162][root][ERROR] - Processed prompts:  48%|####8     | 14/29 [00:04<00:02,  6.32it/s, est. speed input: 2245.26 toks/s, output: 87.63 toks/s]
[2025-01-11 00:59:07,267][root][ERROR] - Processed prompts:  62%|######2   | 18/29 [00:04<00:01,  9.56it/s, est. speed input: 2821.75 toks/s, output: 123.37 toks/s]
[2025-01-11 00:59:07,417][root][ERROR] - Processed prompts:  72%|#######2  | 21/29 [00:05<00:00, 11.36it/s, est. speed input: 3191.52 toks/s, output: 149.80 toks/s]
[2025-01-11 00:59:07,905][root][ERROR] - Processed prompts:  83%|########2 | 24/29 [00:05<00:00,  9.04it/s, est. speed input: 3326.13 toks/s, output: 171.96 toks/s]
[2025-01-11 00:59:08,149][root][ERROR] - Processed prompts:  90%|########9 | 26/29 [00:05<00:00,  8.84it/s, est. speed input: 3445.14 toks/s, output: 192.16 toks/s]
[2025-01-11 00:59:08,500][root][ERROR] - Processed prompts:  97%|#########6| 28/29 [00:06<00:00,  7.80it/s, est. speed input: 3515.43 toks/s, output: 211.27 toks/s]
[2025-01-11 00:59:09,036][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:06<00:00,  4.37it/s, est. speed input: 3355.36 toks/s, output: 214.24 toks/s]
WARNING 01-11 00:59:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:09,252][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:10,021][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.30it/s, est. speed input: 918.71 toks/s, output: 22.12 toks/s]
[2025-01-11 00:59:10,187][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:00<00:00,  2.42it/s, est. speed input: 1608.52 toks/s, output: 46.02 toks/s]
[2025-01-11 00:59:10,554][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.55it/s, est. speed input: 1717.28 toks/s, output: 69.92 toks/s]
[2025-01-11 00:59:10,554][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.30it/s, est. speed input: 1717.28 toks/s, output: 69.92 toks/s]
WARNING 01-11 00:59:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:10,768][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:12,004][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.24s/it, est. speed input: 599.02 toks/s, output: 22.67 toks/s]
[2025-01-11 00:59:12,416][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:02,  1.33it/s, est. speed input: 964.43 toks/s, output: 45.52 toks/s]
[2025-01-11 00:59:13,081][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.69it/s, est. speed input: 1750.37 toks/s, output: 112.86 toks/s]
[2025-01-11 00:59:13,081][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.16it/s, est. speed input: 1750.37 toks/s, output: 112.86 toks/s]
WARNING 01-11 00:59:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:13,342][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:17,091][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:37,  3.75s/it, est. speed input: 188.31 toks/s, output: 6.67 toks/s]
[2025-01-11 00:59:17,249][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:24,  1.03s/it, est. speed input: 619.64 toks/s, output: 20.22 toks/s]
[2025-01-11 00:59:17,387][root][ERROR] - Processed prompts:  33%|###3      | 9/27 [00:04<00:04,  3.78it/s, est. speed input: 1771.46 toks/s, output: 63.04 toks/s]
[2025-01-11 00:59:17,830][root][ERROR] - Processed prompts:  48%|####8     | 13/27 [00:04<00:02,  5.01it/s, est. speed input: 2264.20 toks/s, output: 87.57 toks/s]
[2025-01-11 00:59:18,063][root][ERROR] - Processed prompts:  59%|#####9    | 16/27 [00:04<00:01,  6.17it/s, est. speed input: 2625.92 toks/s, output: 112.47 toks/s]
[2025-01-11 00:59:18,345][root][ERROR] - Processed prompts:  67%|######6   | 18/27 [00:05<00:01,  6.36it/s, est. speed input: 2785.67 toks/s, output: 129.71 toks/s]
[2025-01-11 00:59:18,455][root][ERROR] - Processed prompts:  74%|#######4  | 20/27 [00:05<00:00,  7.57it/s, est. speed input: 3025.64 toks/s, output: 150.98 toks/s]
[2025-01-11 00:59:18,576][root][ERROR] - Processed prompts:  85%|########5 | 23/27 [00:05<00:00,  9.97it/s, est. speed input: 3399.32 toks/s, output: 185.34 toks/s]
[2025-01-11 00:59:18,706][root][ERROR] - Processed prompts:  93%|#########2| 25/27 [00:05<00:00, 10.91it/s, est. speed input: 3601.57 toks/s, output: 208.41 toks/s]
[2025-01-11 00:59:19,102][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  8.38it/s, est. speed input: 3648.48 toks/s, output: 226.40 toks/s]
[2025-01-11 00:59:19,102][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  4.69it/s, est. speed input: 3648.48 toks/s, output: 226.40 toks/s]
WARNING 01-11 00:59:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:19,498][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:22,928][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:15,  3.43s/it, est. speed input: 237.66 toks/s, output: 6.12 toks/s]
[2025-01-11 00:59:23,168][root][ERROR] - Processed prompts:   9%|8         | 2/23 [00:03<00:32,  1.55s/it, est. speed input: 451.58 toks/s, output: 12.81 toks/s]
[2025-01-11 00:59:23,305][root][ERROR] - Processed prompts:  22%|##1       | 5/23 [00:03<00:08,  2.11it/s, est. speed input: 1218.93 toks/s, output: 34.68 toks/s]
[2025-01-11 00:59:23,425][root][ERROR] - Processed prompts:  65%|######5   | 15/23 [00:03<00:00,  8.57it/s, est. speed input: 3477.84 toks/s, output: 109.27 toks/s]
[2025-01-11 00:59:23,910][root][ERROR] - Processed prompts:  83%|########2 | 19/23 [00:04<00:00,  8.46it/s, est. speed input: 3899.15 toks/s, output: 139.40 toks/s]
[2025-01-11 00:59:24,539][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:05<00:00,  7.10it/s, est. speed input: 3939.41 toks/s, output: 163.86 toks/s]
[2025-01-11 00:59:25,146][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.07it/s, est. speed input: 3704.13 toks/s, output: 167.52 toks/s]
WARNING 01-11 00:59:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:25,377][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:26,960][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.58s/it, est. speed input: 478.89 toks/s, output: 18.32 toks/s]
[2025-01-11 00:59:27,144][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.09it/s, est. speed input: 1448.21 toks/s, output: 53.78 toks/s]
[2025-01-11 00:59:27,581][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:02<00:01,  2.15it/s, est. speed input: 1544.32 toks/s, output: 68.98 toks/s]
[2025-01-11 00:59:27,722][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.80it/s, est. speed input: 1880.88 toks/s, output: 92.12 toks/s]
[2025-01-11 00:59:28,329][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  2.29it/s, est. speed input: 1760.18 toks/s, output: 106.03 toks/s]
[2025-01-11 00:59:28,750][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.31it/s, est. speed input: 1812.06 toks/s, output: 128.99 toks/s]
[2025-01-11 00:59:28,750][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.08it/s, est. speed input: 1812.06 toks/s, output: 128.99 toks/s]
WARNING 01-11 00:59:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:29,012][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:30,987][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.98s/it, est. speed input: 559.82 toks/s, output: 14.68 toks/s]
[2025-01-11 00:59:31,258][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:02<00:06,  1.03it/s, est. speed input: 969.93 toks/s, output: 30.27 toks/s]
[2025-01-11 00:59:31,602][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:02<00:02,  2.12it/s, est. speed input: 1552.10 toks/s, output: 62.15 toks/s]
[2025-01-11 00:59:31,763][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  2.67it/s, est. speed input: 1812.12 toks/s, output: 80.32 toks/s]
[2025-01-11 00:59:32,092][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:03<00:00,  3.62it/s, est. speed input: 2343.54 toks/s, output: 116.87 toks/s]
[2025-01-11 00:59:33,118][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  2.69it/s, est. speed input: 2184.18 toks/s, output: 140.04 toks/s]
[2025-01-11 00:59:33,118][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  2.19it/s, est. speed input: 2184.18 toks/s, output: 140.04 toks/s]
WARNING 01-11 00:59:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:33,356][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:36,253][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:43,  2.90s/it, est. speed input: 324.79 toks/s, output: 10.01 toks/s]
[2025-01-11 00:59:36,692][root][ERROR] - Processed prompts:  75%|#######5  | 12/16 [00:03<00:00,  4.74it/s, est. speed input: 3478.42 toks/s, output: 110.91 toks/s]
[2025-01-11 00:59:37,031][root][ERROR] - Processed prompts:  88%|########7 | 14/16 [00:03<00:00,  4.93it/s, est. speed input: 3637.06 toks/s, output: 134.13 toks/s]
[2025-01-11 00:59:37,166][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  5.78it/s, est. speed input: 3975.67 toks/s, output: 165.09 toks/s]
[2025-01-11 00:59:37,166][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.20it/s, est. speed input: 3975.67 toks/s, output: 165.09 toks/s]
WARNING 01-11 00:59:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:37,473][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:39,357][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.88s/it, est. speed input: 566.90 toks/s, output: 15.39 toks/s]
[2025-01-11 00:59:39,707][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.01it/s, est. speed input: 3336.53 toks/s, output: 99.34 toks/s]
[2025-01-11 00:59:39,792][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.45it/s, est. speed input: 3755.79 toks/s, output: 118.55 toks/s]
WARNING 01-11 00:59:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:40,088][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:41,683][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.59s/it, est. speed input: 525.07 toks/s, output: 18.19 toks/s]
[2025-01-11 00:59:42,825][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.08it/s, est. speed input: 1670.55 toks/s, output: 74.92 toks/s]
[2025-01-11 00:59:43,068][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.90it/s, est. speed input: 2146.91 toks/s, output: 132.59 toks/s]
[2025-01-11 00:59:43,068][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.35it/s, est. speed input: 2146.91 toks/s, output: 132.59 toks/s]
WARNING 01-11 00:59:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:43,325][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:44,627][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.30s/it, est. speed input: 803.17 toks/s, output: 22.27 toks/s]
[2025-01-11 00:59:45,217][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.46it/s, est. speed input: 2406.70 toks/s, output: 79.80 toks/s]
[2025-01-11 00:59:45,218][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.11it/s, est. speed input: 2406.70 toks/s, output: 79.80 toks/s]
WARNING 01-11 00:59:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:45,435][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:46,786][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.35s/it, est. speed input: 717.91 toks/s, output: 21.46 toks/s]
[2025-01-11 00:59:47,012][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.31it/s, est. speed input: 1906.73 toks/s, output: 63.43 toks/s]
[2025-01-11 00:59:47,343][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.51it/s, est. speed input: 2049.27 toks/s, output: 82.81 toks/s]
[2025-01-11 00:59:47,394][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.55it/s, est. speed input: 2524.69 toks/s, output: 111.79 toks/s]
WARNING 01-11 00:59:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:47,625][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:49,118][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.49s/it, est. speed input: 1017.58 toks/s, output: 18.76 toks/s]
[2025-01-11 00:59:49,285][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.23it/s, est. speed input: 2389.17 toks/s, output: 56.05 toks/s]
[2025-01-11 00:59:49,487][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.75it/s, est. speed input: 2668.96 toks/s, output: 75.20 toks/s]
[2025-01-11 00:59:49,589][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.61it/s, est. speed input: 3210.12 toks/s, output: 98.31 toks/s]
[2025-01-11 00:59:49,589][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.55it/s, est. speed input: 3210.12 toks/s, output: 98.31 toks/s]
WARNING 01-11 00:59:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:49,802][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:50,617][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1395.49 toks/s, output: 38.08 toks/s]
[2025-01-11 00:59:50,617][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1395.49 toks/s, output: 38.08 toks/s]
WARNING 01-11 00:59:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 00:59:50,848][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 00:59:52,191][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:04,  1.34s/it, est. speed input: 765.87 toks/s, output: 21.61 toks/s]
[2025-01-11 00:59:52,333][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.57it/s, est. speed input: 1521.39 toks/s, output: 43.78 toks/s]
[2025-01-11 00:59:52,665][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.01it/s, est. speed input: 2124.78 toks/s, output: 65.50 toks/s]
[2025-01-11 00:59:52,985][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  2.34it/s, est. speed input: 2307.83 toks/s, output: 89.86 toks/s]
[2025-01-11 00:59:52,985][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.87it/s, est. speed input: 2307.83 toks/s, output: 89.86 toks/s]
[2025-01-11 00:59:55,325][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 00:59:55,378][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 00:59:57,032][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 00:59:58,785][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-01-11 01:00:00,448][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 01:00:01,038][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 01:00:01,039][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-11 01:00:22,831][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:00:22,883][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:00:24,973][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.09s/it]
[2025-01-11 01:00:26,604][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.82s/it]
[2025-01-11 01:00:28,241][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.74s/it]
[2025-01-11 01:00:28,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 01:00:28,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.47s/it]
WARNING 01-11 01:00:48 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:00:48 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:00:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:00:48 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:00:49,246][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:00:50,598][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:00:50,974][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 01:00:52,311][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:00:53,645][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:00:53,645][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:00:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:01:07 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:01:08 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:01:08 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:01:30 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:01:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:30,510][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:34,508][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  4.00s/it, est. speed input: 158.85 toks/s, output: 3.25 toks/s]
[2025-01-11 01:01:34,629][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.72s/it, est. speed input: 308.35 toks/s, output: 6.80 toks/s]
[2025-01-11 01:01:34,802][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:19,  1.44it/s, est. speed input: 591.75 toks/s, output: 14.21 toks/s]
[2025-01-11 01:01:34,917][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:10,  2.54it/s, est. speed input: 864.62 toks/s, output: 22.69 toks/s]
[2025-01-11 01:01:35,017][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  8.66it/s, est. speed input: 1972.58 toks/s, output: 59.24 toks/s]
[2025-01-11 01:01:35,222][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 10.70it/s, est. speed input: 2426.01 toks/s, output: 77.89 toks/s]
[2025-01-11 01:01:35,503][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 11.64it/s, est. speed input: 2797.81 toks/s, output: 99.13 toks/s]
[2025-01-11 01:01:35,619][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:05<00:00, 14.78it/s, est. speed input: 3231.90 toks/s, output: 125.67 toks/s]
[2025-01-11 01:01:35,896][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:05<00:00, 13.52it/s, est. speed input: 3419.34 toks/s, output: 143.72 toks/s]
[2025-01-11 01:01:36,046][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 14.79it/s, est. speed input: 3670.52 toks/s, output: 169.62 toks/s]
[2025-01-11 01:01:36,047][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.78it/s, est. speed input: 3670.52 toks/s, output: 169.62 toks/s]
WARNING 01-11 01:01:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:36,471][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:37,338][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.15it/s, est. speed input: 791.97 toks/s, output: 18.47 toks/s]
[2025-01-11 01:01:37,685][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.78it/s, est. speed input: 1162.00 toks/s, output: 41.20 toks/s]
[2025-01-11 01:01:37,930][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.40it/s, est. speed input: 1460.69 toks/s, output: 67.20 toks/s]
[2025-01-11 01:01:37,930][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.06it/s, est. speed input: 1460.69 toks/s, output: 67.20 toks/s]
WARNING 01-11 01:01:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:38,386][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:41,307][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:02<01:21,  2.92s/it, est. speed input: 223.89 toks/s, output: 3.77 toks/s]
[2025-01-11 01:01:41,700][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:38,  1.43s/it, est. speed input: 394.43 toks/s, output: 8.75 toks/s]
[2025-01-11 01:01:41,905][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:08,  2.72it/s, est. speed input: 1121.42 toks/s, output: 30.69 toks/s]
[2025-01-11 01:01:42,046][root][ERROR] - Processed prompts:  31%|###1      | 9/29 [00:03<00:04,  4.48it/s, est. speed input: 1624.76 toks/s, output: 48.90 toks/s]
[2025-01-11 01:01:42,222][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:03<00:03,  5.48it/s, est. speed input: 1899.14 toks/s, output: 61.26 toks/s]
[2025-01-11 01:01:42,366][root][ERROR] - Processed prompts:  55%|#####5    | 16/29 [00:03<00:01,  9.73it/s, est. speed input: 2657.12 toks/s, output: 96.98 toks/s]
[2025-01-11 01:01:42,591][root][ERROR] - Processed prompts:  66%|######5   | 19/29 [00:04<00:00, 10.61it/s, est. speed input: 2984.92 toks/s, output: 117.96 toks/s]
[2025-01-11 01:01:42,706][root][ERROR] - Processed prompts:  72%|#######2  | 21/29 [00:04<00:00, 11.65it/s, est. speed input: 3209.21 toks/s, output: 134.51 toks/s]
[2025-01-11 01:01:42,808][root][ERROR] - Processed prompts:  83%|########2 | 24/29 [00:04<00:00, 14.51it/s, est. speed input: 3590.46 toks/s, output: 162.83 toks/s]
[2025-01-11 01:01:43,073][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:04<00:00, 13.31it/s, est. speed input: 3813.61 toks/s, output: 190.95 toks/s]
[2025-01-11 01:01:43,146][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.09it/s, est. speed input: 4030.47 toks/s, output: 214.72 toks/s]
WARNING 01-11 01:01:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:43,396][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:47,058][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:38,  3.66s/it, est. speed input: 202.09 toks/s, output: 6.01 toks/s]
[2025-01-11 01:01:47,317][root][ERROR] - Processed prompts:  14%|#4        | 4/28 [00:03<00:18,  1.31it/s, est. speed input: 771.13 toks/s, output: 23.72 toks/s]
[2025-01-11 01:01:47,418][root][ERROR] - Processed prompts:  18%|#7        | 5/28 [00:04<00:13,  1.73it/s, est. speed input: 940.23 toks/s, output: 30.34 toks/s]
[2025-01-11 01:01:47,632][root][ERROR] - Processed prompts:  43%|####2     | 12/28 [00:04<00:02,  5.70it/s, est. speed input: 2180.82 toks/s, output: 79.56 toks/s]
[2025-01-11 01:01:47,739][root][ERROR] - Processed prompts:  57%|#####7    | 16/28 [00:04<00:01,  8.37it/s, est. speed input: 2829.43 toks/s, output: 110.31 toks/s]
[2025-01-11 01:01:48,044][root][ERROR] - Processed prompts:  68%|######7   | 19/28 [00:04<00:01,  8.74it/s, est. speed input: 3125.00 toks/s, output: 130.38 toks/s]
[2025-01-11 01:01:48,402][root][ERROR] - Processed prompts:  75%|#######5  | 21/28 [00:05<00:00,  7.79it/s, est. speed input: 3208.34 toks/s, output: 143.63 toks/s]
[2025-01-11 01:01:48,503][root][ERROR] - Processed prompts:  82%|########2 | 23/28 [00:05<00:00,  9.08it/s, est. speed input: 3451.46 toks/s, output: 165.45 toks/s]
[2025-01-11 01:01:48,645][root][ERROR] - Processed prompts:  93%|#########2| 26/28 [00:05<00:00, 11.23it/s, est. speed input: 3802.22 toks/s, output: 199.08 toks/s]
[2025-01-11 01:01:48,882][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00, 10.41it/s, est. speed input: 3916.79 toks/s, output: 219.13 toks/s]
[2025-01-11 01:01:48,882][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00,  5.10it/s, est. speed input: 3916.79 toks/s, output: 219.13 toks/s]
WARNING 01-11 01:01:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:49,100][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:49,850][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.33it/s, est. speed input: 865.58 toks/s, output: 16.00 toks/s]
[2025-01-11 01:01:50,048][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:00<00:00,  2.35it/s, est. speed input: 1405.60 toks/s, output: 35.88 toks/s]
[2025-01-11 01:01:50,380][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.62it/s, est. speed input: 1562.79 toks/s, output: 57.85 toks/s]
[2025-01-11 01:01:50,430][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.01it/s, est. speed input: 2073.11 toks/s, output: 87.98 toks/s]
WARNING 01-11 01:01:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:50,663][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:52,133][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.47s/it, est. speed input: 581.38 toks/s, output: 17.00 toks/s]
[2025-01-11 01:01:52,377][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:01<00:03,  1.34it/s, est. speed input: 1002.22 toks/s, output: 35.00 toks/s]
[2025-01-11 01:01:52,599][root][ERROR] - Processed prompts:  57%|#####7    | 4/7 [00:01<00:01,  2.86it/s, est. speed input: 1775.81 toks/s, output: 73.84 toks/s]
[2025-01-11 01:01:52,988][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.62it/s, est. speed input: 2187.37 toks/s, output: 109.67 toks/s]
[2025-01-11 01:01:54,482][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  1.71it/s, est. speed input: 1556.60 toks/s, output: 107.35 toks/s]
[2025-01-11 01:01:54,482][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  1.83it/s, est. speed input: 1556.60 toks/s, output: 107.35 toks/s]
WARNING 01-11 01:01:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:54,746][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:01:57,570][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<01:04,  2.82s/it, est. speed input: 244.65 toks/s, output: 5.31 toks/s]
[2025-01-11 01:01:58,058][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:31,  1.45s/it, est. speed input: 430.56 toks/s, output: 12.08 toks/s]
[2025-01-11 01:01:58,246][root][ERROR] - Processed prompts:  17%|#6        | 4/24 [00:03<00:11,  1.67it/s, est. speed input: 820.05 toks/s, output: 27.43 toks/s]
[2025-01-11 01:01:58,454][root][ERROR] - Processed prompts:  50%|#####     | 12/24 [00:03<00:01,  6.61it/s, est. speed input: 2399.96 toks/s, output: 90.87 toks/s]
[2025-01-11 01:01:58,717][root][ERROR] - Processed prompts:  58%|#####8    | 14/24 [00:03<00:01,  6.81it/s, est. speed input: 2610.43 toks/s, output: 106.51 toks/s]
[2025-01-11 01:01:58,833][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:04<00:01,  7.95it/s, est. speed input: 2893.82 toks/s, output: 125.99 toks/s]
[2025-01-11 01:01:58,935][root][ERROR] - Processed prompts:  79%|#######9  | 19/24 [00:04<00:00, 10.55it/s, est. speed input: 3382.91 toks/s, output: 158.02 toks/s]
[2025-01-11 01:01:59,069][root][ERROR] - Processed prompts:  92%|#########1| 22/24 [00:04<00:00, 12.74it/s, est. speed input: 3827.34 toks/s, output: 191.99 toks/s]
[2025-01-11 01:01:59,506][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.04it/s, est. speed input: 3791.94 toks/s, output: 205.85 toks/s]
WARNING 01-11 01:01:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:01:59,776][root][ERROR] - Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:02,398][root][ERROR] - Processed prompts:   5%|5         | 1/20 [00:02<00:49,  2.62s/it, est. speed input: 315.47 toks/s, output: 4.58 toks/s]
[2025-01-11 01:02:03,098][root][ERROR] - Processed prompts:  10%|#         | 2/20 [00:03<00:26,  1.49s/it, est. speed input: 484.46 toks/s, output: 12.04 toks/s]
[2025-01-11 01:02:03,203][root][ERROR] - Processed prompts:  50%|#####     | 10/20 [00:03<00:02,  4.89it/s, est. speed input: 2763.15 toks/s, output: 79.68 toks/s]
[2025-01-11 01:02:03,599][root][ERROR] - Processed prompts:  70%|#######   | 14/20 [00:03<00:00,  6.10it/s, est. speed input: 3388.00 toks/s, output: 108.84 toks/s]
[2025-01-11 01:02:03,965][root][ERROR] - Processed prompts:  85%|########5 | 17/20 [00:04<00:00,  6.59it/s, est. speed input: 3692.75 toks/s, output: 139.45 toks/s]
[2025-01-11 01:02:04,261][root][ERROR] - Processed prompts:  95%|#########5| 19/20 [00:04<00:00,  6.62it/s, est. speed input: 3854.41 toks/s, output: 161.66 toks/s]
[2025-01-11 01:02:04,949][root][ERROR] - Processed prompts: 100%|##########| 20/20 [00:05<00:00,  3.87it/s, est. speed input: 3505.74 toks/s, output: 163.17 toks/s]
WARNING 01-11 01:02:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:05,181][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:06,677][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:13,  1.50s/it, est. speed input: 498.47 toks/s, output: 10.69 toks/s]
[2025-01-11 01:02:06,820][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:05,  1.43it/s, est. speed input: 918.20 toks/s, output: 22.57 toks/s]
[2025-01-11 01:02:07,035][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:01<00:03,  2.09it/s, est. speed input: 1309.24 toks/s, output: 35.59 toks/s]
[2025-01-11 01:02:07,489][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.82it/s, est. speed input: 2004.69 toks/s, output: 75.39 toks/s]
[2025-01-11 01:02:07,729][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.89it/s, est. speed input: 2162.97 toks/s, output: 91.82 toks/s]
[2025-01-11 01:02:07,829][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.61it/s, est. speed input: 2395.38 toks/s, output: 112.90 toks/s]
[2025-01-11 01:02:08,657][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  2.63it/s, est. speed input: 2042.34 toks/s, output: 117.65 toks/s]
[2025-01-11 01:02:08,708][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.84it/s, est. speed input: 2262.95 toks/s, output: 147.99 toks/s]
WARNING 01-11 01:02:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:08,990][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:10,581][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:01<00:19,  1.59s/it, est. speed input: 578.22 toks/s, output: 1.26 toks/s]
[2025-01-11 01:02:11,476][root][ERROR] - Processed prompts:  15%|#5        | 2/13 [00:02<00:12,  1.18s/it, est. speed input: 793.23 toks/s, output: 12.47 toks/s]
[2025-01-11 01:02:11,689][root][ERROR] - Processed prompts:  54%|#####3    | 7/13 [00:02<00:01,  3.82it/s, est. speed input: 2442.10 toks/s, output: 68.93 toks/s]
[2025-01-11 01:02:11,878][root][ERROR] - Processed prompts:  69%|######9   | 9/13 [00:02<00:00,  4.79it/s, est. speed input: 3005.33 toks/s, output: 93.85 toks/s]
[2025-01-11 01:02:12,033][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:03<00:00,  6.88it/s, est. speed input: 3724.55 toks/s, output: 137.36 toks/s]
[2025-01-11 01:02:12,354][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.86it/s, est. speed input: 3689.93 toks/s, output: 145.68 toks/s]
WARNING 01-11 01:02:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:12,580][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:14,553][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.97s/it, est. speed input: 501.91 toks/s, output: 14.70 toks/s]
[2025-01-11 01:02:14,903][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.87it/s, est. speed input: 2692.72 toks/s, output: 94.72 toks/s]
[2025-01-11 01:02:15,314][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  4.12it/s, est. speed input: 2926.45 toks/s, output: 122.91 toks/s]
[2025-01-11 01:02:16,005][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.11it/s, est. speed input: 2601.26 toks/s, output: 129.65 toks/s]
[2025-01-11 01:02:16,005][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  2.92it/s, est. speed input: 2601.26 toks/s, output: 129.65 toks/s]
WARNING 01-11 01:02:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:16,303][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:17,570][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.27s/it, est. speed input: 751.38 toks/s, output: 5.52 toks/s]
[2025-01-11 01:02:18,152][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:05,  1.16it/s, est. speed input: 1066.91 toks/s, output: 19.48 toks/s]
[2025-01-11 01:02:18,381][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.92it/s, est. speed input: 3528.45 toks/s, output: 94.82 toks/s]
[2025-01-11 01:02:18,870][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.12it/s, est. speed input: 3316.95 toks/s, output: 104.02 toks/s]
WARNING 01-11 01:02:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:19,107][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:21,151][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.04s/it, est. speed input: 454.14 toks/s, output: 14.19 toks/s]
[2025-01-11 01:02:21,252][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  5.03it/s, est. speed input: 3605.15 toks/s, output: 110.50 toks/s]
[2025-01-11 01:02:21,929][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.54it/s, est. speed input: 3407.06 toks/s, output: 129.74 toks/s]
WARNING 01-11 01:02:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:22,216][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:23,675][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.46s/it, est. speed input: 877.34 toks/s, output: 19.88 toks/s]
[2025-01-11 01:02:23,744][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.27it/s, est. speed input: 3811.01 toks/s, output: 97.57 toks/s]
WARNING 01-11 01:02:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:23,974][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:25,169][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.19s/it, est. speed input: 626.97 toks/s, output: 24.27 toks/s]
[2025-01-11 01:02:25,488][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.20it/s, est. speed input: 2389.58 toks/s, output: 89.16 toks/s]
[2025-01-11 01:02:25,489][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.64it/s, est. speed input: 2389.58 toks/s, output: 89.16 toks/s]
WARNING 01-11 01:02:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:25,740][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:26,830][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.09s/it, est. speed input: 1071.95 toks/s, output: 25.67 toks/s]
[2025-01-11 01:02:26,849][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.70it/s, est. speed input: 2928.79 toks/s, output: 77.50 toks/s]
WARNING 01-11 01:02:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:27,056][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:28,111][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 977.80 toks/s, output: 42.68 toks/s]
[2025-01-11 01:02:28,111][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 977.80 toks/s, output: 42.68 toks/s]
WARNING 01-11 01:02:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:28,333][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:29,728][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.39s/it, est. speed input: 673.21 toks/s, output: 38.00 toks/s]
[2025-01-11 01:02:30,539][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.05s/it, est. speed input: 993.04 toks/s, output: 69.83 toks/s]
[2025-01-11 01:02:30,539][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.10s/it, est. speed input: 993.04 toks/s, output: 69.83 toks/s]
WARNING 01-11 01:02:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:02:30,760][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:02:31,866][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 927.96 toks/s, output: 43.41 toks/s]
[2025-01-11 01:02:31,866][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 927.96 toks/s, output: 43.41 toks/s]
[2025-01-11 01:02:34,304][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:02:34,357][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:02:36,045][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 01:02:37,752][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 01:02:39,366][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 01:02:39,896][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:02:39,896][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 01:03:01,418][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:03:01,470][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:03:03,862][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.39s/it]
[2025-01-11 01:03:05,982][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.23s/it]
[2025-01-11 01:03:07,955][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.11s/it]
[2025-01-11 01:03:08,590][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.53s/it]
[2025-01-11 01:03:08,590][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.78s/it]
WARNING 01-11 01:03:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:03:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:03:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:03:28 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:03:28,573][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:03:29,897][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:03:30,274][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:03:31,571][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 01:03:32,897][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 01:03:32,897][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 01:03:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:03:46 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:03:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:03:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:04:09 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:04:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:10,001][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:13,768][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.77s/it, est. speed input: 168.62 toks/s, output: 2.92 toks/s]
[2025-01-11 01:04:14,009][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:50,  1.69s/it, est. speed input: 316.92 toks/s, output: 6.49 toks/s]
[2025-01-11 01:04:14,123][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:13,  1.97it/s, est. speed input: 770.31 toks/s, output: 17.95 toks/s]
[2025-01-11 01:04:14,284][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.99it/s, est. speed input: 1037.97 toks/s, output: 26.15 toks/s]
[2025-01-11 01:04:14,466][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  7.81it/s, est. speed input: 1991.07 toks/s, output: 58.68 toks/s]
[2025-01-11 01:04:14,651][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 11.87it/s, est. speed input: 2731.24 toks/s, output: 90.32 toks/s]
[2025-01-11 01:04:14,918][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 12.66it/s, est. speed input: 3099.63 toks/s, output: 112.47 toks/s]
[2025-01-11 01:04:15,019][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:05<00:00, 17.10it/s, est. speed input: 3669.87 toks/s, output: 150.66 toks/s]
[2025-01-11 01:04:15,399][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.93it/s, est. speed input: 3764.70 toks/s, output: 168.78 toks/s]
WARNING 01-11 01:04:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:15,612][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:16,306][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 995.24 toks/s, output: 36.01 toks/s]
[2025-01-11 01:04:16,306][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 995.24 toks/s, output: 36.01 toks/s]
WARNING 01-11 01:04:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:16,552][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:19,647][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:32,  3.09s/it, est. speed input: 211.01 toks/s, output: 4.20 toks/s]
[2025-01-11 01:04:19,929][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:17,  1.51it/s, est. speed input: 771.43 toks/s, output: 17.18 toks/s]
[2025-01-11 01:04:20,089][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:10,  2.45it/s, est. speed input: 1108.80 toks/s, output: 27.43 toks/s]
[2025-01-11 01:04:20,391][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:07,  3.23it/s, est. speed input: 1367.80 toks/s, output: 38.03 toks/s]
[2025-01-11 01:04:20,578][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:04<00:03,  5.05it/s, est. speed input: 1798.58 toks/s, output: 57.87 toks/s]
[2025-01-11 01:04:20,710][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:04<00:02,  6.32it/s, est. speed input: 2065.56 toks/s, output: 71.91 toks/s]
[2025-01-11 01:04:20,828][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:04<00:01, 11.30it/s, est. speed input: 2775.79 toks/s, output: 111.81 toks/s]
[2025-01-11 01:04:20,953][root][ERROR] - Processed prompts:  74%|#######4  | 23/31 [00:04<00:00, 16.25it/s, est. speed input: 3461.86 toks/s, output: 153.39 toks/s]
[2025-01-11 01:04:21,081][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 17.71it/s, est. speed input: 3809.78 toks/s, output: 178.22 toks/s]
[2025-01-11 01:04:21,186][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:04<00:00, 21.61it/s, est. speed input: 4290.43 toks/s, output: 216.70 toks/s]
[2025-01-11 01:04:21,435][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.35it/s, est. speed input: 4205.63 toks/s, output: 219.13 toks/s]
WARNING 01-11 01:04:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:21,686][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:24,704][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:15,  3.02s/it, est. speed input: 240.25 toks/s, output: 4.64 toks/s]
[2025-01-11 01:04:24,962][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:33,  1.39s/it, est. speed input: 445.07 toks/s, output: 10.07 toks/s]
[2025-01-11 01:04:25,264][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:20,  1.12it/s, est. speed input: 618.81 toks/s, output: 16.21 toks/s]
[2025-01-11 01:04:25,452][root][ERROR] - Processed prompts:  23%|##3       | 6/26 [00:03<00:06,  2.90it/s, est. speed input: 1194.38 toks/s, output: 36.91 toks/s]
[2025-01-11 01:04:25,585][root][ERROR] - Processed prompts:  31%|###       | 8/26 [00:03<00:04,  4.22it/s, est. speed input: 1539.87 toks/s, output: 51.81 toks/s]
[2025-01-11 01:04:25,832][root][ERROR] - Processed prompts:  38%|###8      | 10/26 [00:04<00:03,  5.09it/s, est. speed input: 1822.00 toks/s, output: 65.86 toks/s]
[2025-01-11 01:04:25,941][root][ERROR] - Processed prompts:  54%|#####3    | 14/26 [00:04<00:01,  8.99it/s, est. speed input: 2490.47 toks/s, output: 101.76 toks/s]
[2025-01-11 01:04:26,067][root][ERROR] - Processed prompts:  65%|######5   | 17/26 [00:04<00:00, 11.50it/s, est. speed input: 2951.65 toks/s, output: 128.73 toks/s]
[2025-01-11 01:04:26,256][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:04<00:00, 12.67it/s, est. speed input: 3335.39 toks/s, output: 155.81 toks/s]
[2025-01-11 01:04:26,394][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:04<00:00, 14.64it/s, est. speed input: 3724.25 toks/s, output: 186.92 toks/s]
[2025-01-11 01:04:26,527][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:04<00:00, 14.74it/s, est. speed input: 3939.60 toks/s, output: 207.62 toks/s]
[2025-01-11 01:04:27,129][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  4.78it/s, est. speed input: 3649.23 toks/s, output: 203.21 toks/s]
WARNING 01-11 01:04:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:27,381][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:28,761][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.38s/it, est. speed input: 503.97 toks/s, output: 21.75 toks/s]
[2025-01-11 01:04:28,872][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.51it/s, est. speed input: 1428.81 toks/s, output: 65.74 toks/s]
[2025-01-11 01:04:29,060][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.13it/s, est. speed input: 2107.97 toks/s, output: 107.24 toks/s]
[2025-01-11 01:04:29,862][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.42it/s, est. speed input: 1722.94 toks/s, output: 110.05 toks/s]
WARNING 01-11 01:04:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:30,098][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:31,323][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:09,  1.22s/it, est. speed input: 611.03 toks/s, output: 8.17 toks/s]
[2025-01-11 01:04:31,592][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:01<00:04,  1.51it/s, est. speed input: 1007.83 toks/s, output: 20.09 toks/s]
[2025-01-11 01:04:31,695][root][ERROR] - Processed prompts:  44%|####4     | 4/9 [00:01<00:01,  3.58it/s, est. speed input: 1921.09 toks/s, output: 48.23 toks/s]
[2025-01-11 01:04:31,808][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:01<00:00,  5.74it/s, est. speed input: 2789.35 toks/s, output: 78.37 toks/s]
[2025-01-11 01:04:32,238][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  5.25it/s, est. speed input: 3009.03 toks/s, output: 102.36 toks/s]
[2025-01-11 01:04:32,440][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.84it/s, est. speed input: 3120.32 toks/s, output: 120.87 toks/s]
WARNING 01-11 01:04:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:32,681][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:36,033][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:13,  3.35s/it, est. speed input: 225.51 toks/s, output: 8.05 toks/s]
[2025-01-11 01:04:36,164][root][ERROR] - Processed prompts:  43%|####3     | 10/23 [00:03<00:03,  3.92it/s, est. speed input: 2269.69 toks/s, output: 82.69 toks/s]
[2025-01-11 01:04:36,574][root][ERROR] - Processed prompts:  65%|######5   | 15/23 [00:03<00:01,  5.50it/s, est. speed input: 2980.30 toks/s, output: 118.66 toks/s]
[2025-01-11 01:04:36,728][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:04<00:00,  6.80it/s, est. speed input: 3427.12 toks/s, output: 149.50 toks/s]
[2025-01-11 01:04:36,976][root][ERROR] - Processed prompts:  91%|#########1| 21/23 [00:04<00:00,  7.74it/s, est. speed input: 3752.64 toks/s, output: 183.01 toks/s]
[2025-01-11 01:04:37,223][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:04<00:00,  5.06it/s, est. speed input: 3904.52 toks/s, output: 204.53 toks/s]
WARNING 01-11 01:04:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:37,486][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:40,674][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:03<00:57,  3.19s/it, est. speed input: 265.71 toks/s, output: 8.47 toks/s]
[2025-01-11 01:04:40,958][root][ERROR] - Processed prompts:  58%|#####7    | 11/19 [00:03<00:01,  4.25it/s, est. speed input: 3002.13 toks/s, output: 93.32 toks/s]
[2025-01-11 01:04:41,110][root][ERROR] - Processed prompts:  74%|#######3  | 14/19 [00:03<00:00,  5.48it/s, est. speed input: 3587.02 toks/s, output: 121.97 toks/s]
[2025-01-11 01:04:41,284][root][ERROR] - Processed prompts:  89%|########9 | 17/19 [00:03<00:00,  6.83it/s, est. speed input: 4084.84 toks/s, output: 154.02 toks/s]
[2025-01-11 01:04:41,809][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:04<00:00,  5.83it/s, est. speed input: 4026.55 toks/s, output: 168.40 toks/s]
[2025-01-11 01:04:41,809][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:04<00:00,  4.39it/s, est. speed input: 4026.55 toks/s, output: 168.40 toks/s]
WARNING 01-11 01:04:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:42,034][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:43,891][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:18,  1.86s/it, est. speed input: 391.66 toks/s, output: 12.93 toks/s]
[2025-01-11 01:04:44,035][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:02<00:04,  1.87it/s, est. speed input: 1109.33 toks/s, output: 38.99 toks/s]
[2025-01-11 01:04:44,218][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:00,  4.93it/s, est. speed input: 2601.71 toks/s, output: 92.95 toks/s]
[2025-01-11 01:04:44,802][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.32it/s, est. speed input: 2647.90 toks/s, output: 115.28 toks/s]
[2025-01-11 01:04:45,248][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  4.37it/s, est. speed input: 2782.25 toks/s, output: 149.68 toks/s]
[2025-01-11 01:04:45,248][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.42it/s, est. speed input: 2782.25 toks/s, output: 149.68 toks/s]
WARNING 01-11 01:04:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:45,536][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:47,588][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.05s/it, est. speed input: 487.34 toks/s, output: 14.13 toks/s]
[2025-01-11 01:04:47,863][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.33it/s, est. speed input: 2443.98 toks/s, output: 79.92 toks/s]
[2025-01-11 01:04:48,039][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.36it/s, est. speed input: 3040.72 toks/s, output: 113.46 toks/s]
[2025-01-11 01:04:48,313][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  5.01it/s, est. speed input: 3467.29 toks/s, output: 147.67 toks/s]
[2025-01-11 01:04:48,313][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.60it/s, est. speed input: 3467.29 toks/s, output: 147.67 toks/s]
WARNING 01-11 01:04:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:48,578][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:50,647][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:20,  2.07s/it, est. speed input: 394.47 toks/s, output: 13.05 toks/s]
[2025-01-11 01:04:51,049][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.08it/s, est. speed input: 2162.80 toks/s, output: 75.26 toks/s]
[2025-01-11 01:04:51,187][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.48it/s, est. speed input: 2380.58 toks/s, output: 90.06 toks/s]
[2025-01-11 01:04:51,297][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  4.04it/s, est. speed input: 2605.79 toks/s, output: 106.28 toks/s]
[2025-01-11 01:04:51,480][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.31it/s, est. speed input: 2740.22 toks/s, output: 121.31 toks/s]
[2025-01-11 01:04:53,068][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.17it/s, est. speed input: 2212.29 toks/s, output: 128.05 toks/s]
[2025-01-11 01:04:53,069][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:04<00:00,  2.45it/s, est. speed input: 2212.29 toks/s, output: 128.05 toks/s]
WARNING 01-11 01:04:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:53,346][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:55,047][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.70s/it, est. speed input: 637.47 toks/s, output: 17.05 toks/s]
[2025-01-11 01:04:55,323][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  3.89it/s, est. speed input: 3118.10 toks/s, output: 95.59 toks/s]
[2025-01-11 01:04:55,677][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.00it/s, est. speed input: 3100.46 toks/s, output: 108.97 toks/s]
WARNING 01-11 01:04:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:55,900][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:04:57,647][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.75s/it, est. speed input: 535.52 toks/s, output: 16.61 toks/s]
[2025-01-11 01:04:58,174][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:01,  2.69it/s, est. speed input: 2071.22 toks/s, output: 74.33 toks/s]
[2025-01-11 01:04:58,341][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.80it/s, est. speed input: 2738.72 toks/s, output: 117.61 toks/s]
[2025-01-11 01:04:58,593][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.84it/s, est. speed input: 2826.53 toks/s, output: 134.83 toks/s]
[2025-01-11 01:04:58,593][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.97it/s, est. speed input: 2826.53 toks/s, output: 134.83 toks/s]
WARNING 01-11 01:04:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:04:58,843][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:00,428][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.58s/it, est. speed input: 710.65 toks/s, output: 18.30 toks/s]
[2025-01-11 01:05:00,705][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.39it/s, est. speed input: 2886.03 toks/s, output: 85.97 toks/s]
[2025-01-11 01:05:00,924][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.61it/s, est. speed input: 3198.24 toks/s, output: 104.32 toks/s]
[2025-01-11 01:05:00,924][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.88it/s, est. speed input: 3198.24 toks/s, output: 104.32 toks/s]
WARNING 01-11 01:05:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:01,146][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:02,502][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.36s/it, est. speed input: 776.58 toks/s, output: 21.39 toks/s]
[2025-01-11 01:05:02,607][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.46it/s, est. speed input: 3388.23 toks/s, output: 104.71 toks/s]
[2025-01-11 01:05:02,607][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.42it/s, est. speed input: 3388.23 toks/s, output: 104.71 toks/s]
WARNING 01-11 01:05:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:02,871][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:03,832][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 1228.56 toks/s, output: 30.17 toks/s]
[2025-01-11 01:05:04,978][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.07s/it, est. speed input: 1097.50 toks/s, output: 59.81 toks/s]
[2025-01-11 01:05:04,978][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.05s/it, est. speed input: 1097.50 toks/s, output: 59.81 toks/s]
WARNING 01-11 01:05:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:05,189][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:06,671][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.48s/it, est. speed input: 753.90 toks/s, output: 47.24 toks/s]
[2025-01-11 01:05:06,672][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.48s/it, est. speed input: 753.90 toks/s, output: 47.24 toks/s]
WARNING 01-11 01:05:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:06,891][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:07,698][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1564.77 toks/s, output: 35.96 toks/s]
[2025-01-11 01:05:07,698][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1564.77 toks/s, output: 35.96 toks/s]
WARNING 01-11 01:05:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:07,907][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:08,944][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 1163.16 toks/s, output: 41.47 toks/s]
[2025-01-11 01:05:08,944][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 1163.16 toks/s, output: 41.47 toks/s]
WARNING 01-11 01:05:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:05:09,183][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:05:09,955][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1298.18 toks/s, output: 37.57 toks/s]
[2025-01-11 01:05:09,955][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1298.18 toks/s, output: 37.57 toks/s]
[2025-01-11 01:05:12,376][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:05:12,429][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:05:14,102][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 01:05:15,885][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 01:05:17,485][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 01:05:18,012][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 01:05:18,013][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 01:05:39,631][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:05:39,683][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:05:41,637][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 01:05:43,245][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 01:05:44,832][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:05:45,339][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:05:45,339][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 01:06:05 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:06:05 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:06:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:06:05 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:06:05,907][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:06:07,261][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:06:07,638][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 01:06:08,980][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 01:06:10,315][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:06:10,316][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:06:10 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:06:24 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:06:24 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:06:24 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:06:47 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 01:06:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:06:47,781][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:06:51,570][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.59 toks/s, output: 2.90 toks/s]
[2025-01-11 01:06:51,751][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:50,  1.67s/it, est. speed input: 319.89 toks/s, output: 6.30 toks/s]
[2025-01-11 01:06:51,867][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:13,  2.00it/s, est. speed input: 777.16 toks/s, output: 17.38 toks/s]
[2025-01-11 01:06:51,974][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:07,  3.13it/s, est. speed input: 1060.07 toks/s, output: 25.28 toks/s]
[2025-01-11 01:06:52,078][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:05,  4.52it/s, est. speed input: 1330.18 toks/s, output: 33.98 toks/s]
[2025-01-11 01:06:52,306][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  7.28it/s, est. speed input: 1824.35 toks/s, output: 51.93 toks/s]
[2025-01-11 01:06:52,431][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  9.67it/s, est. speed input: 2184.97 toks/s, output: 67.74 toks/s]
[2025-01-11 01:06:52,540][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:00, 13.75it/s, est. speed input: 2668.95 toks/s, output: 91.21 toks/s]
[2025-01-11 01:06:52,667][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 15.73it/s, est. speed input: 2989.33 toks/s, output: 109.71 toks/s]
[2025-01-11 01:06:52,836][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:05<00:00, 21.19it/s, est. speed input: 3643.12 toks/s, output: 151.54 toks/s]
[2025-01-11 01:06:52,909][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.24it/s, est. speed input: 3962.76 toks/s, output: 175.32 toks/s]
WARNING 01-11 01:06:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:06:53,118][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:06:53,818][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 985.98 toks/s, output: 34.29 toks/s]
[2025-01-11 01:06:53,818][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 985.98 toks/s, output: 34.29 toks/s]
WARNING 01-11 01:06:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:06:54,062][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:06:57,407][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:40,  3.35s/it, est. speed input: 202.67 toks/s, output: 5.08 toks/s]
[2025-01-11 01:06:57,518][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:13,  1.92it/s, est. speed input: 951.79 toks/s, output: 25.46 toks/s]
[2025-01-11 01:06:57,621][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:03<00:05,  4.01it/s, est. speed input: 1663.11 toks/s, output: 47.49 toks/s]
[2025-01-11 01:06:57,760][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:03<00:03,  5.74it/s, est. speed input: 2142.37 toks/s, output: 64.36 toks/s]
[2025-01-11 01:06:57,910][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:03<00:01,  9.37it/s, est. speed input: 2911.31 toks/s, output: 94.58 toks/s]
[2025-01-11 01:06:58,114][root][ERROR] - Processed prompts:  65%|######4   | 20/31 [00:04<00:01, 10.46it/s, est. speed input: 3250.20 toks/s, output: 113.28 toks/s]
[2025-01-11 01:06:58,260][root][ERROR] - Processed prompts:  74%|#######4  | 23/31 [00:04<00:00, 12.20it/s, est. speed input: 3609.77 toks/s, output: 135.54 toks/s]
[2025-01-11 01:06:58,433][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 13.36it/s, est. speed input: 3927.43 toks/s, output: 159.01 toks/s]
[2025-01-11 01:06:58,585][root][ERROR] - Processed prompts:  94%|#########3| 29/31 [00:04<00:00, 14.77it/s, est. speed input: 4240.69 toks/s, output: 187.71 toks/s]
[2025-01-11 01:06:58,676][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.72it/s, est. speed input: 4448.50 toks/s, output: 208.93 toks/s]
WARNING 01-11 01:06:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:06:58,926][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:02,349][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:35,  3.42s/it, est. speed input: 213.57 toks/s, output: 4.67 toks/s]
[2025-01-11 01:07:02,459][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:03<00:16,  1.48it/s, est. speed input: 837.78 toks/s, output: 19.25 toks/s]
[2025-01-11 01:07:02,614][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:09,  2.40it/s, est. speed input: 1204.91 toks/s, output: 29.82 toks/s]
[2025-01-11 01:07:03,003][root][ERROR] - Processed prompts:  28%|##7       | 8/29 [00:04<00:06,  3.02it/s, est. speed input: 1458.20 toks/s, output: 40.72 toks/s]
[2025-01-11 01:07:03,211][root][ERROR] - Processed prompts:  45%|####4     | 13/29 [00:04<00:02,  6.11it/s, est. speed input: 2313.28 toks/s, output: 74.45 toks/s]
[2025-01-11 01:07:03,548][root][ERROR] - Processed prompts:  52%|#####1    | 15/29 [00:04<00:02,  6.06it/s, est. speed input: 2465.68 toks/s, output: 86.76 toks/s]
[2025-01-11 01:07:03,677][root][ERROR] - Processed prompts:  69%|######8   | 20/29 [00:04<00:00, 10.05it/s, est. speed input: 3199.23 toks/s, output: 131.77 toks/s]
[2025-01-11 01:07:03,921][root][ERROR] - Processed prompts:  79%|#######9  | 23/29 [00:04<00:00, 10.60it/s, est. speed input: 3509.94 toks/s, output: 157.36 toks/s]
[2025-01-11 01:07:04,052][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:05<00:00, 13.79it/s, est. speed input: 4028.33 toks/s, output: 199.58 toks/s]
[2025-01-11 01:07:04,563][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.15it/s, est. speed input: 3933.43 toks/s, output: 209.71 toks/s]
WARNING 01-11 01:07:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:04,841][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:05,954][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 590.11 toks/s, output: 31.44 toks/s]
[2025-01-11 01:07:06,122][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.85it/s, est. speed input: 1647.59 toks/s, output: 89.80 toks/s]
[2025-01-11 01:07:06,122][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.34it/s, est. speed input: 1647.59 toks/s, output: 89.80 toks/s]
WARNING 01-11 01:07:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:06,345][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:07,605][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.26s/it, est. speed input: 652.71 toks/s, output: 23.03 toks/s]
[2025-01-11 01:07:07,800][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.58it/s, est. speed input: 1120.75 toks/s, output: 46.07 toks/s]
[2025-01-11 01:07:08,082][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.08it/s, est. speed input: 1822.32 toks/s, output: 93.27 toks/s]
[2025-01-11 01:07:08,350][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  3.26it/s, est. speed input: 1985.03 toks/s, output: 115.24 toks/s]
[2025-01-11 01:07:08,350][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.49it/s, est. speed input: 1985.03 toks/s, output: 115.24 toks/s]
WARNING 01-11 01:07:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:08,597][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:11,851][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:21,  3.25s/it, est. speed input: 213.33 toks/s, output: 5.23 toks/s]
[2025-01-11 01:07:12,471][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:40,  1.70s/it, est. speed input: 412.84 toks/s, output: 11.88 toks/s]
[2025-01-11 01:07:12,645][root][ERROR] - Processed prompts:  31%|###       | 8/26 [00:04<00:05,  3.26it/s, est. speed input: 1594.29 toks/s, output: 55.34 toks/s]
[2025-01-11 01:07:12,767][root][ERROR] - Processed prompts:  42%|####2     | 11/26 [00:04<00:03,  4.83it/s, est. speed input: 2083.50 toks/s, output: 78.91 toks/s]
[2025-01-11 01:07:12,947][root][ERROR] - Processed prompts:  50%|#####     | 13/26 [00:04<00:02,  5.67it/s, est. speed input: 2327.96 toks/s, output: 93.80 toks/s]
[2025-01-11 01:07:13,179][root][ERROR] - Processed prompts:  58%|#####7    | 15/26 [00:04<00:01,  6.25it/s, est. speed input: 2520.73 toks/s, output: 108.69 toks/s]
[2025-01-11 01:07:13,364][root][ERROR] - Processed prompts:  65%|######5   | 17/26 [00:04<00:01,  7.09it/s, est. speed input: 2750.48 toks/s, output: 126.71 toks/s]
[2025-01-11 01:07:13,497][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:04<00:00,  9.54it/s, est. speed input: 3156.00 toks/s, output: 157.96 toks/s]
[2025-01-11 01:07:13,631][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:05<00:00, 11.94it/s, est. speed input: 3529.49 toks/s, output: 190.72 toks/s]
[2025-01-11 01:07:13,744][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:05<00:00, 13.00it/s, est. speed input: 3755.15 toks/s, output: 213.16 toks/s]
[2025-01-11 01:07:13,794][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:05<00:00,  5.00it/s, est. speed input: 3867.56 toks/s, output: 225.33 toks/s]
WARNING 01-11 01:07:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:14,072][root][ERROR] - Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:16,646][root][ERROR] - Processed prompts:   5%|5         | 1/20 [00:02<00:48,  2.57s/it, est. speed input: 302.26 toks/s, output: 4.27 toks/s]
[2025-01-11 01:07:17,214][root][ERROR] - Processed prompts:  10%|#         | 2/20 [00:03<00:25,  1.39s/it, est. speed input: 512.46 toks/s, output: 11.14 toks/s]
[2025-01-11 01:07:17,425][root][ERROR] - Processed prompts:  15%|#5        | 3/20 [00:03<00:14,  1.17it/s, est. speed input: 794.30 toks/s, output: 19.09 toks/s]
[2025-01-11 01:07:17,558][root][ERROR] - Processed prompts:  45%|####5     | 9/20 [00:03<00:02,  5.23it/s, est. speed input: 2344.29 toks/s, output: 69.43 toks/s]
[2025-01-11 01:07:17,839][root][ERROR] - Processed prompts:  55%|#####5    | 11/20 [00:03<00:01,  5.63it/s, est. speed input: 2644.67 toks/s, output: 86.27 toks/s]
[2025-01-11 01:07:18,198][root][ERROR] - Processed prompts:  65%|######5   | 13/20 [00:04<00:01,  5.62it/s, est. speed input: 2850.72 toks/s, output: 103.49 toks/s]
[2025-01-11 01:07:18,322][root][ERROR] - Processed prompts:  75%|#######5  | 15/20 [00:04<00:00,  6.91it/s, est. speed input: 3202.83 toks/s, output: 127.76 toks/s]
[2025-01-11 01:07:18,458][root][ERROR] - Processed prompts:  85%|########5 | 17/20 [00:04<00:00,  8.17it/s, est. speed input: 3522.03 toks/s, output: 153.44 toks/s]
[2025-01-11 01:07:18,730][root][ERROR] - Processed prompts:  95%|#########5| 19/20 [00:04<00:00,  7.92it/s, est. speed input: 3684.43 toks/s, output: 177.55 toks/s]
[2025-01-11 01:07:19,267][root][ERROR] - Processed prompts: 100%|##########| 20/20 [00:05<00:00,  3.85it/s, est. speed input: 3477.87 toks/s, output: 180.77 toks/s]
WARNING 01-11 01:07:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:19,484][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:21,021][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.54s/it, est. speed input: 577.73 toks/s, output: 18.87 toks/s]
[2025-01-11 01:07:21,524][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.98it/s, est. speed input: 1973.19 toks/s, output: 84.34 toks/s]
[2025-01-11 01:07:22,514][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.53it/s, est. speed input: 1866.92 toks/s, output: 112.56 toks/s]
[2025-01-11 01:07:22,514][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.31it/s, est. speed input: 1866.92 toks/s, output: 112.56 toks/s]
WARNING 01-11 01:07:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:22,767][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:24,118][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.35s/it, est. speed input: 763.57 toks/s, output: 21.48 toks/s]
[2025-01-11 01:07:24,321][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.22it/s, est. speed input: 2439.80 toks/s, output: 81.75 toks/s]
[2025-01-11 01:07:24,590][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.35it/s, est. speed input: 2574.49 toks/s, output: 100.43 toks/s]
[2025-01-11 01:07:24,590][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.74it/s, est. speed input: 2574.49 toks/s, output: 100.43 toks/s]
WARNING 01-11 01:07:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:24,826][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:27,632][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:42,  2.81s/it, est. speed input: 383.45 toks/s, output: 9.98 toks/s]
[2025-01-11 01:07:28,230][root][ERROR] - Processed prompts:  69%|######8   | 11/16 [00:03<00:01,  4.18it/s, est. speed input: 3016.04 toks/s, output: 100.18 toks/s]
[2025-01-11 01:07:28,550][root][ERROR] - Processed prompts:  75%|#######5  | 12/16 [00:03<00:00,  4.02it/s, est. speed input: 2990.93 toks/s, output: 109.28 toks/s]
[2025-01-11 01:07:28,704][root][ERROR] - Processed prompts:  94%|#########3| 15/16 [00:03<00:00,  5.48it/s, est. speed input: 3560.93 toks/s, output: 158.86 toks/s]
[2025-01-11 01:07:28,738][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.09it/s, est. speed input: 3766.94 toks/s, output: 176.91 toks/s]
WARNING 01-11 01:07:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:29,019][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:30,772][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.75s/it, est. speed input: 668.71 toks/s, output: 16.55 toks/s]
[2025-01-11 01:07:30,806][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  3.92it/s, est. speed input: 4291.39 toks/s, output: 114.71 toks/s]
WARNING 01-11 01:07:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:31,031][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:32,781][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.75s/it, est. speed input: 559.40 toks/s, output: 15.43 toks/s]
[2025-01-11 01:07:33,155][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.19it/s, est. speed input: 3147.28 toks/s, output: 104.52 toks/s]
[2025-01-11 01:07:33,661][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.50it/s, est. speed input: 2982.63 toks/s, output: 113.33 toks/s]
[2025-01-11 01:07:33,661][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.04it/s, est. speed input: 2982.63 toks/s, output: 113.33 toks/s]
WARNING 01-11 01:07:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:33,941][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:35,361][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.42s/it, est. speed input: 822.08 toks/s, output: 20.43 toks/s]
[2025-01-11 01:07:35,490][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.29it/s, est. speed input: 2836.18 toks/s, output: 79.41 toks/s]
[2025-01-11 01:07:35,825][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.65it/s, est. speed input: 2831.05 toks/s, output: 94.99 toks/s]
WARNING 01-11 01:07:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:36,036][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:36,945][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 902.66 toks/s, output: 31.88 toks/s]
[2025-01-11 01:07:37,333][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.66it/s, est. speed input: 1434.30 toks/s, output: 62.46 toks/s]
[2025-01-11 01:07:37,333][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.54it/s, est. speed input: 1434.30 toks/s, output: 62.46 toks/s]
WARNING 01-11 01:07:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:37,562][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:38,677][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 930.24 toks/s, output: 26.01 toks/s]
[2025-01-11 01:07:38,779][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  3.06it/s, est. speed input: 2623.00 toks/s, output: 76.45 toks/s]
[2025-01-11 01:07:38,779][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.47it/s, est. speed input: 2623.00 toks/s, output: 76.45 toks/s]
WARNING 01-11 01:07:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:38,988][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:39,817][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1340.71 toks/s, output: 35.03 toks/s]
[2025-01-11 01:07:39,817][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1340.71 toks/s, output: 35.03 toks/s]
WARNING 01-11 01:07:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:07:40,048][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:07:40,857][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1416.09 toks/s, output: 35.87 toks/s]
[2025-01-11 01:07:40,858][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1416.09 toks/s, output: 35.87 toks/s]
[2025-01-11 01:07:43,267][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:07:43,320][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:07:45,060][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.74s/it]
[2025-01-11 01:07:46,732][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 01:07:48,379][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:07:48,897][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 01:07:48,898][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 01:08:09,660][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:08:09,711][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:08:11,774][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.06s/it]
[2025-01-11 01:08:13,397][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 01:08:14,930][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:08:15,430][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:08:15,431][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 01:08:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:08:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:08:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:08:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:08:35,810][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:08:37,135][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:08:37,512][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:08:38,842][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:08:40,176][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:08:40,176][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:08:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:08:54 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:08:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:08:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:09:16 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:09:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:17,309][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:21,228][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:01,  3.92s/it, est. speed input: 162.04 toks/s, output: 2.81 toks/s]
[2025-01-11 01:09:21,409][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.72s/it, est. speed input: 309.74 toks/s, output: 6.10 toks/s]
[2025-01-11 01:09:21,525][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:13,  1.94it/s, est. speed input: 753.16 toks/s, output: 16.84 toks/s]
[2025-01-11 01:09:21,631][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:06,  3.66it/s, est. speed input: 1175.38 toks/s, output: 28.46 toks/s]
[2025-01-11 01:09:21,732][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:04,  4.98it/s, est. speed input: 1435.62 toks/s, output: 36.85 toks/s]
[2025-01-11 01:09:21,915][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  6.98it/s, est. speed input: 1792.26 toks/s, output: 49.94 toks/s]
[2025-01-11 01:09:22,042][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  9.44it/s, est. speed input: 2146.78 toks/s, output: 65.08 toks/s]
[2025-01-11 01:09:22,153][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 12.17it/s, est. speed input: 2490.80 toks/s, output: 81.55 toks/s]
[2025-01-11 01:09:22,381][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:05<00:00, 12.48it/s, est. speed input: 2754.34 toks/s, output: 98.38 toks/s]
[2025-01-11 01:09:22,494][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:05<00:00, 15.05it/s, est. speed input: 3062.04 toks/s, output: 119.01 toks/s]
[2025-01-11 01:09:22,607][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:05<00:00, 19.22it/s, est. speed input: 3475.81 toks/s, output: 149.11 toks/s]
[2025-01-11 01:09:23,092][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 11.95it/s, est. speed input: 3513.78 toks/s, output: 167.22 toks/s]
[2025-01-11 01:09:23,092][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.53it/s, est. speed input: 3513.78 toks/s, output: 167.22 toks/s]
WARNING 01-11 01:09:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:23,316][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:24,029][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 968.55 toks/s, output: 36.50 toks/s]
[2025-01-11 01:09:24,029][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 968.55 toks/s, output: 36.50 toks/s]
WARNING 01-11 01:09:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:24,276][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:27,331][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:31,  3.05s/it, est. speed input: 213.11 toks/s, output: 3.93 toks/s]
[2025-01-11 01:09:27,507][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:39,  1.36s/it, est. speed input: 410.67 toks/s, output: 8.36 toks/s]
[2025-01-11 01:09:27,733][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.73it/s, est. speed input: 767.06 toks/s, output: 17.93 toks/s]
[2025-01-11 01:09:27,842][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:11,  2.31it/s, est. speed input: 932.99 toks/s, output: 23.28 toks/s]
[2025-01-11 01:09:27,946][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:04,  4.85it/s, est. speed input: 1456.20 toks/s, output: 40.87 toks/s]
[2025-01-11 01:09:28,085][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:03<00:02,  8.55it/s, est. speed input: 2086.84 toks/s, output: 65.36 toks/s]
[2025-01-11 01:09:28,243][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:03<00:01, 11.91it/s, est. speed input: 2671.48 toks/s, output: 90.74 toks/s]
[2025-01-11 01:09:28,448][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:00, 12.65it/s, est. speed input: 3028.24 toks/s, output: 109.53 toks/s]
[2025-01-11 01:09:28,609][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 12.60it/s, est. speed input: 3225.67 toks/s, output: 123.92 toks/s]
[2025-01-11 01:09:28,830][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:04<00:00, 12.90it/s, est. speed input: 3500.97 toks/s, output: 147.77 toks/s]
[2025-01-11 01:09:29,033][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:04<00:00, 14.88it/s, est. speed input: 3907.14 toks/s, output: 186.01 toks/s]
[2025-01-11 01:09:29,589][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:05<00:00,  8.87it/s, est. speed input: 3753.55 toks/s, output: 194.59 toks/s]
[2025-01-11 01:09:29,673][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.74it/s, est. speed input: 3816.21 toks/s, output: 208.81 toks/s]
WARNING 01-11 01:09:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:29,928][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:33,536][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:44,  3.61s/it, est. speed input: 206.22 toks/s, output: 4.71 toks/s]
[2025-01-11 01:09:33,768][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:45,  1.62s/it, est. speed input: 386.77 toks/s, output: 9.90 toks/s]
[2025-01-11 01:09:33,993][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:04<00:26,  1.02it/s, est. speed input: 553.54 toks/s, output: 15.50 toks/s]
[2025-01-11 01:09:34,101][root][ERROR] - Processed prompts:  17%|#6        | 5/30 [00:04<00:11,  2.19it/s, est. speed input: 892.89 toks/s, output: 27.80 toks/s]
[2025-01-11 01:09:34,205][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:04<00:08,  2.81it/s, est. speed input: 1045.23 toks/s, output: 33.91 toks/s]
[2025-01-11 01:09:34,327][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:04<00:01, 11.59it/s, est. speed input: 2646.60 toks/s, output: 93.67 toks/s]
[2025-01-11 01:09:34,538][root][ERROR] - Processed prompts:  63%|######3   | 19/30 [00:04<00:00, 13.31it/s, est. speed input: 3187.89 toks/s, output: 120.83 toks/s]
[2025-01-11 01:09:34,835][root][ERROR] - Processed prompts:  73%|#######3  | 22/30 [00:04<00:00, 12.28it/s, est. speed input: 3468.10 toks/s, output: 140.63 toks/s]
[2025-01-11 01:09:34,987][root][ERROR] - Processed prompts:  83%|########3 | 25/30 [00:05<00:00, 13.69it/s, est. speed input: 3813.33 toks/s, output: 166.84 toks/s]
[2025-01-11 01:09:35,331][root][ERROR] - Processed prompts:  93%|#########3| 28/30 [00:05<00:00, 11.81it/s, est. speed input: 4006.75 toks/s, output: 192.51 toks/s]
[2025-01-11 01:09:35,555][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00, 11.05it/s, est. speed input: 4131.07 toks/s, output: 212.73 toks/s]
[2025-01-11 01:09:35,556][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00,  5.33it/s, est. speed input: 4131.07 toks/s, output: 212.73 toks/s]
WARNING 01-11 01:09:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:35,770][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:36,832][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.06s/it, est. speed input: 621.15 toks/s, output: 36.70 toks/s]
[2025-01-11 01:09:37,100][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.68it/s, est. speed input: 1049.05 toks/s, output: 70.69 toks/s]
[2025-01-11 01:09:37,100][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.50it/s, est. speed input: 1049.05 toks/s, output: 70.69 toks/s]
WARNING 01-11 01:09:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:37,333][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:38,463][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.13s/it, est. speed input: 732.86 toks/s, output: 19.47 toks/s]
[2025-01-11 01:09:38,702][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.62it/s, est. speed input: 1831.65 toks/s, output: 56.96 toks/s]
[2025-01-11 01:09:38,832][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.40it/s, est. speed input: 2244.30 toks/s, output: 79.41 toks/s]
[2025-01-11 01:09:39,233][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.03it/s, est. speed input: 2191.52 toks/s, output: 96.86 toks/s]
[2025-01-11 01:09:39,233][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.63it/s, est. speed input: 2191.52 toks/s, output: 96.86 toks/s]
WARNING 01-11 01:09:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:39,494][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:42,694][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:16,  3.20s/it, est. speed input: 221.32 toks/s, output: 5.94 toks/s]
[2025-01-11 01:09:42,896][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:33,  1.44s/it, est. speed input: 428.97 toks/s, output: 12.35 toks/s]
[2025-01-11 01:09:43,184][root][ERROR] - Processed prompts:  16%|#6        | 4/25 [00:03<00:13,  1.60it/s, est. speed input: 784.85 toks/s, output: 26.02 toks/s]
[2025-01-11 01:09:43,381][root][ERROR] - Processed prompts:  40%|####      | 10/25 [00:03<00:02,  5.16it/s, est. speed input: 2043.21 toks/s, output: 71.27 toks/s]
[2025-01-11 01:09:43,489][root][ERROR] - Processed prompts:  48%|####8     | 12/25 [00:03<00:02,  6.34it/s, est. speed input: 2365.58 toks/s, output: 87.36 toks/s]
[2025-01-11 01:09:43,691][root][ERROR] - Processed prompts:  56%|#####6    | 14/25 [00:04<00:01,  7.02it/s, est. speed input: 2602.04 toks/s, output: 103.18 toks/s]
[2025-01-11 01:09:43,811][root][ERROR] - Processed prompts:  68%|######8   | 17/25 [00:04<00:00,  9.52it/s, est. speed input: 3058.41 toks/s, output: 132.05 toks/s]
[2025-01-11 01:09:44,108][root][ERROR] - Processed prompts:  80%|########  | 20/25 [00:04<00:00,  9.71it/s, est. speed input: 3380.86 toks/s, output: 157.57 toks/s]
[2025-01-11 01:09:44,417][root][ERROR] - Processed prompts:  88%|########8 | 22/25 [00:04<00:00,  8.65it/s, est. speed input: 3468.70 toks/s, output: 175.52 toks/s]
[2025-01-11 01:09:45,023][root][ERROR] - Processed prompts:  96%|#########6| 24/25 [00:05<00:00,  6.08it/s, est. speed input: 3384.66 toks/s, output: 190.46 toks/s]
[2025-01-11 01:09:45,308][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:05<00:00,  5.46it/s, est. speed input: 3352.49 toks/s, output: 202.11 toks/s]
[2025-01-11 01:09:45,308][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:05<00:00,  4.30it/s, est. speed input: 3352.49 toks/s, output: 202.11 toks/s]
WARNING 01-11 01:09:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:45,597][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:48,414][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:02<00:50,  2.82s/it, est. speed input: 281.82 toks/s, output: 7.10 toks/s]
[2025-01-11 01:09:48,710][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:03<00:22,  1.33s/it, est. speed input: 584.34 toks/s, output: 15.10 toks/s]
[2025-01-11 01:09:48,949][root][ERROR] - Processed prompts:  47%|####7     | 9/19 [00:03<00:02,  4.48it/s, est. speed input: 2441.91 toks/s, output: 76.06 toks/s]
[2025-01-11 01:09:49,125][root][ERROR] - Processed prompts:  58%|#####7    | 11/19 [00:03<00:01,  5.31it/s, est. speed input: 2836.24 toks/s, output: 94.37 toks/s]
[2025-01-11 01:09:49,233][root][ERROR] - Processed prompts:  68%|######8   | 13/19 [00:03<00:00,  6.58it/s, est. speed input: 3217.88 toks/s, output: 115.50 toks/s]
[2025-01-11 01:09:49,682][root][ERROR] - Processed prompts:  79%|#######8  | 15/19 [00:04<00:00,  5.81it/s, est. speed input: 3278.74 toks/s, output: 131.45 toks/s]
[2025-01-11 01:09:49,872][root][ERROR] - Processed prompts:  89%|########9 | 17/19 [00:04<00:00,  6.66it/s, est. speed input: 3533.48 toks/s, output: 158.37 toks/s]
[2025-01-11 01:09:50,598][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  4.72it/s, est. speed input: 3365.80 toks/s, output: 175.35 toks/s]
[2025-01-11 01:09:50,598][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  3.80it/s, est. speed input: 3365.80 toks/s, output: 175.35 toks/s]
WARNING 01-11 01:09:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:50,838][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:52,339][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.50s/it, est. speed input: 483.03 toks/s, output: 18.65 toks/s]
[2025-01-11 01:09:52,888][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:02<00:00,  2.94it/s, est. speed input: 1896.52 toks/s, output: 83.90 toks/s]
[2025-01-11 01:09:53,146][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.11it/s, est. speed input: 2087.32 toks/s, output: 104.43 toks/s]
[2025-01-11 01:09:53,414][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.25it/s, est. speed input: 2207.69 toks/s, output: 126.55 toks/s]
[2025-01-11 01:09:53,415][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.72it/s, est. speed input: 2207.69 toks/s, output: 126.55 toks/s]
WARNING 01-11 01:09:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:53,670][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:55,882][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:22,  2.21s/it, est. speed input: 418.76 toks/s, output: 13.11 toks/s]
[2025-01-11 01:09:56,170][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.56it/s, est. speed input: 2044.04 toks/s, output: 62.41 toks/s]
[2025-01-11 01:09:56,292][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.05it/s, est. speed input: 2306.56 toks/s, output: 76.67 toks/s]
[2025-01-11 01:09:56,498][root][ERROR] - Processed prompts:  64%|######3   | 7/11 [00:02<00:01,  3.36it/s, est. speed input: 2460.63 toks/s, output: 90.18 toks/s]
[2025-01-11 01:09:56,629][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  3.94it/s, est. speed input: 2663.71 toks/s, output: 106.47 toks/s]
[2025-01-11 01:09:57,009][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  5.37it/s, est. speed input: 3209.01 toks/s, output: 156.68 toks/s]
[2025-01-11 01:09:57,009][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.30it/s, est. speed input: 3209.01 toks/s, output: 156.68 toks/s]
WARNING 01-11 01:09:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:09:57,241][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:09:59,449][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.21s/it, est. speed input: 344.70 toks/s, output: 11.78 toks/s]
[2025-01-11 01:09:59,593][root][ERROR] - Processed prompts:  58%|#####8    | 7/12 [00:02<00:01,  3.97it/s, est. speed input: 2855.94 toks/s, output: 85.90 toks/s]
[2025-01-11 01:10:00,628][root][ERROR] - Processed prompts:  83%|########3 | 10/12 [00:03<00:00,  3.47it/s, est. speed input: 2740.99 toks/s, output: 114.26 toks/s]
[2025-01-11 01:10:01,221][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.44it/s, est. speed input: 2800.20 toks/s, output: 146.73 toks/s]
[2025-01-11 01:10:01,221][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.01it/s, est. speed input: 2800.20 toks/s, output: 146.73 toks/s]
WARNING 01-11 01:10:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:10:01,524][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:10:03,094][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.57s/it, est. speed input: 693.31 toks/s, output: 18.48 toks/s]
[2025-01-11 01:10:03,316][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.56it/s, est. speed input: 3040.43 toks/s, output: 87.65 toks/s]
[2025-01-11 01:10:03,333][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.32it/s, est. speed input: 3573.56 toks/s, output: 110.03 toks/s]
WARNING 01-11 01:10:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:10:03,562][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:10:05,484][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.92s/it, est. speed input: 566.59 toks/s, output: 15.09 toks/s]
[2025-01-11 01:10:06,009][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:02<00:01,  2.51it/s, est. speed input: 2024.24 toks/s, output: 68.66 toks/s]
[2025-01-11 01:10:06,340][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  3.83it/s, est. speed input: 2744.77 toks/s, output: 123.11 toks/s]
[2025-01-11 01:10:07,282][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.59it/s, est. speed input: 2328.17 toks/s, output: 125.82 toks/s]
[2025-01-11 01:10:07,282][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:03<00:00,  2.42it/s, est. speed input: 2328.17 toks/s, output: 125.82 toks/s]
WARNING 01-11 01:10:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:10:07,547][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:10:08,993][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.45s/it, est. speed input: 812.59 toks/s, output: 20.06 toks/s]
[2025-01-11 01:10:09,218][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.99it/s, est. speed input: 2753.96 toks/s, output: 77.79 toks/s]
[2025-01-11 01:10:09,807][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.47it/s, est. speed input: 2480.24 toks/s, output: 91.17 toks/s]
[2025-01-11 01:10:09,807][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.21it/s, est. speed input: 2480.24 toks/s, output: 91.17 toks/s]
WARNING 01-11 01:10:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:10:10,019][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:10:11,291][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.27s/it, est. speed input: 931.50 toks/s, output: 22.82 toks/s]
[2025-01-11 01:10:11,527][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.29it/s, est. speed input: 2776.67 toks/s, output: 86.25 toks/s]
[2025-01-11 01:10:11,527][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.65it/s, est. speed input: 2776.67 toks/s, output: 86.25 toks/s]
WARNING 01-11 01:10:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:10:11,815][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:10:13,130][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.31s/it, est. speed input: 913.66 toks/s, output: 22.06 toks/s]
[2025-01-11 01:10:13,454][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.37it/s, est. speed input: 1504.60 toks/s, output: 45.17 toks/s]
[2025-01-11 01:10:13,693][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  1.97it/s, est. speed input: 1896.95 toks/s, output: 70.30 toks/s]
[2025-01-11 01:10:15,038][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.19it/s, est. speed input: 1431.10 toks/s, output: 83.78 toks/s]
[2025-01-11 01:10:15,039][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:03<00:00,  1.24it/s, est. speed input: 1431.10 toks/s, output: 83.78 toks/s]
[2025-01-11 01:10:17,461][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:10:17,514][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:10:19,149][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 01:10:20,760][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 01:10:22,342][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.60s/it]
[2025-01-11 01:10:22,867][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 01:10:22,867][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 01:10:43,767][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:10:43,819][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:10:45,807][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.99s/it]
[2025-01-11 01:10:47,416][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 01:10:49,007][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 01:10:49,528][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:10:49,529][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 01:11:08 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:11:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:11:09 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:11:09 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:11:09,450][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:11:10,778][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 01:11:11,158][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:11:12,501][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:11:13,868][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 01:11:13,868][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:11:14 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:11:28 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:11:28 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:11:28 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:11:51 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 01:11:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:11:52,032][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:11:55,908][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.88s/it, est. speed input: 163.80 toks/s, output: 2.84 toks/s]
[2025-01-11 01:11:56,144][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.09s/it, est. speed input: 463.30 toks/s, output: 9.00 toks/s]
[2025-01-11 01:11:56,251][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:09,  2.71it/s, est. speed input: 1053.40 toks/s, output: 23.70 toks/s]
[2025-01-11 01:11:56,399][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.87it/s, est. speed input: 1599.40 toks/s, output: 39.84 toks/s]
[2025-01-11 01:11:56,532][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  6.68it/s, est. speed input: 1975.41 toks/s, output: 53.11 toks/s]
[2025-01-11 01:11:56,647][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  9.83it/s, est. speed input: 2476.61 toks/s, output: 72.80 toks/s]
[2025-01-11 01:11:56,821][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 14.64it/s, est. speed input: 3182.04 toks/s, output: 104.61 toks/s]
[2025-01-11 01:11:57,002][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 15.08it/s, est. speed input: 3449.80 toks/s, output: 122.74 toks/s]
[2025-01-11 01:11:57,485][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 12.89it/s, est. speed input: 3726.17 toks/s, output: 154.58 toks/s]
[2025-01-11 01:11:57,485][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.87it/s, est. speed input: 3726.17 toks/s, output: 154.58 toks/s]
WARNING 01-11 01:11:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:11:57,700][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:11:58,393][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.44it/s, est. speed input: 981.87 toks/s, output: 25.99 toks/s]
[2025-01-11 01:11:58,393][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.89it/s, est. speed input: 1978.53 toks/s, output: 51.95 toks/s]
WARNING 01-11 01:11:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:11:58,633][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:01,588][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:02<01:25,  2.95s/it, est. speed input: 227.49 toks/s, output: 4.06 toks/s]
[2025-01-11 01:12:01,700][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:21,  1.24it/s, est. speed input: 650.56 toks/s, output: 12.39 toks/s]
[2025-01-11 01:12:01,861][root][ERROR] - Processed prompts:  17%|#6        | 5/30 [00:03<00:10,  2.31it/s, est. speed input: 1021.86 toks/s, output: 21.69 toks/s]
[2025-01-11 01:12:02,013][root][ERROR] - Processed prompts:  23%|##3       | 7/30 [00:03<00:06,  3.54it/s, est. speed input: 1363.09 toks/s, output: 31.95 toks/s]
[2025-01-11 01:12:02,185][root][ERROR] - Processed prompts:  43%|####3     | 13/30 [00:03<00:02,  8.42it/s, est. speed input: 2405.31 toks/s, output: 66.16 toks/s]
[2025-01-11 01:12:02,456][root][ERROR] - Processed prompts:  53%|#####3    | 16/30 [00:03<00:01,  9.11it/s, est. speed input: 2752.07 toks/s, output: 84.23 toks/s]
[2025-01-11 01:12:02,558][root][ERROR] - Processed prompts:  63%|######3   | 19/30 [00:03<00:00, 11.60it/s, est. speed input: 3186.21 toks/s, output: 107.02 toks/s]
[2025-01-11 01:12:02,707][root][ERROR] - Processed prompts:  73%|#######3  | 22/30 [00:04<00:00, 13.34it/s, est. speed input: 3556.96 toks/s, output: 130.60 toks/s]
[2025-01-11 01:12:02,861][root][ERROR] - Processed prompts:  83%|########3 | 25/30 [00:04<00:00, 14.75it/s, est. speed input: 3898.07 toks/s, output: 156.12 toks/s]
[2025-01-11 01:12:02,971][root][ERROR] - Processed prompts:  93%|#########3| 28/30 [00:04<00:00, 17.15it/s, est. speed input: 4256.89 toks/s, output: 186.06 toks/s]
[2025-01-11 01:12:03,549][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.10it/s, est. speed input: 4033.21 toks/s, output: 192.45 toks/s]
WARNING 01-11 01:12:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:03,999][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:06,995][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:02<01:20,  2.99s/it, est. speed input: 246.44 toks/s, output: 3.01 toks/s]
[2025-01-11 01:12:07,266][root][ERROR] - Processed prompts:   7%|7         | 2/28 [00:03<00:36,  1.39s/it, est. speed input: 448.80 toks/s, output: 7.04 toks/s]
[2025-01-11 01:12:07,532][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:21,  1.14it/s, est. speed input: 627.68 toks/s, output: 11.89 toks/s]
[2025-01-11 01:12:07,634][root][ERROR] - Processed prompts:  18%|#7        | 5/28 [00:03<00:09,  2.44it/s, est. speed input: 1021.59 toks/s, output: 22.84 toks/s]
[2025-01-11 01:12:07,916][root][ERROR] - Processed prompts:  29%|##8       | 8/28 [00:03<00:04,  4.22it/s, est. speed input: 1515.35 toks/s, output: 39.58 toks/s]
[2025-01-11 01:12:08,087][root][ERROR] - Processed prompts:  43%|####2     | 12/28 [00:04<00:02,  7.26it/s, est. speed input: 2191.79 toks/s, output: 66.80 toks/s]
[2025-01-11 01:12:08,275][root][ERROR] - Processed prompts:  50%|#####     | 14/28 [00:04<00:01,  7.94it/s, est. speed input: 2449.83 toks/s, output: 80.23 toks/s]
[2025-01-11 01:12:08,551][root][ERROR] - Processed prompts:  57%|#####7    | 16/28 [00:04<00:01,  7.73it/s, est. speed input: 2636.14 toks/s, output: 93.60 toks/s]
[2025-01-11 01:12:08,721][root][ERROR] - Processed prompts:  75%|#######5  | 21/28 [00:04<00:00, 12.20it/s, est. speed input: 3341.73 toks/s, output: 139.37 toks/s]
[2025-01-11 01:12:08,865][root][ERROR] - Processed prompts:  86%|########5 | 24/28 [00:04<00:00, 13.93it/s, est. speed input: 3730.33 toks/s, output: 168.15 toks/s]
[2025-01-11 01:12:08,988][root][ERROR] - Processed prompts:  93%|#########2| 26/28 [00:04<00:00, 14.37it/s, est. speed input: 3944.14 toks/s, output: 188.04 toks/s]
[2025-01-11 01:12:09,039][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:05<00:00,  5.56it/s, est. speed input: 4203.36 toks/s, output: 211.35 toks/s]
WARNING 01-11 01:12:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:09,286][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:10,244][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.04it/s, est. speed input: 680.68 toks/s, output: 21.92 toks/s]
[2025-01-11 01:12:10,384][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:00,  2.10it/s, est. speed input: 1188.26 toks/s, output: 44.65 toks/s]
[2025-01-11 01:12:10,679][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.54it/s, est. speed input: 1468.26 toks/s, output: 66.80 toks/s]
[2025-01-11 01:12:10,779][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.60it/s, est. speed input: 1874.98 toks/s, output: 95.79 toks/s]
[2025-01-11 01:12:10,779][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.68it/s, est. speed input: 1874.98 toks/s, output: 95.79 toks/s]
WARNING 01-11 01:12:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:11,005][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:12,857][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:18,  1.85s/it, est. speed input: 426.98 toks/s, output: 12.42 toks/s]
[2025-01-11 01:12:12,975][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:01<00:04,  1.91it/s, est. speed input: 1233.71 toks/s, output: 38.57 toks/s]
[2025-01-11 01:12:13,078][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  4.48it/s, est. speed input: 2355.15 toks/s, output: 79.12 toks/s]
[2025-01-11 01:12:13,259][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  5.73it/s, est. speed input: 2900.03 toks/s, output: 107.34 toks/s]
[2025-01-11 01:12:13,556][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  6.04it/s, est. speed input: 3200.72 toks/s, output: 136.42 toks/s]
[2025-01-11 01:12:13,573][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.28it/s, est. speed input: 3544.43 toks/s, output: 156.91 toks/s]
WARNING 01-11 01:12:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:13,819][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:16,707][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:02<00:57,  2.89s/it, est. speed input: 246.55 toks/s, output: 7.96 toks/s]
[2025-01-11 01:12:16,927][root][ERROR] - Processed prompts:  14%|#4        | 3/21 [00:03<00:14,  1.20it/s, est. speed input: 700.81 toks/s, output: 24.45 toks/s]
[2025-01-11 01:12:17,058][root][ERROR] - Processed prompts:  57%|#####7    | 12/21 [00:03<00:01,  6.41it/s, est. speed input: 2759.91 toks/s, output: 104.97 toks/s]
[2025-01-11 01:12:17,265][root][ERROR] - Processed prompts:  76%|#######6  | 16/21 [00:03<00:00,  8.32it/s, est. speed input: 3429.07 toks/s, output: 140.73 toks/s]
[2025-01-11 01:12:17,573][root][ERROR] - Processed prompts:  90%|######### | 19/21 [00:03<00:00,  8.66it/s, est. speed input: 3752.40 toks/s, output: 169.93 toks/s]
[2025-01-11 01:12:18,179][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:04<00:00,  4.82it/s, est. speed input: 3586.02 toks/s, output: 179.80 toks/s]
WARNING 01-11 01:12:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:18,427][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:21,066][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:44,  2.64s/it, est. speed input: 331.52 toks/s, output: 6.82 toks/s]
[2025-01-11 01:12:21,374][root][ERROR] - Processed prompts:  17%|#6        | 3/18 [00:02<00:11,  1.25it/s, est. speed input: 879.87 toks/s, output: 21.04 toks/s]
[2025-01-11 01:12:21,485][root][ERROR] - Processed prompts:  22%|##2       | 4/18 [00:03<00:07,  1.78it/s, est. speed input: 1189.06 toks/s, output: 29.76 toks/s]
[2025-01-11 01:12:21,735][root][ERROR] - Processed prompts:  78%|#######7  | 14/18 [00:03<00:00,  8.69it/s, est. speed input: 3952.42 toks/s, output: 118.51 toks/s]
[2025-01-11 01:12:21,852][root][ERROR] - Processed prompts:  94%|#########4| 17/18 [00:03<00:00, 10.51it/s, est. speed input: 4537.63 toks/s, output: 152.39 toks/s]
[2025-01-11 01:12:22,940][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  3.99it/s, est. speed input: 3622.03 toks/s, output: 140.27 toks/s]
WARNING 01-11 01:12:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:23,177][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:25,403][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:02<00:26,  2.23s/it, est. speed input: 381.98 toks/s, output: 12.58 toks/s]
[2025-01-11 01:12:25,509][root][ERROR] - Processed prompts:  62%|######1   | 8/13 [00:02<00:01,  4.63it/s, est. speed input: 2807.76 toks/s, output: 100.35 toks/s]
[2025-01-11 01:12:25,947][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:02<00:00,  5.86it/s, est. speed input: 3464.21 toks/s, output: 149.85 toks/s]
[2025-01-11 01:12:26,666][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:03<00:00,  3.73it/s, est. speed input: 2980.92 toks/s, output: 146.48 toks/s]
WARNING 01-11 01:12:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:26,959][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:29,000][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.04s/it, est. speed input: 501.56 toks/s, output: 14.20 toks/s]
[2025-01-11 01:12:29,245][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.81it/s, est. speed input: 2122.35 toks/s, output: 67.79 toks/s]
[2025-01-11 01:12:29,406][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.22it/s, est. speed input: 2322.12 toks/s, output: 82.13 toks/s]
[2025-01-11 01:12:29,736][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  3.17it/s, est. speed input: 2359.71 toks/s, output: 94.34 toks/s]
[2025-01-11 01:12:29,838][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  3.86it/s, est. speed input: 2594.18 toks/s, output: 113.94 toks/s]
[2025-01-11 01:12:30,043][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  5.21it/s, est. speed input: 3078.04 toks/s, output: 153.69 toks/s]
[2025-01-11 01:12:30,043][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.24it/s, est. speed input: 3078.04 toks/s, output: 153.69 toks/s]
WARNING 01-11 01:12:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:30,295][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:32,028][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.73s/it, est. speed input: 463.27 toks/s, output: 12.69 toks/s]
[2025-01-11 01:12:32,199][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:06,  1.23it/s, est. speed input: 844.95 toks/s, output: 26.26 toks/s]
[2025-01-11 01:12:32,558][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  6.17it/s, est. speed input: 3406.02 toks/s, output: 119.76 toks/s]
[2025-01-11 01:12:33,176][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.47it/s, est. speed input: 2958.75 toks/s, output: 123.21 toks/s]
WARNING 01-11 01:12:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:33,446][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:35,133][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.69s/it, est. speed input: 538.34 toks/s, output: 13.64 toks/s]
[2025-01-11 01:12:35,292][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.27it/s, est. speed input: 1064.41 toks/s, output: 28.18 toks/s]
[2025-01-11 01:12:35,719][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.80it/s, est. speed input: 2725.89 toks/s, output: 83.16 toks/s]
[2025-01-11 01:12:35,848][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.27it/s, est. speed input: 3001.66 toks/s, output: 102.41 toks/s]
[2025-01-11 01:12:36,001][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.65it/s, est. speed input: 3294.52 toks/s, output: 122.15 toks/s]
[2025-01-11 01:12:36,001][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.13it/s, est. speed input: 3294.52 toks/s, output: 122.15 toks/s]
WARNING 01-11 01:12:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:36,263][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:37,741][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.48s/it, est. speed input: 627.27 toks/s, output: 16.24 toks/s]
[2025-01-11 01:12:37,858][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.35it/s, est. speed input: 1663.98 toks/s, output: 48.92 toks/s]
[2025-01-11 01:12:38,741][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  2.90it/s, est. speed input: 2202.29 toks/s, output: 85.98 toks/s]
[2025-01-11 01:12:38,926][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.23it/s, est. speed input: 2406.23 toks/s, output: 113.03 toks/s]
[2025-01-11 01:12:38,927][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.63it/s, est. speed input: 2406.23 toks/s, output: 113.03 toks/s]
WARNING 01-11 01:12:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:39,183][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:40,064][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.14it/s, est. speed input: 1126.45 toks/s, output: 14.76 toks/s]
[2025-01-11 01:12:40,206][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:00,  2.24it/s, est. speed input: 1910.60 toks/s, output: 32.28 toks/s]
[2025-01-11 01:12:40,372][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  3.14it/s, est. speed input: 2588.80 toks/s, output: 52.18 toks/s]
[2025-01-11 01:12:40,372][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.37it/s, est. speed input: 3338.04 toks/s, output: 76.57 toks/s]
WARNING 01-11 01:12:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:40,589][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:41,936][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.35s/it, est. speed input: 786.39 toks/s, output: 21.53 toks/s]
[2025-01-11 01:12:42,121][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.28it/s, est. speed input: 2496.31 toks/s, output: 82.27 toks/s]
[2025-01-11 01:12:42,694][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.63it/s, est. speed input: 2307.75 toks/s, output: 94.53 toks/s]
[2025-01-11 01:12:42,694][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.37it/s, est. speed input: 2307.75 toks/s, output: 94.53 toks/s]
WARNING 01-11 01:12:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:42,926][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:44,041][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 1024.36 toks/s, output: 26.03 toks/s]
[2025-01-11 01:12:44,250][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.73it/s, est. speed input: 2399.61 toks/s, output: 77.84 toks/s]
[2025-01-11 01:12:44,250][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.27it/s, est. speed input: 2399.61 toks/s, output: 77.84 toks/s]
WARNING 01-11 01:12:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:44,461][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:45,385][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.08it/s, est. speed input: 987.55 toks/s, output: 24.88 toks/s]
[2025-01-11 01:12:45,533][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  2.14it/s, est. speed input: 1648.83 toks/s, output: 50.36 toks/s]
[2025-01-11 01:12:46,421][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.52it/s, est. speed input: 1333.52 toks/s, output: 70.40 toks/s]
[2025-01-11 01:12:46,421][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.53it/s, est. speed input: 1333.52 toks/s, output: 70.40 toks/s]
WARNING 01-11 01:12:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:46,640][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:47,750][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.11s/it, est. speed input: 985.19 toks/s, output: 26.14 toks/s]
[2025-01-11 01:12:47,935][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.81it/s, est. speed input: 2529.36 toks/s, output: 75.71 toks/s]
[2025-01-11 01:12:47,935][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.32it/s, est. speed input: 2529.36 toks/s, output: 75.71 toks/s]
WARNING 01-11 01:12:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:48,145][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:49,691][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.55s/it, est. speed input: 598.46 toks/s, output: 40.76 toks/s]
[2025-01-11 01:12:49,776][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.23it/s, est. speed input: 1161.70 toks/s, output: 80.35 toks/s]
WARNING 01-11 01:12:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:50,001][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:51,118][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 1021.92 toks/s, output: 25.97 toks/s]
[2025-01-11 01:12:51,118][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.69it/s, est. speed input: 3006.53 toks/s, output: 77.89 toks/s]
WARNING 01-11 01:12:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:12:51,346][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:12:52,471][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 972.41 toks/s, output: 43.59 toks/s]
[2025-01-11 01:12:52,471][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 972.41 toks/s, output: 43.59 toks/s]
[2025-01-11 01:12:54,991][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:12:55,044][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:12:56,711][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 01:12:58,326][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 01:12:59,978][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 01:13:00,508][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:13:00,508][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 01:13:21,419][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:13:21,471][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:13:23,522][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.05s/it]
[2025-01-11 01:13:25,156][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 01:13:26,717][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 01:13:27,228][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:13:27,229][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 01:13:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:13:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:13:46 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:13:46 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:13:46,955][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:13:48,319][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 01:13:48,720][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.25it/s]
[2025-01-11 01:13:50,063][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.05s/it]
[2025-01-11 01:13:51,459][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
[2025-01-11 01:13:51,460][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
INFO 01-11 01:13:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:14:05 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:14:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:14:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:14:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:14:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:28,324][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:32,219][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.89s/it, est. speed input: 163.04 toks/s, output: 2.82 toks/s]
[2025-01-11 01:14:32,393][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:21,  1.28it/s, est. speed input: 624.36 toks/s, output: 11.55 toks/s]
[2025-01-11 01:14:32,561][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:12,  2.09it/s, est. speed input: 899.33 toks/s, output: 19.12 toks/s]
[2025-01-11 01:14:32,718][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:07,  3.07it/s, est. speed input: 1156.30 toks/s, output: 27.09 toks/s]
[2025-01-11 01:14:32,858][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  8.03it/s, est. speed input: 2101.21 toks/s, output: 59.78 toks/s]
[2025-01-11 01:14:32,971][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 12.91it/s, est. speed input: 2870.04 toks/s, output: 90.61 toks/s]
[2025-01-11 01:14:33,172][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.42it/s, est. speed input: 3274.82 toks/s, output: 111.60 toks/s]
[2025-01-11 01:14:33,412][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:05<00:00, 13.89it/s, est. speed input: 3494.44 toks/s, output: 129.52 toks/s]
[2025-01-11 01:14:33,658][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:05<00:00, 13.41it/s, est. speed input: 3690.45 toks/s, output: 150.73 toks/s]
[2025-01-11 01:14:33,776][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.87it/s, est. speed input: 3727.62 toks/s, output: 159.05 toks/s]
WARNING 01-11 01:14:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:33,990][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:34,918][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 743.88 toks/s, output: 33.42 toks/s]
[2025-01-11 01:14:35,235][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.76it/s, est. speed input: 1136.26 toks/s, output: 65.04 toks/s]
[2025-01-11 01:14:35,235][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.61it/s, est. speed input: 1136.26 toks/s, output: 65.04 toks/s]
WARNING 01-11 01:14:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:35,479][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:38,479][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:27,  3.00s/it, est. speed input: 215.32 toks/s, output: 4.33 toks/s]
[2025-01-11 01:14:38,653][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:37,  1.34s/it, est. speed input: 414.38 toks/s, output: 9.14 toks/s]
[2025-01-11 01:14:38,821][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:21,  1.24it/s, est. speed input: 589.52 toks/s, output: 14.36 toks/s]
[2025-01-11 01:14:38,977][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:03<00:07,  3.27it/s, est. speed input: 1124.63 toks/s, output: 31.16 toks/s]
[2025-01-11 01:14:39,269][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:05,  4.10it/s, est. speed input: 1385.46 toks/s, output: 41.96 toks/s]
[2025-01-11 01:14:39,452][root][ERROR] - Processed prompts:  37%|###6      | 11/30 [00:03<00:03,  6.20it/s, est. speed input: 1817.05 toks/s, output: 62.93 toks/s]
[2025-01-11 01:14:39,579][root][ERROR] - Processed prompts:  43%|####3     | 13/30 [00:04<00:02,  7.59it/s, est. speed input: 2080.65 toks/s, output: 77.57 toks/s]
[2025-01-11 01:14:39,699][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:04<00:01,  9.07it/s, est. speed input: 2336.11 toks/s, output: 93.13 toks/s]
[2025-01-11 01:14:39,825][root][ERROR] - Processed prompts:  77%|#######6  | 23/30 [00:04<00:00, 19.80it/s, est. speed input: 3488.17 toks/s, output: 164.05 toks/s]
[2025-01-11 01:14:40,080][root][ERROR] - Processed prompts:  87%|########6 | 26/30 [00:04<00:00, 16.88it/s, est. speed input: 3723.36 toks/s, output: 185.60 toks/s]
[2025-01-11 01:14:40,472][root][ERROR] - Processed prompts:  97%|#########6| 29/30 [00:04<00:00, 12.80it/s, est. speed input: 3835.41 toks/s, output: 210.11 toks/s]
[2025-01-11 01:14:40,556][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:05<00:00,  5.91it/s, est. speed input: 3902.88 toks/s, output: 222.00 toks/s]
WARNING 01-11 01:14:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:40,822][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:44,118][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:25,  3.30s/it, est. speed input: 228.74 toks/s, output: 5.16 toks/s]
[2025-01-11 01:14:44,332][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:37,  1.48s/it, est. speed input: 427.95 toks/s, output: 10.83 toks/s]
[2025-01-11 01:14:44,487][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:21,  1.14it/s, est. speed input: 615.25 toks/s, output: 16.92 toks/s]
[2025-01-11 01:14:44,638][root][ERROR] - Processed prompts:  15%|#4        | 4/27 [00:03<00:13,  1.69it/s, est. speed input: 787.60 toks/s, output: 23.32 toks/s]
[2025-01-11 01:14:44,779][root][ERROR] - Processed prompts:  37%|###7      | 10/27 [00:03<00:02,  6.55it/s, est. speed input: 1943.22 toks/s, output: 66.72 toks/s]
[2025-01-11 01:14:45,179][root][ERROR] - Processed prompts:  44%|####4     | 12/27 [00:04<00:02,  6.04it/s, est. speed input: 2116.11 toks/s, output: 78.26 toks/s]
[2025-01-11 01:14:45,354][root][ERROR] - Processed prompts:  56%|#####5    | 15/27 [00:04<00:01,  7.92it/s, est. speed input: 2526.23 toks/s, output: 103.48 toks/s]
[2025-01-11 01:14:45,513][root][ERROR] - Processed prompts:  63%|######2   | 17/27 [00:04<00:01,  8.75it/s, est. speed input: 2774.01 toks/s, output: 120.43 toks/s]
[2025-01-11 01:14:45,684][root][ERROR] - Processed prompts:  70%|#######   | 19/27 [00:04<00:00,  9.40it/s, est. speed input: 2996.42 toks/s, output: 138.01 toks/s]
[2025-01-11 01:14:45,930][root][ERROR] - Processed prompts:  81%|########1 | 22/27 [00:05<00:00, 10.26it/s, est. speed input: 3308.76 toks/s, output: 166.60 toks/s]
[2025-01-11 01:14:46,037][root][ERROR] - Processed prompts:  93%|#########2| 25/27 [00:05<00:00, 13.10it/s, est. speed input: 3700.94 toks/s, output: 203.04 toks/s]
[2025-01-11 01:14:46,344][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00, 10.49it/s, est. speed input: 3764.03 toks/s, output: 221.27 toks/s]
[2025-01-11 01:14:46,345][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  4.89it/s, est. speed input: 3764.03 toks/s, output: 221.27 toks/s]
WARNING 01-11 01:14:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:46,580][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:47,898][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.32s/it, est. speed input: 505.56 toks/s, output: 25.05 toks/s]
[2025-01-11 01:14:48,043][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.53it/s, est. speed input: 1442.30 toks/s, output: 75.19 toks/s]
[2025-01-11 01:14:48,216][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.23it/s, est. speed input: 2154.82 toks/s, output: 124.13 toks/s]
[2025-01-11 01:14:48,216][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.06it/s, est. speed input: 2154.82 toks/s, output: 124.13 toks/s]
WARNING 01-11 01:14:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:48,447][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:50,352][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.90s/it, est. speed input: 468.78 toks/s, output: 15.22 toks/s]
[2025-01-11 01:14:50,544][root][ERROR] - Processed prompts:  30%|###       | 3/10 [00:02<00:03,  1.77it/s, est. speed input: 1147.41 toks/s, output: 45.78 toks/s]
[2025-01-11 01:14:50,645][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  5.04it/s, est. speed input: 2597.36 toks/s, output: 115.12 toks/s]
[2025-01-11 01:14:51,046][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  5.84it/s, est. speed input: 3180.30 toks/s, output: 155.82 toks/s]
[2025-01-11 01:14:51,046][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.85it/s, est. speed input: 3180.30 toks/s, output: 155.82 toks/s]
WARNING 01-11 01:14:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:51,282][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:54,014][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:02<00:54,  2.73s/it, est. speed input: 266.46 toks/s, output: 6.59 toks/s]
[2025-01-11 01:14:54,282][root][ERROR] - Processed prompts:  10%|9         | 2/21 [00:03<00:24,  1.28s/it, est. speed input: 482.66 toks/s, output: 14.00 toks/s]
[2025-01-11 01:14:54,413][root][ERROR] - Processed prompts:  14%|#4        | 3/21 [00:03<00:13,  1.32it/s, est. speed input: 694.64 toks/s, output: 22.04 toks/s]
[2025-01-11 01:14:54,532][root][ERROR] - Processed prompts:  43%|####2     | 9/21 [00:03<00:02,  5.89it/s, est. speed input: 2189.12 toks/s, output: 75.08 toks/s]
[2025-01-11 01:14:55,006][root][ERROR] - Processed prompts:  57%|#####7    | 12/21 [00:03<00:01,  6.04it/s, est. speed input: 2520.81 toks/s, output: 98.55 toks/s]
[2025-01-11 01:14:55,112][root][ERROR] - Processed prompts:  76%|#######6  | 16/21 [00:03<00:00,  9.20it/s, est. speed input: 3236.98 toks/s, output: 145.19 toks/s]
[2025-01-11 01:14:55,600][root][ERROR] - Processed prompts:  90%|######### | 19/21 [00:04<00:00,  7.99it/s, est. speed input: 3429.50 toks/s, output: 173.00 toks/s]
[2025-01-11 01:14:56,367][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  5.46it/s, est. speed input: 3223.55 toks/s, output: 185.47 toks/s]
[2025-01-11 01:14:56,367][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  4.13it/s, est. speed input: 3223.55 toks/s, output: 185.47 toks/s]
WARNING 01-11 01:14:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:14:56,623][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:14:59,470][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:48,  2.85s/it, est. speed input: 315.73 toks/s, output: 8.08 toks/s]
[2025-01-11 01:14:59,624][root][ERROR] - Processed prompts:  17%|#6        | 3/18 [00:03<00:11,  1.26it/s, est. speed input: 841.27 toks/s, output: 24.32 toks/s]
[2025-01-11 01:14:59,996][root][ERROR] - Processed prompts:  50%|#####     | 9/18 [00:03<00:02,  4.21it/s, est. speed input: 2427.50 toks/s, output: 76.20 toks/s]
[2025-01-11 01:15:00,362][root][ERROR] - Processed prompts:  61%|######1   | 11/18 [00:03<00:01,  4.48it/s, est. speed input: 2671.83 toks/s, output: 95.75 toks/s]
[2025-01-11 01:15:00,493][root][ERROR] - Processed prompts:  67%|######6   | 12/18 [00:03<00:01,  4.80it/s, est. speed input: 2796.07 toks/s, output: 107.22 toks/s]
[2025-01-11 01:15:00,675][root][ERROR] - Processed prompts:  83%|########3 | 15/18 [00:04<00:00,  6.74it/s, est. speed input: 3298.37 toks/s, output: 147.82 toks/s]
[2025-01-11 01:15:00,808][root][ERROR] - Processed prompts:  94%|#########4| 17/18 [00:04<00:00,  7.97it/s, est. speed input: 3637.20 toks/s, output: 176.34 toks/s]
[2025-01-11 01:15:01,753][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:05<00:00,  3.51it/s, est. speed input: 3162.96 toks/s, output: 168.80 toks/s]
WARNING 01-11 01:15:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:01,982][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:03,328][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:13,  1.35s/it, est. speed input: 591.25 toks/s, output: 5.20 toks/s]
[2025-01-11 01:15:03,683][root][ERROR] - Processed prompts:  18%|#8        | 2/11 [00:01<00:06,  1.31it/s, est. speed input: 867.62 toks/s, output: 15.28 toks/s]
[2025-01-11 01:15:03,822][root][ERROR] - Processed prompts:  36%|###6      | 4/11 [00:01<00:02,  3.05it/s, est. speed input: 1623.21 toks/s, output: 38.60 toks/s]
[2025-01-11 01:15:03,953][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:01<00:01,  3.74it/s, est. speed input: 1928.75 toks/s, output: 50.74 toks/s]
[2025-01-11 01:15:04,152][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  7.79it/s, est. speed input: 3424.03 toks/s, output: 104.16 toks/s]
[2025-01-11 01:15:04,422][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  7.67it/s, est. speed input: 3677.13 toks/s, output: 135.26 toks/s]
[2025-01-11 01:15:04,422][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.51it/s, est. speed input: 3677.13 toks/s, output: 135.26 toks/s]
WARNING 01-11 01:15:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:04,722][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:06,966][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.24s/it, est. speed input: 398.34 toks/s, output: 11.58 toks/s]
[2025-01-11 01:15:07,118][root][ERROR] - Processed prompts:  33%|###3      | 4/12 [00:02<00:03,  2.15it/s, est. speed input: 1605.98 toks/s, output: 47.98 toks/s]
[2025-01-11 01:15:07,489][root][ERROR] - Processed prompts:  50%|#####     | 6/12 [00:02<00:02,  2.91it/s, est. speed input: 2091.09 toks/s, output: 70.82 toks/s]
[2025-01-11 01:15:07,742][root][ERROR] - Processed prompts:  67%|######6   | 8/12 [00:03<00:01,  3.84it/s, est. speed input: 2544.09 toks/s, output: 98.66 toks/s]
[2025-01-11 01:15:08,051][root][ERROR] - Processed prompts:  83%|########3 | 10/12 [00:03<00:00,  4.49it/s, est. speed input: 2902.20 toks/s, output: 128.87 toks/s]
[2025-01-11 01:15:08,430][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  4.73it/s, est. speed input: 3087.03 toks/s, output: 161.28 toks/s]
[2025-01-11 01:15:08,430][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.24it/s, est. speed input: 3087.03 toks/s, output: 161.28 toks/s]
WARNING 01-11 01:15:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:08,687][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:10,791][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.10s/it, est. speed input: 395.87 toks/s, output: 13.78 toks/s]
[2025-01-11 01:15:10,896][root][ERROR] - Processed prompts:  45%|####5     | 5/11 [00:02<00:02,  2.98it/s, est. speed input: 1928.45 toks/s, output: 67.43 toks/s]
[2025-01-11 01:15:11,361][root][ERROR] - Processed prompts:  73%|#######2  | 8/11 [00:02<00:00,  4.01it/s, est. speed input: 2542.15 toks/s, output: 108.80 toks/s]
[2025-01-11 01:15:11,519][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  5.10it/s, est. speed input: 3092.55 toks/s, output: 145.11 toks/s]
[2025-01-11 01:15:12,007][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:03<00:00,  3.31it/s, est. speed input: 2922.09 toks/s, output: 150.89 toks/s]
WARNING 01-11 01:15:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:12,261][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:14,051][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.79s/it, est. speed input: 529.74 toks/s, output: 12.29 toks/s]
[2025-01-11 01:15:14,241][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:01<00:05,  1.18it/s, est. speed input: 1057.62 toks/s, output: 25.76 toks/s]
[2025-01-11 01:15:14,375][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  6.29it/s, est. speed input: 3873.75 toks/s, output: 110.73 toks/s]
[2025-01-11 01:15:14,813][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.53it/s, est. speed input: 3627.04 toks/s, output: 115.99 toks/s]
WARNING 01-11 01:15:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:15,036][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:16,904][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:16,  1.87s/it, est. speed input: 406.95 toks/s, output: 12.85 toks/s]
[2025-01-11 01:15:17,047][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:06,  1.17it/s, est. speed input: 855.28 toks/s, output: 26.35 toks/s]
[2025-01-11 01:15:17,300][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:00,  4.09it/s, est. speed input: 2568.65 toks/s, output: 79.52 toks/s]
[2025-01-11 01:15:17,618][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.67it/s, est. speed input: 2874.55 toks/s, output: 106.89 toks/s]
[2025-01-11 01:15:17,941][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  5.09it/s, est. speed input: 3161.08 toks/s, output: 140.79 toks/s]
[2025-01-11 01:15:17,941][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.44it/s, est. speed input: 3161.08 toks/s, output: 140.79 toks/s]
WARNING 01-11 01:15:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:18,189][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:19,684][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.49s/it, est. speed input: 774.72 toks/s, output: 19.40 toks/s]
[2025-01-11 01:15:19,684][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.34it/s, est. speed input: 4058.45 toks/s, output: 96.98 toks/s]
WARNING 01-11 01:15:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:19,906][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:21,899][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.99s/it, est. speed input: 402.84 toks/s, output: 14.55 toks/s]
[2025-01-11 01:15:22,198][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  5.14it/s, est. speed input: 3640.32 toks/s, output: 121.74 toks/s]
[2025-01-11 01:15:22,853][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.39it/s, est. speed input: 3121.83 toks/s, output: 123.18 toks/s]
WARNING 01-11 01:15:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:23,136][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:24,612][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.48s/it, est. speed input: 823.22 toks/s, output: 18.97 toks/s]
[2025-01-11 01:15:25,023][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.25it/s, est. speed input: 3211.07 toks/s, output: 88.49 toks/s]
[2025-01-11 01:15:25,023][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.65it/s, est. speed input: 3211.07 toks/s, output: 88.49 toks/s]
WARNING 01-11 01:15:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:25,253][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:26,469][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.22s/it, est. speed input: 806.10 toks/s, output: 36.19 toks/s]
[2025-01-11 01:15:26,487][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.62it/s, est. speed input: 1576.42 toks/s, output: 72.17 toks/s]
WARNING 01-11 01:15:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:26,729][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:27,707][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.02it/s, est. speed input: 1010.09 toks/s, output: 22.51 toks/s]
[2025-01-11 01:15:28,204][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.44it/s, est. speed input: 1341.72 toks/s, output: 48.16 toks/s]
[2025-01-11 01:15:28,407][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.13it/s, est. speed input: 1952.42 toks/s, output: 78.69 toks/s]
[2025-01-11 01:15:28,407][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.79it/s, est. speed input: 1952.42 toks/s, output: 78.69 toks/s]
WARNING 01-11 01:15:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:15:28,631][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:15:29,741][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.11s/it, est. speed input: 925.83 toks/s, output: 34.22 toks/s]
[2025-01-11 01:15:30,129][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.46it/s, est. speed input: 1406.31 toks/s, output: 66.08 toks/s]
[2025-01-11 01:15:30,129][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.33it/s, est. speed input: 1406.31 toks/s, output: 66.08 toks/s]
[2025-01-11 01:15:32,422][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:15:32,474][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:15:34,097][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 01:15:35,683][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.60s/it]
[2025-01-11 01:15:37,237][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.58s/it]
[2025-01-11 01:15:37,758][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.16s/it]
[2025-01-11 01:15:37,758][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.32s/it]
[2025-01-11 01:15:59,243][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:15:59,295][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:16:01,371][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.08s/it]
[2025-01-11 01:16:02,972][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 01:16:04,585][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 01:16:05,126][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 01:16:05,126][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 01:16:24 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:16:24 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:16:25 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:16:25 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:16:25,245][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:16:26,605][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 01:16:27,008][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 01:16:28,312][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:16:29,648][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:16:29,648][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:16:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:16:43 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:16:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:16:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:17:06 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 01:17:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:07,160][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:10,977][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.38 toks/s, output: 2.88 toks/s]
[2025-01-11 01:17:11,150][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.30it/s, est. speed input: 636.61 toks/s, output: 11.78 toks/s]
[2025-01-11 01:17:11,318][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:12,  2.13it/s, est. speed input: 916.26 toks/s, output: 19.48 toks/s]
[2025-01-11 01:17:11,475][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:07,  3.12it/s, est. speed input: 1177.41 toks/s, output: 27.58 toks/s]
[2025-01-11 01:17:11,614][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  8.14it/s, est. speed input: 2138.39 toks/s, output: 60.84 toks/s]
[2025-01-11 01:17:11,757][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 13.66it/s, est. speed input: 3038.77 toks/s, output: 97.23 toks/s]
[2025-01-11 01:17:11,928][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 15.48it/s, est. speed input: 3462.48 toks/s, output: 120.17 toks/s]
[2025-01-11 01:17:12,089][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 17.37it/s, est. speed input: 3864.73 toks/s, output: 147.29 toks/s]
[2025-01-11 01:17:12,233][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.31it/s, est. speed input: 4005.54 toks/s, output: 161.44 toks/s]
WARNING 01-11 01:17:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:12,449][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:13,379][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 742.02 toks/s, output: 41.94 toks/s]
[2025-01-11 01:17:13,379][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 742.02 toks/s, output: 41.94 toks/s]
WARNING 01-11 01:17:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:13,625][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:16,602][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:29,  2.98s/it, est. speed input: 224.10 toks/s, output: 3.70 toks/s]
[2025-01-11 01:17:16,954][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:41,  1.43s/it, est. speed input: 394.12 toks/s, output: 8.41 toks/s]
[2025-01-11 01:17:17,067][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.76it/s, est. speed input: 761.63 toks/s, output: 18.60 toks/s]
[2025-01-11 01:17:17,321][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:05,  4.08it/s, est. speed input: 1424.48 toks/s, output: 39.78 toks/s]
[2025-01-11 01:17:17,561][root][ERROR] - Processed prompts:  32%|###2      | 10/31 [00:03<00:04,  4.86it/s, est. speed input: 1674.17 toks/s, output: 51.57 toks/s]
[2025-01-11 01:17:17,791][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:04<00:03,  5.63it/s, est. speed input: 1900.07 toks/s, output: 65.05 toks/s]
[2025-01-11 01:17:17,916][root][ERROR] - Processed prompts:  48%|####8     | 15/31 [00:04<00:01,  8.11it/s, est. speed input: 2302.33 toks/s, output: 88.09 toks/s]
[2025-01-11 01:17:18,062][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:04<00:01,  9.13it/s, est. speed input: 2521.12 toks/s, output: 102.77 toks/s]
[2025-01-11 01:17:18,164][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:01, 10.76it/s, est. speed input: 2755.58 toks/s, output: 119.19 toks/s]
[2025-01-11 01:17:18,430][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:04<00:00, 15.08it/s, est. speed input: 3431.92 toks/s, output: 170.67 toks/s]
[2025-01-11 01:17:18,547][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:04<00:00, 17.05it/s, est. speed input: 3756.02 toks/s, output: 201.96 toks/s]
[2025-01-11 01:17:18,933][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00, 12.70it/s, est. speed input: 3853.79 toks/s, output: 227.03 toks/s]
[2025-01-11 01:17:18,933][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.84it/s, est. speed input: 3853.79 toks/s, output: 227.03 toks/s]
WARNING 01-11 01:17:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:19,186][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:22,381][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:23,  3.19s/it, est. speed input: 250.40 toks/s, output: 4.69 toks/s]
[2025-01-11 01:17:22,647][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:36,  1.47s/it, est. speed input: 441.49 toks/s, output: 10.11 toks/s]
[2025-01-11 01:17:22,957][root][ERROR] - Processed prompts:  11%|#1        | 3/27 [00:03<00:22,  1.06it/s, est. speed input: 601.45 toks/s, output: 16.18 toks/s]
[2025-01-11 01:17:23,104][root][ERROR] - Processed prompts:  22%|##2       | 6/27 [00:03<00:07,  2.85it/s, est. speed input: 1196.84 toks/s, output: 37.00 toks/s]
[2025-01-11 01:17:23,216][root][ERROR] - Processed prompts:  48%|####8     | 13/27 [00:04<00:01,  8.30it/s, est. speed input: 2504.54 toks/s, output: 87.33 toks/s]
[2025-01-11 01:17:23,391][root][ERROR] - Processed prompts:  59%|#####9    | 16/27 [00:04<00:01,  9.81it/s, est. speed input: 2944.39 toks/s, output: 109.39 toks/s]
[2025-01-11 01:17:23,602][root][ERROR] - Processed prompts:  70%|#######   | 19/27 [00:04<00:00, 10.80it/s, est. speed input: 3319.19 toks/s, output: 132.69 toks/s]
[2025-01-11 01:17:23,841][root][ERROR] - Processed prompts:  81%|########1 | 22/27 [00:04<00:00, 11.27it/s, est. speed input: 3651.62 toks/s, output: 159.40 toks/s]
[2025-01-11 01:17:24,305][root][ERROR] - Processed prompts:  89%|########8 | 24/27 [00:05<00:00,  8.30it/s, est. speed input: 3624.34 toks/s, output: 171.70 toks/s]
[2025-01-11 01:17:24,526][root][ERROR] - Processed prompts:  96%|#########6| 26/27 [00:05<00:00,  8.47it/s, est. speed input: 3764.06 toks/s, output: 194.56 toks/s]
[2025-01-11 01:17:24,526][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  5.06it/s, est. speed input: 3906.59 toks/s, output: 210.66 toks/s]
WARNING 01-11 01:17:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:24,784][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:26,093][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.31s/it, est. speed input: 514.10 toks/s, output: 24.44 toks/s]
[2025-01-11 01:17:26,337][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.33it/s, est. speed input: 1395.55 toks/s, output: 71.45 toks/s]
[2025-01-11 01:17:26,629][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.52it/s, est. speed input: 1987.47 toks/s, output: 119.24 toks/s]
[2025-01-11 01:17:26,629][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.71it/s, est. speed input: 1987.47 toks/s, output: 119.24 toks/s]
WARNING 01-11 01:17:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:26,866][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:28,483][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.62s/it, est. speed input: 470.65 toks/s, output: 16.70 toks/s]
[2025-01-11 01:17:28,821][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.20it/s, est. speed input: 2194.16 toks/s, output: 80.85 toks/s]
[2025-01-11 01:17:28,921][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.80it/s, est. speed input: 2510.99 toks/s, output: 99.78 toks/s]
[2025-01-11 01:17:29,761][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.07it/s, est. speed input: 2359.94 toks/s, output: 121.59 toks/s]
[2025-01-11 01:17:29,761][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.76it/s, est. speed input: 2359.94 toks/s, output: 121.59 toks/s]
WARNING 01-11 01:17:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:30,003][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:33,511][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:17,  3.51s/it, est. speed input: 257.42 toks/s, output: 8.27 toks/s]
[2025-01-11 01:17:33,687][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:03<00:00,  6.72it/s, est. speed input: 3922.26 toks/s, output: 144.66 toks/s]
[2025-01-11 01:17:35,040][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.57it/s, est. speed input: 3627.08 toks/s, output: 168.53 toks/s]
WARNING 01-11 01:17:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:35,331][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:37,865][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:35,  2.53s/it, est. speed input: 320.03 toks/s, output: 9.08 toks/s]
[2025-01-11 01:17:38,081][root][ERROR] - Processed prompts:  13%|#3        | 2/15 [00:02<00:15,  1.17s/it, est. speed input: 650.67 toks/s, output: 18.91 toks/s]
[2025-01-11 01:17:38,542][root][ERROR] - Processed prompts:  87%|########6 | 13/15 [00:03<00:00,  6.58it/s, est. speed input: 3907.04 toks/s, output: 122.71 toks/s]
[2025-01-11 01:17:38,851][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  6.55it/s, est. speed input: 4090.71 toks/s, output: 147.72 toks/s]
[2025-01-11 01:17:38,852][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  4.26it/s, est. speed input: 4090.71 toks/s, output: 147.72 toks/s]
WARNING 01-11 01:17:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:39,068][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:40,732][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.66s/it, est. speed input: 457.47 toks/s, output: 17.43 toks/s]
[2025-01-11 01:17:41,141][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.62it/s, est. speed input: 2480.86 toks/s, output: 95.57 toks/s]
[2025-01-11 01:17:41,326][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.88it/s, est. speed input: 2726.34 toks/s, output: 115.18 toks/s]
[2025-01-11 01:17:41,326][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.10it/s, est. speed input: 2726.34 toks/s, output: 115.18 toks/s]
WARNING 01-11 01:17:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:41,619][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:42,974][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.36s/it, est. speed input: 795.52 toks/s, output: 21.40 toks/s]
[2025-01-11 01:17:43,116][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.48it/s, est. speed input: 2034.24 toks/s, output: 62.82 toks/s]
[2025-01-11 01:17:43,245][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.24it/s, est. speed input: 2397.09 toks/s, output: 84.27 toks/s]
[2025-01-11 01:17:43,532][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.32it/s, est. speed input: 2520.07 toks/s, output: 103.00 toks/s]
[2025-01-11 01:17:43,532][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.61it/s, est. speed input: 2520.07 toks/s, output: 103.00 toks/s]
WARNING 01-11 01:17:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:43,769][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:45,110][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.34s/it, est. speed input: 583.91 toks/s, output: 21.63 toks/s]
[2025-01-11 01:17:45,416][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.78it/s, est. speed input: 2808.48 toks/s, output: 99.61 toks/s]
[2025-01-11 01:17:45,416][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.04it/s, est. speed input: 2808.48 toks/s, output: 99.61 toks/s]
WARNING 01-11 01:17:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:45,668][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:46,817][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.15s/it, est. speed input: 909.17 toks/s, output: 25.25 toks/s]
[2025-01-11 01:17:47,019][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.68it/s, est. speed input: 2590.98 toks/s, output: 73.29 toks/s]
[2025-01-11 01:17:47,019][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.22it/s, est. speed input: 2590.98 toks/s, output: 73.29 toks/s]
WARNING 01-11 01:17:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:47,242][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:48,323][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 776.54 toks/s, output: 26.84 toks/s]
[2025-01-11 01:17:48,672][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.54it/s, est. speed input: 1144.61 toks/s, output: 53.84 toks/s]
[2025-01-11 01:17:49,158][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.74it/s, est. speed input: 1335.57 toks/s, output: 80.37 toks/s]
[2025-01-11 01:17:49,159][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.57it/s, est. speed input: 1335.57 toks/s, output: 80.37 toks/s]
WARNING 01-11 01:17:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:49,397][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:50,203][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1472.89 toks/s, output: 35.98 toks/s]
[2025-01-11 01:17:50,204][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1472.89 toks/s, output: 35.98 toks/s]
WARNING 01-11 01:17:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:50,415][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:52,120][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.70s/it, est. speed input: 605.93 toks/s, output: 42.23 toks/s]
[2025-01-11 01:17:52,272][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.26it/s, est. speed input: 1033.26 toks/s, output: 82.42 toks/s]
[2025-01-11 01:17:52,272][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.08it/s, est. speed input: 1033.26 toks/s, output: 82.42 toks/s]
WARNING 01-11 01:17:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:52,492][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:53,449][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 1247.03 toks/s, output: 29.27 toks/s]
[2025-01-11 01:17:53,534][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.92it/s, est. speed input: 2026.72 toks/s, output: 58.59 toks/s]
WARNING 01-11 01:17:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:17:53,752][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:17:54,816][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.06s/it, est. speed input: 1199.12 toks/s, output: 32.92 toks/s]
[2025-01-11 01:17:54,833][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.85it/s, est. speed input: 2076.44 toks/s, output: 65.70 toks/s]
[2025-01-11 01:17:57,072][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:17:57,124][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:17:58,717][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.59s/it]
[2025-01-11 01:18:00,389][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 01:18:02,042][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 01:18:02,582][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:18:02,582][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 01:18:23,022][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:18:23,073][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:18:25,121][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.05s/it]
[2025-01-11 01:18:26,767][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 01:18:28,407][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.73s/it]
[2025-01-11 01:18:28,910][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 01:18:28,911][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 01:18:48 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:18:48 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:18:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:18:49 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:18:49,283][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:18:50,613][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 01:18:51,018][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 01:18:52,419][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.07s/it]
[2025-01-11 01:18:53,781][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
[2025-01-11 01:18:53,781][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
INFO 01-11 01:18:54 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:19:07 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:19:08 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:19:08 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:19:30 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:19:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:30,853][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:34,421][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:50,  3.57s/it, est. speed input: 177.99 toks/s, output: 3.08 toks/s]
[2025-01-11 01:19:34,767][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.30it/s, est. speed input: 649.05 toks/s, output: 12.78 toks/s]
[2025-01-11 01:19:34,872][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:07,  3.15it/s, est. speed input: 1264.15 toks/s, output: 30.11 toks/s]
[2025-01-11 01:19:35,018][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:03,  5.34it/s, est. speed input: 1829.82 toks/s, output: 49.23 toks/s]
[2025-01-11 01:19:35,132][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 11.29it/s, est. speed input: 2968.55 toks/s, output: 91.39 toks/s]
[2025-01-11 01:19:35,278][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.58it/s, est. speed input: 3587.83 toks/s, output: 119.56 toks/s]
[2025-01-11 01:19:35,439][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.47it/s, est. speed input: 4015.67 toks/s, output: 144.79 toks/s]
[2025-01-11 01:19:35,512][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.87it/s, est. speed input: 4361.67 toks/s, output: 167.85 toks/s]
WARNING 01-11 01:19:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:35,729][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:36,374][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 1066.41 toks/s, output: 31.04 toks/s]
[2025-01-11 01:19:36,374][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 1066.41 toks/s, output: 31.04 toks/s]
WARNING 01-11 01:19:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:36,617][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:39,641][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:30,  3.02s/it, est. speed input: 213.31 toks/s, output: 3.97 toks/s]
[2025-01-11 01:19:39,759][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:38,  1.31s/it, est. speed input: 413.46 toks/s, output: 8.28 toks/s]
[2025-01-11 01:19:39,874][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:21,  1.30it/s, est. speed input: 602.33 toks/s, output: 12.89 toks/s]
[2025-01-11 01:19:40,039][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:09,  2.63it/s, est. speed input: 963.39 toks/s, output: 22.79 toks/s]
[2025-01-11 01:19:40,145][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:07,  3.32it/s, est. speed input: 1118.89 toks/s, output: 28.06 toks/s]
[2025-01-11 01:19:40,249][root][ERROR] - Processed prompts:  23%|##2       | 7/31 [00:03<00:05,  4.11it/s, est. speed input: 1268.54 toks/s, output: 33.59 toks/s]
[2025-01-11 01:19:40,438][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:03<00:02,  8.27it/s, est. speed input: 1892.95 toks/s, output: 58.09 toks/s]
[2025-01-11 01:19:40,570][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:03<00:01,  9.62it/s, est. speed input: 2158.99 toks/s, output: 70.83 toks/s]
[2025-01-11 01:19:40,692][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:04<00:01, 12.71it/s, est. speed input: 2578.12 toks/s, output: 92.28 toks/s]
[2025-01-11 01:19:40,885][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 16.88it/s, est. speed input: 3239.25 toks/s, output: 129.09 toks/s]
[2025-01-11 01:19:41,010][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 23.96it/s, est. speed input: 4049.45 toks/s, output: 181.43 toks/s]
[2025-01-11 01:19:41,163][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:04<00:00, 22.70it/s, est. speed input: 4348.86 toks/s, output: 206.97 toks/s]
[2025-01-11 01:19:41,181][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.79it/s, est. speed input: 4475.78 toks/s, output: 217.81 toks/s]
WARNING 01-11 01:19:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:41,426][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:43,972][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<00:58,  2.55s/it, est. speed input: 291.16 toks/s, output: 3.54 toks/s]
[2025-01-11 01:19:44,362][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:02<00:28,  1.28s/it, est. speed input: 506.97 toks/s, output: 8.86 toks/s]
[2025-01-11 01:19:44,547][root][ERROR] - Processed prompts:  17%|#6        | 4/24 [00:03<00:10,  1.87it/s, est. speed input: 950.94 toks/s, output: 20.51 toks/s]
[2025-01-11 01:19:44,761][root][ERROR] - Processed prompts:  29%|##9       | 7/24 [00:03<00:04,  3.72it/s, est. speed input: 1572.41 toks/s, output: 40.49 toks/s]
[2025-01-11 01:19:44,881][root][ERROR] - Processed prompts:  38%|###7      | 9/24 [00:03<00:02,  5.12it/s, est. speed input: 1956.46 toks/s, output: 55.58 toks/s]
[2025-01-11 01:19:45,053][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:03<00:00, 11.59it/s, est. speed input: 3324.76 toks/s, output: 110.85 toks/s]
[2025-01-11 01:19:45,232][root][ERROR] - Processed prompts:  79%|#######9  | 19/24 [00:03<00:00, 12.69it/s, est. speed input: 3762.76 toks/s, output: 136.65 toks/s]
[2025-01-11 01:19:45,602][root][ERROR] - Processed prompts:  92%|#########1| 22/24 [00:04<00:00, 10.93it/s, est. speed input: 3967.08 toks/s, output: 160.95 toks/s]
[2025-01-11 01:19:45,883][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  9.80it/s, est. speed input: 4060.37 toks/s, output: 183.10 toks/s]
[2025-01-11 01:19:45,883][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.39it/s, est. speed input: 4060.37 toks/s, output: 183.10 toks/s]
WARNING 01-11 01:19:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:46,103][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:47,563][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.46s/it, est. speed input: 447.41 toks/s, output: 17.13 toks/s]
[2025-01-11 01:19:47,712][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.31it/s, est. speed input: 1296.54 toks/s, output: 51.59 toks/s]
[2025-01-11 01:19:47,877][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.97it/s, est. speed input: 2415.83 toks/s, output: 105.43 toks/s]
[2025-01-11 01:19:48,223][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.24it/s, est. speed input: 2718.13 toks/s, output: 137.75 toks/s]
[2025-01-11 01:19:48,223][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.77it/s, est. speed input: 2718.13 toks/s, output: 137.75 toks/s]
WARNING 01-11 01:19:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:48,446][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:50,208][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.76s/it, est. speed input: 452.48 toks/s, output: 14.76 toks/s]
[2025-01-11 01:19:50,439][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:01<00:01,  3.21it/s, est. speed input: 1958.47 toks/s, output: 74.28 toks/s]
[2025-01-11 01:19:50,733][root][ERROR] - Processed prompts:  70%|#######   | 7/10 [00:02<00:00,  4.01it/s, est. speed input: 2380.39 toks/s, output: 106.69 toks/s]
[2025-01-11 01:19:50,863][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  5.42it/s, est. speed input: 2888.23 toks/s, output: 143.60 toks/s]
[2025-01-11 01:19:50,980][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.95it/s, est. speed input: 3049.01 toks/s, output: 161.43 toks/s]
WARNING 01-11 01:19:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:51,239][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:54,457][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:07,  3.22s/it, est. speed input: 218.49 toks/s, output: 8.08 toks/s]
[2025-01-11 01:19:54,596][root][ERROR] - Processed prompts:   9%|9         | 2/22 [00:03<00:28,  1.41s/it, est. speed input: 421.84 toks/s, output: 16.39 toks/s]
[2025-01-11 01:19:54,765][root][ERROR] - Processed prompts:  77%|#######7  | 17/22 [00:03<00:00,  8.78it/s, est. speed input: 3824.00 toks/s, output: 140.95 toks/s]
[2025-01-11 01:19:55,447][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  8.28it/s, est. speed input: 4097.08 toks/s, output: 183.22 toks/s]
[2025-01-11 01:19:55,447][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  5.23it/s, est. speed input: 4097.08 toks/s, output: 183.22 toks/s]
WARNING 01-11 01:19:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:55,737][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:19:58,434][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:40,  2.70s/it, est. speed input: 305.96 toks/s, output: 9.27 toks/s]
[2025-01-11 01:19:58,581][root][ERROR] - Processed prompts:  12%|#2        | 2/16 [00:02<00:16,  1.20s/it, est. speed input: 569.93 toks/s, output: 18.99 toks/s]
[2025-01-11 01:19:58,845][root][ERROR] - Processed prompts:  81%|########1 | 13/16 [00:03<00:00,  7.22it/s, est. speed input: 3954.45 toks/s, output: 124.22 toks/s]
[2025-01-11 01:19:59,625][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  5.88it/s, est. speed input: 3841.75 toks/s, output: 148.92 toks/s]
[2025-01-11 01:19:59,626][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  4.11it/s, est. speed input: 3841.75 toks/s, output: 148.92 toks/s]
WARNING 01-11 01:19:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:19:59,852][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:01,774][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.92s/it, est. speed input: 416.21 toks/s, output: 15.09 toks/s]
[2025-01-11 01:20:02,089][root][ERROR] - Processed prompts:  50%|#####     | 5/10 [00:02<00:01,  2.83it/s, est. speed input: 2004.56 toks/s, output: 70.63 toks/s]
[2025-01-11 01:20:02,202][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.36it/s, est. speed input: 2265.35 toks/s, output: 87.23 toks/s]
[2025-01-11 01:20:02,641][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  5.26it/s, est. speed input: 3013.44 toks/s, output: 154.20 toks/s]
[2025-01-11 01:20:02,641][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.59it/s, est. speed input: 3013.44 toks/s, output: 154.20 toks/s]
WARNING 01-11 01:20:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:02,940][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:04,506][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.57s/it, est. speed input: 594.62 toks/s, output: 14.05 toks/s]
[2025-01-11 01:20:04,664][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.36it/s, est. speed input: 1131.33 toks/s, output: 29.01 toks/s]
[2025-01-11 01:20:04,828][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.98it/s, est. speed input: 3049.18 toks/s, output: 91.10 toks/s]
[2025-01-11 01:20:05,660][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.61it/s, est. speed input: 2766.18 toks/s, output: 116.57 toks/s]
[2025-01-11 01:20:05,660][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  2.94it/s, est. speed input: 2766.18 toks/s, output: 116.57 toks/s]
WARNING 01-11 01:20:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:05,882][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:07,197][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.31s/it, est. speed input: 610.05 toks/s, output: 18.26 toks/s]
[2025-01-11 01:20:07,311][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.64it/s, est. speed input: 1218.54 toks/s, output: 37.09 toks/s]
[2025-01-11 01:20:07,551][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  5.38it/s, est. speed input: 3101.46 toks/s, output: 111.43 toks/s]
[2025-01-11 01:20:07,552][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.59it/s, est. speed input: 3101.46 toks/s, output: 111.43 toks/s]
WARNING 01-11 01:20:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:07,781][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:08,735][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 1163.07 toks/s, output: 30.41 toks/s]
[2025-01-11 01:20:08,736][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.10it/s, est. speed input: 2336.81 toks/s, output: 60.80 toks/s]
WARNING 01-11 01:20:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:08,956][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:10,803][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.85s/it, est. speed input: 457.71 toks/s, output: 15.71 toks/s]
[2025-01-11 01:20:10,917][root][ERROR] - Processed prompts:  56%|#####5    | 5/9 [00:01<00:01,  3.34it/s, est. speed input: 2222.28 toks/s, output: 76.51 toks/s]
[2025-01-11 01:20:11,488][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  4.04it/s, est. speed input: 2807.61 toks/s, output: 119.32 toks/s]
[2025-01-11 01:20:11,824][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.14it/s, est. speed input: 2802.00 toks/s, output: 134.27 toks/s]
WARNING 01-11 01:20:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:12,096][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:13,548][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.45s/it, est. speed input: 729.92 toks/s, output: 19.97 toks/s]
[2025-01-11 01:20:14,105][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.99it/s, est. speed input: 2837.09 toks/s, output: 88.61 toks/s]
[2025-01-11 01:20:14,105][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.49it/s, est. speed input: 2837.09 toks/s, output: 88.61 toks/s]
WARNING 01-11 01:20:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:14,326][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:15,520][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.19s/it, est. speed input: 730.49 toks/s, output: 23.46 toks/s]
[2025-01-11 01:20:15,746][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.48it/s, est. speed input: 2697.46 toks/s, output: 90.85 toks/s]
[2025-01-11 01:20:15,746][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.82it/s, est. speed input: 2697.46 toks/s, output: 90.85 toks/s]
WARNING 01-11 01:20:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:15,987][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:17,439][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.45s/it, est. speed input: 932.39 toks/s, output: 19.30 toks/s]
[2025-01-11 01:20:17,683][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.94it/s, est. speed input: 2848.76 toks/s, output: 74.92 toks/s]
[2025-01-11 01:20:18,020][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.95it/s, est. speed input: 2948.45 toks/s, output: 92.51 toks/s]
[2025-01-11 01:20:18,020][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.46it/s, est. speed input: 2948.45 toks/s, output: 92.51 toks/s]
WARNING 01-11 01:20:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:20:18,263][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:20:19,400][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 1106.54 toks/s, output: 43.10 toks/s]
[2025-01-11 01:20:19,401][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 1106.54 toks/s, output: 43.10 toks/s]
[2025-01-11 01:20:21,663][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:20:21,716][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:20:23,839][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.12s/it]
[2025-01-11 01:20:25,488][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.84s/it]
[2025-01-11 01:20:27,113][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.74s/it]
[2025-01-11 01:20:27,644][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.27s/it]
[2025-01-11 01:20:27,644][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.48s/it]
[2025-01-11 01:20:48,512][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:20:48,564][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:20:51,000][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.44s/it]
[2025-01-11 01:20:53,090][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.23s/it]
[2025-01-11 01:20:54,771][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:01,  1.98s/it]
[2025-01-11 01:20:55,277][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.40s/it]
[2025-01-11 01:20:55,277][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.68s/it]
WARNING 01-11 01:21:13 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:21:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:21:14 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:21:14 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:21:14,581][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:21:15,911][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 01:21:16,287][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:21:17,598][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:21:18,964][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:21:18,965][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:21:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:21:32 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:21:33 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:21:33 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:21:57 model_runner.py:1335] Graph capturing finished in 24 secs.
WARNING 01-11 01:21:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:21:58,285][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:02,174][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.89s/it, est. speed input: 163.31 toks/s, output: 2.83 toks/s]
[2025-01-11 01:22:02,341][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.97it/s, est. speed input: 939.50 toks/s, output: 17.51 toks/s]
[2025-01-11 01:22:02,500][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.75it/s, est. speed input: 1205.35 toks/s, output: 24.91 toks/s]
[2025-01-11 01:22:02,684][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  5.38it/s, est. speed input: 1877.11 toks/s, output: 44.80 toks/s]
[2025-01-11 01:22:02,891][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.72it/s, est. speed input: 2206.17 toks/s, output: 58.19 toks/s]
[2025-01-11 01:22:03,012][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 13.22it/s, est. speed input: 3224.44 toks/s, output: 102.61 toks/s]
[2025-01-11 01:22:03,191][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 14.94it/s, est. speed input: 3624.65 toks/s, output: 127.41 toks/s]
[2025-01-11 01:22:03,600][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 12.99it/s, est. speed input: 3824.11 toks/s, output: 150.74 toks/s]
[2025-01-11 01:22:03,600][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.02it/s, est. speed input: 3824.11 toks/s, output: 150.74 toks/s]
WARNING 01-11 01:22:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:03,831][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:04,396][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 1222.98 toks/s, output: 30.09 toks/s]
[2025-01-11 01:22:04,396][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 1222.98 toks/s, output: 30.09 toks/s]
WARNING 01-11 01:22:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:04,645][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:07,616][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:29,  2.97s/it, est. speed input: 219.44 toks/s, output: 3.70 toks/s]
[2025-01-11 01:22:07,851][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:39,  1.36s/it, est. speed input: 404.49 toks/s, output: 8.11 toks/s]
[2025-01-11 01:22:07,967][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:22,  1.26it/s, est. speed input: 586.67 toks/s, output: 12.94 toks/s]
[2025-01-11 01:22:08,186][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:10,  2.46it/s, est. speed input: 917.50 toks/s, output: 23.16 toks/s]
[2025-01-11 01:22:08,329][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:03<00:02,  7.39it/s, est. speed input: 1955.34 toks/s, output: 58.08 toks/s]
[2025-01-11 01:22:08,460][root][ERROR] - Processed prompts:  45%|####5     | 14/31 [00:03<00:01,  9.56it/s, est. speed input: 2405.86 toks/s, output: 76.80 toks/s]
[2025-01-11 01:22:08,580][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:03<00:01, 11.96it/s, est. speed input: 2831.41 toks/s, output: 97.07 toks/s]
[2025-01-11 01:22:08,680][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 16.32it/s, est. speed input: 3421.19 toks/s, output: 126.13 toks/s]
[2025-01-11 01:22:08,988][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:04<00:00, 13.65it/s, est. speed input: 3633.80 toks/s, output: 144.59 toks/s]
[2025-01-11 01:22:09,297][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 12.20it/s, est. speed input: 3819.10 toks/s, output: 168.10 toks/s]
[2025-01-11 01:22:10,519][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:05<00:00,  5.62it/s, est. speed input: 3367.27 toks/s, output: 174.48 toks/s]
[2025-01-11 01:22:10,621][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.19it/s, est. speed input: 3421.56 toks/s, output: 193.12 toks/s]
WARNING 01-11 01:22:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:10,875][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:14,061][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:29,  3.18s/it, est. speed input: 230.78 toks/s, output: 3.77 toks/s]
[2025-01-11 01:22:14,342][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:39,  1.48s/it, est. speed input: 428.11 toks/s, output: 8.37 toks/s]
[2025-01-11 01:22:14,502][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:03<00:15,  1.66it/s, est. speed input: 821.41 toks/s, output: 18.20 toks/s]
[2025-01-11 01:22:14,857][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:09,  2.49it/s, est. speed input: 1122.63 toks/s, output: 28.64 toks/s]
[2025-01-11 01:22:15,086][root][ERROR] - Processed prompts:  34%|###4      | 10/29 [00:04<00:03,  4.91it/s, est. speed input: 1817.30 toks/s, output: 54.87 toks/s]
[2025-01-11 01:22:15,243][root][ERROR] - Processed prompts:  48%|####8     | 14/29 [00:04<00:01,  7.72it/s, est. speed input: 2437.88 toks/s, output: 83.58 toks/s]
[2025-01-11 01:22:15,451][root][ERROR] - Processed prompts:  59%|#####8    | 17/29 [00:04<00:01,  9.10it/s, est. speed input: 2818.25 toks/s, output: 105.12 toks/s]
[2025-01-11 01:22:15,567][root][ERROR] - Processed prompts:  72%|#######2  | 21/29 [00:04<00:00, 12.67it/s, est. speed input: 3392.09 toks/s, output: 139.41 toks/s]
[2025-01-11 01:22:15,796][root][ERROR] - Processed prompts:  83%|########2 | 24/29 [00:04<00:00, 12.79it/s, est. speed input: 3688.85 toks/s, output: 164.21 toks/s]
[2025-01-11 01:22:16,039][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:05<00:00, 12.66it/s, est. speed input: 3961.60 toks/s, output: 190.97 toks/s]
[2025-01-11 01:22:16,331][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00, 10.68it/s, est. speed input: 4057.59 toks/s, output: 209.15 toks/s]
[2025-01-11 01:22:16,331][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.32it/s, est. speed input: 4057.59 toks/s, output: 209.15 toks/s]
WARNING 01-11 01:22:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:16,552][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:17,542][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.01it/s, est. speed input: 658.31 toks/s, output: 28.27 toks/s]
[2025-01-11 01:22:18,168][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.29it/s, est. speed input: 858.05 toks/s, output: 55.68 toks/s]
[2025-01-11 01:22:19,209][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.11it/s, est. speed input: 833.53 toks/s, output: 80.53 toks/s]
[2025-01-11 01:22:19,209][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.13it/s, est. speed input: 833.53 toks/s, output: 80.53 toks/s]
WARNING 01-11 01:22:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:19,439][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:20,665][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.23s/it, est. speed input: 652.87 toks/s, output: 17.14 toks/s]
[2025-01-11 01:22:20,818][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.67it/s, est. speed input: 1754.07 toks/s, output: 51.48 toks/s]
[2025-01-11 01:22:21,007][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  5.46it/s, est. speed input: 3176.52 toks/s, output: 107.18 toks/s]
[2025-01-11 01:22:21,007][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.83it/s, est. speed input: 3176.52 toks/s, output: 107.18 toks/s]
WARNING 01-11 01:22:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:21,251][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:24,431][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:16,  3.18s/it, est. speed input: 222.69 toks/s, output: 6.29 toks/s]
[2025-01-11 01:22:24,583][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:32,  1.40s/it, est. speed input: 426.30 toks/s, output: 12.91 toks/s]
[2025-01-11 01:22:24,876][root][ERROR] - Processed prompts:  12%|#2        | 3/25 [00:03<00:19,  1.12it/s, est. speed input: 595.22 toks/s, output: 19.87 toks/s]
[2025-01-11 01:22:25,011][root][ERROR] - Processed prompts:  56%|#####6    | 14/25 [00:03<00:01,  8.33it/s, est. speed input: 2751.33 toks/s, output: 105.62 toks/s]
[2025-01-11 01:22:25,469][root][ERROR] - Processed prompts:  72%|#######2  | 18/25 [00:04<00:00,  8.45it/s, est. speed input: 3175.83 toks/s, output: 133.50 toks/s]
[2025-01-11 01:22:25,729][root][ERROR] - Processed prompts:  84%|########4 | 21/25 [00:04<00:00,  9.06it/s, est. speed input: 3492.88 toks/s, output: 162.17 toks/s]
[2025-01-11 01:22:26,531][root][ERROR] - Processed prompts:  96%|#########6| 24/25 [00:05<00:00,  6.59it/s, est. speed input: 3414.84 toks/s, output: 184.69 toks/s]
[2025-01-11 01:22:27,271][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:06<00:00,  4.15it/s, est. speed input: 3141.67 toks/s, output: 186.08 toks/s]
WARNING 01-11 01:22:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:27,580][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:30,438][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:02<00:57,  2.86s/it, est. speed input: 290.48 toks/s, output: 4.90 toks/s]
[2025-01-11 01:22:30,842][root][ERROR] - Processed prompts:  10%|9         | 2/21 [00:03<00:26,  1.41s/it, est. speed input: 499.23 toks/s, output: 11.35 toks/s]
[2025-01-11 01:22:31,097][root][ERROR] - Processed prompts:  19%|#9        | 4/21 [00:03<00:10,  1.65it/s, est. speed input: 974.60 toks/s, output: 25.59 toks/s]
[2025-01-11 01:22:31,259][root][ERROR] - Processed prompts:  67%|######6   | 14/21 [00:03<00:00,  8.05it/s, est. speed input: 3566.25 toks/s, output: 104.93 toks/s]
[2025-01-11 01:22:31,884][root][ERROR] - Processed prompts:  81%|########  | 17/21 [00:04<00:00,  6.85it/s, est. speed input: 3650.07 toks/s, output: 127.35 toks/s]
[2025-01-11 01:22:32,057][root][ERROR] - Processed prompts:  90%|######### | 19/21 [00:04<00:00,  7.45it/s, est. speed input: 3902.18 toks/s, output: 152.35 toks/s]
[2025-01-11 01:22:32,633][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  5.95it/s, est. speed input: 3844.91 toks/s, output: 168.84 toks/s]
[2025-01-11 01:22:32,633][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:05<00:00,  4.16it/s, est. speed input: 3844.91 toks/s, output: 168.84 toks/s]
WARNING 01-11 01:22:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:32,867][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:34,428][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.56s/it, est. speed input: 436.36 toks/s, output: 16.66 toks/s]
[2025-01-11 01:22:34,549][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.87it/s, est. speed input: 2329.64 toks/s, output: 85.03 toks/s]
[2025-01-11 01:22:35,471][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  2.99it/s, est. speed input: 2133.28 toks/s, output: 98.29 toks/s]
[2025-01-11 01:22:35,489][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.05it/s, est. speed input: 2488.64 toks/s, output: 128.91 toks/s]
WARNING 01-11 01:22:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:35,791][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:37,809][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.02s/it, est. speed input: 510.75 toks/s, output: 14.37 toks/s]
[2025-01-11 01:22:37,931][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:01,  3.71it/s, est. speed input: 2653.14 toks/s, output: 85.51 toks/s]
[2025-01-11 01:22:38,918][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:03<00:00,  3.39it/s, est. speed input: 2705.02 toks/s, output: 126.01 toks/s]
[2025-01-11 01:22:39,052][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:03<00:00,  3.07it/s, est. speed input: 2876.74 toks/s, output: 148.39 toks/s]
WARNING 01-11 01:22:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:39,285][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:40,724][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.44s/it, est. speed input: 540.90 toks/s, output: 13.21 toks/s]
[2025-01-11 01:22:40,908][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.43it/s, est. speed input: 970.55 toks/s, output: 27.73 toks/s]
[2025-01-11 01:22:41,461][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  4.47it/s, est. speed input: 2796.59 toks/s, output: 99.30 toks/s]
[2025-01-11 01:22:41,781][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.13it/s, est. speed input: 2795.65 toks/s, output: 116.23 toks/s]
[2025-01-11 01:22:41,781][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.21it/s, est. speed input: 2795.65 toks/s, output: 116.23 toks/s]
WARNING 01-11 01:22:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:42,063][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:43,968][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:13,  1.90s/it, est. speed input: 541.25 toks/s, output: 15.22 toks/s]
[2025-01-11 01:22:44,055][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.02it/s, est. speed input: 4458.55 toks/s, output: 119.51 toks/s]
WARNING 01-11 01:22:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:44,309][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:46,061][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.75s/it, est. speed input: 480.62 toks/s, output: 16.55 toks/s]
[2025-01-11 01:22:46,345][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:02<00:00,  3.11it/s, est. speed input: 2192.98 toks/s, output: 77.58 toks/s]
[2025-01-11 01:22:46,789][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  2.86it/s, est. speed input: 2135.60 toks/s, output: 89.52 toks/s]
[2025-01-11 01:22:46,918][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:02<00:00,  3.39it/s, est. speed input: 2450.58 toks/s, output: 112.30 toks/s]
[2025-01-11 01:22:47,627][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.48it/s, est. speed input: 2275.32 toks/s, output: 122.34 toks/s]
[2025-01-11 01:22:47,628][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:03<00:00,  2.41it/s, est. speed input: 2275.32 toks/s, output: 122.34 toks/s]
WARNING 01-11 01:22:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:47,914][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:48,856][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 1121.55 toks/s, output: 30.77 toks/s]
[2025-01-11 01:22:48,857][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.12it/s, est. speed input: 2219.85 toks/s, output: 61.51 toks/s]
WARNING 01-11 01:22:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:49,093][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:50,585][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.49s/it, est. speed input: 612.01 toks/s, output: 19.44 toks/s]
[2025-01-11 01:22:50,761][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.21it/s, est. speed input: 1461.31 toks/s, output: 56.94 toks/s]
[2025-01-11 01:22:50,984][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.68it/s, est. speed input: 1754.77 toks/s, output: 75.60 toks/s]
[2025-01-11 01:22:51,279][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.88it/s, est. speed input: 2061.45 toks/s, output: 94.68 toks/s]
[2025-01-11 01:22:51,583][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.00it/s, est. speed input: 2335.58 toks/s, output: 116.06 toks/s]
[2025-01-11 01:22:51,583][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.41it/s, est. speed input: 2335.58 toks/s, output: 116.06 toks/s]
WARNING 01-11 01:22:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:51,822][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:53,187][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.37s/it, est. speed input: 679.83 toks/s, output: 19.05 toks/s]
[2025-01-11 01:22:53,537][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.07it/s, est. speed input: 1838.59 toks/s, output: 56.56 toks/s]
[2025-01-11 01:22:53,703][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.68it/s, est. speed input: 2227.45 toks/s, output: 79.21 toks/s]
[2025-01-11 01:22:53,872][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  3.27it/s, est. speed input: 2647.13 toks/s, output: 102.92 toks/s]
[2025-01-11 01:22:53,872][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.44it/s, est. speed input: 2647.13 toks/s, output: 102.92 toks/s]
WARNING 01-11 01:22:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:54,086][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:54,852][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1061.44 toks/s, output: 37.91 toks/s]
[2025-01-11 01:22:54,852][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1061.44 toks/s, output: 37.91 toks/s]
WARNING 01-11 01:22:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:22:55,082][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:22:56,450][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.37s/it, est. speed input: 809.80 toks/s, output: 17.54 toks/s]
[2025-01-11 01:22:56,561][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.53it/s, est. speed input: 2438.25 toks/s, output: 54.78 toks/s]
[2025-01-11 01:22:56,767][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.08it/s, est. speed input: 3519.31 toks/s, output: 90.79 toks/s]
[2025-01-11 01:22:56,768][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.97it/s, est. speed input: 3519.31 toks/s, output: 90.79 toks/s]
[2025-01-11 01:22:59,112][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:22:59,165][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:23:00,834][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 01:23:02,517][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 01:23:04,183][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 01:23:04,742][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:23:04,743][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 01:23:26,196][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:23:26,247][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:23:28,305][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.06s/it]
[2025-01-11 01:23:29,996][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.84s/it]
[2025-01-11 01:23:31,584][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.73s/it]
[2025-01-11 01:23:32,120][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 01:23:32,121][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.47s/it]
WARNING 01-11 01:23:50 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:23:50 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:23:51 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:23:51 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:23:51,538][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:23:52,861][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:23:53,246][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:23:54,556][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:23:55,915][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:23:55,915][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:23:56 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:24:09 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:24:10 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:24:10 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:24:32 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:24:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:32,986][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:36,834][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.85s/it, est. speed input: 165.05 toks/s, output: 2.86 toks/s]
[2025-01-11 01:24:37,004][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.64it/s, est. speed input: 790.21 toks/s, output: 14.68 toks/s]
[2025-01-11 01:24:37,167][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.44it/s, est. speed input: 1063.24 toks/s, output: 22.25 toks/s]
[2025-01-11 01:24:37,270][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.52it/s, est. speed input: 1334.12 toks/s, output: 30.58 toks/s]
[2025-01-11 01:24:37,413][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.69it/s, est. speed input: 1577.87 toks/s, output: 38.85 toks/s]
[2025-01-11 01:24:37,535][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 11.02it/s, est. speed input: 2512.81 toks/s, output: 73.65 toks/s]
[2025-01-11 01:24:37,685][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 18.39it/s, est. speed input: 3513.70 toks/s, output: 117.90 toks/s]
[2025-01-11 01:24:38,194][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:05<00:00, 13.57it/s, est. speed input: 3658.12 toks/s, output: 138.45 toks/s]
[2025-01-11 01:24:38,933][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.38it/s, est. speed input: 3417.07 toks/s, output: 147.31 toks/s]
WARNING 01-11 01:24:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:39,151][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:40,088][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 739.78 toks/s, output: 33.09 toks/s]
[2025-01-11 01:24:40,940][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.13it/s, est. speed input: 816.25 toks/s, output: 63.17 toks/s]
[2025-01-11 01:24:40,941][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.12it/s, est. speed input: 816.25 toks/s, output: 63.17 toks/s]
WARNING 01-11 01:24:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:41,184][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:44,312][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:30,  3.13s/it, est. speed input: 206.18 toks/s, output: 4.79 toks/s]
[2025-01-11 01:24:44,427][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:23,  1.17it/s, est. speed input: 615.54 toks/s, output: 14.80 toks/s]
[2025-01-11 01:24:44,642][root][ERROR] - Processed prompts:  17%|#6        | 5/30 [00:03<00:11,  2.13it/s, est. speed input: 956.78 toks/s, output: 25.73 toks/s]
[2025-01-11 01:24:44,743][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:05,  4.14it/s, est. speed input: 1479.55 toks/s, output: 43.83 toks/s]
[2025-01-11 01:24:44,881][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:03<00:02,  7.25it/s, est. speed input: 2142.14 toks/s, output: 69.25 toks/s]
[2025-01-11 01:24:45,037][root][ERROR] - Processed prompts:  53%|#####3    | 16/30 [00:03<00:01, 10.34it/s, est. speed input: 2738.53 toks/s, output: 95.77 toks/s]
[2025-01-11 01:24:45,174][root][ERROR] - Processed prompts:  77%|#######6  | 23/30 [00:03<00:00, 17.43it/s, est. speed input: 3790.51 toks/s, output: 146.88 toks/s]
[2025-01-11 01:24:45,714][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 12.47it/s, est. speed input: 3919.58 toks/s, output: 170.20 toks/s]
[2025-01-11 01:24:45,877][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 13.52it/s, est. speed input: 4216.27 toks/s, output: 205.42 toks/s]
[2025-01-11 01:24:45,877][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.39it/s, est. speed input: 4216.27 toks/s, output: 205.42 toks/s]
WARNING 01-11 01:24:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:46,125][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:49,205][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:23,  3.08s/it, est. speed input: 240.59 toks/s, output: 3.90 toks/s]
[2025-01-11 01:24:49,537][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:23,  1.09it/s, est. speed input: 646.19 toks/s, output: 12.60 toks/s]
[2025-01-11 01:24:49,642][root][ERROR] - Processed prompts:  21%|##1       | 6/28 [00:03<00:08,  2.65it/s, est. speed input: 1262.45 toks/s, output: 28.43 toks/s]
[2025-01-11 01:24:49,831][root][ERROR] - Processed prompts:  32%|###2      | 9/28 [00:03<00:04,  4.34it/s, est. speed input: 1807.74 toks/s, output: 44.80 toks/s]
[2025-01-11 01:24:50,036][root][ERROR] - Processed prompts:  46%|####6     | 13/28 [00:03<00:02,  6.84it/s, est. speed input: 2497.08 toks/s, output: 69.28 toks/s]
[2025-01-11 01:24:50,162][root][ERROR] - Processed prompts:  68%|######7   | 19/28 [00:04<00:00, 12.00it/s, est. speed input: 3538.92 toks/s, output: 111.47 toks/s]
[2025-01-11 01:24:50,396][root][ERROR] - Processed prompts:  79%|#######8  | 22/28 [00:04<00:00, 12.21it/s, est. speed input: 3880.33 toks/s, output: 133.47 toks/s]
[2025-01-11 01:24:50,800][root][ERROR] - Processed prompts:  89%|########9 | 25/28 [00:04<00:00, 10.37it/s, est. speed input: 4029.79 toks/s, output: 155.93 toks/s]
[2025-01-11 01:24:51,056][root][ERROR] - Processed prompts:  96%|#########6| 27/28 [00:04<00:00,  9.72it/s, est. speed input: 4138.63 toks/s, output: 175.03 toks/s]
[2025-01-11 01:24:51,091][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  5.64it/s, est. speed input: 4266.96 toks/s, output: 188.48 toks/s]
WARNING 01-11 01:24:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:51,473][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:52,544][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.07s/it, est. speed input: 598.16 toks/s, output: 20.56 toks/s]
[2025-01-11 01:24:52,962][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.31it/s, est. speed input: 1409.32 toks/s, output: 60.46 toks/s]
[2025-01-11 01:24:53,063][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.15it/s, est. speed input: 1739.42 toks/s, output: 88.10 toks/s]
[2025-01-11 01:24:53,063][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.52it/s, est. speed input: 1739.42 toks/s, output: 88.10 toks/s]
WARNING 01-11 01:24:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:53,292][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:54,257][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:00<00:04,  1.04it/s, est. speed input: 773.42 toks/s, output: 9.33 toks/s]
[2025-01-11 01:24:54,713][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.50it/s, est. speed input: 1174.48 toks/s, output: 26.74 toks/s]
[2025-01-11 01:24:55,142][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.43it/s, est. speed input: 2275.82 toks/s, output: 81.63 toks/s]
[2025-01-11 01:24:55,209][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.13it/s, est. speed input: 2683.85 toks/s, output: 107.96 toks/s]
WARNING 01-11 01:24:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:24:55,451][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:24:58,443][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:02<01:14,  2.99s/it, est. speed input: 231.31 toks/s, output: 4.68 toks/s]
[2025-01-11 01:24:58,856][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:35,  1.47s/it, est. speed input: 419.99 toks/s, output: 10.57 toks/s]
[2025-01-11 01:24:59,209][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:22,  1.04it/s, est. speed input: 570.10 toks/s, output: 17.30 toks/s]
[2025-01-11 01:24:59,418][root][ERROR] - Processed prompts:  65%|######5   | 17/26 [00:03<00:00,  9.22it/s, est. speed input: 3299.42 toks/s, output: 120.77 toks/s]
[2025-01-11 01:24:59,577][root][ERROR] - Processed prompts:  77%|#######6  | 20/26 [00:04<00:00, 10.37it/s, est. speed input: 3699.18 toks/s, output: 144.48 toks/s]
[2025-01-11 01:24:59,694][root][ERROR] - Processed prompts:  88%|########8 | 23/26 [00:04<00:00, 12.06it/s, est. speed input: 4133.34 toks/s, output: 173.02 toks/s]
[2025-01-11 01:25:00,417][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  8.14it/s, est. speed input: 3992.45 toks/s, output: 187.29 toks/s]
[2025-01-11 01:25:00,417][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  5.24it/s, est. speed input: 3992.45 toks/s, output: 187.29 toks/s]
WARNING 01-11 01:25:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:00,696][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:04,252][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:03<01:11,  3.56s/it, est. speed input: 273.10 toks/s, output: 8.16 toks/s]
[2025-01-11 01:25:04,464][root][ERROR] - Processed prompts:  81%|########  | 17/21 [00:03<00:00,  6.18it/s, est. speed input: 4230.90 toks/s, output: 134.29 toks/s]
[2025-01-11 01:25:04,921][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:04<00:00,  4.97it/s, est. speed input: 4589.33 toks/s, output: 171.38 toks/s]
WARNING 01-11 01:25:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:05,141][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:06,525][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.38s/it, est. speed input: 677.25 toks/s, output: 20.96 toks/s]
[2025-01-11 01:25:06,764][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  3.07it/s, est. speed input: 1951.22 toks/s, output: 78.91 toks/s]
[2025-01-11 01:25:07,035][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.11it/s, est. speed input: 2485.25 toks/s, output: 120.38 toks/s]
[2025-01-11 01:25:07,036][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.17it/s, est. speed input: 2485.25 toks/s, output: 120.38 toks/s]
WARNING 01-11 01:25:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:07,327][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:08,629][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.30s/it, est. speed input: 723.85 toks/s, output: 16.14 toks/s]
[2025-01-11 01:25:08,812][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.55it/s, est. speed input: 1362.25 toks/s, output: 33.67 toks/s]
[2025-01-11 01:25:09,369][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  3.17it/s, est. speed input: 2323.70 toks/s, output: 83.25 toks/s]
[2025-01-11 01:25:09,573][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.48it/s, est. speed input: 2586.72 toks/s, output: 107.33 toks/s]
[2025-01-11 01:25:09,573][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.67it/s, est. speed input: 2586.72 toks/s, output: 107.33 toks/s]
WARNING 01-11 01:25:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:09,819][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:11,442][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:14,  1.62s/it, est. speed input: 520.19 toks/s, output: 9.86 toks/s]
[2025-01-11 01:25:11,813][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:07,  1.13it/s, est. speed input: 909.83 toks/s, output: 22.57 toks/s]
[2025-01-11 01:25:12,040][root][ERROR] - Processed prompts:  90%|######### | 9/10 [00:02<00:00,  6.39it/s, est. speed input: 3786.10 toks/s, output: 118.44 toks/s]
[2025-01-11 01:25:12,324][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  3.99it/s, est. speed input: 3691.02 toks/s, output: 128.13 toks/s]
WARNING 01-11 01:25:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:12,594][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:13,388][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1429.86 toks/s, output: 36.53 toks/s]
[2025-01-11 01:25:13,388][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1429.86 toks/s, output: 36.53 toks/s]
WARNING 01-11 01:25:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:13,603][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:15,210][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.61s/it, est. speed input: 485.91 toks/s, output: 18.04 toks/s]
[2025-01-11 01:25:15,340][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.54it/s, est. speed input: 3084.27 toks/s, output: 104.21 toks/s]
[2025-01-11 01:25:16,028][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.89it/s, est. speed input: 2571.23 toks/s, output: 106.40 toks/s]
WARNING 01-11 01:25:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:16,270][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:17,705][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.43s/it, est. speed input: 817.65 toks/s, output: 20.21 toks/s]
[2025-01-11 01:25:17,705][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.48it/s, est. speed input: 3827.82 toks/s, output: 101.04 toks/s]
WARNING 01-11 01:25:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:17,913][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:18,731][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1191.03 toks/s, output: 37.91 toks/s]
[2025-01-11 01:25:18,732][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1191.03 toks/s, output: 37.91 toks/s]
WARNING 01-11 01:25:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:25:18,971][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:25:19,788][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1644.41 toks/s, output: 35.48 toks/s]
[2025-01-11 01:25:19,789][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1644.41 toks/s, output: 35.48 toks/s]
[2025-01-11 01:25:22,120][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:25:22,172][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:25:23,883][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-01-11 01:25:25,574][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 01:25:27,246][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 01:25:27,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:25:27,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 01:25:49,454][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:25:49,506][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:25:51,426][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 01:25:53,003][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 01:25:54,589][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 01:25:55,164][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:25:55,164][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 01:26:15 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:26:15 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:26:15 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:26:15 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:26:15,738][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:26:17,079][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 01:26:17,457][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 01:26:18,770][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:26:20,105][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:26:20,105][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:26:20 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:26:34 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:26:34 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:26:34 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:26:56 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:26:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:26:57,215][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:01,009][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.39 toks/s, output: 2.90 toks/s]
[2025-01-11 01:27:01,179][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.66it/s, est. speed input: 800.98 toks/s, output: 14.88 toks/s]
[2025-01-11 01:27:01,342][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.47it/s, est. speed input: 1077.19 toks/s, output: 22.54 toks/s]
[2025-01-11 01:27:01,445][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.56it/s, est. speed input: 1351.18 toks/s, output: 30.97 toks/s]
[2025-01-11 01:27:01,580][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  7.16it/s, est. speed input: 2036.94 toks/s, output: 53.16 toks/s]
[2025-01-11 01:27:01,733][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 10.94it/s, est. speed input: 2670.84 toks/s, output: 77.92 toks/s]
[2025-01-11 01:27:01,849][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 16.59it/s, est. speed input: 3426.52 toks/s, output: 111.59 toks/s]
[2025-01-11 01:27:01,967][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 19.47it/s, est. speed input: 3875.50 toks/s, output: 136.58 toks/s]
[2025-01-11 01:27:02,154][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.48it/s, est. speed input: 4115.06 toks/s, output: 156.95 toks/s]
WARNING 01-11 01:27:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:02,408][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:03,200][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 877.63 toks/s, output: 37.88 toks/s]
[2025-01-11 01:27:03,200][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 877.63 toks/s, output: 37.88 toks/s]
WARNING 01-11 01:27:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:03,444][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:06,480][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:31,  3.04s/it, est. speed input: 216.10 toks/s, output: 3.95 toks/s]
[2025-01-11 01:27:06,657][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:39,  1.35s/it, est. speed input: 415.91 toks/s, output: 8.41 toks/s]
[2025-01-11 01:27:06,883][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.74it/s, est. speed input: 764.30 toks/s, output: 18.03 toks/s]
[2025-01-11 01:27:06,992][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:11,  2.32it/s, est. speed input: 925.55 toks/s, output: 23.40 toks/s]
[2025-01-11 01:27:07,097][root][ERROR] - Processed prompts:  23%|##2       | 7/31 [00:03<00:06,  3.94it/s, est. speed input: 1257.88 toks/s, output: 35.05 toks/s]
[2025-01-11 01:27:07,267][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:03<00:01, 12.93it/s, est. speed input: 2749.61 toks/s, output: 91.57 toks/s]
[2025-01-11 01:27:07,446][root][ERROR] - Processed prompts:  61%|######1   | 19/31 [00:04<00:00, 13.76it/s, est. speed input: 3123.39 toks/s, output: 110.72 toks/s]
[2025-01-11 01:27:07,589][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:04<00:00, 17.95it/s, est. speed input: 3816.82 toks/s, output: 148.16 toks/s]
[2025-01-11 01:27:07,733][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 18.60it/s, est. speed input: 4146.16 toks/s, output: 171.64 toks/s]
[2025-01-11 01:27:07,843][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:04<00:00, 20.26it/s, est. speed input: 4490.47 toks/s, output: 198.00 toks/s]
[2025-01-11 01:27:07,877][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.99it/s, est. speed input: 4606.57 toks/s, output: 208.00 toks/s]
WARNING 01-11 01:27:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:08,128][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:11,801][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:35,  3.67s/it, est. speed input: 208.87 toks/s, output: 6.81 toks/s]
[2025-01-11 01:27:11,902][root][ERROR] - Processed prompts:  15%|#4        | 4/27 [00:03<00:16,  1.38it/s, est. speed input: 794.48 toks/s, output: 27.03 toks/s]
[2025-01-11 01:27:12,036][root][ERROR] - Processed prompts:  52%|#####1    | 14/27 [00:03<00:02,  6.28it/s, est. speed input: 2691.30 toks/s, output: 100.58 toks/s]
[2025-01-11 01:27:12,325][root][ERROR] - Processed prompts:  70%|#######   | 19/27 [00:04<00:00,  8.16it/s, est. speed input: 3396.58 toks/s, output: 137.03 toks/s]
[2025-01-11 01:27:12,445][root][ERROR] - Processed prompts:  89%|########8 | 24/27 [00:04<00:00, 11.24it/s, est. speed input: 4178.30 toks/s, output: 181.39 toks/s]
[2025-01-11 01:27:12,705][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  5.90it/s, est. speed input: 4448.77 toks/s, output: 206.28 toks/s]
WARNING 01-11 01:27:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:12,929][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:14,049][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.12s/it, est. speed input: 641.47 toks/s, output: 21.44 toks/s]
[2025-01-11 01:27:14,229][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.80it/s, est. speed input: 1613.67 toks/s, output: 63.10 toks/s]
[2025-01-11 01:27:14,358][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.60it/s, est. speed input: 1966.07 toks/s, output: 85.39 toks/s]
[2025-01-11 01:27:14,426][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.34it/s, est. speed input: 2365.79 toks/s, output: 110.94 toks/s]
WARNING 01-11 01:27:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:14,649][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:16,285][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.64s/it, est. speed input: 499.41 toks/s, output: 17.73 toks/s]
[2025-01-11 01:27:16,403][root][ERROR] - Processed prompts:  62%|######2   | 5/8 [00:01<00:00,  3.72it/s, est. speed input: 2303.40 toks/s, output: 89.51 toks/s]
[2025-01-11 01:27:16,548][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  6.11it/s, est. speed input: 3376.54 toks/s, output: 143.78 toks/s]
[2025-01-11 01:27:16,548][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.21it/s, est. speed input: 3376.54 toks/s, output: 143.78 toks/s]
WARNING 01-11 01:27:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:16,790][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:19,973][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:03<01:13,  3.18s/it, est. speed input: 226.23 toks/s, output: 6.28 toks/s]
[2025-01-11 01:27:20,315][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:33,  1.51s/it, est. speed input: 413.05 toks/s, output: 13.33 toks/s]
[2025-01-11 01:27:20,465][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:03<00:01,  7.79it/s, est. speed input: 3549.72 toks/s, output: 123.27 toks/s]
[2025-01-11 01:27:21,089][root][ERROR] - Processed prompts:  88%|########7 | 21/24 [00:04<00:00,  7.86it/s, est. speed input: 3906.20 toks/s, output: 162.14 toks/s]
[2025-01-11 01:27:21,811][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:05<00:00,  6.60it/s, est. speed input: 3794.03 toks/s, output: 186.62 toks/s]
[2025-01-11 01:27:21,811][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:05<00:00,  4.78it/s, est. speed input: 3794.03 toks/s, output: 186.62 toks/s]
WARNING 01-11 01:27:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:22,118][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:24,715][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:36,  2.60s/it, est. speed input: 385.83 toks/s, output: 10.40 toks/s]
[2025-01-11 01:27:24,933][root][ERROR] - Processed prompts:  67%|######6   | 10/15 [00:02<00:01,  4.76it/s, est. speed input: 3201.32 toks/s, output: 104.42 toks/s]
[2025-01-11 01:27:25,399][root][ERROR] - Processed prompts:  87%|########6 | 13/15 [00:03<00:00,  5.18it/s, est. speed input: 3613.22 toks/s, output: 133.19 toks/s]
[2025-01-11 01:27:25,502][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  6.20it/s, est. speed input: 4030.02 toks/s, output: 164.89 toks/s]
[2025-01-11 01:27:25,502][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:03<00:00,  4.43it/s, est. speed input: 4030.02 toks/s, output: 164.89 toks/s]
WARNING 01-11 01:27:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:25,748][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:27,097][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:09,  1.35s/it, est. speed input: 533.19 toks/s, output: 14.09 toks/s]
[2025-01-11 01:27:27,306][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.47it/s, est. speed input: 972.70 toks/s, output: 29.53 toks/s]
[2025-01-11 01:27:27,913][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:02<00:00,  3.67it/s, est. speed input: 2190.73 toks/s, output: 88.26 toks/s]
[2025-01-11 01:27:28,022][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.13it/s, est. speed input: 2748.03 toks/s, output: 138.98 toks/s]
[2025-01-11 01:27:28,022][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.52it/s, est. speed input: 2748.03 toks/s, output: 138.98 toks/s]
WARNING 01-11 01:27:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:28,283][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:30,125][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.84s/it, est. speed input: 484.87 toks/s, output: 14.12 toks/s]
[2025-01-11 01:27:31,012][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  4.00it/s, est. speed input: 3144.44 toks/s, output: 112.16 toks/s]
[2025-01-11 01:27:31,012][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.30it/s, est. speed input: 3144.44 toks/s, output: 112.16 toks/s]
WARNING 01-11 01:27:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:31,236][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:32,797][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.56s/it, est. speed input: 510.52 toks/s, output: 14.09 toks/s]
[2025-01-11 01:27:32,981][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.33it/s, est. speed input: 1014.60 toks/s, output: 29.23 toks/s]
[2025-01-11 01:27:32,981][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.58it/s, est. speed input: 4313.86 toks/s, output: 128.95 toks/s]
WARNING 01-11 01:27:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:33,255][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:34,474][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.22s/it, est. speed input: 721.71 toks/s, output: 22.14 toks/s]
[2025-01-11 01:27:34,600][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.78it/s, est. speed input: 3128.62 toks/s, output: 88.50 toks/s]
[2025-01-11 01:27:34,600][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.97it/s, est. speed input: 3128.62 toks/s, output: 88.50 toks/s]
WARNING 01-11 01:27:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:34,817][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:35,849][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.03s/it, est. speed input: 804.69 toks/s, output: 20.36 toks/s]
[2025-01-11 01:27:36,008][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.93it/s, est. speed input: 1554.07 toks/s, output: 41.98 toks/s]
[2025-01-11 01:27:36,119][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  3.01it/s, est. speed input: 2065.36 toks/s, output: 65.29 toks/s]
[2025-01-11 01:27:36,338][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.48it/s, est. speed input: 2333.79 toks/s, output: 87.48 toks/s]
[2025-01-11 01:27:36,338][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.63it/s, est. speed input: 2333.79 toks/s, output: 87.48 toks/s]
WARNING 01-11 01:27:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:36,565][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:37,337][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1124.37 toks/s, output: 37.61 toks/s]
[2025-01-11 01:27:37,337][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1124.37 toks/s, output: 37.61 toks/s]
WARNING 01-11 01:27:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:37,548][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:38,679][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.13s/it, est. speed input: 726.12 toks/s, output: 22.11 toks/s]
[2025-01-11 01:27:38,761][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.30it/s, est. speed input: 3024.45 toks/s, output: 92.42 toks/s]
WARNING 01-11 01:27:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:38,986][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:39,980][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.01it/s, est. speed input: 1230.39 toks/s, output: 29.20 toks/s]
[2025-01-11 01:27:40,771][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.14it/s, est. speed input: 1334.03 toks/s, output: 58.83 toks/s]
[2025-01-11 01:27:40,772][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.12it/s, est. speed input: 1334.03 toks/s, output: 58.83 toks/s]
WARNING 01-11 01:27:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:40,978][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:41,737][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1165.41 toks/s, output: 38.19 toks/s]
[2025-01-11 01:27:41,737][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1165.41 toks/s, output: 38.19 toks/s]
WARNING 01-11 01:27:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:41,949][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:43,072][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 1062.44 toks/s, output: 23.15 toks/s]
[2025-01-11 01:27:43,313][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.63it/s, est. speed input: 2761.61 toks/s, output: 69.63 toks/s]
[2025-01-11 01:27:43,313][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.20it/s, est. speed input: 2761.61 toks/s, output: 69.63 toks/s]
WARNING 01-11 01:27:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:27:43,537][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:27:44,804][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 1004.98 toks/s, output: 44.21 toks/s]
[2025-01-11 01:27:44,805][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 1004.98 toks/s, output: 44.21 toks/s]
[2025-01-11 01:27:47,172][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:27:47,225][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:27:48,941][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 01:27:50,689][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 01:27:52,352][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 01:27:52,885][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 01:27:52,886][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-11 01:28:13,328][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:28:13,380][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:28:15,401][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.02s/it]
[2025-01-11 01:28:17,007][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 01:28:18,653][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 01:28:19,165][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 01:28:19,165][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 01:28:38 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:28:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:28:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:28:38 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:28:39,139][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:28:40,491][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:28:40,865][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 01:28:42,160][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 01:28:43,510][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:28:43,510][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:28:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:28:57 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:28:58 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:28:58 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:29:20 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:29:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:20,460][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:24,212][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.75s/it, est. speed input: 169.25 toks/s, output: 2.93 toks/s]
[2025-01-11 01:29:24,443][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.29it/s, est. speed input: 637.74 toks/s, output: 12.05 toks/s]
[2025-01-11 01:29:24,555][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.70it/s, est. speed input: 775.28 toks/s, output: 15.87 toks/s]
[2025-01-11 01:29:24,743][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  6.31it/s, est. speed input: 1927.32 toks/s, output: 49.26 toks/s]
[2025-01-11 01:29:24,865][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  9.59it/s, est. speed input: 2594.66 toks/s, output: 73.78 toks/s]
[2025-01-11 01:29:24,987][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 14.31it/s, est. speed input: 3366.72 toks/s, output: 105.60 toks/s]
[2025-01-11 01:29:25,147][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 17.40it/s, est. speed input: 3929.16 toks/s, output: 134.63 toks/s]
[2025-01-11 01:29:25,394][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.49it/s, est. speed input: 4118.90 toks/s, output: 153.44 toks/s]
WARNING 01-11 01:29:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:25,615][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:26,365][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 907.65 toks/s, output: 22.69 toks/s]
[2025-01-11 01:29:26,365][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 907.65 toks/s, output: 22.69 toks/s]
WARNING 01-11 01:29:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:26,603][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:29,636][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:30,  3.03s/it, est. speed input: 212.67 toks/s, output: 3.96 toks/s]
[2025-01-11 01:29:29,872][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:40,  1.39s/it, est. speed input: 399.56 toks/s, output: 8.57 toks/s]
[2025-01-11 01:29:30,042][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.75it/s, est. speed input: 761.16 toks/s, output: 18.61 toks/s]
[2025-01-11 01:29:30,149][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:08,  3.06it/s, est. speed input: 1107.44 toks/s, output: 29.33 toks/s]
[2025-01-11 01:29:30,335][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:03<00:02,  7.77it/s, est. speed input: 2112.14 toks/s, output: 64.04 toks/s]
[2025-01-11 01:29:30,592][root][ERROR] - Processed prompts:  45%|####5     | 14/31 [00:03<00:02,  7.77it/s, est. speed input: 2308.31 toks/s, output: 74.95 toks/s]
[2025-01-11 01:29:30,707][root][ERROR] - Processed prompts:  52%|#####1    | 16/31 [00:04<00:01,  9.06it/s, est. speed input: 2570.97 toks/s, output: 88.70 toks/s]
[2025-01-11 01:29:30,869][root][ERROR] - Processed prompts:  68%|######7   | 21/31 [00:04<00:00, 13.68it/s, est. speed input: 3243.61 toks/s, output: 127.27 toks/s]
[2025-01-11 01:29:31,090][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:04<00:00, 13.66it/s, est. speed input: 3522.92 toks/s, output: 149.77 toks/s]
[2025-01-11 01:29:31,309][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 12.33it/s, est. speed input: 3638.02 toks/s, output: 164.90 toks/s]
[2025-01-11 01:29:31,465][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:04<00:00, 12.44it/s, est. speed input: 3792.04 toks/s, output: 184.90 toks/s]
[2025-01-11 01:29:31,761][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:05<00:00, 10.26it/s, est. speed input: 3831.43 toks/s, output: 202.00 toks/s]
[2025-01-11 01:29:31,945][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.80it/s, est. speed input: 3821.78 toks/s, output: 211.90 toks/s]
WARNING 01-11 01:29:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:32,194][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:35,172][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:02<01:11,  2.98s/it, est. speed input: 247.46 toks/s, output: 5.04 toks/s]
[2025-01-11 01:29:35,274][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:29,  1.29s/it, est. speed input: 482.83 toks/s, output: 10.39 toks/s]
[2025-01-11 01:29:35,467][root][ERROR] - Processed prompts:  16%|#6        | 4/25 [00:03<00:11,  1.85it/s, est. speed input: 918.24 toks/s, output: 21.70 toks/s]
[2025-01-11 01:29:35,647][root][ERROR] - Processed prompts:  24%|##4       | 6/25 [00:03<00:06,  3.05it/s, est. speed input: 1299.64 toks/s, output: 34.17 toks/s]
[2025-01-11 01:29:35,802][root][ERROR] - Processed prompts:  44%|####4     | 11/25 [00:03<00:01,  7.08it/s, est. speed input: 2327.15 toks/s, output: 69.57 toks/s]
[2025-01-11 01:29:36,061][root][ERROR] - Processed prompts:  68%|######8   | 17/25 [00:03<00:00, 11.06it/s, est. speed input: 3421.76 toks/s, output: 112.76 toks/s]
[2025-01-11 01:29:36,286][root][ERROR] - Processed prompts:  88%|########8 | 22/25 [00:04<00:00, 13.67it/s, est. speed input: 4150.60 toks/s, output: 156.87 toks/s]
[2025-01-11 01:29:38,464][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:06<00:00,  4.27it/s, est. speed input: 3086.59 toks/s, output: 150.24 toks/s]
[2025-01-11 01:29:38,464][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:06<00:00,  3.99it/s, est. speed input: 3086.59 toks/s, output: 150.24 toks/s]
WARNING 01-11 01:29:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:38,683][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:39,973][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:07,  1.29s/it, est. speed input: 551.03 toks/s, output: 17.05 toks/s]
[2025-01-11 01:29:40,248][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.29it/s, est. speed input: 1328.01 toks/s, output: 51.10 toks/s]
[2025-01-11 01:29:40,793][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.57it/s, est. speed input: 2035.53 toks/s, output: 102.37 toks/s]
[2025-01-11 01:29:40,894][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  4.17it/s, est. speed input: 2293.39 toks/s, output: 128.89 toks/s]
[2025-01-11 01:29:40,894][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.17it/s, est. speed input: 2293.39 toks/s, output: 128.89 toks/s]
WARNING 01-11 01:29:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:41,160][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:42,620][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:13,  1.46s/it, est. speed input: 515.23 toks/s, output: 9.59 toks/s]
[2025-01-11 01:29:43,047][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:01<00:06,  1.17it/s, est. speed input: 794.30 toks/s, output: 22.80 toks/s]
[2025-01-11 01:29:43,213][root][ERROR] - Processed prompts:  60%|######    | 6/10 [00:02<00:00,  4.40it/s, est. speed input: 2335.58 toks/s, output: 82.84 toks/s]
[2025-01-11 01:29:43,377][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  5.69it/s, est. speed input: 2896.15 toks/s, output: 113.72 toks/s]
[2025-01-11 01:29:43,540][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  6.94it/s, est. speed input: 3381.06 toks/s, output: 149.61 toks/s]
[2025-01-11 01:29:43,540][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.20it/s, est. speed input: 3381.06 toks/s, output: 149.61 toks/s]
WARNING 01-11 01:29:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:43,784][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:45,997][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:02<00:39,  2.21s/it, est. speed input: 322.69 toks/s, output: 5.42 toks/s]
[2025-01-11 01:29:46,542][root][ERROR] - Processed prompts:  11%|#         | 2/19 [00:02<00:20,  1.23s/it, est. speed input: 528.26 toks/s, output: 13.41 toks/s]
[2025-01-11 01:29:46,706][root][ERROR] - Processed prompts:  16%|#5        | 3/19 [00:02<00:11,  1.34it/s, est. speed input: 751.93 toks/s, output: 22.59 toks/s]
[2025-01-11 01:29:46,916][root][ERROR] - Processed prompts:  68%|######8   | 13/19 [00:03<00:00,  8.54it/s, est. speed input: 3163.21 toks/s, output: 116.54 toks/s]
[2025-01-11 01:29:47,269][root][ERROR] - Processed prompts:  84%|########4 | 16/19 [00:03<00:00,  8.53it/s, est. speed input: 3477.21 toks/s, output: 142.35 toks/s]
[2025-01-11 01:29:48,079][root][ERROR] - Processed prompts:  95%|#########4| 18/19 [00:04<00:00,  5.67it/s, est. speed input: 3168.10 toks/s, output: 150.64 toks/s]
[2025-01-11 01:29:48,971][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:05<00:00,  3.66it/s, est. speed input: 2802.74 toks/s, output: 153.65 toks/s]
WARNING 01-11 01:29:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:49,244][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:51,618][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:35,  2.37s/it, est. speed input: 356.51 toks/s, output: 7.16 toks/s]
[2025-01-11 01:29:52,051][root][ERROR] - Processed prompts:  19%|#8        | 3/16 [00:02<00:10,  1.29it/s, est. speed input: 893.00 toks/s, output: 23.16 toks/s]
[2025-01-11 01:29:52,289][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  8.74it/s, est. speed input: 4830.93 toks/s, output: 150.42 toks/s]
[2025-01-11 01:29:52,289][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  5.25it/s, est. speed input: 4830.93 toks/s, output: 150.42 toks/s]
WARNING 01-11 01:29:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:52,513][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:53,958][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:11,  1.45s/it, est. speed input: 501.01 toks/s, output: 12.46 toks/s]
[2025-01-11 01:29:54,066][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:01<00:04,  1.52it/s, est. speed input: 914.23 toks/s, output: 25.75 toks/s]
[2025-01-11 01:29:54,197][root][ERROR] - Processed prompts:  33%|###3      | 3/9 [00:01<00:02,  2.39it/s, est. speed input: 1322.96 toks/s, output: 39.78 toks/s]
[2025-01-11 01:29:54,420][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:01<00:00,  5.25it/s, est. speed input: 2447.27 toks/s, output: 84.95 toks/s]
[2025-01-11 01:29:54,620][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  5.19it/s, est. speed input: 2632.64 toks/s, output: 99.17 toks/s]
[2025-01-11 01:29:54,731][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  5.82it/s, est. speed input: 2888.33 toks/s, output: 118.11 toks/s]
[2025-01-11 01:29:55,033][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  4.84it/s, est. speed input: 2851.13 toks/s, output: 132.12 toks/s]
[2025-01-11 01:29:55,034][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.57it/s, est. speed input: 2851.13 toks/s, output: 132.12 toks/s]
WARNING 01-11 01:29:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:55,313][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:56,987][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:13,  1.67s/it, est. speed input: 531.18 toks/s, output: 12.55 toks/s]
[2025-01-11 01:29:57,204][root][ERROR] - Processed prompts:  22%|##2       | 2/9 [00:01<00:05,  1.22it/s, est. speed input: 1063.65 toks/s, output: 26.45 toks/s]
[2025-01-11 01:29:57,564][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.65it/s, est. speed input: 3013.12 toks/s, output: 94.62 toks/s]
[2025-01-11 01:29:57,694][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  5.02it/s, est. speed input: 3208.34 toks/s, output: 112.17 toks/s]
[2025-01-11 01:29:57,828][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  5.39it/s, est. speed input: 3396.93 toks/s, output: 130.82 toks/s]
[2025-01-11 01:29:57,828][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.58it/s, est. speed input: 3396.93 toks/s, output: 130.82 toks/s]
WARNING 01-11 01:29:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:29:58,047][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:29:59,586][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.54s/it, est. speed input: 534.90 toks/s, output: 17.55 toks/s]
[2025-01-11 01:30:00,239][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.87it/s, est. speed input: 2822.26 toks/s, output: 108.16 toks/s]
[2025-01-11 01:30:00,239][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.19it/s, est. speed input: 2822.26 toks/s, output: 108.16 toks/s]
WARNING 01-11 01:30:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:30:00,497][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:30:01,756][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.26s/it, est. speed input: 833.00 toks/s, output: 23.03 toks/s]
[2025-01-11 01:30:01,757][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.18it/s, est. speed input: 3319.77 toks/s, output: 92.08 toks/s]
WARNING 01-11 01:30:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:30:01,971][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:30:03,285][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.31s/it, est. speed input: 574.38 toks/s, output: 22.06 toks/s]
[2025-01-11 01:30:03,420][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.45it/s, est. speed input: 3092.30 toks/s, output: 105.56 toks/s]
[2025-01-11 01:30:03,421][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.45it/s, est. speed input: 3092.30 toks/s, output: 105.56 toks/s]
WARNING 01-11 01:30:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:30:03,684][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:30:04,643][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 1229.41 toks/s, output: 30.24 toks/s]
[2025-01-11 01:30:04,643][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.08it/s, est. speed input: 2156.70 toks/s, output: 60.46 toks/s]
WARNING 01-11 01:30:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:30:04,853][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:30:05,761][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 1049.13 toks/s, output: 31.96 toks/s]
[2025-01-11 01:30:05,761][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.20it/s, est. speed input: 1939.82 toks/s, output: 63.89 toks/s]
WARNING 01-11 01:30:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:30:05,996][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:30:06,786][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1416.64 toks/s, output: 36.68 toks/s]
[2025-01-11 01:30:06,787][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1416.64 toks/s, output: 36.68 toks/s]
[2025-01-11 01:30:09,114][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:30:09,167][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:30:10,832][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 01:30:12,559][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 01:30:14,168][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 01:30:14,694][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:30:14,695][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 01:30:35,032][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:30:35,085][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:30:36,989][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.90s/it]
[2025-01-11 01:30:38,652][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 01:30:40,240][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:30:40,752][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 01:30:40,752][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 01:30:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:30:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:30:59 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:30:59 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:30:59,603][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:31:00,931][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 01:31:01,306][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 01:31:02,641][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:31:03,979][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:31:03,979][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:31:04 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:31:18 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:31:18 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:31:18 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:31:40 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:31:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:31:40,608][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:31:44,496][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.89s/it, est. speed input: 163.31 toks/s, output: 2.83 toks/s]
[2025-01-11 01:31:44,727][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.25it/s, est. speed input: 616.66 toks/s, output: 11.65 toks/s]
[2025-01-11 01:31:44,840][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.65it/s, est. speed input: 750.32 toks/s, output: 15.36 toks/s]
[2025-01-11 01:31:44,983][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  6.30it/s, est. speed input: 1886.97 toks/s, output: 48.00 toks/s]
[2025-01-11 01:31:45,107][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01,  8.79it/s, est. speed input: 2399.42 toks/s, output: 66.24 toks/s]
[2025-01-11 01:31:45,211][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.85it/s, est. speed input: 2897.08 toks/s, output: 86.25 toks/s]
[2025-01-11 01:31:45,355][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.56it/s, est. speed input: 3344.62 toks/s, output: 107.87 toks/s]
[2025-01-11 01:31:45,492][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 18.65it/s, est. speed input: 3900.42 toks/s, output: 140.25 toks/s]
[2025-01-11 01:31:45,665][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.33it/s, est. speed input: 4018.46 toks/s, output: 152.67 toks/s]
WARNING 01-11 01:31:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:31:45,891][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:31:46,583][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 982.84 toks/s, output: 36.13 toks/s]
[2025-01-11 01:31:46,583][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 982.84 toks/s, output: 36.13 toks/s]
WARNING 01-11 01:31:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:31:46,831][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:31:49,922][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:32,  3.09s/it, est. speed input: 213.25 toks/s, output: 4.21 toks/s]
[2025-01-11 01:31:50,211][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:25,  1.10it/s, est. speed input: 586.67 toks/s, output: 13.31 toks/s]
[2025-01-11 01:31:50,376][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.07it/s, est. speed input: 930.31 toks/s, output: 23.98 toks/s]
[2025-01-11 01:31:50,526][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:03<00:04,  4.58it/s, est. speed input: 1606.15 toks/s, output: 47.10 toks/s]
[2025-01-11 01:31:50,667][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:03<00:03,  5.74it/s, est. speed input: 1886.08 toks/s, output: 59.18 toks/s]
[2025-01-11 01:31:50,801][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:03<00:02,  7.04it/s, est. speed input: 2154.64 toks/s, output: 72.30 toks/s]
[2025-01-11 01:31:50,915][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:04<00:01, 11.13it/s, est. speed input: 2740.51 toks/s, output: 100.88 toks/s]
[2025-01-11 01:31:51,130][root][ERROR] - Processed prompts:  71%|#######   | 22/31 [00:04<00:00, 14.56it/s, est. speed input: 3371.17 toks/s, output: 137.01 toks/s]
[2025-01-11 01:31:51,428][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:04<00:00, 12.95it/s, est. speed input: 3581.85 toks/s, output: 159.02 toks/s]
[2025-01-11 01:31:51,592][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 12.77it/s, est. speed input: 3738.35 toks/s, output: 177.27 toks/s]
[2025-01-11 01:31:51,830][root][ERROR] - Processed prompts:  94%|#########3| 29/31 [00:04<00:00, 11.42it/s, est. speed input: 3824.13 toks/s, output: 194.46 toks/s]
[2025-01-11 01:31:52,876][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:06<00:00,  5.11it/s, est. speed input: 3380.86 toks/s, output: 195.37 toks/s]
[2025-01-11 01:31:52,876][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:06<00:00,  5.13it/s, est. speed input: 3380.86 toks/s, output: 195.37 toks/s]
WARNING 01-11 01:31:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:31:53,129][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:31:56,608][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:33,  3.48s/it, est. speed input: 217.65 toks/s, output: 5.18 toks/s]
[2025-01-11 01:31:56,716][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:23,  1.06it/s, est. speed input: 624.26 toks/s, output: 15.89 toks/s]
[2025-01-11 01:31:56,868][root][ERROR] - Processed prompts:  18%|#7        | 5/28 [00:03<00:11,  2.01it/s, est. speed input: 1003.67 toks/s, output: 26.75 toks/s]
[2025-01-11 01:31:57,139][root][ERROR] - Processed prompts:  32%|###2      | 9/28 [00:04<00:04,  4.14it/s, est. speed input: 1721.12 toks/s, output: 49.88 toks/s]
[2025-01-11 01:31:57,246][root][ERROR] - Processed prompts:  54%|#####3    | 15/28 [00:04<00:01,  8.61it/s, est. speed input: 2848.99 toks/s, output: 91.57 toks/s]
[2025-01-11 01:31:57,414][root][ERROR] - Processed prompts:  64%|######4   | 18/28 [00:04<00:00, 10.11it/s, est. speed input: 3273.08 toks/s, output: 113.43 toks/s]
[2025-01-11 01:31:57,559][root][ERROR] - Processed prompts:  75%|#######5  | 21/28 [00:04<00:00, 11.87it/s, est. speed input: 3677.00 toks/s, output: 137.48 toks/s]
[2025-01-11 01:31:57,679][root][ERROR] - Processed prompts:  86%|########5 | 24/28 [00:04<00:00, 14.02it/s, est. speed input: 4088.39 toks/s, output: 163.52 toks/s]
[2025-01-11 01:31:57,892][root][ERROR] - Processed prompts:  96%|#########6| 27/28 [00:04<00:00, 14.05it/s, est. speed input: 4386.22 toks/s, output: 189.40 toks/s]
[2025-01-11 01:31:57,993][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  5.76it/s, est. speed input: 4454.78 toks/s, output: 198.64 toks/s]
WARNING 01-11 01:31:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:31:58,211][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:31:59,175][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.04it/s, est. speed input: 729.30 toks/s, output: 20.75 toks/s]
[2025-01-11 01:31:59,334][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:00,  2.04it/s, est. speed input: 1305.11 toks/s, output: 42.73 toks/s]
[2025-01-11 01:32:01,000][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.42it/s, est. speed input: 1062.12 toks/s, output: 73.51 toks/s]
[2025-01-11 01:32:01,000][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.43it/s, est. speed input: 1062.12 toks/s, output: 73.51 toks/s]
WARNING 01-11 01:32:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:01,275][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:02,403][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.13s/it, est. speed input: 736.68 toks/s, output: 20.39 toks/s]
[2025-01-11 01:32:02,533][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.85it/s, est. speed input: 1400.02 toks/s, output: 41.32 toks/s]
[2025-01-11 01:32:02,755][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.69it/s, est. speed input: 2229.51 toks/s, output: 82.45 toks/s]
[2025-01-11 01:32:03,256][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.92it/s, est. speed input: 2030.86 toks/s, output: 97.43 toks/s]
[2025-01-11 01:32:03,256][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.52it/s, est. speed input: 2030.86 toks/s, output: 97.43 toks/s]
WARNING 01-11 01:32:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:03,511][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:06,406][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<01:06,  2.89s/it, est. speed input: 257.73 toks/s, output: 5.53 toks/s]
[2025-01-11 01:32:06,747][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:30,  1.39s/it, est. speed input: 450.59 toks/s, output: 12.05 toks/s]
[2025-01-11 01:32:06,985][root][ERROR] - Processed prompts:  12%|#2        | 3/24 [00:03<00:18,  1.16it/s, est. speed input: 627.43 toks/s, output: 19.28 toks/s]
[2025-01-11 01:32:07,104][root][ERROR] - Processed prompts:  42%|####1     | 10/24 [00:03<00:02,  5.90it/s, est. speed input: 2074.82 toks/s, output: 75.42 toks/s]
[2025-01-11 01:32:07,408][root][ERROR] - Processed prompts:  54%|#####4    | 13/24 [00:03<00:01,  6.82it/s, est. speed input: 2544.72 toks/s, output: 97.25 toks/s]
[2025-01-11 01:32:07,528][root][ERROR] - Processed prompts:  67%|######6   | 16/24 [00:04<00:00,  8.91it/s, est. speed input: 3014.88 toks/s, output: 126.21 toks/s]
[2025-01-11 01:32:07,989][root][ERROR] - Processed prompts:  88%|########7 | 21/24 [00:04<00:00,  9.66it/s, est. speed input: 3535.48 toks/s, output: 168.15 toks/s]
[2025-01-11 01:32:08,195][root][ERROR] - Processed prompts:  96%|#########5| 23/24 [00:04<00:00,  9.67it/s, est. speed input: 3704.62 toks/s, output: 191.29 toks/s]
[2025-01-11 01:32:08,229][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.09it/s, est. speed input: 3843.55 toks/s, output: 206.45 toks/s]
WARNING 01-11 01:32:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:08,512][root][ERROR] - Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:12,015][root][ERROR] - Processed prompts:   5%|4         | 1/21 [00:03<01:10,  3.50s/it, est. speed input: 235.28 toks/s, output: 7.99 toks/s]
[2025-01-11 01:32:12,154][root][ERROR] - Processed prompts:  81%|########  | 17/21 [00:03<00:00,  6.45it/s, est. speed input: 4301.30 toks/s, output: 136.47 toks/s]
[2025-01-11 01:32:12,722][root][ERROR] - Processed prompts: 100%|##########| 21/21 [00:04<00:00,  4.99it/s, est. speed input: 4579.80 toks/s, output: 172.23 toks/s]
WARNING 01-11 01:32:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:12,942][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:14,109][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.17s/it, est. speed input: 690.06 toks/s, output: 22.29 toks/s]
[2025-01-11 01:32:14,377][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.41it/s, est. speed input: 2124.92 toks/s, output: 86.45 toks/s]
[2025-01-11 01:32:14,594][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.68it/s, est. speed input: 2316.54 toks/s, output: 107.17 toks/s]
[2025-01-11 01:32:14,594][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.03it/s, est. speed input: 2316.54 toks/s, output: 107.17 toks/s]
WARNING 01-11 01:32:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:14,874][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:16,243][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.37s/it, est. speed input: 582.32 toks/s, output: 17.54 toks/s]
[2025-01-11 01:32:16,357][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.52it/s, est. speed input: 1918.02 toks/s, output: 54.63 toks/s]
[2025-01-11 01:32:16,425][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.87it/s, est. speed input: 3589.36 toks/s, output: 110.96 toks/s]
WARNING 01-11 01:32:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:16,652][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:18,803][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.15s/it, est. speed input: 451.85 toks/s, output: 13.48 toks/s]
[2025-01-11 01:32:19,337][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  4.76it/s, est. speed input: 3415.33 toks/s, output: 118.82 toks/s]
[2025-01-11 01:32:19,421][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  3.97it/s, est. speed input: 3633.48 toks/s, output: 137.96 toks/s]
WARNING 01-11 01:32:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:19,675][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:20,782][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.11s/it, est. speed input: 839.69 toks/s, output: 13.56 toks/s]
[2025-01-11 01:32:21,091][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.57it/s, est. speed input: 1441.30 toks/s, output: 31.09 toks/s]
[2025-01-11 01:32:21,091][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.53it/s, est. speed input: 3858.99 toks/s, output: 92.54 toks/s]
WARNING 01-11 01:32:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:21,313][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:22,563][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.25s/it, est. speed input: 697.20 toks/s, output: 15.21 toks/s]
[2025-01-11 01:32:22,791][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.54it/s, est. speed input: 1318.64 toks/s, output: 32.48 toks/s]
[2025-01-11 01:32:24,157][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.43it/s, est. speed input: 1997.45 toks/s, output: 87.90 toks/s]
[2025-01-11 01:32:24,158][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.11it/s, est. speed input: 1997.45 toks/s, output: 87.90 toks/s]
WARNING 01-11 01:32:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:24,437][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:25,249][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1415.76 toks/s, output: 35.70 toks/s]
[2025-01-11 01:32:25,249][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1415.76 toks/s, output: 35.70 toks/s]
WARNING 01-11 01:32:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:25,462][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:26,614][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.15s/it, est. speed input: 789.01 toks/s, output: 22.57 toks/s]
[2025-01-11 01:32:26,988][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.29it/s, est. speed input: 1781.66 toks/s, output: 66.18 toks/s]
[2025-01-11 01:32:27,476][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  2.20it/s, est. speed input: 1851.01 toks/s, output: 87.39 toks/s]
[2025-01-11 01:32:27,476][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.99it/s, est. speed input: 1851.01 toks/s, output: 87.39 toks/s]
WARNING 01-11 01:32:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:27,706][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:28,808][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.10s/it, est. speed input: 1119.73 toks/s, output: 25.41 toks/s]
[2025-01-11 01:32:28,894][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.52it/s, est. speed input: 2641.34 toks/s, output: 75.73 toks/s]
WARNING 01-11 01:32:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:29,100][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:29,863][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1291.50 toks/s, output: 36.71 toks/s]
[2025-01-11 01:32:29,864][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1291.50 toks/s, output: 36.71 toks/s]
WARNING 01-11 01:32:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:30,083][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:31,163][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 893.39 toks/s, output: 25.92 toks/s]
[2025-01-11 01:32:32,462][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.31it/s, est. speed input: 1334.68 toks/s, output: 68.10 toks/s]
[2025-01-11 01:32:32,462][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:02<00:00,  1.26it/s, est. speed input: 1334.68 toks/s, output: 68.10 toks/s]
WARNING 01-11 01:32:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:32:32,678][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:32:33,282][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 1710.83 toks/s, output: 29.84 toks/s]
[2025-01-11 01:32:33,282][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 1710.83 toks/s, output: 29.84 toks/s]
[2025-01-11 01:32:35,708][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:32:35,761][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:32:37,364][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 01:32:38,993][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 01:32:40,624][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 01:32:41,166][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:32:41,166][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 01:33:02,572][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:33:02,624][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:33:04,671][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.05s/it]
[2025-01-11 01:33:06,336][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.82s/it]
[2025-01-11 01:33:07,924][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 01:33:08,440][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 01:33:08,440][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 01:33:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:33:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:33:27 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:33:27 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:33:27,934][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:33:29,287][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:33:29,665][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 01:33:31,001][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:33:32,362][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 01:33:32,362][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 01:33:32 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:33:46 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:33:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:33:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:34:08 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:34:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:08,893][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:12,740][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.85s/it, est. speed input: 165.08 toks/s, output: 2.86 toks/s]
[2025-01-11 01:34:13,086][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.22it/s, est. speed input: 605.83 toks/s, output: 11.93 toks/s]
[2025-01-11 01:34:13,269][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.88it/s, est. speed input: 2177.21 toks/s, output: 57.37 toks/s]
[2025-01-11 01:34:13,371][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 10.19it/s, est. speed input: 3261.83 toks/s, output: 96.03 toks/s]
[2025-01-11 01:34:13,547][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 12.51it/s, est. speed input: 3820.86 toks/s, output: 122.06 toks/s]
[2025-01-11 01:34:13,713][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.64it/s, est. speed input: 4216.77 toks/s, output: 149.21 toks/s]
WARNING 01-11 01:34:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:13,996][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:16,883][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.89s/it, est. speed input: 229.00 toks/s, output: 2.77 toks/s]
[2025-01-11 01:34:17,365][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:44,  1.47s/it, est. speed input: 389.75 toks/s, output: 7.12 toks/s]
[2025-01-11 01:34:17,480][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:12,  2.24it/s, est. speed input: 940.52 toks/s, output: 21.81 toks/s]
[2025-01-11 01:34:17,584][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.21it/s, est. speed input: 1461.74 toks/s, output: 36.79 toks/s]
[2025-01-11 01:34:17,724][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:03<00:02,  7.23it/s, est. speed input: 2113.97 toks/s, output: 57.95 toks/s]
[2025-01-11 01:34:17,853][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:03<00:01, 10.57it/s, est. speed input: 2731.78 toks/s, output: 81.93 toks/s]
[2025-01-11 01:34:17,962][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:00, 14.35it/s, est. speed input: 3312.39 toks/s, output: 107.92 toks/s]
[2025-01-11 01:34:18,110][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 18.62it/s, est. speed input: 3996.07 toks/s, output: 142.68 toks/s]
[2025-01-11 01:34:18,472][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 15.33it/s, est. speed input: 4260.01 toks/s, output: 168.45 toks/s]
[2025-01-11 01:34:18,703][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 14.68it/s, est. speed input: 4467.78 toks/s, output: 198.66 toks/s]
[2025-01-11 01:34:18,703][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.80it/s, est. speed input: 4467.78 toks/s, output: 198.66 toks/s]
WARNING 01-11 01:34:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:18,969][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:22,436][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:40,  3.47s/it, est. speed input: 217.51 toks/s, output: 4.33 toks/s]
[2025-01-11 01:34:22,710][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:18,  1.37it/s, est. speed input: 798.24 toks/s, output: 17.64 toks/s]
[2025-01-11 01:34:22,813][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:06,  3.31it/s, est. speed input: 1552.18 toks/s, output: 39.29 toks/s]
[2025-01-11 01:34:22,948][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:03<00:03,  5.63it/s, est. speed input: 2256.41 toks/s, output: 61.57 toks/s]
[2025-01-11 01:34:23,109][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:04<00:02,  7.33it/s, est. speed input: 2757.77 toks/s, output: 79.23 toks/s]
[2025-01-11 01:34:23,213][root][ERROR] - Processed prompts:  60%|######    | 18/30 [00:04<00:01,  9.64it/s, est. speed input: 3212.98 toks/s, output: 98.73 toks/s]
[2025-01-11 01:34:23,401][root][ERROR] - Processed prompts:  70%|#######   | 21/30 [00:04<00:00, 11.01it/s, est. speed input: 3586.85 toks/s, output: 118.91 toks/s]
[2025-01-11 01:34:23,511][root][ERROR] - Processed prompts:  80%|########  | 24/30 [00:04<00:00, 13.52it/s, est. speed input: 3997.08 toks/s, output: 143.13 toks/s]
[2025-01-11 01:34:23,658][root][ERROR] - Processed prompts:  93%|#########3| 28/30 [00:04<00:00, 16.64it/s, est. speed input: 4504.15 toks/s, output: 176.83 toks/s]
[2025-01-11 01:34:23,965][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.01it/s, est. speed input: 4530.92 toks/s, output: 189.77 toks/s]
WARNING 01-11 01:34:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:24,180][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:25,453][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.27s/it, est. speed input: 590.58 toks/s, output: 38.48 toks/s]
[2025-01-11 01:34:25,605][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.63it/s, est. speed input: 1057.78 toks/s, output: 75.10 toks/s]
[2025-01-11 01:34:25,605][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.40it/s, est. speed input: 1057.78 toks/s, output: 75.10 toks/s]
WARNING 01-11 01:34:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:25,849][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:27,840][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:19,  1.99s/it, est. speed input: 360.58 toks/s, output: 13.56 toks/s]
[2025-01-11 01:34:27,948][root][ERROR] - Processed prompts:  55%|#####4    | 6/11 [00:02<00:01,  3.80it/s, est. speed input: 2354.76 toks/s, output: 82.41 toks/s]
[2025-01-11 01:34:28,094][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  5.84it/s, est. speed input: 3293.73 toks/s, output: 122.93 toks/s]
[2025-01-11 01:34:28,353][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.39it/s, est. speed input: 3631.86 toks/s, output: 148.16 toks/s]
WARNING 01-11 01:34:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:28,592][root][ERROR] - Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:31,006][root][ERROR] - Processed prompts:   5%|5         | 1/20 [00:02<00:45,  2.41s/it, est. speed input: 299.95 toks/s, output: 6.63 toks/s]
[2025-01-11 01:34:31,425][root][ERROR] - Processed prompts:  15%|#5        | 3/20 [00:02<00:13,  1.28it/s, est. speed input: 764.08 toks/s, output: 20.82 toks/s]
[2025-01-11 01:34:31,546][root][ERROR] - Processed prompts:  25%|##5       | 5/20 [00:02<00:06,  2.44it/s, est. speed input: 1218.45 toks/s, output: 39.27 toks/s]
[2025-01-11 01:34:31,884][root][ERROR] - Processed prompts:  75%|#######5  | 15/20 [00:03<00:00,  8.74it/s, est. speed input: 3282.05 toks/s, output: 127.59 toks/s]
[2025-01-11 01:34:32,240][root][ERROR] - Processed prompts:  95%|#########5| 19/20 [00:03<00:00,  9.38it/s, est. speed input: 3769.04 toks/s, output: 168.58 toks/s]
[2025-01-11 01:34:32,358][root][ERROR] - Processed prompts: 100%|##########| 20/20 [00:03<00:00,  5.31it/s, est. speed input: 3848.36 toks/s, output: 181.64 toks/s]
WARNING 01-11 01:34:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:32,609][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:35,846][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:03<00:58,  3.24s/it, est. speed input: 249.29 toks/s, output: 8.65 toks/s]
[2025-01-11 01:34:36,042][root][ERROR] - Processed prompts:  84%|########4 | 16/19 [00:03<00:00,  6.38it/s, est. speed input: 4324.82 toks/s, output: 136.93 toks/s]
[2025-01-11 01:34:36,229][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:03<00:00,  5.25it/s, est. speed input: 4808.21 toks/s, output: 165.49 toks/s]
WARNING 01-11 01:34:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:36,484][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:38,215][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:01<00:17,  1.73s/it, est. speed input: 398.17 toks/s, output: 11.56 toks/s]
[2025-01-11 01:34:38,444][root][ERROR] - Processed prompts:  27%|##7       | 3/11 [00:01<00:04,  1.87it/s, est. speed input: 1099.97 toks/s, output: 35.20 toks/s]
[2025-01-11 01:34:38,839][root][ERROR] - Processed prompts:  91%|######### | 10/11 [00:02<00:00,  6.33it/s, est. speed input: 3314.85 toks/s, output: 123.57 toks/s]
[2025-01-11 01:34:39,041][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.30it/s, est. speed input: 3383.50 toks/s, output: 137.69 toks/s]
WARNING 01-11 01:34:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:39,363][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:40,847][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.48s/it, est. speed input: 713.59 toks/s, output: 18.87 toks/s]
[2025-01-11 01:34:41,192][root][ERROR] - Processed prompts:  67%|######6   | 4/6 [00:01<00:00,  2.67it/s, est. speed input: 2161.24 toks/s, output: 71.62 toks/s]
[2025-01-11 01:34:41,358][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  3.16it/s, est. speed input: 2422.59 toks/s, output: 92.73 toks/s]
[2025-01-11 01:34:41,761][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.93it/s, est. speed input: 2389.92 toks/s, output: 109.66 toks/s]
[2025-01-11 01:34:41,762][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.50it/s, est. speed input: 2389.92 toks/s, output: 109.66 toks/s]
WARNING 01-11 01:34:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:41,993][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:43,961][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:17,  1.97s/it, est. speed input: 398.44 toks/s, output: 14.23 toks/s]
[2025-01-11 01:34:44,133][root][ERROR] - Processed prompts:  80%|########  | 8/10 [00:02<00:00,  4.97it/s, est. speed input: 3402.38 toks/s, output: 111.71 toks/s]
[2025-01-11 01:34:44,481][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.02it/s, est. speed input: 3650.48 toks/s, output: 139.89 toks/s]
WARNING 01-11 01:34:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:44,732][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:46,221][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.49s/it, est. speed input: 760.36 toks/s, output: 17.46 toks/s]
[2025-01-11 01:34:46,290][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.85it/s, est. speed input: 4003.03 toks/s, output: 109.77 toks/s]
WARNING 01-11 01:34:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:46,507][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:47,833][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.33s/it, est. speed input: 587.52 toks/s, output: 21.12 toks/s]
[2025-01-11 01:34:47,907][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.57it/s, est. speed input: 3392.81 toks/s, output: 105.02 toks/s]
WARNING 01-11 01:34:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:48,158][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:48,958][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1306.10 toks/s, output: 36.25 toks/s]
[2025-01-11 01:34:48,958][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1306.10 toks/s, output: 36.25 toks/s]
WARNING 01-11 01:34:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:49,172][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:50,129][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.05it/s, est. speed input: 884.35 toks/s, output: 23.00 toks/s]
[2025-01-11 01:34:50,258][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  2.13it/s, est. speed input: 1672.08 toks/s, output: 46.96 toks/s]
[2025-01-11 01:34:50,258][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.76it/s, est. speed input: 2780.04 toks/s, output: 73.64 toks/s]
WARNING 01-11 01:34:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:50,493][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:51,235][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1287.63 toks/s, output: 35.06 toks/s]
[2025-01-11 01:34:51,236][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1287.63 toks/s, output: 35.06 toks/s]
WARNING 01-11 01:34:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:34:51,441][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:34:52,444][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 997.08 toks/s, output: 41.88 toks/s]
[2025-01-11 01:34:52,444][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 997.08 toks/s, output: 41.88 toks/s]
[2025-01-11 01:34:54,903][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:34:54,956][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:34:56,623][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 01:34:58,223][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 01:34:59,808][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 01:35:00,332][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 01:35:00,333][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 01:35:20,651][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:35:20,703][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:35:22,745][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.04s/it]
[2025-01-11 01:35:24,442][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.84s/it]
[2025-01-11 01:35:25,989][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 01:35:26,491][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:35:26,492][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 01:35:44 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:35:44 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:35:45 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:35:45 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:35:46,143][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:35:47,464][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:35:47,838][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 01:35:49,136][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 01:35:50,464][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 01:35:50,464][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 01:35:50 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:36:04 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:36:05 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:36:05 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:36:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:36:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:27,366][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:31,134][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.77s/it, est. speed input: 168.53 toks/s, output: 2.92 toks/s]
[2025-01-11 01:36:31,470][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.58it/s, est. speed input: 773.55 toks/s, output: 14.86 toks/s]
[2025-01-11 01:36:31,654][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.42it/s, est. speed input: 2073.03 toks/s, output: 52.47 toks/s]
[2025-01-11 01:36:31,768][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  8.59it/s, est. speed input: 2885.25 toks/s, output: 82.01 toks/s]
[2025-01-11 01:36:31,882][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.68it/s, est. speed input: 3515.33 toks/s, output: 108.28 toks/s]
[2025-01-11 01:36:32,022][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 13.92it/s, est. speed input: 3955.26 toks/s, output: 131.88 toks/s]
[2025-01-11 01:36:32,116][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.74it/s, est. speed input: 4277.73 toks/s, output: 153.05 toks/s]
WARNING 01-11 01:36:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:32,384][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:35,579][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:39,  3.19s/it, est. speed input: 204.08 toks/s, output: 4.07 toks/s]
[2025-01-11 01:36:35,815][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:26,  1.09it/s, est. speed input: 575.13 toks/s, output: 12.53 toks/s]
[2025-01-11 01:36:35,981][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:10,  2.58it/s, est. speed input: 1093.93 toks/s, output: 27.52 toks/s]
[2025-01-11 01:36:36,138][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:06,  3.65it/s, est. speed input: 1392.24 toks/s, output: 38.10 toks/s]
[2025-01-11 01:36:36,281][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:03<00:03,  6.60it/s, est. speed input: 2016.20 toks/s, output: 61.85 toks/s]
[2025-01-11 01:36:36,406][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01, 10.98it/s, est. speed input: 2771.89 toks/s, output: 94.49 toks/s]
[2025-01-11 01:36:36,537][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 15.44it/s, est. speed input: 3477.12 toks/s, output: 128.34 toks/s]
[2025-01-11 01:36:36,651][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 17.29it/s, est. speed input: 3848.45 toks/s, output: 150.01 toks/s]
[2025-01-11 01:36:36,775][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 18.71it/s, est. speed input: 4189.97 toks/s, output: 173.77 toks/s]
[2025-01-11 01:36:37,204][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 12.76it/s, est. speed input: 4229.73 toks/s, output: 192.32 toks/s]
[2025-01-11 01:36:37,521][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.23it/s, est. speed input: 4095.88 toks/s, output: 196.62 toks/s]
WARNING 01-11 01:36:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:37,785][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:41,067][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:31,  3.28s/it, est. speed input: 229.20 toks/s, output: 3.96 toks/s]
[2025-01-11 01:36:41,301][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:40,  1.49s/it, est. speed input: 428.74 toks/s, output: 8.53 toks/s]
[2025-01-11 01:36:41,639][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:25,  1.04it/s, est. speed input: 589.58 toks/s, output: 13.75 toks/s]
[2025-01-11 01:36:41,801][root][ERROR] - Processed prompts:  17%|#7        | 5/29 [00:04<00:11,  2.16it/s, est. speed input: 937.97 toks/s, output: 25.65 toks/s]
[2025-01-11 01:36:41,903][root][ERROR] - Processed prompts:  24%|##4       | 7/29 [00:04<00:06,  3.58it/s, est. speed input: 1280.42 toks/s, output: 38.37 toks/s]
[2025-01-11 01:36:42,087][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:04<00:02,  6.74it/s, est. speed input: 1920.00 toks/s, output: 64.40 toks/s]
[2025-01-11 01:36:42,216][root][ERROR] - Processed prompts:  45%|####4     | 13/29 [00:04<00:01,  8.03it/s, est. speed input: 2200.32 toks/s, output: 77.87 toks/s]
[2025-01-11 01:36:42,329][root][ERROR] - Processed prompts:  55%|#####5    | 16/29 [00:04<00:01, 10.94it/s, est. speed input: 2646.54 toks/s, output: 99.72 toks/s]
[2025-01-11 01:36:42,446][root][ERROR] - Processed prompts:  79%|#######9  | 23/29 [00:04<00:00, 20.12it/s, est. speed input: 3703.37 toks/s, output: 156.63 toks/s]
[2025-01-11 01:36:42,585][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:04<00:00, 22.22it/s, est. speed input: 4224.20 toks/s, output: 190.02 toks/s]
[2025-01-11 01:36:43,343][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.22it/s, est. speed input: 3928.89 toks/s, output: 189.86 toks/s]
WARNING 01-11 01:36:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:43,740][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:44,726][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.02it/s, est. speed input: 715.67 toks/s, output: 21.32 toks/s]
[2025-01-11 01:36:45,725][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.60it/s, est. speed input: 1111.17 toks/s, output: 61.98 toks/s]
[2025-01-11 01:36:45,726][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.51it/s, est. speed input: 1111.17 toks/s, output: 61.98 toks/s]
WARNING 01-11 01:36:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:46,118][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:47,992][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:14,  1.87s/it, est. speed input: 399.93 toks/s, output: 14.42 toks/s]
[2025-01-11 01:36:48,387][root][ERROR] - Processed prompts:  67%|######6   | 6/9 [00:02<00:00,  3.34it/s, est. speed input: 2141.03 toks/s, output: 82.02 toks/s]
[2025-01-11 01:36:48,509][root][ERROR] - Processed prompts:  89%|########8 | 8/9 [00:02<00:00,  4.55it/s, est. speed input: 2713.30 toks/s, output: 118.42 toks/s]
[2025-01-11 01:36:48,771][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:02<00:00,  3.39it/s, est. speed input: 2761.61 toks/s, output: 131.23 toks/s]
WARNING 01-11 01:36:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:49,225][root][ERROR] - Processed prompts:   0%|          | 0/23 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:52,510][root][ERROR] - Processed prompts:   4%|4         | 1/23 [00:03<01:12,  3.28s/it, est. speed input: 225.65 toks/s, output: 7.31 toks/s]
[2025-01-11 01:36:52,748][root][ERROR] - Processed prompts:   9%|8         | 2/23 [00:03<00:31,  1.49s/it, est. speed input: 417.04 toks/s, output: 15.05 toks/s]
[2025-01-11 01:36:53,052][root][ERROR] - Processed prompts:  61%|######    | 14/23 [00:03<00:01,  6.34it/s, est. speed input: 2743.59 toks/s, output: 108.48 toks/s]
[2025-01-11 01:36:53,509][root][ERROR] - Processed prompts:  70%|######9   | 16/23 [00:04<00:01,  5.87it/s, est. speed input: 2806.21 toks/s, output: 119.30 toks/s]
[2025-01-11 01:36:53,995][root][ERROR] - Processed prompts:  78%|#######8  | 18/23 [00:04<00:00,  5.41it/s, est. speed input: 2830.14 toks/s, output: 134.84 toks/s]
[2025-01-11 01:36:54,192][root][ERROR] - Processed prompts:  87%|########6 | 20/23 [00:04<00:00,  6.04it/s, est. speed input: 3021.95 toks/s, output: 162.10 toks/s]
[2025-01-11 01:36:54,433][root][ERROR] - Processed prompts:  96%|#########5| 22/23 [00:05<00:00,  6.46it/s, est. speed input: 3187.13 toks/s, output: 189.92 toks/s]
[2025-01-11 01:36:54,451][root][ERROR] - Processed prompts: 100%|##########| 23/23 [00:05<00:00,  4.40it/s, est. speed input: 3319.00 toks/s, output: 208.24 toks/s]
WARNING 01-11 01:36:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:54,747][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:36:57,793][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:03<00:51,  3.05s/it, est. speed input: 274.86 toks/s, output: 8.54 toks/s]
[2025-01-11 01:36:57,917][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:03<00:21,  1.33s/it, est. speed input: 575.55 toks/s, output: 17.35 toks/s]
[2025-01-11 01:36:58,202][root][ERROR] - Processed prompts:  89%|########8 | 16/18 [00:03<00:00,  8.15it/s, est. speed input: 4359.85 toks/s, output: 138.07 toks/s]
[2025-01-11 01:36:59,016][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:04<00:00,  4.22it/s, est. speed input: 3957.37 toks/s, output: 144.30 toks/s]
WARNING 01-11 01:36:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:36:59,249][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:01,280][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:20,  2.03s/it, est. speed input: 461.05 toks/s, output: 14.28 toks/s]
[2025-01-11 01:37:01,729][root][ERROR] - Processed prompts:  82%|########1 | 9/11 [00:02<00:00,  4.65it/s, est. speed input: 2996.77 toks/s, output: 116.98 toks/s]
[2025-01-11 01:37:01,798][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  4.32it/s, est. speed input: 3528.62 toks/s, output: 155.79 toks/s]
WARNING 01-11 01:37:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:02,108][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:03,470][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.36s/it, est. speed input: 688.89 toks/s, output: 13.95 toks/s]
[2025-01-11 01:37:03,698][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.29it/s, est. speed input: 1774.58 toks/s, output: 42.15 toks/s]
[2025-01-11 01:37:04,039][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:01<00:00,  3.33it/s, est. speed input: 2391.85 toks/s, output: 73.55 toks/s]
[2025-01-11 01:37:04,297][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:02<00:00,  3.46it/s, est. speed input: 2543.80 toks/s, output: 92.29 toks/s]
[2025-01-11 01:37:04,297][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.20it/s, est. speed input: 2935.03 toks/s, output: 119.68 toks/s]
WARNING 01-11 01:37:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:04,531][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:06,427][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.90s/it, est. speed input: 557.52 toks/s, output: 14.77 toks/s]
[2025-01-11 01:37:06,695][root][ERROR] - Processed prompts:  78%|#######7  | 7/9 [00:02<00:00,  4.20it/s, est. speed input: 3177.02 toks/s, output: 98.46 toks/s]
[2025-01-11 01:37:09,048][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  1.93it/s, est. speed input: 1901.73 toks/s, output: 98.52 toks/s]
[2025-01-11 01:37:09,048][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:04<00:00,  1.99it/s, est. speed input: 1901.73 toks/s, output: 98.52 toks/s]
WARNING 01-11 01:37:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:09,309][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:10,427][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.12s/it, est. speed input: 960.15 toks/s, output: 25.95 toks/s]
[2025-01-11 01:37:10,562][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.94it/s, est. speed input: 2477.79 toks/s, output: 75.83 toks/s]
[2025-01-11 01:37:10,562][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.39it/s, est. speed input: 2477.79 toks/s, output: 75.83 toks/s]
WARNING 01-11 01:37:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:10,780][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:12,196][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.42s/it, est. speed input: 651.17 toks/s, output: 14.13 toks/s]
[2025-01-11 01:37:12,418][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:01<00:03,  1.40it/s, est. speed input: 1021.74 toks/s, output: 29.93 toks/s]
[2025-01-11 01:37:12,523][root][ERROR] - Processed prompts:  71%|#######1  | 5/7 [00:01<00:00,  4.39it/s, est. speed input: 2737.24 toks/s, output: 82.04 toks/s]
[2025-01-11 01:37:13,118][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.91it/s, est. speed input: 2910.26 toks/s, output: 110.78 toks/s]
[2025-01-11 01:37:13,118][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  2.99it/s, est. speed input: 2910.26 toks/s, output: 110.78 toks/s]
WARNING 01-11 01:37:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:13,358][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:14,804][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.45s/it, est. speed input: 790.79 toks/s, output: 19.37 toks/s]
[2025-01-11 01:37:14,827][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.41it/s, est. speed input: 3961.88 toks/s, output: 98.08 toks/s]
WARNING 01-11 01:37:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:15,051][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:15,998][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 1181.61 toks/s, output: 30.65 toks/s]
[2025-01-11 01:37:16,183][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  2.01it/s, est. speed input: 1855.21 toks/s, output: 60.99 toks/s]
[2025-01-11 01:37:16,183][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.77it/s, est. speed input: 1855.21 toks/s, output: 60.99 toks/s]
WARNING 01-11 01:37:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:16,439][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:17,835][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 628.83 toks/s, output: 47.27 toks/s]
[2025-01-11 01:37:17,835][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 628.83 toks/s, output: 47.27 toks/s]
WARNING 01-11 01:37:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:37:18,069][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:37:19,213][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 859.04 toks/s, output: 43.69 toks/s]
[2025-01-11 01:37:19,214][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 859.04 toks/s, output: 43.69 toks/s]
[2025-01-11 01:37:21,631][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:37:21,684][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:37:23,350][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.67s/it]
[2025-01-11 01:37:25,002][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 01:37:28,066][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.30s/it]
[2025-01-11 01:37:29,061][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.78s/it]
[2025-01-11 01:37:29,062][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.84s/it]
[2025-01-11 01:37:51,081][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:37:51,133][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:37:53,196][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.06s/it]
[2025-01-11 01:37:54,847][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.82s/it]
[2025-01-11 01:37:56,417][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 01:37:56,953][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 01:37:56,953][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 01:38:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:38:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:38:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:38:18 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:38:18,187][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:38:21,253][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.07s/it]
[2025-01-11 01:38:21,639][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:02,  1.49s/it]
[2025-01-11 01:38:23,179][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.51s/it]
[2025-01-11 01:38:24,526][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.45s/it]
[2025-01-11 01:38:24,526][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.58s/it]
INFO 01-11 01:38:24 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:38:38 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:38:39 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:38:39 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:39:06 model_runner.py:1335] Graph capturing finished in 27 secs.
WARNING 01-11 01:39:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:06,996][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:10,677][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:54,  3.68s/it, est. speed input: 172.54 toks/s, output: 2.99 toks/s]
[2025-01-11 01:39:10,794][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.74it/s, est. speed input: 835.98 toks/s, output: 15.01 toks/s]
[2025-01-11 01:39:11,018][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.98it/s, est. speed input: 1263.15 toks/s, output: 26.61 toks/s]
[2025-01-11 01:39:11,118][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  5.96it/s, est. speed input: 2002.81 toks/s, output: 47.80 toks/s]
[2025-01-11 01:39:11,251][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01,  8.56it/s, est. speed input: 2537.32 toks/s, output: 66.52 toks/s]
[2025-01-11 01:39:11,361][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.69it/s, est. speed input: 3055.20 toks/s, output: 86.60 toks/s]
[2025-01-11 01:39:11,482][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 16.15it/s, est. speed input: 3680.46 toks/s, output: 114.80 toks/s]
[2025-01-11 01:39:11,605][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 19.10it/s, est. speed input: 4133.75 toks/s, output: 140.39 toks/s]
[2025-01-11 01:39:11,995][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.40it/s, est. speed input: 4065.18 toks/s, output: 148.24 toks/s]
WARNING 01-11 01:39:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:12,454][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:15,655][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:39,  3.20s/it, est. speed input: 203.76 toks/s, output: 3.44 toks/s]
[2025-01-11 01:39:15,838][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:18,  1.53it/s, est. speed input: 771.30 toks/s, output: 14.48 toks/s]
[2025-01-11 01:39:15,956][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:13,  1.98it/s, est. speed input: 930.62 toks/s, output: 18.57 toks/s]
[2025-01-11 01:39:16,067][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:07,  3.27it/s, est. speed input: 1264.92 toks/s, output: 27.68 toks/s]
[2025-01-11 01:39:16,229][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:05,  4.57it/s, est. speed input: 1554.90 toks/s, output: 37.36 toks/s]
[2025-01-11 01:39:16,412][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:01,  9.88it/s, est. speed input: 2482.57 toks/s, output: 69.74 toks/s]
[2025-01-11 01:39:16,535][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 12.11it/s, est. speed input: 2901.74 toks/s, output: 87.50 toks/s]
[2025-01-11 01:39:16,640][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 14.69it/s, est. speed input: 3298.92 toks/s, output: 106.32 toks/s]
[2025-01-11 01:39:16,760][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 18.50it/s, est. speed input: 3816.64 toks/s, output: 134.03 toks/s]
[2025-01-11 01:39:16,885][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 21.63it/s, est. speed input: 4301.79 toks/s, output: 164.54 toks/s]
[2025-01-11 01:39:17,156][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 17.21it/s, est. speed input: 4478.51 toks/s, output: 186.75 toks/s]
[2025-01-11 01:39:17,156][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.81it/s, est. speed input: 4478.51 toks/s, output: 186.75 toks/s]
WARNING 01-11 01:39:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:17,625][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:21,162][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:42,  3.54s/it, est. speed input: 208.93 toks/s, output: 3.96 toks/s]
[2025-01-11 01:39:21,278][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:42,  1.52s/it, est. speed input: 402.94 toks/s, output: 8.21 toks/s]
[2025-01-11 01:39:21,390][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:15,  1.66it/s, est. speed input: 784.70 toks/s, output: 17.27 toks/s]
[2025-01-11 01:39:21,539][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:05,  4.16it/s, est. speed input: 1516.45 toks/s, output: 36.28 toks/s]
[2025-01-11 01:39:21,683][root][ERROR] - Processed prompts:  33%|###3      | 10/30 [00:04<00:03,  5.34it/s, est. speed input: 1826.62 toks/s, output: 46.82 toks/s]
[2025-01-11 01:39:21,817][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:04<00:02,  6.66it/s, est. speed input: 2128.85 toks/s, output: 58.21 toks/s]
[2025-01-11 01:39:21,967][root][ERROR] - Processed prompts:  63%|######3   | 19/30 [00:04<00:00, 13.90it/s, est. speed input: 3347.95 toks/s, output: 103.40 toks/s]
[2025-01-11 01:39:22,083][root][ERROR] - Processed prompts:  77%|#######6  | 23/30 [00:04<00:00, 17.34it/s, est. speed input: 3931.37 toks/s, output: 130.54 toks/s]
[2025-01-11 01:39:22,226][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 19.75it/s, est. speed input: 4456.06 toks/s, output: 160.18 toks/s]
[2025-01-11 01:39:22,473][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 17.03it/s, est. speed input: 4698.23 toks/s, output: 181.72 toks/s]
[2025-01-11 01:39:22,473][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.19it/s, est. speed input: 4698.23 toks/s, output: 181.72 toks/s]
WARNING 01-11 01:39:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:22,696][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:23,641][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 763.29 toks/s, output: 32.82 toks/s]
[2025-01-11 01:39:24,076][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.55it/s, est. speed input: 1067.30 toks/s, output: 63.76 toks/s]
[2025-01-11 01:39:24,076][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.45it/s, est. speed input: 1067.30 toks/s, output: 63.76 toks/s]
WARNING 01-11 01:39:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:24,315][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:25,547][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.23s/it, est. speed input: 608.19 toks/s, output: 17.86 toks/s]
[2025-01-11 01:39:25,707][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.66it/s, est. speed input: 1139.45 toks/s, output: 36.66 toks/s]
[2025-01-11 01:39:25,948][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.38it/s, est. speed input: 2440.96 toks/s, output: 93.10 toks/s]
[2025-01-11 01:39:25,966][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.64it/s, est. speed input: 2941.46 toks/s, output: 118.19 toks/s]
WARNING 01-11 01:39:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:26,232][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:28,782][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<00:58,  2.55s/it, est. speed input: 274.19 toks/s, output: 3.53 toks/s]
[2025-01-11 01:39:29,756][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:35,  1.62s/it, est. speed input: 398.20 toks/s, output: 10.79 toks/s]
[2025-01-11 01:39:30,009][root][ERROR] - Processed prompts:  92%|#########1| 22/24 [00:03<00:00,  9.70it/s, est. speed input: 4419.59 toks/s, output: 169.19 toks/s]
[2025-01-11 01:39:30,289][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.92it/s, est. speed input: 4483.40 toks/s, output: 183.40 toks/s]
WARNING 01-11 01:39:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:30,594][root][ERROR] - Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:33,642][root][ERROR] - Processed prompts:   6%|5         | 1/17 [00:03<00:48,  3.05s/it, est. speed input: 315.95 toks/s, output: 9.51 toks/s]
[2025-01-11 01:39:33,911][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:03<00:00,  6.95it/s, est. speed input: 4844.09 toks/s, output: 153.46 toks/s]
[2025-01-11 01:39:33,911][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:03<00:00,  5.12it/s, est. speed input: 4844.09 toks/s, output: 153.46 toks/s]
WARNING 01-11 01:39:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:34,140][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:35,781][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.64s/it, est. speed input: 548.94 toks/s, output: 17.67 toks/s]
[2025-01-11 01:39:35,964][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  2.78it/s, est. speed input: 1809.99 toks/s, output: 67.99 toks/s]
[2025-01-11 01:39:36,125][root][ERROR] - Processed prompts:  75%|#######5  | 6/8 [00:01<00:00,  4.20it/s, est. speed input: 2419.86 toks/s, output: 104.27 toks/s]
[2025-01-11 01:39:36,295][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.57it/s, est. speed input: 2995.81 toks/s, output: 142.92 toks/s]
[2025-01-11 01:39:36,295][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.71it/s, est. speed input: 2995.81 toks/s, output: 142.92 toks/s]
WARNING 01-11 01:39:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:36,608][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:37,761][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.15s/it, est. speed input: 714.75 toks/s, output: 13.88 toks/s]
[2025-01-11 01:39:38,046][root][ERROR] - Processed prompts:  50%|#####     | 3/6 [00:01<00:01,  2.47it/s, est. speed input: 1767.71 toks/s, output: 43.79 toks/s]
[2025-01-11 01:39:38,709][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.73it/s, est. speed input: 2132.96 toks/s, output: 74.73 toks/s]
[2025-01-11 01:39:38,726][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.83it/s, est. speed input: 2536.49 toks/s, output: 105.27 toks/s]
WARNING 01-11 01:39:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:38,946][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:40,166][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.22s/it, est. speed input: 752.43 toks/s, output: 23.77 toks/s]
[2025-01-11 01:39:40,451][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.25it/s, est. speed input: 2454.05 toks/s, output: 88.38 toks/s]
[2025-01-11 01:39:40,452][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.66it/s, est. speed input: 2454.05 toks/s, output: 88.38 toks/s]
WARNING 01-11 01:39:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:40,696][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:41,773][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 921.95 toks/s, output: 26.95 toks/s]
[2025-01-11 01:39:42,012][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.71it/s, est. speed input: 1399.49 toks/s, output: 53.97 toks/s]
[2025-01-11 01:39:42,282][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.27it/s, est. speed input: 1827.75 toks/s, output: 81.36 toks/s]
[2025-01-11 01:39:42,282][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.89it/s, est. speed input: 1827.75 toks/s, output: 81.36 toks/s]
WARNING 01-11 01:39:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:42,497][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:43,290][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.26it/s, est. speed input: 1101.32 toks/s, output: 20.16 toks/s]
[2025-01-11 01:39:43,530][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  2.14it/s, est. speed input: 1817.98 toks/s, output: 43.56 toks/s]
[2025-01-11 01:39:43,681][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  3.10it/s, est. speed input: 2312.00 toks/s, output: 70.11 toks/s]
[2025-01-11 01:39:43,681][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.53it/s, est. speed input: 2312.00 toks/s, output: 70.11 toks/s]
WARNING 01-11 01:39:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:43,913][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:44,918][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.00s/it, est. speed input: 924.69 toks/s, output: 13.93 toks/s]
[2025-01-11 01:39:45,246][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.65it/s, est. speed input: 1540.19 toks/s, output: 32.27 toks/s]
[2025-01-11 01:39:45,406][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.48it/s, est. speed input: 1988.51 toks/s, output: 53.60 toks/s]
[2025-01-11 01:39:45,572][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.23it/s, est. speed input: 2328.17 toks/s, output: 75.98 toks/s]
[2025-01-11 01:39:46,091][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.60it/s, est. speed input: 2139.15 toks/s, output: 93.25 toks/s]
[2025-01-11 01:39:46,091][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.30it/s, est. speed input: 2139.15 toks/s, output: 93.25 toks/s]
WARNING 01-11 01:39:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:46,318][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:47,141][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.22it/s, est. speed input: 1000.70 toks/s, output: 27.97 toks/s]
[2025-01-11 01:39:47,242][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.51it/s, est. speed input: 1824.03 toks/s, output: 56.32 toks/s]
[2025-01-11 01:39:47,242][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.17it/s, est. speed input: 1824.03 toks/s, output: 56.32 toks/s]
WARNING 01-11 01:39:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:47,457][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:48,610][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.15s/it, est. speed input: 844.03 toks/s, output: 36.43 toks/s]
[2025-01-11 01:39:48,846][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.63it/s, est. speed input: 1342.51 toks/s, output: 70.54 toks/s]
[2025-01-11 01:39:48,847][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.44it/s, est. speed input: 1342.51 toks/s, output: 70.54 toks/s]
WARNING 01-11 01:39:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:49,088][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:50,259][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.17s/it, est. speed input: 738.67 toks/s, output: 25.62 toks/s]
[2025-01-11 01:39:50,480][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.63it/s, est. speed input: 1276.22 toks/s, output: 51.71 toks/s]
[2025-01-11 01:39:50,751][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.19it/s, est. speed input: 1865.44 toks/s, output: 78.18 toks/s]
[2025-01-11 01:39:50,751][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.80it/s, est. speed input: 1865.44 toks/s, output: 78.18 toks/s]
WARNING 01-11 01:39:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:39:50,975][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:39:51,751][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1254.94 toks/s, output: 37.40 toks/s]
[2025-01-11 01:39:51,751][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1254.94 toks/s, output: 37.40 toks/s]
[2025-01-11 01:39:54,241][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:39:54,294][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:39:55,962][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 01:39:57,627][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 01:39:59,217][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 01:39:59,749][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:39:59,749][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 01:40:28,125][root][ERROR] - /home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 9c36463e-7536-43db-ab1e-0a26a7cb64f0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.
  warnings.warn(
[2025-01-11 01:40:28,126][root][ERROR] - /home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[2025-01-11 01:40:29,608][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:40:29,660][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:40:31,641][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 01:40:33,242][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 01:40:34,775][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 01:40:35,282][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:40:35,282][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 01:40:53 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:40:53 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:40:53 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:40:53 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:40:53,968][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:40:55,311][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 01:40:55,721][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 01:40:57,029][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:40:58,366][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:40:58,366][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 01:40:58 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:41:12 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:41:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:41:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:41:35 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:41:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:35,593][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:39,218][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:52,  3.62s/it, est. speed input: 175.22 toks/s, output: 3.04 toks/s]
[2025-01-11 01:41:39,499][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.67it/s, est. speed input: 813.12 toks/s, output: 15.37 toks/s]
[2025-01-11 01:41:39,604][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.99it/s, est. speed input: 1583.77 toks/s, output: 36.41 toks/s]
[2025-01-11 01:41:39,730][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.47it/s, est. speed input: 2456.60 toks/s, output: 62.87 toks/s]
[2025-01-11 01:41:39,835][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.95it/s, est. speed input: 3144.59 toks/s, output: 87.72 toks/s]
[2025-01-11 01:41:39,950][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.83it/s, est. speed input: 3790.31 toks/s, output: 115.71 toks/s]
[2025-01-11 01:41:40,183][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 16.54it/s, est. speed input: 4289.77 toks/s, output: 146.88 toks/s]
[2025-01-11 01:41:40,200][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.95it/s, est. speed input: 4411.58 toks/s, output: 155.01 toks/s]
WARNING 01-11 01:41:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:40,455][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:43,522][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:35,  3.07s/it, est. speed input: 218.19 toks/s, output: 3.59 toks/s]
[2025-01-11 01:41:43,642][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:24,  1.19it/s, est. speed input: 622.07 toks/s, output: 11.30 toks/s]
[2025-01-11 01:41:43,756][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:11,  2.29it/s, est. speed input: 997.13 toks/s, output: 19.69 toks/s]
[2025-01-11 01:41:43,862][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:06,  3.64it/s, est. speed input: 1349.36 toks/s, output: 28.47 toks/s]
[2025-01-11 01:41:44,004][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:03<00:02,  8.87it/s, est. speed input: 2397.52 toks/s, output: 58.34 toks/s]
[2025-01-11 01:41:44,122][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:03<00:01, 12.33it/s, est. speed input: 3037.73 toks/s, output: 79.64 toks/s]
[2025-01-11 01:41:44,228][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:03<00:00, 16.17it/s, est. speed input: 3652.86 toks/s, output: 103.92 toks/s]
[2025-01-11 01:41:44,485][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 15.95it/s, est. speed input: 4068.72 toks/s, output: 128.05 toks/s]
[2025-01-11 01:41:44,823][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 13.18it/s, est. speed input: 4210.84 toks/s, output: 148.38 toks/s]
[2025-01-11 01:41:44,994][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.14it/s, est. speed input: 4489.46 toks/s, output: 180.23 toks/s]
[2025-01-11 01:41:45,062][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.95it/s, est. speed input: 4564.22 toks/s, output: 190.84 toks/s]
WARNING 01-11 01:41:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:45,326][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:48,557][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:36,  3.23s/it, est. speed input: 224.07 toks/s, output: 3.09 toks/s]
[2025-01-11 01:41:48,793][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:42,  1.47s/it, est. speed input: 421.97 toks/s, output: 6.92 toks/s]
[2025-01-11 01:41:48,908][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.71it/s, est. speed input: 816.63 toks/s, output: 15.36 toks/s]
[2025-01-11 01:41:49,069][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:08,  2.89it/s, est. speed input: 1179.26 toks/s, output: 24.31 toks/s]
[2025-01-11 01:41:49,223][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:05,  4.21it/s, est. speed input: 1510.12 toks/s, output: 34.38 toks/s]
[2025-01-11 01:41:49,361][root][ERROR] - Processed prompts:  35%|###5      | 11/31 [00:04<00:02,  6.71it/s, est. speed input: 2009.30 toks/s, output: 50.30 toks/s]
[2025-01-11 01:41:49,539][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:04<00:02,  7.66it/s, est. speed input: 2277.72 toks/s, output: 61.71 toks/s]
[2025-01-11 01:41:49,659][root][ERROR] - Processed prompts:  74%|#######4  | 23/31 [00:04<00:00, 20.32it/s, est. speed input: 3956.20 toks/s, output: 128.77 toks/s]
[2025-01-11 01:41:49,986][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 17.13it/s, est. speed input: 4336.21 toks/s, output: 155.57 toks/s]
[2025-01-11 01:41:50,280][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:04<00:00, 14.80it/s, est. speed input: 4532.88 toks/s, output: 179.43 toks/s]
[2025-01-11 01:41:50,331][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  6.19it/s, est. speed input: 4639.77 toks/s, output: 190.40 toks/s]
WARNING 01-11 01:41:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:50,624][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:51,392][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 929.90 toks/s, output: 37.77 toks/s]
[2025-01-11 01:41:51,392][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 929.90 toks/s, output: 37.77 toks/s]
WARNING 01-11 01:41:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:51,613][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:52,364][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.33it/s, est. speed input: 1005.20 toks/s, output: 13.31 toks/s]
[2025-01-11 01:41:52,743][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.88it/s, est. speed input: 1432.27 toks/s, output: 34.52 toks/s]
[2025-01-11 01:41:53,074][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.27it/s, est. speed input: 1677.32 toks/s, output: 58.85 toks/s]
[2025-01-11 01:41:53,798][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.81it/s, est. speed input: 1506.75 toks/s, output: 80.55 toks/s]
[2025-01-11 01:41:53,798][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.83it/s, est. speed input: 1506.75 toks/s, output: 80.55 toks/s]
WARNING 01-11 01:41:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:54,054][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:41:57,210][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:25,  3.16s/it, est. speed input: 228.75 toks/s, output: 4.12 toks/s]
[2025-01-11 01:41:57,429][root][ERROR] - Processed prompts:   7%|7         | 2/28 [00:03<00:37,  1.43s/it, est. speed input: 428.50 toks/s, output: 8.89 toks/s]
[2025-01-11 01:41:57,589][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:21,  1.18it/s, est. speed input: 610.81 toks/s, output: 14.15 toks/s]
[2025-01-11 01:41:58,043][root][ERROR] - Processed prompts:  18%|#7        | 5/28 [00:03<00:11,  2.01it/s, est. speed input: 902.38 toks/s, output: 25.07 toks/s]
[2025-01-11 01:41:58,431][root][ERROR] - Processed prompts:  93%|#########2| 26/28 [00:04<00:00, 14.51it/s, est. speed input: 4526.11 toks/s, output: 167.91 toks/s]
[2025-01-11 01:41:58,791][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  5.91it/s, est. speed input: 4503.63 toks/s, output: 180.91 toks/s]
WARNING 01-11 01:41:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:41:59,074][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:02,792][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:18,  3.72s/it, est. speed input: 260.38 toks/s, output: 7.80 toks/s]
[2025-01-11 01:42:02,923][root][ERROR] - Processed prompts:  95%|#########5| 21/22 [00:03<00:00,  7.57it/s, est. speed input: 5039.85 toks/s, output: 160.29 toks/s]
[2025-01-11 01:42:03,008][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:03<00:00,  5.59it/s, est. speed input: 5171.87 toks/s, output: 167.28 toks/s]
WARNING 01-11 01:42:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:03,230][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:04,326][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.10s/it, est. speed input: 644.44 toks/s, output: 19.17 toks/s]
[2025-01-11 01:42:04,500][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.81it/s, est. speed input: 1161.83 toks/s, output: 39.38 toks/s]
[2025-01-11 01:42:04,868][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  3.11it/s, est. speed input: 1883.98 toks/s, output: 78.14 toks/s]
[2025-01-11 01:42:04,987][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.86it/s, est. speed input: 2321.40 toks/s, output: 104.77 toks/s]
[2025-01-11 01:42:04,987][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.85it/s, est. speed input: 2321.40 toks/s, output: 104.77 toks/s]
WARNING 01-11 01:42:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:05,294][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:06,357][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.06s/it, est. speed input: 934.82 toks/s, output: 15.05 toks/s]
[2025-01-11 01:42:06,626][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.67it/s, est. speed input: 2133.75 toks/s, output: 48.80 toks/s]
[2025-01-11 01:42:06,677][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.62it/s, est. speed input: 3480.77 toks/s, output: 91.10 toks/s]
WARNING 01-11 01:42:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:06,897][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:08,522][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.62s/it, est. speed input: 585.30 toks/s, output: 17.85 toks/s]
[2025-01-11 01:42:10,216][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.37it/s, est. speed input: 1958.16 toks/s, output: 91.91 toks/s]
[2025-01-11 01:42:10,216][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.11it/s, est. speed input: 1958.16 toks/s, output: 91.91 toks/s]
WARNING 01-11 01:42:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:10,486][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:11,453][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.03it/s, est. speed input: 1141.76 toks/s, output: 29.99 toks/s]
[2025-01-11 01:42:11,454][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.07it/s, est. speed input: 2331.00 toks/s, output: 59.95 toks/s]
WARNING 01-11 01:42:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:11,674][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:12,687][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.01s/it, est. speed input: 772.07 toks/s, output: 28.63 toks/s]
[2025-01-11 01:42:12,687][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.96it/s, est. speed input: 2307.50 toks/s, output: 85.86 toks/s]
WARNING 01-11 01:42:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:42:12,918][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:42:14,077][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.16s/it, est. speed input: 1053.20 toks/s, output: 25.03 toks/s]
[2025-01-11 01:42:14,077][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.59it/s, est. speed input: 3000.60 toks/s, output: 75.08 toks/s]
[2025-01-11 01:42:16,528][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:42:16,582][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:42:18,186][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 01:42:19,872][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 01:42:21,486][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 01:42:22,016][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:42:22,016][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 01:42:42,597][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:42:42,649][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:42:45,209][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.56s/it]
[2025-01-11 01:42:47,038][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.13s/it]
[2025-01-11 01:42:48,906][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.01s/it]
[2025-01-11 01:42:49,414][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.42s/it]
[2025-01-11 01:42:49,414][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.69s/it]
WARNING 01-11 01:43:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:43:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:43:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:43:07 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:43:08,056][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:43:09,393][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 01:43:09,770][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 01:43:11,061][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 01:43:12,388][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 01:43:12,388][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 01:43:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:43:26 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:43:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:43:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:43:48 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 01:43:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:43:48,784][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:43:52,580][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 167.30 toks/s, output: 2.90 toks/s]
[2025-01-11 01:43:52,693][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.70it/s, est. speed input: 812.36 toks/s, output: 14.58 toks/s]
[2025-01-11 01:43:52,909][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.92it/s, est. speed input: 1231.67 toks/s, output: 25.94 toks/s]
[2025-01-11 01:43:53,091][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  6.16it/s, est. speed input: 2064.59 toks/s, output: 50.63 toks/s]
[2025-01-11 01:43:53,206][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  8.69it/s, est. speed input: 2585.36 toks/s, output: 69.44 toks/s]
[2025-01-11 01:43:53,327][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.46it/s, est. speed input: 3494.48 toks/s, output: 107.42 toks/s]
[2025-01-11 01:43:53,442][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 17.26it/s, est. speed input: 3953.63 toks/s, output: 131.18 toks/s]
[2025-01-11 01:43:53,516][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.76it/s, est. speed input: 4294.66 toks/s, output: 151.54 toks/s]
WARNING 01-11 01:43:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:43:53,793][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:43:56,868][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:35,  3.07s/it, est. speed input: 217.63 toks/s, output: 3.58 toks/s]
[2025-01-11 01:43:56,989][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:40,  1.34s/it, est. speed input: 416.24 toks/s, output: 7.51 toks/s]
[2025-01-11 01:43:57,215][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:11,  2.31it/s, est. speed input: 962.13 toks/s, output: 19.87 toks/s]
[2025-01-11 01:43:57,321][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.30it/s, est. speed input: 1491.52 toks/s, output: 34.86 toks/s]
[2025-01-11 01:43:57,463][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:03<00:02,  8.27it/s, est. speed input: 2325.44 toks/s, output: 61.59 toks/s]
[2025-01-11 01:43:57,654][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 12.85it/s, est. speed input: 3227.60 toks/s, output: 96.10 toks/s]
[2025-01-11 01:43:57,770][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:03<00:00, 19.80it/s, est. speed input: 4291.39 toks/s, output: 144.09 toks/s]
[2025-01-11 01:43:57,926][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 21.08it/s, est. speed input: 4765.92 toks/s, output: 173.24 toks/s]
[2025-01-11 01:43:58,217][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.23it/s, est. speed input: 4754.50 toks/s, output: 183.80 toks/s]
WARNING 01-11 01:43:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:43:58,479][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:01,957][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:44,  3.48s/it, est. speed input: 213.93 toks/s, output: 4.03 toks/s]
[2025-01-11 01:44:02,134][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:44,  1.54s/it, est. speed input: 406.32 toks/s, output: 8.48 toks/s]
[2025-01-11 01:44:02,351][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.05it/s, est. speed input: 959.17 toks/s, output: 22.21 toks/s]
[2025-01-11 01:44:02,452][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:03<00:04,  4.52it/s, est. speed input: 1685.60 toks/s, output: 43.79 toks/s]
[2025-01-11 01:44:02,589][root][ERROR] - Processed prompts:  39%|###8      | 12/31 [00:04<00:02,  6.49it/s, est. speed input: 2172.66 toks/s, output: 60.34 toks/s]
[2025-01-11 01:44:02,716][root][ERROR] - Processed prompts:  48%|####8     | 15/31 [00:04<00:01,  8.72it/s, est. speed input: 2631.66 toks/s, output: 78.35 toks/s]
[2025-01-11 01:44:02,844][root][ERROR] - Processed prompts:  71%|#######   | 22/31 [00:04<00:00, 15.99it/s, est. speed input: 3757.40 toks/s, output: 124.63 toks/s]
[2025-01-11 01:44:03,108][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 15.73it/s, est. speed input: 4187.53 toks/s, output: 151.23 toks/s]
[2025-01-11 01:44:03,384][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00, 16.50it/s, est. speed input: 4722.71 toks/s, output: 190.83 toks/s]
[2025-01-11 01:44:03,384][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.32it/s, est. speed input: 4722.71 toks/s, output: 190.83 toks/s]
WARNING 01-11 01:44:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:03,629][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:04,746][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 655.26 toks/s, output: 44.76 toks/s]
[2025-01-11 01:44:04,746][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 655.26 toks/s, output: 44.76 toks/s]
WARNING 01-11 01:44:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:04,967][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:06,148][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.18s/it, est. speed input: 712.38 toks/s, output: 24.56 toks/s]
[2025-01-11 01:44:06,399][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.43it/s, est. speed input: 2377.62 toks/s, output: 91.50 toks/s]
[2025-01-11 01:44:06,399][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.79it/s, est. speed input: 2377.62 toks/s, output: 91.50 toks/s]
WARNING 01-11 01:44:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:06,662][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:09,960][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:29,  3.30s/it, est. speed input: 214.69 toks/s, output: 4.25 toks/s]
[2025-01-11 01:44:10,233][root][ERROR] - Processed prompts:   7%|7         | 2/28 [00:03<00:39,  1.52s/it, est. speed input: 406.14 toks/s, output: 9.24 toks/s]
[2025-01-11 01:44:10,446][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:23,  1.08it/s, est. speed input: 571.98 toks/s, output: 14.80 toks/s]
[2025-01-11 01:44:10,601][root][ERROR] - Processed prompts:  14%|#4        | 4/28 [00:03<00:14,  1.61it/s, est. speed input: 729.42 toks/s, output: 20.82 toks/s]
[2025-01-11 01:44:10,753][root][ERROR] - Processed prompts:  18%|#7        | 5/28 [00:04<00:10,  2.22it/s, est. speed input: 875.73 toks/s, output: 27.14 toks/s]
[2025-01-11 01:44:10,907][root][ERROR] - Processed prompts:  89%|########9 | 25/28 [00:04<00:00, 21.33it/s, est. speed input: 4643.13 toks/s, output: 164.93 toks/s]
[2025-01-11 01:44:11,407][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  5.90it/s, est. speed input: 4629.21 toks/s, output: 178.52 toks/s]
WARNING 01-11 01:44:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:11,713][root][ERROR] - Processed prompts:   0%|          | 0/17 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:14,777][root][ERROR] - Processed prompts:   6%|5         | 1/17 [00:03<00:49,  3.06s/it, est. speed input: 268.65 toks/s, output: 9.47 toks/s]
[2025-01-11 01:44:15,199][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:03<00:00,  6.51it/s, est. speed input: 4524.41 toks/s, output: 149.20 toks/s]
[2025-01-11 01:44:15,199][root][ERROR] - Processed prompts: 100%|##########| 17/17 [00:03<00:00,  4.88it/s, est. speed input: 4524.41 toks/s, output: 149.20 toks/s]
WARNING 01-11 01:44:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:15,418][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:16,583][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.16s/it, est. speed input: 780.14 toks/s, output: 24.92 toks/s]
[2025-01-11 01:44:16,770][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.69it/s, est. speed input: 2405.38 toks/s, output: 95.47 toks/s]
[2025-01-11 01:44:16,770][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  2.96it/s, est. speed input: 2405.38 toks/s, output: 95.47 toks/s]
WARNING 01-11 01:44:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:17,054][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:17,998][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 1021.60 toks/s, output: 30.73 toks/s]
[2025-01-11 01:44:18,166][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  2.05it/s, est. speed input: 1690.76 toks/s, output: 61.15 toks/s]
[2025-01-11 01:44:18,166][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.80it/s, est. speed input: 1690.76 toks/s, output: 61.15 toks/s]
WARNING 01-11 01:44:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:18,386][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:19,875][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.49s/it, est. speed input: 651.70 toks/s, output: 19.48 toks/s]
[2025-01-11 01:44:19,875][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.03it/s, est. speed input: 3753.92 toks/s, output: 116.87 toks/s]
WARNING 01-11 01:44:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:20,125][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:20,932][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1332.62 toks/s, output: 35.95 toks/s]
[2025-01-11 01:44:20,932][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1332.62 toks/s, output: 35.95 toks/s]
WARNING 01-11 01:44:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:21,145][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:21,981][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 935.92 toks/s, output: 39.49 toks/s]
[2025-01-11 01:44:21,981][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 935.92 toks/s, output: 39.49 toks/s]
WARNING 01-11 01:44:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:44:22,221][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:44:23,105][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 1318.08 toks/s, output: 32.81 toks/s]
[2025-01-11 01:44:23,105][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 1318.08 toks/s, output: 32.81 toks/s]
[2025-01-11 01:44:25,628][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:44:25,680][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:44:27,407][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.73s/it]
[2025-01-11 01:44:29,051][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 01:44:30,663][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 01:44:31,199][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:44:31,200][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 01:44:50,501][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:44:50,554][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:44:52,539][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 01:44:54,224][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 01:44:55,862][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.73s/it]
[2025-01-11 01:44:56,373][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 01:44:56,374][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 01:45:14 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:45:14 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:45:15 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:45:15 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:45:15,286][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:45:16,642][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 01:45:17,019][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 01:45:18,314][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 01:45:19,663][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:45:19,664][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:45:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:45:33 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:45:34 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:45:34 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:45:57 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:45:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:45:57,580][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:01,437][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.86s/it, est. speed input: 164.67 toks/s, output: 2.85 toks/s]
[2025-01-11 01:46:01,717][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.58it/s, est. speed input: 767.59 toks/s, output: 14.51 toks/s]
[2025-01-11 01:46:01,817][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.71it/s, est. speed input: 1798.57 toks/s, output: 42.25 toks/s]
[2025-01-11 01:46:01,942][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.77it/s, est. speed input: 2329.26 toks/s, output: 58.69 toks/s]
[2025-01-11 01:46:02,049][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.15it/s, est. speed input: 2984.54 toks/s, output: 82.59 toks/s]
[2025-01-11 01:46:02,161][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.01it/s, est. speed input: 3604.05 toks/s, output: 108.93 toks/s]
[2025-01-11 01:46:02,436][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 15.20it/s, est. speed input: 4054.51 toks/s, output: 139.03 toks/s]
[2025-01-11 01:46:02,503][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.50it/s, est. speed input: 4128.30 toks/s, output: 146.28 toks/s]
WARNING 01-11 01:46:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:02,750][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:05,687][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:31,  2.94s/it, est. speed input: 227.14 toks/s, output: 3.06 toks/s]
[2025-01-11 01:46:05,808][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:38,  1.28s/it, est. speed input: 431.38 toks/s, output: 6.54 toks/s]
[2025-01-11 01:46:05,926][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:21,  1.33it/s, est. speed input: 618.82 toks/s, output: 10.39 toks/s]
[2025-01-11 01:46:06,041][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:13,  2.00it/s, est. speed input: 793.40 toks/s, output: 14.59 toks/s]
[2025-01-11 01:46:06,154][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:09,  2.78it/s, est. speed input: 965.81 toks/s, output: 19.10 toks/s]
[2025-01-11 01:46:06,263][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:07,  3.64it/s, est. speed input: 1121.84 toks/s, output: 23.92 toks/s]
[2025-01-11 01:46:06,365][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:03<00:02,  9.08it/s, est. speed input: 1813.06 toks/s, output: 45.65 toks/s]
[2025-01-11 01:46:06,498][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:01, 13.74it/s, est. speed input: 2447.59 toks/s, output: 67.79 toks/s]
[2025-01-11 01:46:06,617][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:00, 18.11it/s, est. speed input: 3048.66 toks/s, output: 92.59 toks/s]
[2025-01-11 01:46:06,751][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 19.22it/s, est. speed input: 3440.25 toks/s, output: 111.23 toks/s]
[2025-01-11 01:46:06,920][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 20.67it/s, est. speed input: 3934.72 toks/s, output: 139.11 toks/s]
[2025-01-11 01:46:07,408][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 12.40it/s, est. speed input: 3947.25 toks/s, output: 157.82 toks/s]
[2025-01-11 01:46:07,572][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 12.36it/s, est. speed input: 4085.46 toks/s, output: 178.39 toks/s]
[2025-01-11 01:46:07,947][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  9.45it/s, est. speed input: 4046.83 toks/s, output: 195.69 toks/s]
[2025-01-11 01:46:07,948][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.16it/s, est. speed input: 4046.83 toks/s, output: 195.69 toks/s]
WARNING 01-11 01:46:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:08,215][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:11,300][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:26,  3.08s/it, est. speed input: 240.90 toks/s, output: 2.92 toks/s]
[2025-01-11 01:46:11,750][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:41,  1.54s/it, est. speed input: 420.96 toks/s, output: 7.36 toks/s]
[2025-01-11 01:46:12,177][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:03<00:17,  1.42it/s, est. speed input: 750.88 toks/s, output: 17.42 toks/s]
[2025-01-11 01:46:12,281][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:04<00:09,  2.52it/s, est. speed input: 1102.55 toks/s, output: 30.01 toks/s]
[2025-01-11 01:46:12,415][root][ERROR] - Processed prompts:  52%|#####1    | 15/29 [00:04<00:01,  9.13it/s, est. speed input: 2828.57 toks/s, output: 91.21 toks/s]
[2025-01-11 01:46:12,812][root][ERROR] - Processed prompts:  66%|######5   | 19/29 [00:04<00:01,  9.41it/s, est. speed input: 3231.68 toks/s, output: 115.08 toks/s]
[2025-01-11 01:46:12,937][root][ERROR] - Processed prompts:  90%|########9 | 26/29 [00:04<00:00, 14.98it/s, est. speed input: 4258.69 toks/s, output: 177.89 toks/s]
[2025-01-11 01:46:15,566][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:07<00:00,  3.95it/s, est. speed input: 3051.71 toks/s, output: 159.71 toks/s]
WARNING 01-11 01:46:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:15,808][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:16,521][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.40it/s, est. speed input: 969.59 toks/s, output: 19.64 toks/s]
[2025-01-11 01:46:16,632][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:00<00:00,  2.79it/s, est. speed input: 1684.16 toks/s, output: 41.28 toks/s]
[2025-01-11 01:46:17,199][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.20it/s, est. speed input: 1536.13 toks/s, output: 63.26 toks/s]
[2025-01-11 01:46:17,199][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.16it/s, est. speed input: 1536.13 toks/s, output: 63.26 toks/s]
WARNING 01-11 01:46:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:17,453][root][ERROR] - Processed prompts:   0%|          | 0/13 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:19,261][root][ERROR] - Processed prompts:   8%|7         | 1/13 [00:01<00:21,  1.81s/it, est. speed input: 400.96 toks/s, output: 7.74 toks/s]
[2025-01-11 01:46:19,526][root][ERROR] - Processed prompts:  15%|#5        | 2/13 [00:02<00:09,  1.11it/s, est. speed input: 701.49 toks/s, output: 17.37 toks/s]
[2025-01-11 01:46:19,710][root][ERROR] - Processed prompts:  31%|###       | 4/13 [00:02<00:03,  2.54it/s, est. speed input: 1339.77 toks/s, output: 39.43 toks/s]
[2025-01-11 01:46:19,861][root][ERROR] - Processed prompts:  92%|#########2| 12/13 [00:02<00:00,  9.94it/s, est. speed input: 4153.66 toks/s, output: 138.32 toks/s]
[2025-01-11 01:46:19,878][root][ERROR] - Processed prompts: 100%|##########| 13/13 [00:02<00:00,  5.36it/s, est. speed input: 4458.62 toks/s, output: 152.18 toks/s]
WARNING 01-11 01:46:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:20,123][root][ERROR] - Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:22,719][root][ERROR] - Processed prompts:   6%|6         | 1/16 [00:02<00:38,  2.60s/it, est. speed input: 274.70 toks/s, output: 11.17 toks/s]
[2025-01-11 01:46:22,938][root][ERROR] - Processed prompts:  88%|########7 | 14/16 [00:02<00:00,  6.73it/s, est. speed input: 3839.24 toks/s, output: 148.14 toks/s]
[2025-01-11 01:46:23,186][root][ERROR] - Processed prompts: 100%|##########| 16/16 [00:03<00:00,  5.22it/s, est. speed input: 4023.39 toks/s, output: 169.45 toks/s]
WARNING 01-11 01:46:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:23,457][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:25,828][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:26,  2.37s/it, est. speed input: 409.92 toks/s, output: 12.23 toks/s]
[2025-01-11 01:46:26,675][root][ERROR] - Processed prompts:  92%|#########1| 11/12 [00:03<00:00,  4.27it/s, est. speed input: 3325.55 toks/s, output: 113.44 toks/s]
[2025-01-11 01:46:26,709][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.69it/s, est. speed input: 3564.09 toks/s, output: 135.93 toks/s]
WARNING 01-11 01:46:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:26,938][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:29,108][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:23,  2.17s/it, est. speed input: 408.30 toks/s, output: 13.36 toks/s]
[2025-01-11 01:46:29,433][root][ERROR] - Processed prompts:  92%|#########1| 11/12 [00:02<00:00,  5.80it/s, est. speed input: 3593.64 toks/s, output: 137.48 toks/s]
[2025-01-11 01:46:30,671][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:03<00:00,  3.21it/s, est. speed input: 2619.52 toks/s, output: 124.04 toks/s]
WARNING 01-11 01:46:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:31,008][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:32,561][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.55s/it, est. speed input: 690.96 toks/s, output: 18.67 toks/s]
[2025-01-11 01:46:33,150][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:02<00:00,  2.80it/s, est. speed input: 2314.97 toks/s, output: 82.64 toks/s]
[2025-01-11 01:46:33,370][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  3.08it/s, est. speed input: 2618.08 toks/s, output: 106.28 toks/s]
[2025-01-11 01:46:33,370][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:02<00:00,  2.54it/s, est. speed input: 2618.08 toks/s, output: 106.28 toks/s]
WARNING 01-11 01:46:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:33,589][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:34,651][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 826.50 toks/s, output: 27.30 toks/s]
[2025-01-11 01:46:34,964][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.61it/s, est. speed input: 1308.92 toks/s, output: 54.54 toks/s]
[2025-01-11 01:46:36,845][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:03<00:00,  1.20s/it, est. speed input: 834.57 toks/s, output: 71.57 toks/s]
[2025-01-11 01:46:36,845][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:03<00:00,  1.09s/it, est. speed input: 834.57 toks/s, output: 71.57 toks/s]
WARNING 01-11 01:46:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:37,097][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:37,925][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.21it/s, est. speed input: 932.78 toks/s, output: 19.33 toks/s]
[2025-01-11 01:46:38,165][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  2.08it/s, est. speed input: 1789.80 toks/s, output: 42.17 toks/s]
[2025-01-11 01:46:38,501][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.41it/s, est. speed input: 2078.43 toks/s, output: 66.95 toks/s]
[2025-01-11 01:46:38,502][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.14it/s, est. speed input: 2078.43 toks/s, output: 66.95 toks/s]
WARNING 01-11 01:46:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:38,708][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:40,058][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.35s/it, est. speed input: 757.31 toks/s, output: 37.79 toks/s]
[2025-01-11 01:46:40,648][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.11it/s, est. speed input: 1101.25 toks/s, output: 70.63 toks/s]
[2025-01-11 01:46:40,648][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.03it/s, est. speed input: 1101.25 toks/s, output: 70.63 toks/s]
WARNING 01-11 01:46:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:40,863][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:41,947][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 910.59 toks/s, output: 43.36 toks/s]
[2025-01-11 01:46:41,948][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 910.59 toks/s, output: 43.36 toks/s]
WARNING 01-11 01:46:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:42,160][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:43,234][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.07s/it, est. speed input: 1017.49 toks/s, output: 27.02 toks/s]
[2025-01-11 01:46:43,621][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.37it/s, est. speed input: 1983.67 toks/s, output: 75.32 toks/s]
[2025-01-11 01:46:43,621][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.05it/s, est. speed input: 1983.67 toks/s, output: 75.32 toks/s]
WARNING 01-11 01:46:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:43,838][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:45,005][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.17s/it, est. speed input: 1086.15 toks/s, output: 24.00 toks/s]
[2025-01-11 01:46:45,226][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.60it/s, est. speed input: 2759.22 toks/s, output: 70.62 toks/s]
[2025-01-11 01:46:45,226][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.16it/s, est. speed input: 2759.22 toks/s, output: 70.62 toks/s]
WARNING 01-11 01:46:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:45,441][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:46,581][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 952.19 toks/s, output: 43.00 toks/s]
[2025-01-11 01:46:46,581][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 952.19 toks/s, output: 43.00 toks/s]
WARNING 01-11 01:46:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:46:46,811][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:46:47,788][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.02it/s, est. speed input: 1020.85 toks/s, output: 29.66 toks/s]
[2025-01-11 01:46:48,008][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.88it/s, est. speed input: 1961.10 toks/s, output: 59.33 toks/s]
[2025-01-11 01:46:48,008][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.67it/s, est. speed input: 1961.10 toks/s, output: 59.33 toks/s]
[2025-01-11 01:46:50,529][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:46:50,582][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:46:52,241][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 01:46:53,856][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 01:46:55,430][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 01:46:55,964][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 01:46:55,964][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 01:47:16,546][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:47:16,598][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:47:19,294][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:08,  2.70s/it]
[2025-01-11 01:47:21,010][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.12s/it]
[2025-01-11 01:47:22,561][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.86s/it]
[2025-01-11 01:47:23,075][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.33s/it]
[2025-01-11 01:47:23,076][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.62s/it]
WARNING 01-11 01:47:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:47:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:47:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:47:41 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:47:41,947][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:47:43,265][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:47:43,642][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 01:47:44,931][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-01-11 01:47:46,298][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 01:47:46,298][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:47:46 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:48:00 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:48:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:48:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:48:22 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:48:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:22,912][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:26,641][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.73s/it, est. speed input: 170.30 toks/s, output: 2.95 toks/s]
[2025-01-11 01:48:26,934][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.62it/s, est. speed input: 789.52 toks/s, output: 14.92 toks/s]
[2025-01-11 01:48:27,041][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.34it/s, est. speed input: 1691.76 toks/s, output: 39.24 toks/s]
[2025-01-11 01:48:27,174][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.05it/s, est. speed input: 2384.00 toks/s, output: 60.54 toks/s]
[2025-01-11 01:48:27,323][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  9.34it/s, est. speed input: 2879.51 toks/s, output: 79.13 toks/s]
[2025-01-11 01:48:27,477][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.70it/s, est. speed input: 3477.40 toks/s, output: 106.46 toks/s]
[2025-01-11 01:48:27,595][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 16.79it/s, est. speed input: 4068.07 toks/s, output: 138.16 toks/s]
[2025-01-11 01:48:27,725][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4221.98 toks/s, output: 151.05 toks/s]
WARNING 01-11 01:48:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:28,198][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:31,337][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:37,  3.14s/it, est. speed input: 207.71 toks/s, output: 3.19 toks/s]
[2025-01-11 01:48:31,464][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:41,  1.37s/it, est. speed input: 397.13 toks/s, output: 6.74 toks/s]
[2025-01-11 01:48:31,582][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:08,  2.98it/s, est. speed input: 1164.51 toks/s, output: 21.87 toks/s]
[2025-01-11 01:48:31,744][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.07it/s, est. speed input: 1487.71 toks/s, output: 29.61 toks/s]
[2025-01-11 01:48:31,848][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:03<00:03,  5.54it/s, est. speed input: 1802.09 toks/s, output: 38.63 toks/s]
[2025-01-11 01:48:31,990][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:03<00:02,  8.02it/s, est. speed input: 2253.67 toks/s, output: 53.54 toks/s]
[2025-01-11 01:48:32,117][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01, 13.31it/s, est. speed input: 3015.39 toks/s, output: 82.16 toks/s]
[2025-01-11 01:48:32,260][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 14.96it/s, est. speed input: 3396.32 toks/s, output: 99.96 toks/s]
[2025-01-11 01:48:32,377][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 22.22it/s, est. speed input: 4241.65 toks/s, output: 142.15 toks/s]
[2025-01-11 01:48:33,090][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 11.68it/s, est. speed input: 4165.14 toks/s, output: 167.23 toks/s]
[2025-01-11 01:48:33,213][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.38it/s, est. speed input: 4195.15 toks/s, output: 177.46 toks/s]
WARNING 01-11 01:48:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:33,686][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:36,921][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:30,  3.23s/it, est. speed input: 228.78 toks/s, output: 3.09 toks/s]
[2025-01-11 01:48:37,156][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:03<00:39,  1.47s/it, est. speed input: 425.72 toks/s, output: 6.92 toks/s]
[2025-01-11 01:48:37,383][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:23,  1.11it/s, est. speed input: 596.80 toks/s, output: 11.36 toks/s]
[2025-01-11 01:48:37,494][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:03<00:14,  1.69it/s, est. speed input: 774.41 toks/s, output: 16.28 toks/s]
[2025-01-11 01:48:37,654][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:07,  3.13it/s, est. speed input: 1119.49 toks/s, output: 26.71 toks/s]
[2025-01-11 01:48:37,800][root][ERROR] - Processed prompts:  34%|###4      | 10/29 [00:04<00:02,  6.71it/s, est. speed input: 1804.93 toks/s, output: 49.83 toks/s]
[2025-01-11 01:48:37,936][root][ERROR] - Processed prompts:  41%|####1     | 12/29 [00:04<00:02,  8.02it/s, est. speed input: 2100.24 toks/s, output: 61.65 toks/s]
[2025-01-11 01:48:38,039][root][ERROR] - Processed prompts:  66%|######5   | 19/29 [00:04<00:00, 17.06it/s, est. speed input: 3281.43 toks/s, output: 107.99 toks/s]
[2025-01-11 01:48:38,214][root][ERROR] - Processed prompts:  79%|#######9  | 23/29 [00:04<00:00, 18.59it/s, est. speed input: 3803.89 toks/s, output: 135.38 toks/s]
[2025-01-11 01:48:38,565][root][ERROR] - Processed prompts:  93%|#########3| 27/29 [00:04<00:00, 15.47it/s, est. speed input: 4156.36 toks/s, output: 161.92 toks/s]
[2025-01-11 01:48:39,148][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.31it/s, est. speed input: 3987.39 toks/s, output: 170.63 toks/s]
WARNING 01-11 01:48:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:39,369][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:40,978][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:03,  1.61s/it, est. speed input: 480.03 toks/s, output: 36.06 toks/s]
[2025-01-11 01:48:41,089][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.37it/s, est. speed input: 891.30 toks/s, output: 70.93 toks/s]
[2025-01-11 01:48:41,140][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.69it/s, est. speed input: 1289.28 toks/s, output: 106.73 toks/s]
WARNING 01-11 01:48:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:41,387][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:42,581][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.19s/it, est. speed input: 666.79 toks/s, output: 16.75 toks/s]
[2025-01-11 01:48:42,695][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.79it/s, est. speed input: 1170.08 toks/s, output: 34.41 toks/s]
[2025-01-11 01:48:43,250][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.25it/s, est. speed input: 2612.89 toks/s, output: 101.44 toks/s]
[2025-01-11 01:48:43,251][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.22it/s, est. speed input: 2612.89 toks/s, output: 101.44 toks/s]
WARNING 01-11 01:48:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:43,508][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:46,828][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:19,  3.32s/it, est. speed input: 216.01 toks/s, output: 6.63 toks/s]
[2025-01-11 01:48:47,130][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:35,  1.54s/it, est. speed input: 415.00 toks/s, output: 13.81 toks/s]
[2025-01-11 01:48:47,276][root][ERROR] - Processed prompts:  80%|########  | 20/25 [00:03<00:00,  9.67it/s, est. speed input: 4069.86 toks/s, output: 152.87 toks/s]
[2025-01-11 01:48:47,813][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:04<00:00,  5.81it/s, est. speed input: 4408.90 toks/s, output: 189.08 toks/s]
WARNING 01-11 01:48:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:48,110][root][ERROR] - Processed prompts:   0%|          | 0/19 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:51,227][root][ERROR] - Processed prompts:   5%|5         | 1/19 [00:03<00:56,  3.12s/it, est. speed input: 271.46 toks/s, output: 8.02 toks/s]
[2025-01-11 01:48:51,391][root][ERROR] - Processed prompts:  16%|#5        | 3/19 [00:03<00:13,  1.15it/s, est. speed input: 807.70 toks/s, output: 24.08 toks/s]
[2025-01-11 01:48:51,527][root][ERROR] - Processed prompts:  95%|#########4| 18/19 [00:03<00:00,  9.50it/s, est. speed input: 4866.02 toks/s, output: 153.63 toks/s]
[2025-01-11 01:48:51,645][root][ERROR] - Processed prompts: 100%|##########| 19/19 [00:03<00:00,  5.37it/s, est. speed input: 4936.84 toks/s, output: 160.68 toks/s]
WARNING 01-11 01:48:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:51,870][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:53,192][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.32s/it, est. speed input: 556.60 toks/s, output: 19.66 toks/s]
[2025-01-11 01:48:53,335][root][ERROR] - Processed prompts:  83%|########3 | 5/6 [00:01<00:00,  4.40it/s, est. speed input: 2685.29 toks/s, output: 99.68 toks/s]
[2025-01-11 01:48:53,536][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.60it/s, est. speed input: 2879.62 toks/s, output: 114.61 toks/s]
WARNING 01-11 01:48:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:53,828][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:55,220][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:08,  1.39s/it, est. speed input: 646.35 toks/s, output: 14.38 toks/s]
[2025-01-11 01:48:55,415][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:01<00:03,  1.45it/s, est. speed input: 1207.45 toks/s, output: 30.25 toks/s]
[2025-01-11 01:48:55,513][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.16it/s, est. speed input: 3818.59 toks/s, output: 119.35 toks/s]
WARNING 01-11 01:48:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:55,758][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:57,207][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.45s/it, est. speed input: 654.75 toks/s, output: 20.01 toks/s]
[2025-01-11 01:48:57,208][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.14it/s, est. speed input: 3638.26 toks/s, output: 120.01 toks/s]
WARNING 01-11 01:48:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:57,448][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:48:58,884][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.44s/it, est. speed input: 778.82 toks/s, output: 20.20 toks/s]
[2025-01-11 01:48:59,205][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.55it/s, est. speed input: 3102.24 toks/s, output: 93.39 toks/s]
[2025-01-11 01:48:59,205][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.85it/s, est. speed input: 3102.24 toks/s, output: 93.39 toks/s]
WARNING 01-11 01:48:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:48:59,425][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:49:00,744][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.32s/it, est. speed input: 603.34 toks/s, output: 21.98 toks/s]
[2025-01-11 01:49:01,372][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.03it/s, est. speed input: 2205.45 toks/s, output: 96.04 toks/s]
[2025-01-11 01:49:01,372][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.57it/s, est. speed input: 2205.45 toks/s, output: 96.04 toks/s]
WARNING 01-11 01:49:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:49:01,619][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:49:02,581][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 1174.61 toks/s, output: 30.17 toks/s]
[2025-01-11 01:49:02,581][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.08it/s, est. speed input: 2345.19 toks/s, output: 60.32 toks/s]
WARNING 01-11 01:49:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:49:02,795][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:49:03,706][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 950.79 toks/s, output: 31.88 toks/s]
[2025-01-11 01:49:03,723][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.16it/s, est. speed input: 1871.32 toks/s, output: 63.63 toks/s]
WARNING 01-11 01:49:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:49:03,948][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:49:04,917][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.03it/s, est. speed input: 1173.71 toks/s, output: 29.96 toks/s]
[2025-01-11 01:49:04,917][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.07it/s, est. speed input: 2344.38 toks/s, output: 59.90 toks/s]
[2025-01-11 01:49:07,551][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:49:07,604][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:49:09,265][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 01:49:10,905][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 01:49:12,568][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 01:49:13,101][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:49:13,102][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 01:49:33,272][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:49:33,324][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:49:35,301][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 01:49:36,894][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 01:49:38,447][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 01:49:38,954][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:49:38,954][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 01:49:56 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:49:56 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:49:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:49:57 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:49:57,762][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:49:59,096][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
[2025-01-11 01:49:59,508][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 01:50:00,803][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:50:02,125][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:50:02,125][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:50:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:50:16 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:50:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:50:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:50:38 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:50:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:38,610][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:50:42,403][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.43 toks/s, output: 2.90 toks/s]
[2025-01-11 01:50:42,683][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.60it/s, est. speed input: 779.54 toks/s, output: 14.73 toks/s]
[2025-01-11 01:50:42,783][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.78it/s, est. speed input: 1825.97 toks/s, output: 42.89 toks/s]
[2025-01-11 01:50:42,908][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.86it/s, est. speed input: 2363.63 toks/s, output: 59.56 toks/s]
[2025-01-11 01:50:43,014][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.28it/s, est. speed input: 3027.60 toks/s, output: 83.78 toks/s]
[2025-01-11 01:50:43,181][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 13.43it/s, est. speed input: 3611.84 toks/s, output: 110.04 toks/s]
[2025-01-11 01:50:43,457][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.72it/s, est. speed input: 3930.11 toks/s, output: 136.37 toks/s]
[2025-01-11 01:50:43,655][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.34it/s, est. speed input: 4027.43 toks/s, output: 151.42 toks/s]
WARNING 01-11 01:50:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:43,903][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:50:47,050][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:37,  3.15s/it, est. speed input: 208.49 toks/s, output: 3.81 toks/s]
[2025-01-11 01:50:47,166][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:17,  1.60it/s, est. speed input: 802.81 toks/s, output: 15.33 toks/s]
[2025-01-11 01:50:47,278][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:09,  2.64it/s, est. speed input: 1170.71 toks/s, output: 24.30 toks/s]
[2025-01-11 01:50:47,595][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:07,  3.40it/s, est. speed input: 1425.15 toks/s, output: 33.86 toks/s]
[2025-01-11 01:50:47,729][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:01,  8.83it/s, est. speed input: 2566.42 toks/s, output: 75.28 toks/s]
[2025-01-11 01:50:47,924][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 10.06it/s, est. speed input: 2932.70 toks/s, output: 93.02 toks/s]
[2025-01-11 01:50:48,061][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.94it/s, est. speed input: 3313.25 toks/s, output: 113.53 toks/s]
[2025-01-11 01:50:48,348][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 11.46it/s, est. speed input: 3542.95 toks/s, output: 132.07 toks/s]
[2025-01-11 01:50:48,468][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 14.98it/s, est. speed input: 4030.48 toks/s, output: 169.11 toks/s]
[2025-01-11 01:50:48,712][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.13it/s, est. speed input: 4240.39 toks/s, output: 194.45 toks/s]
[2025-01-11 01:50:48,712][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4382.43 toks/s, output: 207.33 toks/s]
WARNING 01-11 01:50:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:48,983][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:50:52,089][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:26,  3.11s/it, est. speed input: 237.02 toks/s, output: 3.22 toks/s]
[2025-01-11 01:50:52,307][root][ERROR] - Processed prompts:  17%|#7        | 5/29 [00:03<00:12,  1.97it/s, est. speed input: 1111.97 toks/s, output: 16.55 toks/s]
[2025-01-11 01:50:52,458][root][ERROR] - Processed prompts:  28%|##7       | 8/29 [00:03<00:06,  3.45it/s, est. speed input: 1718.36 toks/s, output: 29.07 toks/s]
[2025-01-11 01:50:52,598][root][ERROR] - Processed prompts:  38%|###7      | 11/29 [00:03<00:03,  5.22it/s, est. speed input: 2277.37 toks/s, output: 43.44 toks/s]
[2025-01-11 01:50:52,729][root][ERROR] - Processed prompts:  45%|####4     | 13/29 [00:03<00:02,  6.39it/s, est. speed input: 2598.63 toks/s, output: 53.94 toks/s]
[2025-01-11 01:50:52,838][root][ERROR] - Processed prompts:  62%|######2   | 18/29 [00:03<00:00, 11.20it/s, est. speed input: 3499.18 toks/s, output: 84.07 toks/s]
[2025-01-11 01:50:53,234][root][ERROR] - Processed prompts:  86%|########6 | 25/29 [00:04<00:00, 13.70it/s, est. speed input: 4414.93 toks/s, output: 126.58 toks/s]
[2025-01-11 01:50:53,535][root][ERROR] - Processed prompts:  97%|#########6| 28/29 [00:04<00:00, 12.60it/s, est. speed input: 4626.93 toks/s, output: 150.29 toks/s]
[2025-01-11 01:50:53,736][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.10it/s, est. speed input: 4591.42 toks/s, output: 158.66 toks/s]
WARNING 01-11 01:50:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:53,960][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:50:55,111][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.15s/it, est. speed input: 636.29 toks/s, output: 31.29 toks/s]
[2025-01-11 01:50:55,295][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:01<00:00,  1.72it/s, est. speed input: 1094.59 toks/s, output: 61.43 toks/s]
[2025-01-11 01:50:55,681][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.03it/s, est. speed input: 1302.80 toks/s, output: 87.78 toks/s]
[2025-01-11 01:50:55,681][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.74it/s, est. speed input: 1302.80 toks/s, output: 87.78 toks/s]
WARNING 01-11 01:50:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:55,899][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:50:56,824][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:00<00:03,  1.08it/s, est. speed input: 826.53 toks/s, output: 14.06 toks/s]
[2025-01-11 01:50:57,150][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.75it/s, est. speed input: 1261.57 toks/s, output: 32.80 toks/s]
[2025-01-11 01:50:57,390][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.55it/s, est. speed input: 2762.21 toks/s, output: 95.29 toks/s]
[2025-01-11 01:50:57,390][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.35it/s, est. speed input: 2762.21 toks/s, output: 95.29 toks/s]
WARNING 01-11 01:50:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:50:57,636][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:00,544][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:02<01:15,  2.91s/it, est. speed input: 248.69 toks/s, output: 3.78 toks/s]
[2025-01-11 01:51:01,500][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:43,  1.76s/it, est. speed input: 372.16 toks/s, output: 10.35 toks/s]
[2025-01-11 01:51:01,844][root][ERROR] - Processed prompts:  93%|#########2| 25/27 [00:04<00:00,  9.93it/s, est. speed input: 4468.55 toks/s, output: 173.05 toks/s]
[2025-01-11 01:51:02,117][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  6.03it/s, est. speed input: 4529.90 toks/s, output: 189.28 toks/s]
WARNING 01-11 01:51:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:02,400][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:06,167][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:19,  3.77s/it, est. speed input: 258.59 toks/s, output: 7.43 toks/s]
[2025-01-11 01:51:06,401][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  7.56it/s, est. speed input: 5353.02 toks/s, output: 162.22 toks/s]
[2025-01-11 01:51:06,401][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:04<00:00,  5.50it/s, est. speed input: 5353.02 toks/s, output: 162.22 toks/s]
WARNING 01-11 01:51:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:06,618][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:07,826][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.21s/it, est. speed input: 573.68 toks/s, output: 22.35 toks/s]
[2025-01-11 01:51:08,047][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.53it/s, est. speed input: 1614.82 toks/s, output: 65.07 toks/s]
[2025-01-11 01:51:08,305][root][ERROR] - Processed prompts:  80%|########  | 4/5 [00:01<00:00,  2.88it/s, est. speed input: 1786.76 toks/s, output: 85.93 toks/s]
[2025-01-11 01:51:08,339][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.90it/s, est. speed input: 2217.97 toks/s, output: 115.60 toks/s]
WARNING 01-11 01:51:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:08,663][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:09,881][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:07,  1.22s/it, est. speed input: 859.90 toks/s, output: 10.68 toks/s]
[2025-01-11 01:51:10,100][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:01<00:03,  1.59it/s, est. speed input: 1342.52 toks/s, output: 24.36 toks/s]
[2025-01-11 01:51:10,259][root][ERROR] - Processed prompts:  43%|####2     | 3/7 [00:01<00:01,  2.41it/s, est. speed input: 1704.08 toks/s, output: 40.10 toks/s]
[2025-01-11 01:51:11,976][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.35it/s, est. speed input: 1976.72 toks/s, output: 86.02 toks/s]
[2025-01-11 01:51:11,976][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:03<00:00,  2.11it/s, est. speed input: 1976.72 toks/s, output: 86.02 toks/s]
WARNING 01-11 01:51:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:12,219][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:13,137][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.09it/s, est. speed input: 1028.85 toks/s, output: 31.61 toks/s]
[2025-01-11 01:51:13,171][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.10it/s, est. speed input: 1911.35 toks/s, output: 63.05 toks/s]
WARNING 01-11 01:51:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:13,406][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:14,157][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.33it/s, est. speed input: 1340.87 toks/s, output: 23.97 toks/s]
[2025-01-11 01:51:14,344][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.39it/s, est. speed input: 2306.80 toks/s, output: 50.15 toks/s]
[2025-01-11 01:51:14,344][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.13it/s, est. speed input: 2306.80 toks/s, output: 50.15 toks/s]
WARNING 01-11 01:51:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:14,565][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:15,617][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.05s/it, est. speed input: 988.50 toks/s, output: 26.64 toks/s]
[2025-01-11 01:51:15,636][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.80it/s, est. speed input: 2671.58 toks/s, output: 80.36 toks/s]
WARNING 01-11 01:51:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:15,857][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:16,630][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1441.35 toks/s, output: 36.23 toks/s]
[2025-01-11 01:51:16,630][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1441.35 toks/s, output: 36.23 toks/s]
WARNING 01-11 01:51:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:16,842][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:18,134][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.29s/it, est. speed input: 868.06 toks/s, output: 37.94 toks/s]
[2025-01-11 01:51:18,973][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.03s/it, est. speed input: 943.70 toks/s, output: 69.45 toks/s]
[2025-01-11 01:51:18,973][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.07s/it, est. speed input: 943.70 toks/s, output: 69.45 toks/s]
WARNING 01-11 01:51:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:19,192][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:20,204][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 1181.38 toks/s, output: 40.50 toks/s]
[2025-01-11 01:51:20,205][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 1181.38 toks/s, output: 40.50 toks/s]
WARNING 01-11 01:51:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:20,418][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:21,712][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.29s/it, est. speed input: 778.68 toks/s, output: 45.58 toks/s]
[2025-01-11 01:51:21,713][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.29s/it, est. speed input: 778.68 toks/s, output: 45.58 toks/s]
WARNING 01-11 01:51:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:21,935][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:22,650][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 1464.13 toks/s, output: 33.59 toks/s]
[2025-01-11 01:51:22,650][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 1464.13 toks/s, output: 33.59 toks/s]
WARNING 01-11 01:51:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:51:22,865][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:51:23,520][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 1663.17 toks/s, output: 30.54 toks/s]
[2025-01-11 01:51:23,520][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 1663.17 toks/s, output: 30.54 toks/s]
[2025-01-11 01:51:26,046][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:51:26,099][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:51:27,707][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 01:51:29,415][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 01:51:31,435][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.83s/it]
[2025-01-11 01:51:32,100][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.37s/it]
[2025-01-11 01:51:32,100][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.50s/it]
[2025-01-11 01:51:52,643][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:51:52,694][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:51:54,635][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 01:51:56,214][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 01:51:57,834][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:51:58,336][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 01:51:58,336][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 01:52:16 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:52:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:52:16 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:52:16 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:52:17,080][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:52:18,430][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:52:18,814][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 01:52:20,123][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 01:52:21,455][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:52:21,455][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:52:21 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:52:35 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:52:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:52:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:52:58 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:52:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:52:58,567][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:02,359][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.48 toks/s, output: 2.64 toks/s]
[2025-01-11 01:53:02,700][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.57it/s, est. speed input: 768.33 toks/s, output: 14.28 toks/s]
[2025-01-11 01:53:02,844][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  5.04it/s, est. speed input: 1930.17 toks/s, output: 46.06 toks/s]
[2025-01-11 01:53:02,966][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:02,  7.04it/s, est. speed input: 2453.88 toks/s, output: 63.65 toks/s]
[2025-01-11 01:53:03,068][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  9.59it/s, est. speed input: 2962.78 toks/s, output: 83.10 toks/s]
[2025-01-11 01:53:03,171][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 15.60it/s, est. speed input: 3862.45 toks/s, output: 121.65 toks/s]
[2025-01-11 01:53:03,632][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.32it/s, est. speed input: 4012.01 toks/s, output: 141.37 toks/s]
WARNING 01-11 01:53:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:03,920][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:06,924][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.00s/it, est. speed input: 221.38 toks/s, output: 3.33 toks/s]
[2025-01-11 01:53:07,045][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:39,  1.31s/it, est. speed input: 422.08 toks/s, output: 7.04 toks/s]
[2025-01-11 01:53:07,156][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:08,  3.11it/s, est. speed input: 1216.06 toks/s, output: 22.56 toks/s]
[2025-01-11 01:53:07,367][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.09it/s, est. speed input: 1520.09 toks/s, output: 31.34 toks/s]
[2025-01-11 01:53:07,514][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:03<00:03,  6.25it/s, est. speed input: 2007.27 toks/s, output: 46.75 toks/s]
[2025-01-11 01:53:07,765][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:03<00:01,  9.58it/s, est. speed input: 2734.52 toks/s, output: 73.87 toks/s]
[2025-01-11 01:53:07,905][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:00, 12.67it/s, est. speed input: 3291.31 toks/s, output: 100.13 toks/s]
[2025-01-11 01:53:08,022][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 18.95it/s, est. speed input: 4160.15 toks/s, output: 146.04 toks/s]
[2025-01-11 01:53:08,129][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 22.19it/s, est. speed input: 4676.09 toks/s, output: 177.74 toks/s]
[2025-01-11 01:53:08,547][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.92it/s, est. speed input: 4545.05 toks/s, output: 188.26 toks/s]
WARNING 01-11 01:53:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:08,811][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:11,967][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:31,  3.16s/it, est. speed input: 235.74 toks/s, output: 3.17 toks/s]
[2025-01-11 01:53:12,141][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:39,  1.40s/it, est. speed input: 447.17 toks/s, output: 6.91 toks/s]
[2025-01-11 01:53:12,307][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:14,  1.74it/s, est. speed input: 840.57 toks/s, output: 15.16 toks/s]
[2025-01-11 01:53:12,410][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:03<00:07,  3.05it/s, est. speed input: 1232.00 toks/s, output: 24.17 toks/s]
[2025-01-11 01:53:12,559][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:04,  4.43it/s, est. speed input: 1578.43 toks/s, output: 33.89 toks/s]
[2025-01-11 01:53:12,696][root][ERROR] - Processed prompts:  40%|####      | 12/30 [00:03<00:02,  8.12it/s, est. speed input: 2295.53 toks/s, output: 56.13 toks/s]
[2025-01-11 01:53:12,821][root][ERROR] - Processed prompts:  47%|####6     | 14/30 [00:04<00:01,  9.40it/s, est. speed input: 2599.64 toks/s, output: 67.59 toks/s]
[2025-01-11 01:53:12,946][root][ERROR] - Processed prompts:  77%|#######6  | 23/30 [00:04<00:00, 21.17it/s, est. speed input: 4186.77 toks/s, output: 128.18 toks/s]
[2025-01-11 01:53:13,175][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 19.95it/s, est. speed input: 4650.55 toks/s, output: 154.44 toks/s]
[2025-01-11 01:53:13,617][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 13.68it/s, est. speed input: 4704.76 toks/s, output: 172.08 toks/s]
[2025-01-11 01:53:13,617][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.24it/s, est. speed input: 4704.76 toks/s, output: 172.08 toks/s]
WARNING 01-11 01:53:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:13,833][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:14,785][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 747.13 toks/s, output: 33.63 toks/s]
[2025-01-11 01:53:14,902][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  2.17it/s, est. speed input: 1347.46 toks/s, output: 66.44 toks/s]
[2025-01-11 01:53:14,902][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.87it/s, est. speed input: 1347.46 toks/s, output: 66.44 toks/s]
WARNING 01-11 01:53:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:15,128][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:16,143][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.01s/it, est. speed input: 734.33 toks/s, output: 17.74 toks/s]
[2025-01-11 01:53:16,370][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.88it/s, est. speed input: 1867.88 toks/s, output: 55.58 toks/s]
[2025-01-11 01:53:16,370][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.03it/s, est. speed input: 3210.73 toks/s, output: 102.27 toks/s]
WARNING 01-11 01:53:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:16,620][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:19,676][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:16,  3.06s/it, est. speed input: 237.26 toks/s, output: 4.91 toks/s]
[2025-01-11 01:53:19,987][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:34,  1.44s/it, est. speed input: 422.73 toks/s, output: 10.69 toks/s]
[2025-01-11 01:53:20,389][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:22,  1.03it/s, est. speed input: 565.96 toks/s, output: 17.25 toks/s]
[2025-01-11 01:53:20,969][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:04<00:00, 11.41it/s, est. speed input: 4362.21 toks/s, output: 170.16 toks/s]
[2025-01-11 01:53:21,188][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  5.69it/s, est. speed input: 4336.51 toks/s, output: 178.01 toks/s]
WARNING 01-11 01:53:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:21,474][root][ERROR] - Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:24,238][root][ERROR] - Processed prompts:   5%|5         | 1/20 [00:02<00:52,  2.76s/it, est. speed input: 293.45 toks/s, output: 4.70 toks/s]
[2025-01-11 01:53:24,762][root][ERROR] - Processed prompts:  10%|#         | 2/20 [00:03<00:26,  1.45s/it, est. speed input: 490.92 toks/s, output: 11.56 toks/s]
[2025-01-11 01:53:24,888][root][ERROR] - Processed prompts:  15%|#5        | 3/20 [00:03<00:14,  1.19it/s, est. speed input: 761.73 toks/s, output: 19.33 toks/s]
[2025-01-11 01:53:25,118][root][ERROR] - Processed prompts:  95%|#########5| 19/20 [00:03<00:00, 11.51it/s, est. speed input: 5010.79 toks/s, output: 148.72 toks/s]
[2025-01-11 01:53:25,186][root][ERROR] - Processed prompts: 100%|##########| 20/20 [00:03<00:00,  5.39it/s, est. speed input: 5130.27 toks/s, output: 157.60 toks/s]
WARNING 01-11 01:53:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:25,408][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:26,706][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.30s/it, est. speed input: 598.00 toks/s, output: 22.35 toks/s]
[2025-01-11 01:53:26,740][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.75it/s, est. speed input: 3203.57 toks/s, output: 110.36 toks/s]
WARNING 01-11 01:53:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:27,066][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:28,238][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.17s/it, est. speed input: 729.64 toks/s, output: 22.19 toks/s]
[2025-01-11 01:53:28,350][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.97it/s, est. speed input: 3025.26 toks/s, output: 90.35 toks/s]
[2025-01-11 01:53:28,350][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.11it/s, est. speed input: 3025.26 toks/s, output: 90.35 toks/s]
WARNING 01-11 01:53:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:28,560][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:29,520][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.04it/s, est. speed input: 809.45 toks/s, output: 27.09 toks/s]
[2025-01-11 01:53:29,576][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.95it/s, est. speed input: 2482.82 toks/s, output: 82.69 toks/s]
WARNING 01-11 01:53:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:29,827][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:30,790][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 1268.09 toks/s, output: 30.12 toks/s]
[2025-01-11 01:53:30,790][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.08it/s, est. speed input: 2345.37 toks/s, output: 60.22 toks/s]
WARNING 01-11 01:53:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:31,002][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:31,895][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.12it/s, est. speed input: 921.29 toks/s, output: 32.50 toks/s]
[2025-01-11 01:53:31,929][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.16it/s, est. speed input: 1708.82 toks/s, output: 64.77 toks/s]
WARNING 01-11 01:53:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:53:32,172][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:53:33,113][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.06it/s, est. speed input: 1098.17 toks/s, output: 30.80 toks/s]
[2025-01-11 01:53:33,114][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.12it/s, est. speed input: 2256.04 toks/s, output: 61.58 toks/s]
[2025-01-11 01:53:35,632][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:53:35,685][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:53:37,326][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 01:53:38,961][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 01:53:40,623][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 01:53:41,218][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 01:53:41,218][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 01:54:01,150][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:54:01,202][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:54:03,197][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.99s/it]
[2025-01-11 01:54:04,827][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 01:54:06,466][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 01:54:06,992][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 01:54:06,992][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 01:54:24 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:54:24 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:54:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:54:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:54:25,124][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:54:26,476][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 01:54:26,855][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 01:54:28,150][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 01:54:29,470][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:54:29,471][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:54:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:54:43 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:54:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:54:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:55:05 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:55:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:06,233][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:10,054][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.21 toks/s, output: 2.62 toks/s]
[2025-01-11 01:55:10,395][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.56it/s, est. speed input: 762.94 toks/s, output: 14.18 toks/s]
[2025-01-11 01:55:10,535][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.46it/s, est. speed input: 2066.51 toks/s, output: 49.51 toks/s]
[2025-01-11 01:55:10,654][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  7.44it/s, est. speed input: 2585.61 toks/s, output: 66.96 toks/s]
[2025-01-11 01:55:10,805][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.83it/s, est. speed input: 3472.57 toks/s, output: 101.93 toks/s]
[2025-01-11 01:55:11,033][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.81it/s, est. speed input: 3968.86 toks/s, output: 128.96 toks/s]
[2025-01-11 01:55:11,214][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.42it/s, est. speed input: 4079.60 toks/s, output: 143.15 toks/s]
WARNING 01-11 01:55:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:11,460][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:14,462][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.00s/it, est. speed input: 221.87 toks/s, output: 3.33 toks/s]
[2025-01-11 01:55:14,580][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:23,  1.22it/s, est. speed input: 635.96 toks/s, output: 10.26 toks/s]
[2025-01-11 01:55:14,694][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:11,  2.33it/s, est. speed input: 1017.02 toks/s, output: 18.24 toks/s]
[2025-01-11 01:55:14,845][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:04,  5.10it/s, est. speed input: 1743.15 toks/s, output: 34.86 toks/s]
[2025-01-11 01:55:15,065][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  8.50it/s, est. speed input: 2545.01 toks/s, output: 58.26 toks/s]
[2025-01-11 01:55:15,181][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01, 11.79it/s, est. speed input: 3168.40 toks/s, output: 81.71 toks/s]
[2025-01-11 01:55:15,290][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:03<00:00, 22.07it/s, est. speed input: 4624.80 toks/s, output: 141.27 toks/s]
[2025-01-11 01:55:15,998][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 13.45it/s, est. speed input: 4633.98 toks/s, output: 167.92 toks/s]
[2025-01-11 01:55:15,998][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.05it/s, est. speed input: 4633.98 toks/s, output: 167.92 toks/s]
WARNING 01-11 01:55:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:16,251][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:19,393][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:31,  3.14s/it, est. speed input: 233.31 toks/s, output: 3.18 toks/s]
[2025-01-11 01:55:19,625][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:40,  1.43s/it, est. speed input: 435.15 toks/s, output: 7.11 toks/s]
[2025-01-11 01:55:19,782][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:03<00:08,  2.79it/s, est. speed input: 1259.01 toks/s, output: 23.79 toks/s]
[2025-01-11 01:55:20,013][root][ERROR] - Processed prompts:  37%|###6      | 11/30 [00:03<00:03,  5.63it/s, est. speed input: 2167.19 toks/s, output: 47.59 toks/s]
[2025-01-11 01:55:20,136][root][ERROR] - Processed prompts:  50%|#####     | 15/30 [00:03<00:01,  8.40it/s, est. speed input: 2858.45 toks/s, output: 70.53 toks/s]
[2025-01-11 01:55:20,283][root][ERROR] - Processed prompts:  60%|######    | 18/30 [00:04<00:01, 10.19it/s, est. speed input: 3304.54 toks/s, output: 89.54 toks/s]
[2025-01-11 01:55:20,527][root][ERROR] - Processed prompts:  83%|########3 | 25/30 [00:04<00:00, 15.03it/s, est. speed input: 4396.03 toks/s, output: 134.47 toks/s]
[2025-01-11 01:55:20,958][root][ERROR] - Processed prompts:  97%|#########6| 29/30 [00:04<00:00, 12.75it/s, est. speed input: 4627.46 toks/s, output: 161.26 toks/s]
[2025-01-11 01:55:23,264][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:07<00:00,  4.28it/s, est. speed input: 3211.61 toks/s, output: 136.74 toks/s]
WARNING 01-11 01:55:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:23,477][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:24,373][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.12it/s, est. speed input: 785.56 toks/s, output: 32.36 toks/s]
[2025-01-11 01:55:24,808][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.60it/s, est. speed input: 1088.84 toks/s, output: 63.12 toks/s]
[2025-01-11 01:55:24,808][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.50it/s, est. speed input: 1088.84 toks/s, output: 63.12 toks/s]
WARNING 01-11 01:55:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:25,042][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:26,329][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.29s/it, est. speed input: 641.02 toks/s, output: 22.53 toks/s]
[2025-01-11 01:55:27,438][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.34it/s, est. speed input: 1728.25 toks/s, output: 88.08 toks/s]
[2025-01-11 01:55:27,438][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:02<00:00,  2.09it/s, est. speed input: 1728.25 toks/s, output: 88.08 toks/s]
WARNING 01-11 01:55:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:27,703][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:30,960][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:21,  3.26s/it, est. speed input: 223.81 toks/s, output: 5.83 toks/s]
[2025-01-11 01:55:31,322][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:37,  1.55s/it, est. speed input: 395.65 toks/s, output: 12.43 toks/s]
[2025-01-11 01:55:31,424][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:20,  1.12it/s, est. speed input: 575.43 toks/s, output: 19.62 toks/s]
[2025-01-11 01:55:31,547][root][ERROR] - Processed prompts:  96%|#########6| 25/26 [00:03<00:00, 15.80it/s, est. speed input: 4963.38 toks/s, output: 186.00 toks/s]
[2025-01-11 01:55:31,681][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:03<00:00,  6.54it/s, est. speed input: 4979.73 toks/s, output: 190.05 toks/s]
WARNING 01-11 01:55:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:31,980][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:34,872][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:02<00:49,  2.89s/it, est. speed input: 274.30 toks/s, output: 7.26 toks/s]
[2025-01-11 01:55:35,201][root][ERROR] - Processed prompts:  11%|#1        | 2/18 [00:03<00:22,  1.38s/it, est. speed input: 542.14 toks/s, output: 15.53 toks/s]
[2025-01-11 01:55:35,202][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:03<00:00,  5.59it/s, est. speed input: 5395.43 toks/s, output: 159.57 toks/s]
WARNING 01-11 01:55:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:35,423][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:36,723][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.30s/it, est. speed input: 680.03 toks/s, output: 22.31 toks/s]
[2025-01-11 01:55:37,074][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.73it/s, est. speed input: 2598.44 toks/s, output: 100.54 toks/s]
[2025-01-11 01:55:37,075][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.03it/s, est. speed input: 2598.44 toks/s, output: 100.54 toks/s]
WARNING 01-11 01:55:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:37,403][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:38,339][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 1032.10 toks/s, output: 30.98 toks/s]
[2025-01-11 01:55:39,416][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.02s/it, est. speed input: 982.50 toks/s, output: 60.60 toks/s]
[2025-01-11 01:55:39,417][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:02<00:00,  1.01s/it, est. speed input: 982.50 toks/s, output: 60.60 toks/s]
WARNING 01-11 01:55:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:39,632][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:40,376][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.34it/s, est. speed input: 1030.14 toks/s, output: 26.90 toks/s]
[2025-01-11 01:55:40,527][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.53it/s, est. speed input: 1926.43 toks/s, output: 54.75 toks/s]
[2025-01-11 01:55:40,527][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.23it/s, est. speed input: 1926.43 toks/s, output: 54.75 toks/s]
WARNING 01-11 01:55:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:40,749][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:42,081][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 851.89 toks/s, output: 45.78 toks/s]
[2025-01-11 01:55:42,081][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 851.89 toks/s, output: 45.78 toks/s]
WARNING 01-11 01:55:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:42,292][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:43,055][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1054.22 toks/s, output: 37.98 toks/s]
[2025-01-11 01:55:43,056][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 1054.22 toks/s, output: 37.98 toks/s]
WARNING 01-11 01:55:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:43,267][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:44,080][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1260.21 toks/s, output: 38.15 toks/s]
[2025-01-11 01:55:44,080][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1260.21 toks/s, output: 38.15 toks/s]
WARNING 01-11 01:55:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:44,290][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:45,062][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1137.54 toks/s, output: 37.57 toks/s]
[2025-01-11 01:55:45,063][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1137.54 toks/s, output: 37.57 toks/s]
WARNING 01-11 01:55:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:55:45,279][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:55:46,101][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1711.67 toks/s, output: 35.30 toks/s]
[2025-01-11 01:55:46,101][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 1711.67 toks/s, output: 35.30 toks/s]
[2025-01-11 01:55:48,649][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:55:48,702][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:55:50,397][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-01-11 01:55:52,100][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 01:55:53,759][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 01:55:54,347][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 01:55:54,347][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-11 01:56:14,425][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:56:14,477][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:56:16,430][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 01:56:18,014][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 01:56:19,654][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 01:56:20,159][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 01:56:20,159][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 01:56:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:56:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:56:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:56:38 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:56:38,535][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:56:39,873][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 01:56:40,277][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 01:56:41,591][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 01:56:42,912][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:56:42,912][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:56:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:56:56 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:56:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:56:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:57:19 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:57:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:19,441][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:23,242][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 167.07 toks/s, output: 2.63 toks/s]
[2025-01-11 01:57:23,416][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.66it/s, est. speed input: 798.97 toks/s, output: 14.09 toks/s]
[2025-01-11 01:57:23,578][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.46it/s, est. speed input: 1074.58 toks/s, output: 21.27 toks/s]
[2025-01-11 01:57:23,787][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01,  8.01it/s, est. speed input: 2484.51 toks/s, output: 60.76 toks/s]
[2025-01-11 01:57:23,892][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.43it/s, est. speed input: 2996.42 toks/s, output: 79.99 toks/s]
[2025-01-11 01:57:24,006][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.15it/s, est. speed input: 3617.53 toks/s, output: 106.71 toks/s]
[2025-01-11 01:57:24,175][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 15.95it/s, est. speed input: 4024.33 toks/s, output: 129.71 toks/s]
[2025-01-11 01:57:24,385][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.47it/s, est. speed input: 4110.81 toks/s, output: 142.22 toks/s]
WARNING 01-11 01:57:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:24,634][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:27,761][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:36,  3.13s/it, est. speed input: 206.30 toks/s, output: 3.84 toks/s]
[2025-01-11 01:57:27,938][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:17,  1.56it/s, est. speed input: 783.53 toks/s, output: 16.05 toks/s]
[2025-01-11 01:57:28,046][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:09,  2.60it/s, est. speed input: 1140.45 toks/s, output: 24.91 toks/s]
[2025-01-11 01:57:28,153][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:06,  3.89it/s, est. speed input: 1479.42 toks/s, output: 34.96 toks/s]
[2025-01-11 01:57:28,291][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  8.96it/s, est. speed input: 2505.64 toks/s, output: 67.01 toks/s]
[2025-01-11 01:57:28,397][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:03<00:00, 17.16it/s, est. speed input: 3835.46 toks/s, output: 114.56 toks/s]
[2025-01-11 01:57:28,557][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:03<00:00, 20.12it/s, est. speed input: 4515.25 toks/s, output: 146.09 toks/s]
[2025-01-11 01:57:28,719][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 21.18it/s, est. speed input: 4986.39 toks/s, output: 174.81 toks/s]
[2025-01-11 01:57:28,786][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.71it/s, est. speed input: 5061.82 toks/s, output: 182.34 toks/s]
WARNING 01-11 01:57:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:29,028][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:32,100][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:29,  3.07s/it, est. speed input: 240.61 toks/s, output: 2.93 toks/s]
[2025-01-11 01:57:32,496][root][ERROR] - Processed prompts:  10%|#         | 3/30 [00:03<00:25,  1.06it/s, est. speed input: 641.79 toks/s, output: 10.10 toks/s]
[2025-01-11 01:57:32,605][root][ERROR] - Processed prompts:  13%|#3        | 4/30 [00:03<00:17,  1.53it/s, est. speed input: 831.42 toks/s, output: 14.82 toks/s]
[2025-01-11 01:57:32,811][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:03<00:09,  2.63it/s, est. speed input: 1176.29 toks/s, output: 24.59 toks/s]
[2025-01-11 01:57:32,912][root][ERROR] - Processed prompts:  23%|##3       | 7/30 [00:03<00:07,  3.26it/s, est. speed input: 1335.77 toks/s, output: 30.13 toks/s]
[2025-01-11 01:57:33,141][root][ERROR] - Processed prompts:  37%|###6      | 11/30 [00:04<00:02,  6.36it/s, est. speed input: 1982.90 toks/s, output: 53.99 toks/s]
[2025-01-11 01:57:33,266][root][ERROR] - Processed prompts:  67%|######6   | 20/30 [00:04<00:00, 15.89it/s, est. speed input: 3510.09 toks/s, output: 114.92 toks/s]
[2025-01-11 01:57:33,428][root][ERROR] - Processed prompts:  80%|########  | 24/30 [00:04<00:00, 17.77it/s, est. speed input: 4067.44 toks/s, output: 143.20 toks/s]
[2025-01-11 01:57:33,704][root][ERROR] - Processed prompts:  90%|######### | 27/30 [00:04<00:00, 15.43it/s, est. speed input: 4304.85 toks/s, output: 165.13 toks/s]
[2025-01-11 01:57:33,940][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 14.63it/s, est. speed input: 4555.42 toks/s, output: 193.24 toks/s]
[2025-01-11 01:57:33,940][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.11it/s, est. speed input: 4555.42 toks/s, output: 193.24 toks/s]
WARNING 01-11 01:57:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:34,154][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:34,778][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.60it/s, est. speed input: 1107.79 toks/s, output: 22.44 toks/s]
[2025-01-11 01:57:35,228][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.92it/s, est. speed input: 1323.60 toks/s, output: 51.23 toks/s]
[2025-01-11 01:57:35,228][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.86it/s, est. speed input: 1323.60 toks/s, output: 51.23 toks/s]
WARNING 01-11 01:57:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:35,450][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:36,660][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.21s/it, est. speed input: 599.45 toks/s, output: 10.75 toks/s]
[2025-01-11 01:57:36,816][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.69it/s, est. speed input: 1126.83 toks/s, output: 23.43 toks/s]
[2025-01-11 01:57:37,059][root][ERROR] - Processed prompts:  38%|###7      | 3/8 [00:01<00:02,  2.32it/s, est. speed input: 1473.41 toks/s, output: 37.92 toks/s]
[2025-01-11 01:57:37,577][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  5.54it/s, est. speed input: 3035.27 toks/s, output: 111.41 toks/s]
[2025-01-11 01:57:37,578][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.76it/s, est. speed input: 3035.27 toks/s, output: 111.41 toks/s]
WARNING 01-11 01:57:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:37,830][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:40,908][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:03<01:10,  3.08s/it, est. speed input: 231.34 toks/s, output: 5.85 toks/s]
[2025-01-11 01:57:41,396][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:34,  1.55s/it, est. speed input: 407.49 toks/s, output: 12.90 toks/s]
[2025-01-11 01:57:41,508][root][ERROR] - Processed prompts:  92%|#########1| 22/24 [00:03<00:00, 10.80it/s, est. speed input: 4763.29 toks/s, output: 171.58 toks/s]
[2025-01-11 01:57:41,947][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.83it/s, est. speed input: 4617.40 toks/s, output: 177.81 toks/s]
WARNING 01-11 01:57:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:42,266][root][ERROR] - Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:44,831][root][ERROR] - Processed prompts:   7%|7         | 1/14 [00:02<00:33,  2.56s/it, est. speed input: 309.28 toks/s, output: 11.31 toks/s]
[2025-01-11 01:57:44,868][root][ERROR] - Processed prompts: 100%|##########| 14/14 [00:02<00:00,  5.38it/s, est. speed input: 4960.56 toks/s, output: 157.59 toks/s]
WARNING 01-11 01:57:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:45,090][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:46,763][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.67s/it, est. speed input: 532.69 toks/s, output: 17.34 toks/s]
[2025-01-11 01:57:46,996][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  5.48it/s, est. speed input: 3512.92 toks/s, output: 129.06 toks/s]
[2025-01-11 01:57:46,996][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.20it/s, est. speed input: 3512.92 toks/s, output: 129.06 toks/s]
WARNING 01-11 01:57:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:47,316][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:48,250][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 1116.96 toks/s, output: 31.06 toks/s]
[2025-01-11 01:57:48,703][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.54it/s, est. speed input: 1442.35 toks/s, output: 61.27 toks/s]
[2025-01-11 01:57:48,704][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.44it/s, est. speed input: 1442.35 toks/s, output: 61.27 toks/s]
WARNING 01-11 01:57:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:48,924][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:50,397][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:07,  1.47s/it, est. speed input: 638.08 toks/s, output: 19.69 toks/s]
[2025-01-11 01:57:50,451][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.93it/s, est. speed input: 3609.94 toks/s, output: 117.17 toks/s]
WARNING 01-11 01:57:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:50,703][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:51,791][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.09s/it, est. speed input: 720.79 toks/s, output: 26.66 toks/s]
[2025-01-11 01:57:52,212][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.28it/s, est. speed input: 1924.64 toks/s, output: 74.20 toks/s]
[2025-01-11 01:57:52,212][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.99it/s, est. speed input: 1924.64 toks/s, output: 74.20 toks/s]
WARNING 01-11 01:57:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:52,426][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:53,384][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.04it/s, est. speed input: 997.58 toks/s, output: 30.29 toks/s]
[2025-01-11 01:57:53,418][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.02it/s, est. speed input: 2006.75 toks/s, output: 60.50 toks/s]
WARNING 01-11 01:57:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:57:53,637][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:57:54,465][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1352.50 toks/s, output: 37.43 toks/s]
[2025-01-11 01:57:54,465][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1352.50 toks/s, output: 37.43 toks/s]
[2025-01-11 01:57:57,026][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:57:57,079][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:57:58,741][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 01:58:00,334][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 01:58:01,975][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 01:58:02,513][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:58:02,514][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 01:58:21,890][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 01:58:21,942][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:58:23,867][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 01:58:25,440][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 01:58:26,987][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 01:58:27,517][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 01:58:27,518][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 01:58:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 01:58:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 01:58:46 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 01:58:46 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 01:58:47,061][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 01:58:48,384][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 01:58:48,760][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 01:58:50,056][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 01:58:51,407][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 01:58:51,408][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 01:58:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 01:59:05 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 01:59:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 01:59:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 01:59:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 01:59:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:28,152][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:31,682][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:49,  3.53s/it, est. speed input: 179.94 toks/s, output: 2.83 toks/s]
[2025-01-11 01:59:32,023][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.67it/s, est. speed input: 820.43 toks/s, output: 15.25 toks/s]
[2025-01-11 01:59:32,161][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.31it/s, est. speed input: 2376.02 toks/s, output: 57.62 toks/s]
[2025-01-11 01:59:32,316][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  8.17it/s, est. speed input: 2897.98 toks/s, output: 76.38 toks/s]
[2025-01-11 01:59:32,445][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 10.48it/s, est. speed input: 3402.16 toks/s, output: 98.07 toks/s]
[2025-01-11 01:59:32,556][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 13.33it/s, est. speed input: 3893.80 toks/s, output: 123.09 toks/s]
[2025-01-11 01:59:32,750][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.91it/s, est. speed input: 4281.76 toks/s, output: 148.34 toks/s]
[2025-01-11 01:59:32,817][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.86it/s, est. speed input: 4356.19 toks/s, output: 155.85 toks/s]
WARNING 01-11 01:59:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:33,089][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:36,209][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:36,  3.12s/it, est. speed input: 209.64 toks/s, output: 3.85 toks/s]
[2025-01-11 01:59:36,380][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:13,  2.00it/s, est. speed input: 995.95 toks/s, output: 19.45 toks/s]
[2025-01-11 01:59:36,484][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:05,  4.16it/s, est. speed input: 1738.60 toks/s, output: 37.71 toks/s]
[2025-01-11 01:59:36,619][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  7.42it/s, est. speed input: 2602.06 toks/s, output: 62.32 toks/s]
[2025-01-11 01:59:36,774][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:03<00:01,  9.09it/s, est. speed input: 3026.14 toks/s, output: 77.34 toks/s]
[2025-01-11 01:59:36,910][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:01, 11.03it/s, est. speed input: 3433.58 toks/s, output: 94.50 toks/s]
[2025-01-11 01:59:37,037][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00, 13.12it/s, est. speed input: 3820.47 toks/s, output: 115.02 toks/s]
[2025-01-11 01:59:37,172][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.95it/s, est. speed input: 4183.27 toks/s, output: 137.18 toks/s]
[2025-01-11 01:59:37,370][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 15.02it/s, est. speed input: 4448.42 toks/s, output: 159.58 toks/s]
[2025-01-11 01:59:37,405][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.41it/s, est. speed input: 4874.82 toks/s, output: 190.92 toks/s]
WARNING 01-11 01:59:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:37,658][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:40,490][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:02<01:13,  2.83s/it, est. speed input: 263.83 toks/s, output: 3.18 toks/s]
[2025-01-11 01:59:40,862][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:34,  1.38s/it, est. speed input: 462.00 toks/s, output: 7.80 toks/s]
[2025-01-11 01:59:41,367][root][ERROR] - Processed prompts:  15%|#4        | 4/27 [00:03<00:15,  1.48it/s, est. speed input: 795.78 toks/s, output: 18.34 toks/s]
[2025-01-11 01:59:41,513][root][ERROR] - Processed prompts:  22%|##2       | 6/27 [00:03<00:08,  2.56it/s, est. speed input: 1199.65 toks/s, output: 32.43 toks/s]
[2025-01-11 01:59:41,883][root][ERROR] - Processed prompts:  85%|########5 | 23/27 [00:04<00:00, 13.08it/s, est. speed input: 4145.45 toks/s, output: 151.04 toks/s]
[2025-01-11 01:59:42,058][root][ERROR] - Processed prompts:  96%|#########6| 26/27 [00:04<00:00, 13.64it/s, est. speed input: 4492.74 toks/s, output: 179.10 toks/s]
[2025-01-11 01:59:42,727][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:05<00:00,  5.33it/s, est. speed input: 4047.19 toks/s, output: 174.03 toks/s]
WARNING 01-11 01:59:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:42,949][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:44,150][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.20s/it, est. speed input: 594.79 toks/s, output: 22.49 toks/s]
[2025-01-11 01:59:44,253][root][ERROR] - Processed prompts:  60%|######    | 3/5 [00:01<00:00,  2.86it/s, est. speed input: 1648.02 toks/s, output: 67.52 toks/s]
[2025-01-11 01:59:44,826][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.15it/s, est. speed input: 1900.49 toks/s, output: 100.73 toks/s]
[2025-01-11 01:59:44,826][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.66it/s, est. speed input: 1900.49 toks/s, output: 100.73 toks/s]
WARNING 01-11 01:59:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:45,072][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:46,232][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.16s/it, est. speed input: 657.53 toks/s, output: 9.48 toks/s]
[2025-01-11 01:59:46,337][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:03,  1.85it/s, est. speed input: 1194.43 toks/s, output: 20.55 toks/s]
[2025-01-11 01:59:46,612][root][ERROR] - Processed prompts:  50%|#####     | 4/8 [00:01<00:01,  3.48it/s, est. speed input: 2093.73 toks/s, output: 45.44 toks/s]
[2025-01-11 01:59:47,391][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.39it/s, est. speed input: 2735.99 toks/s, output: 99.16 toks/s]
[2025-01-11 01:59:47,391][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.45it/s, est. speed input: 2735.99 toks/s, output: 99.16 toks/s]
WARNING 01-11 01:59:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:47,633][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:50,961][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:09,  3.33s/it, est. speed input: 214.83 toks/s, output: 7.81 toks/s]
[2025-01-11 01:59:51,096][root][ERROR] - Processed prompts:  14%|#3        | 3/22 [00:03<00:17,  1.10it/s, est. speed input: 678.36 toks/s, output: 23.39 toks/s]
[2025-01-11 01:59:51,303][root][ERROR] - Processed prompts:  95%|#########5| 21/22 [00:03<00:00, 10.29it/s, est. speed input: 4732.75 toks/s, output: 168.10 toks/s]
[2025-01-11 01:59:51,354][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:03<00:00,  5.91it/s, est. speed input: 4856.65 toks/s, output: 177.38 toks/s]
WARNING 01-11 01:59:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:51,677][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:53,705][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.03s/it, est. speed input: 444.25 toks/s, output: 14.30 toks/s]
[2025-01-11 01:59:53,706][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.93it/s, est. speed input: 4661.51 toks/s, output: 142.95 toks/s]
WARNING 01-11 01:59:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:53,944][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:55,590][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:11,  1.65s/it, est. speed input: 532.17 toks/s, output: 17.62 toks/s]
[2025-01-11 01:59:55,658][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.67it/s, est. speed input: 3820.31 toks/s, output: 137.71 toks/s]
WARNING 01-11 01:59:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:55,951][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:57,238][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.29s/it, est. speed input: 632.74 toks/s, output: 20.21 toks/s]
[2025-01-11 01:59:57,304][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.70it/s, est. speed input: 3555.13 toks/s, output: 104.95 toks/s]
WARNING 01-11 01:59:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:57,520][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:58,616][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.10s/it, est. speed input: 956.30 toks/s, output: 26.46 toks/s]
[2025-01-11 01:59:58,617][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.74it/s, est. speed input: 2753.85 toks/s, output: 79.36 toks/s]
WARNING 01-11 01:59:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:58,869][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 01:59:59,434][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 1541.87 toks/s, output: 30.13 toks/s]
[2025-01-11 01:59:59,434][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 1541.87 toks/s, output: 30.13 toks/s]
WARNING 01-11 01:59:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 01:59:59,634][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:00:00,212][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.73it/s, est. speed input: 1428.01 toks/s, output: 31.16 toks/s]
[2025-01-11 02:00:00,213][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.73it/s, est. speed input: 1428.01 toks/s, output: 31.16 toks/s]
WARNING 01-11 02:00:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:00:00,422][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:00:01,137][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 1205.48 toks/s, output: 36.36 toks/s]
[2025-01-11 02:00:01,137][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 1205.48 toks/s, output: 36.36 toks/s]
WARNING 01-11 02:00:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:00:01,346][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:00:01,841][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.02it/s, est. speed input: 1836.51 toks/s, output: 24.27 toks/s]
[2025-01-11 02:00:01,841][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.02it/s, est. speed input: 1836.51 toks/s, output: 24.27 toks/s]
WARNING 01-11 02:00:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:00:02,049][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:00:02,608][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 1680.73 toks/s, output: 28.64 toks/s]
[2025-01-11 02:00:02,609][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 1680.73 toks/s, output: 28.64 toks/s]
[2025-01-11 02:00:05,267][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:00:05,319][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:00:07,314][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.99s/it]
[2025-01-11 02:00:08,980][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 02:00:10,575][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 02:00:11,104][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 02:00:11,104][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
[2025-01-11 02:00:30,490][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:00:30,543][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:00:32,472][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 02:00:34,122][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 02:00:35,768][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 02:00:36,311][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 02:00:36,311][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 02:00:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:00:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:00:55 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:00:55 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:00:55,722][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:00:57,377][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 02:00:57,882][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:01,  1.02it/s]
[2025-01-11 02:00:59,601][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.32s/it]
[2025-01-11 02:01:00,941][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.33s/it]
[2025-01-11 02:01:00,942][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.30s/it]
INFO 01-11 02:01:01 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:01:14 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:01:15 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:01:15 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:01:37 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:01:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:37,625][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:41,415][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.55 toks/s, output: 2.64 toks/s]
[2025-01-11 02:01:41,756][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.57it/s, est. speed input: 768.61 toks/s, output: 14.28 toks/s]
[2025-01-11 02:01:41,974][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.18it/s, est. speed input: 2336.34 toks/s, output: 57.49 toks/s]
[2025-01-11 02:01:42,079][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  8.70it/s, est. speed input: 2993.54 toks/s, output: 81.49 toks/s]
[2025-01-11 02:01:42,193][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.74it/s, est. speed input: 3614.50 toks/s, output: 108.15 toks/s]
[2025-01-11 02:01:42,554][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 12.33it/s, est. speed input: 3993.94 toks/s, output: 133.91 toks/s]
[2025-01-11 02:01:42,571][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.47it/s, est. speed input: 4108.43 toks/s, output: 143.15 toks/s]
WARNING 01-11 02:01:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:42,850][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:45,668][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.82s/it, est. speed input: 241.68 toks/s, output: 2.48 toks/s]
[2025-01-11 02:01:45,910][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:39,  1.30s/it, est. speed input: 435.69 toks/s, output: 5.88 toks/s]
[2025-01-11 02:01:46,019][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.33it/s, est. speed input: 1654.95 toks/s, output: 28.40 toks/s]
[2025-01-11 02:01:46,154][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  8.51it/s, est. speed input: 2781.67 toks/s, output: 52.67 toks/s]
[2025-01-11 02:01:46,274][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01, 11.41it/s, est. speed input: 3454.98 toks/s, output: 71.86 toks/s]
[2025-01-11 02:01:46,474][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:03<00:00, 13.28it/s, est. speed input: 3986.11 toks/s, output: 93.27 toks/s]
[2025-01-11 02:01:46,689][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:03<00:00, 13.44it/s, est. speed input: 4277.05 toks/s, output: 109.91 toks/s]
[2025-01-11 02:01:46,924][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 13.25it/s, est. speed input: 4518.25 toks/s, output: 131.81 toks/s]
[2025-01-11 02:01:47,069][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.74it/s, est. speed input: 4829.43 toks/s, output: 161.90 toks/s]
[2025-01-11 02:01:47,152][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.44it/s, est. speed input: 4885.76 toks/s, output: 171.54 toks/s]
WARNING 01-11 02:01:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:47,419][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:50,965][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:46,  3.55s/it, est. speed input: 208.72 toks/s, output: 3.95 toks/s]
[2025-01-11 02:01:51,088][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:27,  1.04it/s, est. speed input: 599.11 toks/s, output: 12.27 toks/s]
[2025-01-11 02:01:51,204][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.01it/s, est. speed input: 977.89 toks/s, output: 21.14 toks/s]
[2025-01-11 02:01:51,472][root][ERROR] - Processed prompts:  23%|##2       | 7/31 [00:04<00:08,  2.90it/s, est. speed input: 1273.02 toks/s, output: 30.10 toks/s]
[2025-01-11 02:01:51,778][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:04<00:05,  3.67it/s, est. speed input: 1521.51 toks/s, output: 40.14 toks/s]
[2025-01-11 02:01:51,884][root][ERROR] - Processed prompts:  84%|########3 | 26/31 [00:04<00:00, 18.55it/s, est. speed input: 4437.84 toks/s, output: 150.94 toks/s]
[2025-01-11 02:01:52,277][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.38it/s, est. speed input: 4842.86 toks/s, output: 183.84 toks/s]
WARNING 01-11 02:01:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:52,651][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:53,860][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 603.01 toks/s, output: 34.74 toks/s]
[2025-01-11 02:01:53,860][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 603.01 toks/s, output: 34.74 toks/s]
WARNING 01-11 02:01:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:54,270][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:55,273][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.00s/it, est. speed input: 818.72 toks/s, output: 16.95 toks/s]
[2025-01-11 02:01:55,507][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.88it/s, est. speed input: 2008.40 toks/s, output: 52.55 toks/s]
[2025-01-11 02:01:55,525][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.19it/s, est. speed input: 2555.84 toks/s, output: 75.69 toks/s]
WARNING 01-11 02:01:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:01:55,949][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:01:59,824][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:32,  3.87s/it, est. speed input: 227.40 toks/s, output: 7.23 toks/s]
[2025-01-11 02:02:00,015][root][ERROR] - Processed prompts:  96%|#########6| 24/25 [00:04<00:00,  8.17it/s, est. speed input: 4727.25 toks/s, output: 172.95 toks/s]
[2025-01-11 02:02:00,453][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:04<00:00,  5.55it/s, est. speed input: 4432.96 toks/s, output: 169.66 toks/s]
WARNING 01-11 02:02:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:00,959][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:03,312][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:23,  2.35s/it, est. speed input: 425.57 toks/s, output: 12.33 toks/s]
[2025-01-11 02:02:03,751][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  5.13it/s, est. speed input: 3783.84 toks/s, output: 123.25 toks/s]
[2025-01-11 02:02:03,751][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  3.94it/s, est. speed input: 3783.84 toks/s, output: 123.25 toks/s]
WARNING 01-11 02:02:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:04,162][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:05,831][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.67s/it, est. speed input: 571.82 toks/s, output: 17.38 toks/s]
[2025-01-11 02:02:06,027][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.18it/s, est. speed input: 2621.30 toks/s, output: 99.79 toks/s]
[2025-01-11 02:02:06,259][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.34it/s, est. speed input: 2720.64 toks/s, output: 113.56 toks/s]
WARNING 01-11 02:02:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:06,772][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:08,253][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.48s/it, est. speed input: 645.07 toks/s, output: 19.59 toks/s]
[2025-01-11 02:02:08,622][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.34it/s, est. speed input: 2618.37 toks/s, output: 89.75 toks/s]
[2025-01-11 02:02:08,622][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.70it/s, est. speed input: 2618.37 toks/s, output: 89.75 toks/s]
WARNING 01-11 02:02:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:09,039][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:10,315][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 704.07 toks/s, output: 33.71 toks/s]
[2025-01-11 02:02:10,316][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 704.07 toks/s, output: 33.71 toks/s]
WARNING 01-11 02:02:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:10,705][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:11,683][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 792.94 toks/s, output: 29.67 toks/s]
[2025-01-11 02:02:11,684][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 792.94 toks/s, output: 29.67 toks/s]
WARNING 01-11 02:02:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:12,066][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:13,099][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.03s/it, est. speed input: 921.73 toks/s, output: 28.08 toks/s]
[2025-01-11 02:02:13,100][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.93it/s, est. speed input: 1687.37 toks/s, output: 56.12 toks/s]
WARNING 01-11 02:02:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:13,453][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:15,964][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.51s/it, est. speed input: 469.99 toks/s, output: 51.78 toks/s]
[2025-01-11 02:02:15,964][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:02<00:00,  2.51s/it, est. speed input: 469.99 toks/s, output: 51.78 toks/s]
WARNING 01-11 02:02:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:16,169][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:16,868][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 1411.33 toks/s, output: 34.35 toks/s]
[2025-01-11 02:02:16,868][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 1411.33 toks/s, output: 34.35 toks/s]
WARNING 01-11 02:02:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:17,080][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:18,409][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 1027.22 toks/s, output: 44.40 toks/s]
[2025-01-11 02:02:18,409][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 1027.22 toks/s, output: 44.40 toks/s]
WARNING 01-11 02:02:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:18,620][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:19,422][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1309.36 toks/s, output: 36.20 toks/s]
[2025-01-11 02:02:19,422][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1309.36 toks/s, output: 36.20 toks/s]
WARNING 01-11 02:02:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:19,642][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:20,426][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1294.07 toks/s, output: 37.01 toks/s]
[2025-01-11 02:02:20,426][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1294.07 toks/s, output: 37.01 toks/s]
WARNING 01-11 02:02:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:02:20,637][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:02:21,425][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1365.71 toks/s, output: 36.84 toks/s]
[2025-01-11 02:02:21,425][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1365.71 toks/s, output: 36.84 toks/s]
[2025-01-11 02:02:23,829][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:02:23,883][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:02:25,496][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 02:02:27,187][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 02:02:28,804][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:02:29,342][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:02:29,343][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:02:49,701][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:02:49,806][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:02:53,008][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.20s/it]
[2025-01-11 02:02:56,293][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:06,  3.25s/it]
[2025-01-11 02:02:59,485][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:09<00:03,  3.22s/it]
[2025-01-11 02:03:00,002][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.16s/it]
[2025-01-11 02:03:00,002][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.55s/it]
WARNING 01-11 02:03:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:03:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:03:18 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:03:18 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:03:18,424][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:03:19,750][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:03:20,132][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:03:21,473][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:03:22,813][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:03:22,813][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:03:23 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:03:36 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:03:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:03:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:03:59 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:03:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:03:59,716][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:03,333][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:52,  3.62s/it, est. speed input: 175.59 toks/s, output: 2.77 toks/s]
[2025-01-11 02:04:03,674][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.64it/s, est. speed input: 802.21 toks/s, output: 14.91 toks/s]
[2025-01-11 02:04:03,776][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.40it/s, est. speed input: 1720.61 toks/s, output: 39.66 toks/s]
[2025-01-11 02:04:03,949][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.36it/s, est. speed input: 2250.49 toks/s, output: 56.94 toks/s]
[2025-01-11 02:04:04,056][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  9.78it/s, est. speed input: 2926.57 toks/s, output: 82.27 toks/s]
[2025-01-11 02:04:04,188][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.41it/s, est. speed input: 3691.96 toks/s, output: 114.94 toks/s]
[2025-01-11 02:04:04,343][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 16.46it/s, est. speed input: 4117.97 toks/s, output: 140.72 toks/s]
[2025-01-11 02:04:04,376][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.87it/s, est. speed input: 4360.53 toks/s, output: 156.01 toks/s]
WARNING 01-11 02:04:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:04,624][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:07,624][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:32,  3.00s/it, est. speed input: 223.01 toks/s, output: 3.33 toks/s]
[2025-01-11 02:04:07,745][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:39,  1.31s/it, est. speed input: 421.01 toks/s, output: 7.05 toks/s]
[2025-01-11 02:04:07,854][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.32it/s, est. speed input: 1621.68 toks/s, output: 30.66 toks/s]
[2025-01-11 02:04:08,001][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:03<00:03,  6.13it/s, est. speed input: 2133.69 toks/s, output: 43.54 toks/s]
[2025-01-11 02:04:08,132][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:03<00:01, 10.12it/s, est. speed input: 2987.82 toks/s, output: 68.98 toks/s]
[2025-01-11 02:04:08,280][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 11.83it/s, est. speed input: 3401.53 toks/s, output: 85.07 toks/s]
[2025-01-11 02:04:08,474][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:03<00:00, 12.70it/s, est. speed input: 3745.24 toks/s, output: 102.09 toks/s]
[2025-01-11 02:04:08,584][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:03<00:00, 15.06it/s, est. speed input: 4144.51 toks/s, output: 123.49 toks/s]
[2025-01-11 02:04:09,053][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 12.88it/s, est. speed input: 4454.52 toks/s, output: 156.04 toks/s]
[2025-01-11 02:04:09,599][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.43it/s, est. speed input: 4229.20 toks/s, output: 169.67 toks/s]
WARNING 01-11 02:04:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:09,846][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:12,974][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:03<01:24,  3.13s/it, est. speed input: 236.90 toks/s, output: 4.16 toks/s]
[2025-01-11 02:04:13,136][root][ERROR] - Processed prompts:  11%|#         | 3/28 [00:03<00:21,  1.15it/s, est. speed input: 674.30 toks/s, output: 13.07 toks/s]
[2025-01-11 02:04:13,282][root][ERROR] - Processed prompts:  21%|##1       | 6/28 [00:03<00:08,  2.73it/s, est. speed input: 1289.92 toks/s, output: 27.36 toks/s]
[2025-01-11 02:04:13,423][root][ERROR] - Processed prompts:  29%|##8       | 8/28 [00:03<00:05,  3.89it/s, est. speed input: 1660.07 toks/s, output: 38.03 toks/s]
[2025-01-11 02:04:13,729][root][ERROR] - Processed prompts:  36%|###5      | 10/28 [00:03<00:03,  4.52it/s, est. speed input: 1911.50 toks/s, output: 48.42 toks/s]
[2025-01-11 02:04:13,841][root][ERROR] - Processed prompts:  89%|########9 | 25/28 [00:03<00:00, 18.86it/s, est. speed input: 4700.46 toks/s, output: 158.20 toks/s]
[2025-01-11 02:04:14,357][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  6.21it/s, est. speed input: 4657.74 toks/s, output: 171.15 toks/s]
WARNING 01-11 02:04:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:14,604][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:15,361][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.32it/s, est. speed input: 933.00 toks/s, output: 14.54 toks/s]
[2025-01-11 02:04:15,698][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.96it/s, est. speed input: 1352.90 toks/s, output: 35.65 toks/s]
[2025-01-11 02:04:15,919][root][ERROR] - Processed prompts:  75%|#######5  | 3/4 [00:01<00:00,  2.65it/s, est. speed input: 1692.87 toks/s, output: 60.08 toks/s]
[2025-01-11 02:04:16,740][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.81it/s, est. speed input: 1399.78 toks/s, output: 78.68 toks/s]
[2025-01-11 02:04:16,740][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:02<00:00,  1.87it/s, est. speed input: 1399.78 toks/s, output: 78.68 toks/s]
WARNING 01-11 02:04:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:16,980][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:18,493][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.51s/it, est. speed input: 530.91 toks/s, output: 19.17 toks/s]
[2025-01-11 02:04:18,604][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.87it/s, est. speed input: 2893.20 toks/s, output: 110.87 toks/s]
[2025-01-11 02:04:18,621][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.27it/s, est. speed input: 3321.79 toks/s, output: 131.65 toks/s]
WARNING 01-11 02:04:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:18,867][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:21,680][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:02<01:04,  2.81s/it, est. speed input: 251.34 toks/s, output: 4.27 toks/s]
[2025-01-11 02:04:22,023][root][ERROR] - Processed prompts:   8%|8         | 2/24 [00:03<00:29,  1.36s/it, est. speed input: 451.32 toks/s, output: 9.83 toks/s]
[2025-01-11 02:04:22,500][root][ERROR] - Processed prompts:  12%|#2        | 3/24 [00:03<00:20,  1.05it/s, est. speed input: 587.75 toks/s, output: 16.52 toks/s]
[2025-01-11 02:04:22,534][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:03<00:00,  6.55it/s, est. speed input: 5297.66 toks/s, output: 183.00 toks/s]
WARNING 01-11 02:04:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:22,849][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:24,868][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:02<00:18,  2.02s/it, est. speed input: 480.56 toks/s, output: 14.37 toks/s]
[2025-01-11 02:04:24,902][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.87it/s, est. speed input: 4552.02 toks/s, output: 142.25 toks/s]
WARNING 01-11 02:04:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:25,123][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:26,564][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:10,  1.44s/it, est. speed input: 622.45 toks/s, output: 13.18 toks/s]
[2025-01-11 02:04:26,826][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.34it/s, est. speed input: 1062.18 toks/s, output: 28.18 toks/s]
[2025-01-11 02:04:26,975][root][ERROR] - Processed prompts:  88%|########7 | 7/8 [00:01<00:00,  5.97it/s, est. speed input: 3335.65 toks/s, output: 108.58 toks/s]
[2025-01-11 02:04:27,360][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.58it/s, est. speed input: 3120.94 toks/s, output: 116.72 toks/s]
WARNING 01-11 02:04:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:27,647][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:29,001][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.35s/it, est. speed input: 596.07 toks/s, output: 21.42 toks/s]
[2025-01-11 02:04:29,052][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.56it/s, est. speed input: 3359.77 toks/s, output: 105.33 toks/s]
WARNING 01-11 02:04:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:29,264][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:30,180][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.09it/s, est. speed input: 1033.10 toks/s, output: 31.67 toks/s]
[2025-01-11 02:04:30,181][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.18it/s, est. speed input: 2046.98 toks/s, output: 63.32 toks/s]
WARNING 01-11 02:04:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:04:30,424][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:04:31,373][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 1185.64 toks/s, output: 30.54 toks/s]
[2025-01-11 02:04:31,374][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.11it/s, est. speed input: 2343.06 toks/s, output: 61.05 toks/s]
[2025-01-11 02:04:33,777][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:04:33,830][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:04:35,494][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 02:04:37,145][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 02:04:38,726][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 02:04:39,258][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:04:39,259][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:04:58,014][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:04:58,066][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:05:00,039][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.97s/it]
[2025-01-11 02:05:01,641][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 02:05:03,186][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:05:03,689][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:05:03,689][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 02:05:22 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:05:22 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:05:22 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:05:22 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:05:23,066][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:05:24,455][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.39s/it]
[2025-01-11 02:05:24,942][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.17it/s]
[2025-01-11 02:05:26,530][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.19s/it]
[2025-01-11 02:05:28,059][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.32s/it]
[2025-01-11 02:05:28,059][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.25s/it]
INFO 01-11 02:05:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:05:42 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:05:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:05:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:06:04 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:06:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:04,593][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:08,352][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.76s/it, est. speed input: 168.94 toks/s, output: 2.66 toks/s]
[2025-01-11 02:06:08,525][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.68it/s, est. speed input: 807.53 toks/s, output: 14.24 toks/s]
[2025-01-11 02:06:08,687][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.49it/s, est. speed input: 1085.89 toks/s, output: 21.25 toks/s]
[2025-01-11 02:06:08,822][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.83it/s, est. speed input: 2402.50 toks/s, output: 57.46 toks/s]
[2025-01-11 02:06:09,005][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  9.78it/s, est. speed input: 2878.84 toks/s, output: 75.48 toks/s]
[2025-01-11 02:06:09,177][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.81it/s, est. speed input: 3463.66 toks/s, output: 101.67 toks/s]
[2025-01-11 02:06:09,370][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 15.44it/s, est. speed input: 3988.29 toks/s, output: 132.94 toks/s]
[2025-01-11 02:06:09,806][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.14it/s, est. speed input: 3898.00 toks/s, output: 143.68 toks/s]
WARNING 01-11 02:06:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:10,075][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:12,949][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.87s/it, est. speed input: 231.37 toks/s, output: 2.78 toks/s]
[2025-01-11 02:06:13,070][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:02<00:37,  1.25s/it, est. speed input: 438.71 toks/s, output: 6.01 toks/s]
[2025-01-11 02:06:13,187][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:14,  1.98it/s, est. speed input: 838.64 toks/s, output: 13.17 toks/s]
[2025-01-11 02:06:13,326][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:01, 10.46it/s, est. speed input: 3016.62 toks/s, output: 56.91 toks/s]
[2025-01-11 02:06:13,548][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 11.97it/s, est. speed input: 3577.28 toks/s, output: 74.00 toks/s]
[2025-01-11 02:06:13,727][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:03<00:00, 14.97it/s, est. speed input: 4315.76 toks/s, output: 102.42 toks/s]
[2025-01-11 02:06:13,931][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:03<00:00, 14.90it/s, est. speed input: 4595.41 toks/s, output: 122.15 toks/s]
[2025-01-11 02:06:14,073][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:03<00:00, 16.08it/s, est. speed input: 4930.90 toks/s, output: 146.34 toks/s]
[2025-01-11 02:06:14,247][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.67it/s, est. speed input: 5048.78 toks/s, output: 163.71 toks/s]
WARNING 01-11 02:06:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:14,508][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:17,845][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:43,  3.34s/it, est. speed input: 218.44 toks/s, output: 3.00 toks/s]
[2025-01-11 02:06:18,027][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:44,  1.48s/it, est. speed input: 414.30 toks/s, output: 6.54 toks/s]
[2025-01-11 02:06:18,144][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:16,  1.70it/s, est. speed input: 801.31 toks/s, output: 14.30 toks/s]
[2025-01-11 02:06:18,257][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:11,  2.26it/s, est. speed input: 971.61 toks/s, output: 18.40 toks/s]
[2025-01-11 02:06:18,366][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:08,  2.94it/s, est. speed input: 1136.18 toks/s, output: 22.81 toks/s]
[2025-01-11 02:06:18,473][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:06,  3.72it/s, est. speed input: 1290.98 toks/s, output: 27.49 toks/s]
[2025-01-11 02:06:18,783][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:06,  3.55it/s, est. speed input: 1368.87 toks/s, output: 31.81 toks/s]
[2025-01-11 02:06:18,883][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 27.09it/s, est. speed input: 4500.25 toks/s, output: 150.17 toks/s]
[2025-01-11 02:06:19,899][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  5.94it/s, est. speed input: 4477.26 toks/s, output: 171.94 toks/s]
WARNING 01-11 02:06:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:20,128][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:21,101][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:00<00:03,  1.03it/s, est. speed input: 797.74 toks/s, output: 15.42 toks/s]
[2025-01-11 02:06:21,404][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.73it/s, est. speed input: 1252.68 toks/s, output: 34.47 toks/s]
[2025-01-11 02:06:21,890][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.57it/s, est. speed input: 2331.83 toks/s, output: 90.80 toks/s]
[2025-01-11 02:06:21,890][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  2.84it/s, est. speed input: 2331.83 toks/s, output: 90.80 toks/s]
WARNING 01-11 02:06:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:22,137][root][ERROR] - Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:25,505][root][ERROR] - Processed prompts:   4%|4         | 1/25 [00:03<01:20,  3.37s/it, est. speed input: 210.80 toks/s, output: 5.94 toks/s]
[2025-01-11 02:06:25,607][root][ERROR] - Processed prompts:   8%|8         | 2/25 [00:03<00:33,  1.45s/it, est. speed input: 408.41 toks/s, output: 12.11 toks/s]
[2025-01-11 02:06:25,949][root][ERROR] - Processed prompts:  12%|#2        | 3/25 [00:03<00:20,  1.06it/s, est. speed input: 609.71 toks/s, output: 18.63 toks/s]
[2025-01-11 02:06:25,949][root][ERROR] - Processed prompts: 100%|##########| 25/25 [00:03<00:00,  6.56it/s, est. speed input: 5370.08 toks/s, output: 185.98 toks/s]
WARNING 01-11 02:06:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:26,274][root][ERROR] - Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:28,022][root][ERROR] - Processed prompts:  10%|#         | 1/10 [00:01<00:15,  1.75s/it, est. speed input: 454.30 toks/s, output: 11.44 toks/s]
[2025-01-11 02:06:28,279][root][ERROR] - Processed prompts:  20%|##        | 2/10 [00:02<00:06,  1.15it/s, est. speed input: 885.65 toks/s, output: 24.45 toks/s]
[2025-01-11 02:06:28,279][root][ERROR] - Processed prompts: 100%|##########| 10/10 [00:02<00:00,  4.99it/s, est. speed input: 4633.49 toks/s, output: 140.18 toks/s]
WARNING 01-11 02:06:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:28,498][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:29,778][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.28s/it, est. speed input: 696.08 toks/s, output: 22.66 toks/s]
[2025-01-11 02:06:29,817][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.79it/s, est. speed input: 3094.24 toks/s, output: 113.70 toks/s]
WARNING 01-11 02:06:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:30,093][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:31,016][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 1084.77 toks/s, output: 31.43 toks/s]
[2025-01-11 02:06:31,738][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.24it/s, est. speed input: 1165.88 toks/s, output: 61.43 toks/s]
[2025-01-11 02:06:31,738][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.22it/s, est. speed input: 1165.88 toks/s, output: 61.43 toks/s]
WARNING 01-11 02:06:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:31,960][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:32,865][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.11it/s, est. speed input: 1037.92 toks/s, output: 32.05 toks/s]
[2025-01-11 02:06:32,865][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.21it/s, est. speed input: 1881.68 toks/s, output: 64.08 toks/s]
WARNING 01-11 02:06:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:33,093][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:34,031][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 1091.90 toks/s, output: 30.92 toks/s]
[2025-01-11 02:06:34,503][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.51it/s, est. speed input: 1448.51 toks/s, output: 61.00 toks/s]
[2025-01-11 02:06:34,503][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.42it/s, est. speed input: 1448.51 toks/s, output: 61.00 toks/s]
WARNING 01-11 02:06:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:34,722][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:35,479][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1008.51 toks/s, output: 38.33 toks/s]
[2025-01-11 02:06:35,479][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1008.51 toks/s, output: 38.33 toks/s]
WARNING 01-11 02:06:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:35,701][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:37,423][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.72s/it, est. speed input: 746.34 toks/s, output: 47.63 toks/s]
[2025-01-11 02:06:37,423][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.72s/it, est. speed input: 746.34 toks/s, output: 47.63 toks/s]
WARNING 01-11 02:06:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:06:37,634][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:06:38,473][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 1693.52 toks/s, output: 34.56 toks/s]
[2025-01-11 02:06:38,473][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 1693.52 toks/s, output: 34.56 toks/s]
[2025-01-11 02:06:40,865][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:06:40,918][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:06:42,532][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 02:06:44,208][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:06:45,918][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 02:06:46,470][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 02:06:46,470][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 02:07:05,546][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:07:05,598][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:07:07,576][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 02:07:09,206][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 02:07:10,771][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 02:07:11,279][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:07:11,279][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 02:07:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:07:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:07:30 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:07:30 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:07:31,036][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:07:32,370][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:07:32,784][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 02:07:34,114][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 02:07:35,478][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
[2025-01-11 02:07:35,478][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 02:07:35 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:07:49 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:07:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:07:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:08:11 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:08:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:11,916][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:15,726][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.81s/it, est. speed input: 166.68 toks/s, output: 2.62 toks/s]
[2025-01-11 02:08:15,843][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.69it/s, est. speed input: 808.54 toks/s, output: 14.01 toks/s]
[2025-01-11 02:08:16,056][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.91it/s, est. speed input: 1227.30 toks/s, output: 23.92 toks/s]
[2025-01-11 02:08:16,188][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.54it/s, est. speed input: 2378.32 toks/s, output: 55.48 toks/s]
[2025-01-11 02:08:16,291][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 12.50it/s, est. speed input: 3338.70 toks/s, output: 87.32 toks/s]
[2025-01-11 02:08:16,566][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 16.45it/s, est. speed input: 4233.60 toks/s, output: 125.38 toks/s]
[2025-01-11 02:08:16,566][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.88it/s, est. speed input: 4369.96 toks/s, output: 133.12 toks/s]
WARNING 01-11 02:08:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:16,794][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:17,333][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.86it/s, est. speed input: 1266.26 toks/s, output: 27.89 toks/s]
[2025-01-11 02:08:17,333][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.86it/s, est. speed input: 1266.26 toks/s, output: 27.89 toks/s]
WARNING 01-11 02:08:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:17,582][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:20,442][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:25,  2.86s/it, est. speed input: 225.86 toks/s, output: 3.15 toks/s]
[2025-01-11 02:08:20,603][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:09,  2.64it/s, est. speed input: 1297.61 toks/s, output: 19.20 toks/s]
[2025-01-11 02:08:20,745][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:03<00:02,  6.64it/s, est. speed input: 2682.46 toks/s, output: 48.38 toks/s]
[2025-01-11 02:08:20,858][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:03<00:01,  9.97it/s, est. speed input: 3585.87 toks/s, output: 71.73 toks/s]
[2025-01-11 02:08:20,975][root][ERROR] - Processed prompts:  74%|#######4  | 23/31 [00:03<00:00, 13.78it/s, est. speed input: 4426.07 toks/s, output: 97.84 toks/s]
[2025-01-11 02:08:21,172][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:03<00:00, 15.21it/s, est. speed input: 4920.36 toks/s, output: 122.01 toks/s]
[2025-01-11 02:08:21,436][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00, 15.19it/s, est. speed input: 5262.53 toks/s, output: 155.68 toks/s]
[2025-01-11 02:08:21,436][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  8.04it/s, est. speed input: 5262.53 toks/s, output: 155.68 toks/s]
WARNING 01-11 02:08:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:21,715][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:24,646][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:02<01:24,  2.93s/it, est. speed input: 245.68 toks/s, output: 2.39 toks/s]
[2025-01-11 02:08:24,877][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:37,  1.34s/it, est. speed input: 458.26 toks/s, output: 5.69 toks/s]
[2025-01-11 02:08:25,038][root][ERROR] - Processed prompts:  17%|#6        | 5/30 [00:03<00:10,  2.38it/s, est. speed input: 1099.29 toks/s, output: 16.55 toks/s]
[2025-01-11 02:08:25,193][root][ERROR] - Processed prompts:  20%|##        | 6/30 [00:03<00:08,  2.84it/s, est. speed input: 1262.19 toks/s, output: 20.70 toks/s]
[2025-01-11 02:08:25,294][root][ERROR] - Processed prompts:  23%|##3       | 7/30 [00:03<00:06,  3.50it/s, est. speed input: 1429.42 toks/s, output: 25.43 toks/s]
[2025-01-11 02:08:25,586][root][ERROR] - Processed prompts:  27%|##6       | 8/30 [00:03<00:06,  3.48it/s, est. speed input: 1512.90 toks/s, output: 29.96 toks/s]
[2025-01-11 02:08:25,729][root][ERROR] - Processed prompts:  30%|###       | 9/30 [00:04<00:05,  4.06it/s, est. speed input: 1640.56 toks/s, output: 35.87 toks/s]
[2025-01-11 02:08:25,880][root][ERROR] - Processed prompts:  80%|########  | 24/30 [00:04<00:00, 22.95it/s, est. speed input: 4294.93 toks/s, output: 139.97 toks/s]
[2025-01-11 02:08:26,262][root][ERROR] - Processed prompts:  93%|#########3| 28/30 [00:04<00:00, 17.75it/s, est. speed input: 4583.71 toks/s, output: 167.81 toks/s]
[2025-01-11 02:08:26,443][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.34it/s, est. speed input: 4716.18 toks/s, output: 186.33 toks/s]
WARNING 01-11 02:08:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:26,651][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:27,265][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.63it/s, est. speed input: 1060.47 toks/s, output: 22.84 toks/s]
[2025-01-11 02:08:27,515][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.50it/s, est. speed input: 1597.59 toks/s, output: 49.82 toks/s]
[2025-01-11 02:08:27,515][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.32it/s, est. speed input: 1597.59 toks/s, output: 49.82 toks/s]
WARNING 01-11 02:08:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:27,746][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:29,035][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:07,  1.29s/it, est. speed input: 562.28 toks/s, output: 15.51 toks/s]
[2025-01-11 02:08:29,253][root][ERROR] - Processed prompts:  29%|##8       | 2/7 [00:01<00:03,  1.52it/s, est. speed input: 1022.10 toks/s, output: 32.50 toks/s]
[2025-01-11 02:08:29,759][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  4.80it/s, est. speed input: 2750.08 toks/s, output: 112.25 toks/s]
[2025-01-11 02:08:29,759][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.48it/s, est. speed input: 2750.08 toks/s, output: 112.25 toks/s]
WARNING 01-11 02:08:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:30,002][root][ERROR] - Processed prompts:   0%|          | 0/24 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:33,308][root][ERROR] - Processed prompts:   4%|4         | 1/24 [00:03<01:16,  3.30s/it, est. speed input: 229.37 toks/s, output: 6.66 toks/s]
[2025-01-11 02:08:33,644][root][ERROR] - Processed prompts:  12%|#2        | 3/24 [00:03<00:20,  1.02it/s, est. speed input: 588.88 toks/s, output: 20.32 toks/s]
[2025-01-11 02:08:34,115][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  9.84it/s, est. speed input: 4655.01 toks/s, output: 173.15 toks/s]
[2025-01-11 02:08:34,115][root][ERROR] - Processed prompts: 100%|##########| 24/24 [00:04<00:00,  5.84it/s, est. speed input: 4655.01 toks/s, output: 173.15 toks/s]
WARNING 01-11 02:08:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:34,441][root][ERROR] - Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:36,618][root][ERROR] - Processed prompts:   9%|9         | 1/11 [00:02<00:21,  2.18s/it, est. speed input: 440.62 toks/s, output: 13.32 toks/s]
[2025-01-11 02:08:36,619][root][ERROR] - Processed prompts: 100%|##########| 11/11 [00:02<00:00,  5.05it/s, est. speed input: 4731.17 toks/s, output: 146.53 toks/s]
WARNING 01-11 02:08:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:36,856][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:38,009][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:08,  1.15s/it, est. speed input: 688.45 toks/s, output: 8.67 toks/s]
[2025-01-11 02:08:38,402][root][ERROR] - Processed prompts:  25%|##5       | 2/8 [00:01<00:04,  1.42it/s, est. speed input: 1086.88 toks/s, output: 22.64 toks/s]
[2025-01-11 02:08:39,014][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  4.95it/s, est. speed input: 3132.82 toks/s, output: 109.82 toks/s]
[2025-01-11 02:08:39,014][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:02<00:00,  3.71it/s, est. speed input: 3132.82 toks/s, output: 109.82 toks/s]
WARNING 01-11 02:08:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:39,301][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:40,380][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 713.17 toks/s, output: 26.89 toks/s]
[2025-01-11 02:08:40,380][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.78it/s, est. speed input: 2628.26 toks/s, output: 80.65 toks/s]
WARNING 01-11 02:08:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:40,598][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:41,719][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.12s/it, est. speed input: 853.66 toks/s, output: 22.32 toks/s]
[2025-01-11 02:08:41,833][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  4.12it/s, est. speed input: 2968.86 toks/s, output: 92.35 toks/s]
[2025-01-11 02:08:41,833][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.24it/s, est. speed input: 2968.86 toks/s, output: 92.35 toks/s]
WARNING 01-11 02:08:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:42,073][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:42,864][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1369.57 toks/s, output: 36.71 toks/s]
[2025-01-11 02:08:42,864][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1369.57 toks/s, output: 36.71 toks/s]
WARNING 01-11 02:08:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:08:43,077][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:08:43,861][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1294.26 toks/s, output: 37.01 toks/s]
[2025-01-11 02:08:43,861][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1294.26 toks/s, output: 37.01 toks/s]
[2025-01-11 02:08:46,293][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:08:46,347][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:08:48,063][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 02:08:49,745][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 02:08:51,349][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 02:08:51,886][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:08:51,886][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 02:09:10,438][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:09:10,491][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:09:12,486][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  2.00s/it]
[2025-01-11 02:09:14,186][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.82s/it]
[2025-01-11 02:09:15,781][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 02:09:16,286][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 02:09:16,287][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 02:09:34 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:09:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:09:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:09:34 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:09:35,072][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:09:36,400][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:09:36,907][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.18it/s]
[2025-01-11 02:09:38,313][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.10s/it]
[2025-01-11 02:09:39,929][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.30s/it]
[2025-01-11 02:09:39,930][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
INFO 01-11 02:09:40 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:09:53 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:09:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:09:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:10:16 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:10:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:16,551][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:20,343][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.47 toks/s, output: 2.64 toks/s]
[2025-01-11 02:10:20,460][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.70it/s, est. speed input: 812.25 toks/s, output: 14.07 toks/s]
[2025-01-11 02:10:20,617][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  3.00it/s, est. speed input: 1249.26 toks/s, output: 23.61 toks/s]
[2025-01-11 02:10:20,756][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  7.09it/s, est. speed input: 2264.83 toks/s, output: 50.65 toks/s]
[2025-01-11 02:10:20,866][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 10.52it/s, est. speed input: 2943.23 toks/s, output: 72.07 toks/s]
[2025-01-11 02:10:20,980][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 15.42it/s, est. speed input: 3727.50 toks/s, output: 101.15 toks/s]
[2025-01-11 02:10:21,149][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 18.22it/s, est. speed input: 4280.77 toks/s, output: 129.39 toks/s]
[2025-01-11 02:10:21,582][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.36it/s, est. speed input: 4038.73 toks/s, output: 129.99 toks/s]
WARNING 01-11 02:10:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:21,863][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:24,789][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:30,  2.93s/it, est. speed input: 221.47 toks/s, output: 3.08 toks/s]
[2025-01-11 02:10:24,907][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:23,  1.25it/s, est. speed input: 645.86 toks/s, output: 9.53 toks/s]
[2025-01-11 02:10:25,013][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.25it/s, est. speed input: 1660.10 toks/s, output: 27.93 toks/s]
[2025-01-11 02:10:25,148][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:01,  9.31it/s, est. speed input: 2977.51 toks/s, output: 57.23 toks/s]
[2025-01-11 02:10:25,291][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:03<00:00, 13.83it/s, est. speed input: 3996.48 toks/s, output: 86.07 toks/s]
[2025-01-11 02:10:25,443][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:03<00:00, 16.00it/s, est. speed input: 4558.13 toks/s, output: 109.50 toks/s]
[2025-01-11 02:10:25,634][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:03<00:00, 17.18it/s, est. speed input: 5035.85 toks/s, output: 136.05 toks/s]
[2025-01-11 02:10:25,684][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.37it/s, est. speed input: 5486.80 toks/s, output: 160.95 toks/s]
WARNING 01-11 02:10:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:25,950][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:29,200][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:40,  3.25s/it, est. speed input: 224.64 toks/s, output: 2.77 toks/s]
[2025-01-11 02:10:29,614][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:28,  1.00it/s, est. speed input: 601.85 toks/s, output: 9.55 toks/s]
[2025-01-11 02:10:29,727][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:13,  1.95it/s, est. speed input: 969.08 toks/s, output: 18.27 toks/s]
[2025-01-11 02:10:29,999][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.23it/s, est. speed input: 1085.05 toks/s, output: 22.72 toks/s]
[2025-01-11 02:10:30,317][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.43it/s, est. speed input: 1179.62 toks/s, output: 27.70 toks/s]
[2025-01-11 02:10:30,419][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 19.72it/s, est. speed input: 4519.63 toks/s, output: 158.21 toks/s]
[2025-01-11 02:10:30,805][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.59it/s, est. speed input: 4909.58 toks/s, output: 190.31 toks/s]
WARNING 01-11 02:10:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:31,041][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:32,308][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.27s/it, est. speed input: 642.74 toks/s, output: 22.90 toks/s]
[2025-01-11 02:10:32,596][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.00it/s, est. speed input: 2580.14 toks/s, output: 105.47 toks/s]
[2025-01-11 02:10:32,597][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.21it/s, est. speed input: 2580.14 toks/s, output: 105.47 toks/s]
WARNING 01-11 02:10:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:32,850][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:35,972][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:18,  3.12s/it, est. speed input: 228.02 toks/s, output: 4.16 toks/s]
[2025-01-11 02:10:36,593][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:39,  1.65s/it, est. speed input: 374.56 toks/s, output: 10.15 toks/s]
[2025-01-11 02:10:36,795][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:22,  1.01it/s, est. speed input: 584.23 toks/s, output: 16.98 toks/s]
[2025-01-11 02:10:36,870][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  6.47it/s, est. speed input: 5297.42 toks/s, output: 184.60 toks/s]
WARNING 01-11 02:10:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:37,203][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:38,944][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.74s/it, est. speed input: 555.06 toks/s, output: 16.66 toks/s]
[2025-01-11 02:10:38,944][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.60it/s, est. speed input: 4254.87 toks/s, output: 133.27 toks/s]
WARNING 01-11 02:10:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:39,163][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:40,686][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.52s/it, est. speed input: 489.23 toks/s, output: 18.39 toks/s]
[2025-01-11 02:10:40,877][root][ERROR] - Processed prompts:  86%|########5 | 6/7 [00:01<00:00,  4.53it/s, est. speed input: 2928.99 toks/s, output: 106.19 toks/s]
[2025-01-11 02:10:41,077][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  3.66it/s, est. speed input: 2977.98 toks/s, output: 121.21 toks/s]
WARNING 01-11 02:10:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:41,349][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:42,395][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.05s/it, est. speed input: 1014.76 toks/s, output: 27.71 toks/s]
[2025-01-11 02:10:42,899][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.18it/s, est. speed input: 1684.09 toks/s, output: 75.46 toks/s]
[2025-01-11 02:10:42,900][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  1.93it/s, est. speed input: 1684.09 toks/s, output: 75.46 toks/s]
WARNING 01-11 02:10:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:43,134][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:44,056][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 1040.47 toks/s, output: 31.46 toks/s]
[2025-01-11 02:10:44,056][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.17it/s, est. speed input: 2054.10 toks/s, output: 62.90 toks/s]
WARNING 01-11 02:10:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:10:44,297][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:10:45,225][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 1059.81 toks/s, output: 31.27 toks/s]
[2025-01-11 02:10:45,293][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.01it/s, est. speed input: 1998.52 toks/s, output: 62.26 toks/s]
[2025-01-11 02:10:47,727][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:10:47,780][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:10:49,421][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 02:10:51,072][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:10:52,700][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:10:53,259][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:10:53,259][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:11:11,609][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:11:11,661][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:11:13,667][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.01s/it]
[2025-01-11 02:11:15,298][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 02:11:16,909][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 02:11:17,438][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 02:11:17,438][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 02:11:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:11:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:11:36 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:11:36 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:11:36,655][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:11:38,014][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 02:11:38,398][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 02:11:39,719][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:11:41,051][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:11:41,052][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:11:41 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:11:55 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:11:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:11:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:12:17 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:12:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:17,599][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:21,403][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 166.96 toks/s, output: 2.63 toks/s]
[2025-01-11 02:12:21,520][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.69it/s, est. speed input: 809.85 toks/s, output: 14.03 toks/s]
[2025-01-11 02:12:21,676][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.99it/s, est. speed input: 1246.04 toks/s, output: 23.30 toks/s]
[2025-01-11 02:12:21,817][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  7.07it/s, est. speed input: 2258.24 toks/s, output: 50.50 toks/s]
[2025-01-11 02:12:21,921][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 10.53it/s, est. speed input: 2938.54 toks/s, output: 71.03 toks/s]
[2025-01-11 02:12:22,028][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 17.62it/s, est. speed input: 4014.88 toks/s, output: 110.42 toks/s]
[2025-01-11 02:12:22,238][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.90it/s, est. speed input: 4380.30 toks/s, output: 130.63 toks/s]
WARNING 01-11 02:12:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:22,451][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:23,240][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 863.56 toks/s, output: 36.77 toks/s]
[2025-01-11 02:12:23,240][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 863.56 toks/s, output: 36.77 toks/s]
WARNING 01-11 02:12:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:23,485][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:26,328][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:25,  2.84s/it, est. speed input: 229.71 toks/s, output: 3.17 toks/s]
[2025-01-11 02:12:26,445][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:02<00:21,  1.28it/s, est. speed input: 658.84 toks/s, output: 10.14 toks/s]
[2025-01-11 02:12:26,551][root][ERROR] - Processed prompts:  23%|##2       | 7/31 [00:03<00:06,  3.73it/s, est. speed input: 1492.19 toks/s, output: 25.44 toks/s]
[2025-01-11 02:12:26,691][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:03<00:02,  8.12it/s, est. speed input: 2641.73 toks/s, output: 51.77 toks/s]
[2025-01-11 02:12:26,812][root][ERROR] - Processed prompts:  55%|#####4    | 17/31 [00:03<00:01, 11.16it/s, est. speed input: 3334.59 toks/s, output: 71.84 toks/s]
[2025-01-11 02:12:26,960][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:03<00:00, 17.48it/s, est. speed input: 4514.13 toks/s, output: 111.35 toks/s]
[2025-01-11 02:12:27,096][root][ERROR] - Processed prompts:  90%|######### | 28/31 [00:03<00:00, 19.72it/s, est. speed input: 5070.04 toks/s, output: 136.79 toks/s]
[2025-01-11 02:12:27,322][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  8.08it/s, est. speed input: 5282.63 toks/s, output: 156.90 toks/s]
WARNING 01-11 02:12:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:27,570][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:30,671][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:03<01:26,  3.10s/it, est. speed input: 233.79 toks/s, output: 3.87 toks/s]
[2025-01-11 02:12:30,837][root][ERROR] - Processed prompts:  10%|#         | 3/29 [00:03<00:22,  1.16it/s, est. speed input: 666.16 toks/s, output: 12.25 toks/s]
[2025-01-11 02:12:30,996][root][ERROR] - Processed prompts:  14%|#3        | 4/29 [00:03<00:15,  1.61it/s, est. speed input: 847.40 toks/s, output: 16.93 toks/s]
[2025-01-11 02:12:31,349][root][ERROR] - Processed prompts:  21%|##        | 6/29 [00:03<00:09,  2.50it/s, est. speed input: 1153.92 toks/s, output: 26.99 toks/s]
[2025-01-11 02:12:31,491][root][ERROR] - Processed prompts:  31%|###1      | 9/29 [00:03<00:04,  4.58it/s, est. speed input: 1682.90 toks/s, output: 46.41 toks/s]
[2025-01-11 02:12:31,777][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00, 20.50it/s, est. speed input: 5089.46 toks/s, output: 185.39 toks/s]
[2025-01-11 02:12:31,778][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.89it/s, est. speed input: 5089.46 toks/s, output: 185.39 toks/s]
WARNING 01-11 02:12:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:32,009][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:32,751][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.35it/s, est. speed input: 935.66 toks/s, output: 21.57 toks/s]
[2025-01-11 02:12:32,974][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:00<00:00,  3.64it/s, est. speed input: 2155.17 toks/s, output: 66.38 toks/s]
[2025-01-11 02:12:32,974][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:00<00:00,  3.11it/s, est. speed input: 2155.17 toks/s, output: 66.38 toks/s]
WARNING 01-11 02:12:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:33,201][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:34,404][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:06,  1.20s/it, est. speed input: 655.34 toks/s, output: 17.46 toks/s]
[2025-01-11 02:12:34,587][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.66it/s, est. speed input: 1098.47 toks/s, output: 36.09 toks/s]
[2025-01-11 02:12:34,829][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  5.41it/s, est. speed input: 2980.84 toks/s, output: 113.06 toks/s]
[2025-01-11 02:12:34,829][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  3.69it/s, est. speed input: 2980.84 toks/s, output: 113.06 toks/s]
WARNING 01-11 02:12:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:35,084][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:38,113][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:03<01:15,  3.03s/it, est. speed input: 229.48 toks/s, output: 3.63 toks/s]
[2025-01-11 02:12:38,372][root][ERROR] - Processed prompts:   8%|7         | 2/26 [00:03<00:33,  1.40s/it, est. speed input: 425.25 toks/s, output: 8.21 toks/s]
[2025-01-11 02:12:38,474][root][ERROR] - Processed prompts:  12%|#1        | 3/26 [00:03<00:18,  1.24it/s, est. speed input: 627.65 toks/s, output: 13.28 toks/s]
[2025-01-11 02:12:39,011][root][ERROR] - Processed prompts:  15%|#5        | 4/26 [00:03<00:15,  1.43it/s, est. speed input: 767.11 toks/s, output: 18.85 toks/s]
[2025-01-11 02:12:39,250][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00, 16.06it/s, est. speed input: 5176.24 toks/s, output: 174.79 toks/s]
[2025-01-11 02:12:39,250][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  6.24it/s, est. speed input: 5176.24 toks/s, output: 174.79 toks/s]
WARNING 01-11 02:12:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:39,588][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:41,309][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.72s/it, est. speed input: 555.05 toks/s, output: 16.85 toks/s]
[2025-01-11 02:12:41,309][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.65it/s, est. speed input: 4163.78 toks/s, output: 134.80 toks/s]
WARNING 01-11 02:12:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:41,530][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:42,533][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.00s/it, est. speed input: 695.54 toks/s, output: 17.94 toks/s]
[2025-01-11 02:12:42,773][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.80it/s, est. speed input: 1126.60 toks/s, output: 37.79 toks/s]
[2025-01-11 02:12:42,774][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.02it/s, est. speed input: 3172.42 toks/s, output: 107.73 toks/s]
WARNING 01-11 02:12:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:43,036][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:44,117][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.08s/it, est. speed input: 943.44 toks/s, output: 26.85 toks/s]
[2025-01-11 02:12:44,386][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.63it/s, est. speed input: 2192.95 toks/s, output: 76.33 toks/s]
[2025-01-11 02:12:44,386][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.22it/s, est. speed input: 2192.95 toks/s, output: 76.33 toks/s]
WARNING 01-11 02:12:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:44,603][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:45,661][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 865.89 toks/s, output: 27.41 toks/s]
[2025-01-11 02:12:45,661][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.83it/s, est. speed input: 2647.79 toks/s, output: 82.21 toks/s]
WARNING 01-11 02:12:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:45,904][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:46,913][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 956.39 toks/s, output: 42.62 toks/s]
[2025-01-11 02:12:46,913][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 956.39 toks/s, output: 42.62 toks/s]
WARNING 01-11 02:12:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:47,129][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:47,822][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 1064.01 toks/s, output: 36.09 toks/s]
[2025-01-11 02:12:47,822][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 1064.01 toks/s, output: 36.09 toks/s]
WARNING 01-11 02:12:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:48,034][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:48,803][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1016.21 toks/s, output: 37.73 toks/s]
[2025-01-11 02:12:48,803][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1016.21 toks/s, output: 37.73 toks/s]
WARNING 01-11 02:12:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:12:49,020][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:12:49,829][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1506.49 toks/s, output: 35.87 toks/s]
[2025-01-11 02:12:49,829][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 1506.49 toks/s, output: 35.87 toks/s]
[2025-01-11 02:12:52,280][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:12:52,334][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:12:53,981][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 02:12:55,614][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 02:12:57,241][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 02:12:57,774][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:12:57,775][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:13:16,144][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:13:16,196][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:13:18,154][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.96s/it]
[2025-01-11 02:13:19,772][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 02:13:21,362][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 02:13:21,886][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:13:21,887][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 02:13:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:13:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:13:40 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:13:40 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:13:41,138][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:13:42,472][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:13:42,878][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 02:13:44,172][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:13:45,528][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:13:45,528][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:13:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:13:59 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:14:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:14:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:14:21 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 02:14:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:21,862][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:25,590][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.73s/it, est. speed input: 170.31 toks/s, output: 2.68 toks/s]
[2025-01-11 02:14:25,708][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.72it/s, est. speed input: 825.61 toks/s, output: 14.30 toks/s]
[2025-01-11 02:14:25,924][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.96it/s, est. speed input: 1250.72 toks/s, output: 25.11 toks/s]
[2025-01-11 02:14:26,132][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01,  7.90it/s, est. speed input: 2527.93 toks/s, output: 61.35 toks/s]
[2025-01-11 02:14:26,234][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 11.15it/s, est. speed input: 3195.00 toks/s, output: 85.99 toks/s]
[2025-01-11 02:14:26,348][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 13.84it/s, est. speed input: 3680.19 toks/s, output: 107.89 toks/s]
[2025-01-11 02:14:26,712][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 12.87it/s, est. speed input: 3927.45 toks/s, output: 129.88 toks/s]
[2025-01-11 02:14:26,800][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.48it/s, est. speed input: 4114.84 toks/s, output: 147.22 toks/s]
WARNING 01-11 02:14:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:27,062][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:30,071][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.01s/it, est. speed input: 216.70 toks/s, output: 3.32 toks/s]
[2025-01-11 02:14:30,190][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:16,  1.66it/s, est. speed input: 834.85 toks/s, output: 14.07 toks/s]
[2025-01-11 02:14:30,320][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01,  9.83it/s, est. speed input: 3620.24 toks/s, output: 69.69 toks/s]
[2025-01-11 02:14:30,425][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:03<00:00, 13.69it/s, est. speed input: 4682.37 toks/s, output: 99.33 toks/s]
[2025-01-11 02:14:30,745][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:03<00:00, 15.02it/s, est. speed input: 5352.80 toks/s, output: 134.96 toks/s]
[2025-01-11 02:14:31,102][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.92it/s, est. speed input: 5208.47 toks/s, output: 144.58 toks/s]
WARNING 01-11 02:14:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:31,359][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:34,502][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:34,  3.14s/it, est. speed input: 231.98 toks/s, output: 2.86 toks/s]
[2025-01-11 02:14:34,734][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:03<00:25,  1.11it/s, est. speed input: 650.25 toks/s, output: 9.19 toks/s]
[2025-01-11 02:14:34,845][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:03<00:12,  2.15it/s, est. speed input: 1052.43 toks/s, output: 17.22 toks/s]
[2025-01-11 02:14:35,350][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:07,  3.23it/s, est. speed input: 1474.22 toks/s, output: 29.32 toks/s]
[2025-01-11 02:14:35,545][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:04<00:06,  3.48it/s, est. speed input: 1581.20 toks/s, output: 34.88 toks/s]
[2025-01-11 02:14:35,703][root][ERROR] - Processed prompts:  87%|########7 | 27/31 [00:04<00:00, 18.48it/s, est. speed input: 4595.64 toks/s, output: 155.41 toks/s]
[2025-01-11 02:14:36,162][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.46it/s, est. speed input: 4777.77 toks/s, output: 178.87 toks/s]
WARNING 01-11 02:14:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:36,376][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:37,237][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 847.51 toks/s, output: 39.47 toks/s]
[2025-01-11 02:14:37,237][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 847.51 toks/s, output: 39.47 toks/s]
WARNING 01-11 02:14:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:37,456][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:38,736][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.28s/it, est. speed input: 597.38 toks/s, output: 22.65 toks/s]
[2025-01-11 02:14:38,737][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.90it/s, est. speed input: 3131.18 toks/s, output: 113.19 toks/s]
WARNING 01-11 02:14:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:38,984][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:43,124][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:04<01:47,  4.14s/it, est. speed input: 212.61 toks/s, output: 7.01 toks/s]
[2025-01-11 02:14:43,525][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  8.10it/s, est. speed input: 4914.82 toks/s, output: 177.75 toks/s]
[2025-01-11 02:14:43,525][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  5.95it/s, est. speed input: 4914.82 toks/s, output: 177.75 toks/s]
WARNING 01-11 02:14:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:43,883][root][ERROR] - Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:45,669][root][ERROR] - Processed prompts:  12%|#2        | 1/8 [00:01<00:12,  1.79s/it, est. speed input: 536.00 toks/s, output: 16.24 toks/s]
[2025-01-11 02:14:45,670][root][ERROR] - Processed prompts: 100%|##########| 8/8 [00:01<00:00,  4.48it/s, est. speed input: 4331.17 toks/s, output: 129.87 toks/s]
WARNING 01-11 02:14:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:45,924][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:47,532][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.61s/it, est. speed input: 573.62 toks/s, output: 18.04 toks/s]
[2025-01-11 02:14:47,933][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  4.39it/s, est. speed input: 3127.33 toks/s, output: 112.99 toks/s]
[2025-01-11 02:14:47,934][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:02<00:00,  3.48it/s, est. speed input: 3127.33 toks/s, output: 112.99 toks/s]
WARNING 01-11 02:14:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:48,232][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:49,060][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 956.81 toks/s, output: 39.87 toks/s]
[2025-01-11 02:14:49,060][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 956.81 toks/s, output: 39.87 toks/s]
WARNING 01-11 02:14:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:49,269][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:50,043][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1098.41 toks/s, output: 37.47 toks/s]
[2025-01-11 02:14:50,043][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1098.41 toks/s, output: 37.47 toks/s]
WARNING 01-11 02:14:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:14:50,256][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:14:51,049][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1349.11 toks/s, output: 36.56 toks/s]
[2025-01-11 02:14:51,049][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1349.11 toks/s, output: 36.56 toks/s]
[2025-01-11 02:14:53,469][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:14:53,522][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:14:55,152][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 02:14:56,791][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 02:14:58,439][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:14:58,977][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:14:58,977][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:15:17,028][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:15:17,080][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:15:19,079][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  2.00s/it]
[2025-01-11 02:15:20,704][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 02:15:22,283][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 02:15:22,791][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:15:22,791][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 02:15:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:15:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:15:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:15:41 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:15:41,989][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:15:43,368][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.38s/it]
[2025-01-11 02:15:43,775][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]
[2025-01-11 02:15:45,078][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:15:46,404][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:15:46,404][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:15:46 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:16:00 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:16:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:16:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:16:22 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:16:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:23,077][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:26,872][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.35 toks/s, output: 2.64 toks/s]
[2025-01-11 02:16:26,989][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.69it/s, est. speed input: 811.67 toks/s, output: 14.06 toks/s]
[2025-01-11 02:16:27,256][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.85it/s, est. speed input: 1215.57 toks/s, output: 24.65 toks/s]
[2025-01-11 02:16:27,382][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.42it/s, est. speed input: 2360.13 toks/s, output: 56.68 toks/s]
[2025-01-11 02:16:27,488][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 11.50it/s, est. speed input: 3167.26 toks/s, output: 85.02 toks/s]
[2025-01-11 02:16:27,652][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.34it/s, est. speed input: 4025.18 toks/s, output: 120.66 toks/s]
[2025-01-11 02:16:27,779][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.81it/s, est. speed input: 4322.02 toks/s, output: 139.10 toks/s]
WARNING 01-11 02:16:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:28,031][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:30,970][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:31,  2.94s/it, est. speed input: 223.25 toks/s, output: 3.06 toks/s]
[2025-01-11 02:16:31,087][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:16,  1.70it/s, est. speed input: 862.64 toks/s, output: 12.76 toks/s]
[2025-01-11 02:16:31,192][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:04,  4.66it/s, est. speed input: 1872.52 toks/s, output: 31.32 toks/s]
[2025-01-11 02:16:31,314][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:03<00:01, 10.56it/s, est. speed input: 3391.81 toks/s, output: 63.67 toks/s]
[2025-01-11 02:16:31,438][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00, 15.22it/s, est. speed input: 4422.06 toks/s, output: 91.59 toks/s]
[2025-01-11 02:16:31,550][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:03<00:00, 20.48it/s, est. speed input: 5397.80 toks/s, output: 125.05 toks/s]
[2025-01-11 02:16:31,688][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.75it/s, est. speed input: 5734.47 toks/s, output: 146.32 toks/s]
WARNING 01-11 02:16:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:31,947][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:35,182][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:40,  3.24s/it, est. speed input: 223.77 toks/s, output: 2.78 toks/s]
[2025-01-11 02:16:35,354][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:13,  1.93it/s, est. speed input: 1069.37 toks/s, output: 14.67 toks/s]
[2025-01-11 02:16:35,461][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:08,  2.92it/s, est. speed input: 1463.68 toks/s, output: 21.62 toks/s]
[2025-01-11 02:16:35,922][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:06,  3.30it/s, est. speed input: 1663.57 toks/s, output: 29.94 toks/s]
[2025-01-11 02:16:36,212][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:05,  3.99it/s, est. speed input: 1899.34 toks/s, output: 40.80 toks/s]
[2025-01-11 02:16:36,370][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 19.19it/s, est. speed input: 5040.33 toks/s, output: 165.71 toks/s]
[2025-01-11 02:16:36,454][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.10it/s, est. speed input: 5270.83 toks/s, output: 180.16 toks/s]
WARNING 01-11 02:16:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:36,683][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:37,580][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.12it/s, est. speed input: 892.71 toks/s, output: 32.36 toks/s]
[2025-01-11 02:16:37,580][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.23it/s, est. speed input: 1781.40 toks/s, output: 64.70 toks/s]
WARNING 01-11 02:16:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:37,838][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:42,221][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:04<02:02,  4.38s/it, est. speed input: 202.60 toks/s, output: 6.62 toks/s]
[2025-01-11 02:16:43,286][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  6.94it/s, est. speed input: 4338.07 toks/s, output: 167.04 toks/s]
[2025-01-11 02:16:43,286][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:05<00:00,  5.32it/s, est. speed input: 4338.07 toks/s, output: 167.04 toks/s]
WARNING 01-11 02:16:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:43,649][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:45,583][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.93s/it, est. speed input: 487.67 toks/s, output: 15.00 toks/s]
[2025-01-11 02:16:45,583][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:01<00:00,  4.65it/s, est. speed input: 4458.16 toks/s, output: 134.94 toks/s]
WARNING 01-11 02:16:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:45,811][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:46,523][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:00<00:02,  1.40it/s, est. speed input: 1104.81 toks/s, output: 11.23 toks/s]
[2025-01-11 02:16:46,941][root][ERROR] - Processed prompts:  50%|#####     | 2/4 [00:01<00:01,  1.85it/s, est. speed input: 1468.23 toks/s, output: 32.73 toks/s]
[2025-01-11 02:16:46,959][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.48it/s, est. speed input: 2937.53 toks/s, output: 83.63 toks/s]
WARNING 01-11 02:16:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:47,238][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:48,150][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 873.10 toks/s, output: 31.81 toks/s]
[2025-01-11 02:16:48,150][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.19it/s, est. speed input: 1950.56 toks/s, output: 63.59 toks/s]
WARNING 01-11 02:16:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:16:48,372][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:16:49,162][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1273.40 toks/s, output: 36.74 toks/s]
[2025-01-11 02:16:49,162][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1273.40 toks/s, output: 36.74 toks/s]
[2025-01-11 02:16:51,712][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:16:51,765][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:16:53,438][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 02:16:55,116][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 02:16:56,724][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:16:57,256][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:16:57,256][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:17:16,513][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:17:16,611][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:17:19,940][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.33s/it]
[2025-01-11 02:17:21,757][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:05<00:04,  2.44s/it]
[2025-01-11 02:17:23,346][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.05s/it]
[2025-01-11 02:17:23,850][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.44s/it]
[2025-01-11 02:17:23,851][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.81s/it]
WARNING 01-11 02:17:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:17:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:17:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:17:42 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:17:42,586][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:17:43,911][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 02:17:44,297][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 02:17:45,600][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 02:17:46,931][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:17:46,931][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:17:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:18:00 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:18:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:18:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:18:23 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:18:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:23,646][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:27,068][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:46,  3.42s/it, est. speed input: 185.59 toks/s, output: 2.92 toks/s]
[2025-01-11 02:18:27,241][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:14,  1.83it/s, est. speed input: 883.23 toks/s, output: 15.58 toks/s]
[2025-01-11 02:18:27,350][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:09,  2.77it/s, est. speed input: 1200.22 toks/s, output: 23.22 toks/s]
[2025-01-11 02:18:27,484][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 10.69it/s, est. speed input: 3143.66 toks/s, output: 74.78 toks/s]
[2025-01-11 02:18:27,608][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:03<00:00, 13.88it/s, est. speed input: 3846.54 toks/s, output: 97.93 toks/s]
[2025-01-11 02:18:27,762][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.86it/s, est. speed input: 4474.09 toks/s, output: 126.82 toks/s]
[2025-01-11 02:18:27,892][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.54it/s, est. speed input: 4785.96 toks/s, output: 146.03 toks/s]
WARNING 01-11 02:18:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:28,139][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:31,012][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.87s/it, est. speed input: 232.86 toks/s, output: 2.78 toks/s]
[2025-01-11 02:18:31,129][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:02<00:12,  2.21it/s, est. speed input: 1098.57 toks/s, output: 15.05 toks/s]
[2025-01-11 02:18:31,234][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:05,  4.57it/s, est. speed input: 1905.41 toks/s, output: 29.08 toks/s]
[2025-01-11 02:18:31,384][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:03<00:00, 13.51it/s, est. speed input: 4230.36 toks/s, output: 77.04 toks/s]
[2025-01-11 02:18:31,612][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:03<00:00, 15.23it/s, est. speed input: 4889.35 toks/s, output: 102.21 toks/s]
[2025-01-11 02:18:31,737][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 18.66it/s, est. speed input: 5635.93 toks/s, output: 136.19 toks/s]
[2025-01-11 02:18:31,804][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.73it/s, est. speed input: 5710.79 toks/s, output: 142.98 toks/s]
WARNING 01-11 02:18:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:32,049][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:35,448][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:45,  3.40s/it, est. speed input: 215.12 toks/s, output: 3.53 toks/s]
[2025-01-11 02:18:35,911][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.30it/s, est. speed input: 757.74 toks/s, output: 14.76 toks/s]
[2025-01-11 02:18:36,192][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.58it/s, est. speed input: 882.04 toks/s, output: 19.79 toks/s]
[2025-01-11 02:18:36,353][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:09,  2.58it/s, est. speed input: 1192.18 toks/s, output: 31.60 toks/s]
[2025-01-11 02:18:36,500][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 17.61it/s, est. speed input: 4631.96 toks/s, output: 168.74 toks/s]
[2025-01-11 02:18:37,008][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.45it/s, est. speed input: 4749.34 toks/s, output: 190.40 toks/s]
WARNING 01-11 02:18:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:37,224][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:38,225][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.00s/it, est. speed input: 782.50 toks/s, output: 16.99 toks/s]
[2025-01-11 02:18:38,485][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.77it/s, est. speed input: 1269.36 toks/s, output: 36.49 toks/s]
[2025-01-11 02:18:38,586][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  5.42it/s, est. speed input: 2964.80 toks/s, output: 102.11 toks/s]
[2025-01-11 02:18:38,586][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.67it/s, est. speed input: 2964.80 toks/s, output: 102.11 toks/s]
WARNING 01-11 02:18:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:38,833][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:42,933][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:04<01:46,  4.10s/it, est. speed input: 214.13 toks/s, output: 6.83 toks/s]
[2025-01-11 02:18:43,007][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  6.47it/s, est. speed input: 5409.07 toks/s, output: 187.86 toks/s]
WARNING 01-11 02:18:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:43,341][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:44,964][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.62s/it, est. speed input: 595.94 toks/s, output: 17.26 toks/s]
[2025-01-11 02:18:44,989][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.25it/s, est. speed input: 4090.27 toks/s, output: 122.59 toks/s]
WARNING 01-11 02:18:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:45,215][root][ERROR] - Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:46,339][root][ERROR] - Processed prompts:  17%|#6        | 1/6 [00:01<00:05,  1.12s/it, est. speed input: 607.74 toks/s, output: 15.13 toks/s]
[2025-01-11 02:18:46,613][root][ERROR] - Processed prompts:  33%|###3      | 2/6 [00:01<00:02,  1.60it/s, est. speed input: 1108.93 toks/s, output: 32.91 toks/s]
[2025-01-11 02:18:46,697][root][ERROR] - Processed prompts: 100%|##########| 6/6 [00:01<00:00,  4.05it/s, est. speed input: 3352.50 toks/s, output: 112.67 toks/s]
WARNING 01-11 02:18:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:46,998][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:47,607][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.64it/s, est. speed input: 1417.10 toks/s, output: 19.73 toks/s]
[2025-01-11 02:18:47,893][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.39it/s, est. speed input: 2093.11 toks/s, output: 45.84 toks/s]
[2025-01-11 02:18:47,893][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.24it/s, est. speed input: 2093.11 toks/s, output: 45.84 toks/s]
WARNING 01-11 02:18:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:48,108][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:48,871][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 953.79 toks/s, output: 39.36 toks/s]
[2025-01-11 02:18:48,871][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 953.79 toks/s, output: 39.36 toks/s]
WARNING 01-11 02:18:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:18:49,088][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:18:49,872][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1383.43 toks/s, output: 37.01 toks/s]
[2025-01-11 02:18:49,872][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1383.43 toks/s, output: 37.01 toks/s]
[2025-01-11 02:18:52,387][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:18:52,440][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:18:54,049][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 02:18:55,705][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 02:18:57,360][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:18:57,899][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:18:57,900][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:19:16,193][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:19:16,246][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:19:18,187][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 02:19:19,829][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 02:19:21,388][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 02:19:21,887][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:19:21,888][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 02:19:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:19:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:19:40 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:19:40 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:19:40,905][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:19:42,245][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 02:19:42,618][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:19:43,915][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 02:19:45,250][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:19:45,251][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:19:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:19:59 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:19:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:19:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:20:21 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:20:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:22,268][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:26,003][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.74s/it, est. speed input: 170.01 toks/s, output: 2.68 toks/s]
[2025-01-11 02:20:26,176][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.69it/s, est. speed input: 812.40 toks/s, output: 14.33 toks/s]
[2025-01-11 02:20:26,339][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:09,  2.50it/s, est. speed input: 1091.89 toks/s, output: 21.62 toks/s]
[2025-01-11 02:20:26,516][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.66it/s, est. speed input: 2391.43 toks/s, output: 57.90 toks/s]
[2025-01-11 02:20:26,655][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.69it/s, est. speed input: 3039.47 toks/s, output: 80.92 toks/s]
[2025-01-11 02:20:26,797][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.00it/s, est. speed input: 3644.90 toks/s, output: 107.96 toks/s]
[2025-01-11 02:20:27,044][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 14.56it/s, est. speed input: 3988.71 toks/s, output: 130.23 toks/s]
[2025-01-11 02:20:27,114][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.60it/s, est. speed input: 4193.22 toks/s, output: 146.31 toks/s]
WARNING 01-11 02:20:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:27,366][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:30,201][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.83s/it, est. speed input: 229.35 toks/s, output: 2.47 toks/s]
[2025-01-11 02:20:30,390][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:38,  1.28s/it, est. speed input: 431.29 toks/s, output: 5.62 toks/s]
[2025-01-11 02:20:30,508][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:08,  3.17it/s, est. speed input: 1245.13 toks/s, output: 19.42 toks/s]
[2025-01-11 02:20:30,651][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:01,  9.01it/s, est. speed input: 2786.23 toks/s, output: 49.93 toks/s]
[2025-01-11 02:20:30,772][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 12.83it/s, est. speed input: 3657.72 toks/s, output: 72.53 toks/s]
[2025-01-11 02:20:30,897][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:03<00:00, 19.24it/s, est. speed input: 4831.79 toks/s, output: 109.62 toks/s]
[2025-01-11 02:20:31,194][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 18.44it/s, est. speed input: 5316.08 toks/s, output: 140.83 toks/s]
[2025-01-11 02:20:31,405][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.92it/s, est. speed input: 5205.36 toks/s, output: 145.11 toks/s]
WARNING 01-11 02:20:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:31,881][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:35,179][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:38,  3.30s/it, est. speed input: 219.26 toks/s, output: 2.73 toks/s]
[2025-01-11 02:20:35,299][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:03<00:41,  1.43s/it, est. speed input: 434.48 toks/s, output: 5.85 toks/s]
[2025-01-11 02:20:35,417][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:03<00:15,  1.75it/s, est. speed input: 832.28 toks/s, output: 12.44 toks/s]
[2025-01-11 02:20:35,695][root][ERROR] - Processed prompts:  19%|#9        | 6/31 [00:03<00:09,  2.73it/s, est. speed input: 1156.86 toks/s, output: 20.19 toks/s]
[2025-01-11 02:20:35,800][root][ERROR] - Processed prompts:  26%|##5       | 8/31 [00:03<00:05,  4.17it/s, est. speed input: 1500.09 toks/s, output: 29.34 toks/s]
[2025-01-11 02:20:36,255][root][ERROR] - Processed prompts:  32%|###2      | 10/31 [00:04<00:04,  4.25it/s, est. speed input: 1691.67 toks/s, output: 39.55 toks/s]
[2025-01-11 02:20:36,515][root][ERROR] - Processed prompts:  94%|#########3| 29/31 [00:04<00:00, 19.65it/s, est. speed input: 4639.77 toks/s, output: 159.90 toks/s]
[2025-01-11 02:20:36,648][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.50it/s, est. speed input: 4818.10 toks/s, output: 174.97 toks/s]
WARNING 01-11 02:20:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:37,120][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:38,396][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 545.83 toks/s, output: 33.72 toks/s]
[2025-01-11 02:20:38,396][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 545.83 toks/s, output: 33.72 toks/s]
WARNING 01-11 02:20:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:38,826][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:39,880][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.05s/it, est. speed input: 760.88 toks/s, output: 27.55 toks/s]
[2025-01-11 02:20:39,880][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.90it/s, est. speed input: 1626.03 toks/s, output: 55.05 toks/s]
WARNING 01-11 02:20:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:40,356][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:44,622][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:03,  4.27s/it, est. speed input: 174.63 toks/s, output: 5.39 toks/s]
[2025-01-11 02:20:44,912][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:04<00:53,  1.93s/it, est. speed input: 358.39 toks/s, output: 11.19 toks/s]
[2025-01-11 02:20:45,175][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 11.42it/s, est. speed input: 5156.71 toks/s, output: 182.00 toks/s]
[2025-01-11 02:20:45,175][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.23it/s, est. speed input: 5156.71 toks/s, output: 182.00 toks/s]
WARNING 01-11 02:20:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:45,516][root][ERROR] - Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:47,458][root][ERROR] - Processed prompts:  11%|#1        | 1/9 [00:01<00:15,  1.94s/it, est. speed input: 496.92 toks/s, output: 14.93 toks/s]
[2025-01-11 02:20:47,459][root][ERROR] - Processed prompts: 100%|##########| 9/9 [00:01<00:00,  4.63it/s, est. speed input: 4466.00 toks/s, output: 134.37 toks/s]
WARNING 01-11 02:20:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:47,677][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:48,733][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 821.79 toks/s, output: 27.49 toks/s]
[2025-01-11 02:20:48,784][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.71it/s, est. speed input: 2374.38 toks/s, output: 81.38 toks/s]
WARNING 01-11 02:20:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:20:49,059][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:20:49,860][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1253.06 toks/s, output: 36.19 toks/s]
[2025-01-11 02:20:49,861][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1253.06 toks/s, output: 36.19 toks/s]
[2025-01-11 02:20:52,348][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:20:52,400][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:20:54,047][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 02:20:55,733][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 02:20:57,333][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:20:57,893][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:20:57,894][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:21:16,143][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:21:16,195][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:21:18,150][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 02:21:19,857][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 02:21:21,529][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.75s/it]
[2025-01-11 02:21:22,038][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 02:21:22,038][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 02:21:40 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:21:40 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:21:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:21:41 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:21:41,252][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:21:42,583][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:21:42,963][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:21:44,291][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:21:45,622][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:21:45,623][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:21:45 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:21:59 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:22:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:22:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:22:27 model_runner.py:1335] Graph capturing finished in 27 secs.
WARNING 01-11 02:22:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:28,081][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:31,927][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.85s/it, est. speed input: 165.11 toks/s, output: 2.60 toks/s]
[2025-01-11 02:22:32,044][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.67it/s, est. speed input: 801.13 toks/s, output: 13.88 toks/s]
[2025-01-11 02:22:32,313][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.81it/s, est. speed input: 1200.49 toks/s, output: 24.58 toks/s]
[2025-01-11 02:22:32,440][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.33it/s, est. speed input: 2331.11 toks/s, output: 56.44 toks/s]
[2025-01-11 02:22:32,548][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.59it/s, est. speed input: 2985.36 toks/s, output: 79.70 toks/s]
[2025-01-11 02:22:32,666][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 13.28it/s, est. speed input: 3462.54 toks/s, output: 100.11 toks/s]
[2025-01-11 02:22:32,786][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.16it/s, est. speed input: 3914.05 toks/s, output: 123.28 toks/s]
[2025-01-11 02:22:32,964][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.55it/s, est. speed input: 4161.71 toks/s, output: 141.73 toks/s]
WARNING 01-11 02:22:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:33,233][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:36,053][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.82s/it, est. speed input: 234.77 toks/s, output: 2.48 toks/s]
[2025-01-11 02:22:36,173][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:02<00:22,  1.29it/s, est. speed input: 670.75 toks/s, output: 8.16 toks/s]
[2025-01-11 02:22:36,283][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:06,  3.74it/s, est. speed input: 1502.43 toks/s, output: 20.99 toks/s]
[2025-01-11 02:22:36,426][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:03<00:02,  8.13it/s, est. speed input: 2662.73 toks/s, output: 43.54 toks/s]
[2025-01-11 02:22:36,564][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:03<00:00, 14.93it/s, est. speed input: 4131.75 toks/s, output: 78.06 toks/s]
[2025-01-11 02:22:36,677][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:03<00:00, 18.97it/s, est. speed input: 4945.42 toks/s, output: 105.12 toks/s]
[2025-01-11 02:22:36,896][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 20.03it/s, est. speed input: 5551.25 toks/s, output: 138.13 toks/s]
[2025-01-11 02:22:37,130][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.21it/s, est. speed input: 5389.80 toks/s, output: 141.66 toks/s]
WARNING 01-11 02:22:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:37,378][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:40,623][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:40,  3.25s/it, est. speed input: 235.44 toks/s, output: 2.77 toks/s]
[2025-01-11 02:22:40,986][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:46,  1.55s/it, est. speed input: 412.73 toks/s, output: 6.65 toks/s]
[2025-01-11 02:22:41,221][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:27,  1.05it/s, est. speed input: 580.51 toks/s, output: 11.19 toks/s]
[2025-01-11 02:22:41,395][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:18,  1.56it/s, est. speed input: 740.94 toks/s, output: 16.18 toks/s]
[2025-01-11 02:22:41,606][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:05,  4.27it/s, est. speed input: 1402.12 toks/s, output: 37.84 toks/s]
[2025-01-11 02:22:41,749][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:02,  7.38it/s, est. speed input: 2033.04 toks/s, output: 61.77 toks/s]
[2025-01-11 02:22:41,912][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 20.12it/s, est. speed input: 4075.55 toks/s, output: 144.23 toks/s]
[2025-01-11 02:22:42,142][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 19.41it/s, est. speed input: 4494.52 toks/s, output: 170.86 toks/s]
[2025-01-11 02:22:42,302][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.50it/s, est. speed input: 4803.25 toks/s, output: 195.59 toks/s]
WARNING 01-11 02:22:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:42,516][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:43,634][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:04,  1.12s/it, est. speed input: 718.18 toks/s, output: 19.68 toks/s]
[2025-01-11 02:22:43,743][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.91it/s, est. speed input: 1327.97 toks/s, output: 39.94 toks/s]
[2025-01-11 02:22:44,119][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  4.19it/s, est. speed input: 2554.21 toks/s, output: 97.33 toks/s]
[2025-01-11 02:22:44,119][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.12it/s, est. speed input: 2554.21 toks/s, output: 97.33 toks/s]
WARNING 01-11 02:22:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:44,366][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:47,622][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:03<01:24,  3.26s/it, est. speed input: 220.85 toks/s, output: 4.61 toks/s]
[2025-01-11 02:22:48,314][root][ERROR] - Processed prompts:   7%|7         | 2/27 [00:03<00:43,  1.75s/it, est. speed input: 361.98 toks/s, output: 10.89 toks/s]
[2025-01-11 02:22:48,420][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00, 11.98it/s, est. speed input: 5257.51 toks/s, output: 190.66 toks/s]
[2025-01-11 02:22:48,421][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  6.66it/s, est. speed input: 5257.51 toks/s, output: 190.66 toks/s]
WARNING 01-11 02:22:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:48,731][root][ERROR] - Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:51,506][root][ERROR] - Processed prompts:   7%|6         | 1/15 [00:02<00:38,  2.77s/it, est. speed input: 357.86 toks/s, output: 10.45 toks/s]
[2025-01-11 02:22:51,506][root][ERROR] - Processed prompts: 100%|##########| 15/15 [00:02<00:00,  5.40it/s, est. speed input: 5205.21 toks/s, output: 156.73 toks/s]
WARNING 01-11 02:22:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:51,763][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:53,073][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.31s/it, est. speed input: 683.95 toks/s, output: 22.14 toks/s]
[2025-01-11 02:22:53,074][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.81it/s, est. speed input: 3158.05 toks/s, output: 110.63 toks/s]
WARNING 01-11 02:22:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:53,403][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:54,370][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.03it/s, est. speed input: 1069.92 toks/s, output: 29.98 toks/s]
[2025-01-11 02:22:54,371][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.07it/s, est. speed input: 2227.91 toks/s, output: 59.93 toks/s]
WARNING 01-11 02:22:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:22:54,584][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:22:55,385][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1177.96 toks/s, output: 36.19 toks/s]
[2025-01-11 02:22:55,386][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1177.96 toks/s, output: 36.19 toks/s]
[2025-01-11 02:22:57,995][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:22:58,047][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:23:00,186][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.14s/it]
[2025-01-11 02:23:02,167][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.05s/it]
[2025-01-11 02:23:03,991][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.94s/it]
[2025-01-11 02:23:04,538][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.39s/it]
[2025-01-11 02:23:04,539][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.62s/it]
[2025-01-11 02:23:23,787][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:23:23,838][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:23:25,720][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.88s/it]
[2025-01-11 02:23:27,328][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 02:23:28,914][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:23:29,415][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:23:29,415][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 02:23:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:23:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:23:47 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:23:47 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:23:47,582][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:23:48,918][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 02:23:49,301][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 02:23:50,616][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:23:51,990][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 02:23:51,990][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:23:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:24:06 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:24:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:24:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:24:28 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:24:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:29,141][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:33,010][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.87s/it, est. speed input: 164.11 toks/s, output: 2.58 toks/s]
[2025-01-11 02:24:33,184][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.63it/s, est. speed input: 785.36 toks/s, output: 13.85 toks/s]
[2025-01-11 02:24:33,345][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.43it/s, est. speed input: 1057.19 toks/s, output: 20.69 toks/s]
[2025-01-11 02:24:33,512][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  8.69it/s, est. speed input: 2614.84 toks/s, output: 63.83 toks/s]
[2025-01-11 02:24:33,707][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.36it/s, est. speed input: 3059.43 toks/s, output: 81.03 toks/s]
[2025-01-11 02:24:33,846][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 18.11it/s, est. speed input: 4318.83 toks/s, output: 138.15 toks/s]
[2025-01-11 02:24:33,846][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.80it/s, est. speed input: 4318.83 toks/s, output: 138.15 toks/s]
WARNING 01-11 02:24:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:34,103][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:36,913][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.81s/it, est. speed input: 233.44 toks/s, output: 2.49 toks/s]
[2025-01-11 02:24:37,031][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:02<00:22,  1.29it/s, est. speed input: 672.76 toks/s, output: 7.85 toks/s]
[2025-01-11 02:24:37,138][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.40it/s, est. speed input: 1730.08 toks/s, output: 23.73 toks/s]
[2025-01-11 02:24:37,267][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01, 12.10it/s, est. speed input: 3715.34 toks/s, output: 62.26 toks/s]
[2025-01-11 02:24:37,367][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00, 16.05it/s, est. speed input: 4607.62 toks/s, output: 84.86 toks/s]
[2025-01-11 02:24:37,481][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 23.91it/s, est. speed input: 6008.58 toks/s, output: 127.87 toks/s]
[2025-01-11 02:24:37,648][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  9.03it/s, est. speed input: 5913.57 toks/s, output: 130.89 toks/s]
WARNING 01-11 02:24:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:37,904][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:41,127][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:39,  3.22s/it, est. speed input: 225.84 toks/s, output: 2.79 toks/s]
[2025-01-11 02:24:41,469][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:14,  1.81it/s, est. speed input: 1021.16 toks/s, output: 15.14 toks/s]
[2025-01-11 02:24:42,212][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:12,  2.07it/s, est. speed input: 1189.90 toks/s, output: 22.75 toks/s]
[2025-01-11 02:24:42,381][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.08it/s, est. speed input: 4433.20 toks/s, output: 152.99 toks/s]
[2025-01-11 02:24:42,812][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 11.97it/s, est. speed input: 4791.96 toks/s, output: 186.84 toks/s]
[2025-01-11 02:24:42,812][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.52it/s, est. speed input: 4791.96 toks/s, output: 186.84 toks/s]
WARNING 01-11 02:24:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:43,053][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:44,017][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:00<00:03,  1.04it/s, est. speed input: 849.23 toks/s, output: 15.55 toks/s]
[2025-01-11 02:24:44,320][root][ERROR] - Processed prompts:  40%|####      | 2/5 [00:01<00:01,  1.74it/s, est. speed input: 1297.24 toks/s, output: 34.72 toks/s]
[2025-01-11 02:24:44,321][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.94it/s, est. speed input: 3205.25 toks/s, output: 103.34 toks/s]
WARNING 01-11 02:24:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:44,578][root][ERROR] - Processed prompts:   0%|          | 0/27 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:48,726][root][ERROR] - Processed prompts:   4%|3         | 1/27 [00:04<01:47,  4.15s/it, est. speed input: 210.48 toks/s, output: 6.99 toks/s]
[2025-01-11 02:24:48,764][root][ERROR] - Processed prompts: 100%|##########| 27/27 [00:04<00:00,  6.45it/s, est. speed input: 5375.46 toks/s, output: 188.02 toks/s]
WARNING 01-11 02:24:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:49,103][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:50,772][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:10,  1.67s/it, est. speed input: 564.89 toks/s, output: 17.37 toks/s]
[2025-01-11 02:24:50,773][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.19it/s, est. speed input: 4000.46 toks/s, output: 121.57 toks/s]
WARNING 01-11 02:24:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:50,986][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:52,282][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.30s/it, est. speed input: 686.92 toks/s, output: 22.38 toks/s]
[2025-01-11 02:24:52,282][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.86it/s, est. speed input: 3249.92 toks/s, output: 111.88 toks/s]
WARNING 01-11 02:24:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:24:52,561][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:24:53,359][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1309.51 toks/s, output: 36.37 toks/s]
[2025-01-11 02:24:53,359][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1309.51 toks/s, output: 36.37 toks/s]
[2025-01-11 02:24:55,933][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:24:55,986][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:24:57,632][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 02:24:59,299][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 02:25:00,910][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:25:01,476][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:25:01,477][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:25:19,529][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:25:19,581][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:25:21,441][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.86s/it]
[2025-01-11 02:25:23,053][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-01-11 02:25:24,600][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 02:25:25,099][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:25:25,099][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 02:25:43 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:25:43 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:25:43 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:25:43 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:25:44,044][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:25:45,419][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.37s/it]
[2025-01-11 02:25:45,811][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 02:25:47,116][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:25:48,453][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:25:48,453][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:25:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:26:02 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:26:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:26:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:26:24 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 02:26:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:24,916][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:28,592][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:53,  3.68s/it, est. speed input: 172.76 toks/s, output: 2.72 toks/s]
[2025-01-11 02:26:28,765][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:15,  1.71it/s, est. speed input: 824.95 toks/s, output: 14.55 toks/s]
[2025-01-11 02:26:28,873][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:09,  2.60it/s, est. speed input: 1123.21 toks/s, output: 21.73 toks/s]
[2025-01-11 02:26:28,974][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:04,  4.40it/s, est. speed input: 1564.63 toks/s, output: 33.26 toks/s]
[2025-01-11 02:26:29,095][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:01,  9.86it/s, est. speed input: 2583.29 toks/s, output: 62.22 toks/s]
[2025-01-11 02:26:29,195][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 14.17it/s, est. speed input: 3264.80 toks/s, output: 85.77 toks/s]
[2025-01-11 02:26:29,310][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 17.25it/s, est. speed input: 3757.14 toks/s, output: 107.41 toks/s]
[2025-01-11 02:26:29,404][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.13it/s, est. speed input: 4527.84 toks/s, output: 143.50 toks/s]
WARNING 01-11 02:26:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:29,663][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:32,593][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:30,  2.93s/it, est. speed input: 225.64 toks/s, output: 3.07 toks/s]
[2025-01-11 02:26:32,706][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:08,  3.10it/s, est. speed input: 1515.11 toks/s, output: 22.68 toks/s]
[2025-01-11 02:26:32,836][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01,  9.54it/s, est. speed input: 3718.20 toks/s, output: 64.93 toks/s]
[2025-01-11 02:26:32,972][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:03<00:00, 13.19it/s, est. speed input: 4750.92 toks/s, output: 93.40 toks/s]
[2025-01-11 02:26:33,077][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 18.77it/s, est. speed input: 5944.04 toks/s, output: 131.24 toks/s]
[2025-01-11 02:26:33,228][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.98it/s, est. speed input: 5879.58 toks/s, output: 134.68 toks/s]
WARNING 01-11 02:26:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:33,478][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:36,999][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:49,  3.52s/it, est. speed input: 206.21 toks/s, output: 3.98 toks/s]
[2025-01-11 02:26:37,181][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:46,  1.56s/it, est. speed input: 394.89 toks/s, output: 8.37 toks/s]
[2025-01-11 02:26:37,592][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:29,  1.03s/it, est. speed input: 534.26 toks/s, output: 13.37 toks/s]
[2025-01-11 02:26:37,824][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:20,  1.39it/s, est. speed input: 674.76 toks/s, output: 19.10 toks/s]
[2025-01-11 02:26:38,138][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 18.14it/s, est. speed input: 4890.08 toks/s, output: 188.85 toks/s]
[2025-01-11 02:26:38,156][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.84it/s, est. speed input: 5030.24 toks/s, output: 197.56 toks/s]
WARNING 01-11 02:26:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:38,371][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:39,166][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.26it/s, est. speed input: 1001.67 toks/s, output: 30.20 toks/s]
[2025-01-11 02:26:39,250][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.28it/s, est. speed input: 1835.38 toks/s, output: 60.31 toks/s]
WARNING 01-11 02:26:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:39,499][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:44,136][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:14,  4.64s/it, est. speed input: 191.09 toks/s, output: 6.25 toks/s]
[2025-01-11 02:26:44,292][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  8.73it/s, est. speed input: 5399.15 toks/s, output: 183.83 toks/s]
[2025-01-11 02:26:44,292][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.26it/s, est. speed input: 5399.15 toks/s, output: 183.83 toks/s]
WARNING 01-11 02:26:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:44,668][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:45,722][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.05s/it, est. speed input: 757.18 toks/s, output: 27.52 toks/s]
[2025-01-11 02:26:45,722][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.85it/s, est. speed input: 2578.07 toks/s, output: 82.52 toks/s]
WARNING 01-11 02:26:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:45,934][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:46,964][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.03s/it, est. speed input: 859.93 toks/s, output: 28.18 toks/s]
[2025-01-11 02:26:46,964][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.91it/s, est. speed input: 2440.66 toks/s, output: 84.49 toks/s]
WARNING 01-11 02:26:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:47,220][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:48,016][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1295.58 toks/s, output: 36.48 toks/s]
[2025-01-11 02:26:48,016][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1295.58 toks/s, output: 36.48 toks/s]
WARNING 01-11 02:26:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:26:48,227][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:26:49,012][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1202.31 toks/s, output: 36.93 toks/s]
[2025-01-11 02:26:49,013][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1202.31 toks/s, output: 36.93 toks/s]
[2025-01-11 02:26:51,594][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:26:51,647][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:26:53,280][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 02:26:54,964][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 02:26:56,602][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 02:26:57,144][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:26:57,144][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:27:14,210][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:27:14,263][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:27:16,225][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.96s/it]
[2025-01-11 02:27:17,927][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.81s/it]
[2025-01-11 02:27:19,578][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.74s/it]
[2025-01-11 02:27:20,206][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.30s/it]
[2025-01-11 02:27:20,206][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.49s/it]
WARNING 01-11 02:27:39 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:27:39 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:27:39 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:27:39 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:27:40,057][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:27:41,391][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
[2025-01-11 02:27:41,805][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 02:27:43,143][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 02:27:44,476][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 02:27:44,477][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 02:27:44 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:27:58 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:27:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:27:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:28:21 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:28:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:28:21,366][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:28:25,078][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.71s/it, est. speed input: 171.07 toks/s, output: 2.69 toks/s]
[2025-01-11 02:28:25,363][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:16,  1.63it/s, est. speed input: 794.39 toks/s, output: 14.51 toks/s]
[2025-01-11 02:28:25,465][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.91it/s, est. speed input: 1549.03 toks/s, output: 33.66 toks/s]
[2025-01-11 02:28:25,589][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.34it/s, est. speed input: 2405.90 toks/s, output: 57.78 toks/s]
[2025-01-11 02:28:25,697][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  9.93it/s, est. speed input: 2931.94 toks/s, output: 76.41 toks/s]
[2025-01-11 02:28:25,808][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 13.94it/s, est. speed input: 3573.84 toks/s, output: 101.76 toks/s]
[2025-01-11 02:28:25,940][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 19.03it/s, est. speed input: 4303.05 toks/s, output: 138.59 toks/s]
[2025-01-11 02:28:25,958][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.97it/s, est. speed input: 4425.21 toks/s, output: 145.47 toks/s]
WARNING 01-11 02:28:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:28:26,232][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:28:28,982][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:25,  2.75s/it, est. speed input: 241.86 toks/s, output: 2.18 toks/s]
[2025-01-11 02:28:29,101][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:02<00:21,  1.32it/s, est. speed input: 693.12 toks/s, output: 7.32 toks/s]
[2025-01-11 02:28:29,208][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:02<00:04,  5.13it/s, est. speed input: 1994.96 toks/s, output: 25.20 toks/s]
[2025-01-11 02:28:29,332][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:00, 13.76it/s, est. speed input: 4223.05 toks/s, output: 65.15 toks/s]
[2025-01-11 02:28:29,473][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:03<00:00, 17.89it/s, est. speed input: 5247.38 toks/s, output: 90.70 toks/s]
[2025-01-11 02:28:29,839][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 16.44it/s, est. speed input: 5631.31 toks/s, output: 120.61 toks/s]
[2025-01-11 02:28:29,873][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.79it/s, est. speed input: 5762.47 toks/s, output: 129.65 toks/s]
WARNING 01-11 02:28:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:28:30,138][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:28:34,611][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:18,  4.47s/it, est. speed input: 165.23 toks/s, output: 6.48 toks/s]
[2025-01-11 02:28:35,083][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  8.80it/s, est. speed input: 4794.90 toks/s, output: 193.74 toks/s]
[2025-01-11 02:28:35,083][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.47it/s, est. speed input: 4794.90 toks/s, output: 193.74 toks/s]
WARNING 01-11 02:28:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:28:35,386][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:28:40,196][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:24,  4.81s/it, est. speed input: 183.59 toks/s, output: 6.03 toks/s]
[2025-01-11 02:28:40,231][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.40it/s, est. speed input: 5600.59 toks/s, output: 185.96 toks/s]
WARNING 01-11 02:28:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:28:40,625][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:28:41,411][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1278.32 toks/s, output: 36.89 toks/s]
[2025-01-11 02:28:41,412][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1278.32 toks/s, output: 36.89 toks/s]
[2025-01-11 02:28:43,997][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:28:44,053][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:28:45,660][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 02:28:47,266][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.61s/it]
[2025-01-11 02:28:48,902][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 02:28:49,458][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:28:49,458][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 02:29:06,247][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:29:06,299][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:29:08,204][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 02:29:09,809][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 02:29:11,448][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 02:29:11,954][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:29:11,954][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 02:29:30 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:29:30 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:29:31 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:29:31 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:29:31,502][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:29:32,831][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:29:33,212][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:29:34,539][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:29:35,872][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:29:35,872][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:29:36 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:29:50 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:29:50 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:29:50 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:30:12 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:30:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:13,061][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:16,719][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:53,  3.66s/it, est. speed input: 173.62 toks/s, output: 2.73 toks/s]
[2025-01-11 02:30:16,992][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:10,  2.36it/s, est. speed input: 1130.97 toks/s, output: 20.35 toks/s]
[2025-01-11 02:30:17,132][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.97it/s, est. speed input: 2339.72 toks/s, output: 52.32 toks/s]
[2025-01-11 02:30:17,243][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  8.06it/s, est. speed input: 2885.02 toks/s, output: 69.58 toks/s]
[2025-01-11 02:30:17,360][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.16it/s, est. speed input: 3692.73 toks/s, output: 99.33 toks/s]
[2025-01-11 02:30:17,563][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 14.53it/s, est. speed input: 4231.42 toks/s, output: 128.16 toks/s]
[2025-01-11 02:30:17,782][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.78it/s, est. speed input: 4304.26 toks/s, output: 139.59 toks/s]
WARNING 01-11 02:30:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:18,023][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:18,657][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 1151.61 toks/s, output: 31.55 toks/s]
[2025-01-11 02:30:18,657][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 1151.61 toks/s, output: 31.55 toks/s]
WARNING 01-11 02:30:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:18,907][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:21,648][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:22,  2.74s/it, est. speed input: 239.71 toks/s, output: 2.55 toks/s]
[2025-01-11 02:30:21,764][root][ERROR] - Processed prompts:  13%|#2        | 4/31 [00:02<00:14,  1.82it/s, est. speed input: 916.89 toks/s, output: 11.20 toks/s]
[2025-01-11 02:30:21,866][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:02<00:04,  4.97it/s, est. speed input: 1995.85 toks/s, output: 27.72 toks/s]
[2025-01-11 02:30:21,992][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:03<00:01, 11.99it/s, est. speed input: 3815.85 toks/s, output: 63.54 toks/s]
[2025-01-11 02:30:22,158][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:03<00:00, 15.98it/s, est. speed input: 4822.31 toks/s, output: 89.21 toks/s]
[2025-01-11 02:30:22,422][root][ERROR] - Processed prompts:  94%|#########3| 29/31 [00:03<00:00, 16.80it/s, est. speed input: 5394.27 toks/s, output: 122.92 toks/s]
[2025-01-11 02:30:22,524][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  8.57it/s, est. speed input: 5610.51 toks/s, output: 138.52 toks/s]
WARNING 01-11 02:30:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:22,778][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:26,760][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:03<01:55,  3.98s/it, est. speed input: 181.83 toks/s, output: 6.28 toks/s]
[2025-01-11 02:30:26,934][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:04<00:48,  1.74s/it, est. speed input: 351.33 toks/s, output: 12.75 toks/s]
[2025-01-11 02:30:26,991][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  7.12it/s, est. speed input: 5282.00 toks/s, output: 205.33 toks/s]
WARNING 01-11 02:30:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:27,208][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:27,749][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.85it/s, est. speed input: 1211.80 toks/s, output: 18.50 toks/s]
[2025-01-11 02:30:28,049][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.50it/s, est. speed input: 1603.10 toks/s, output: 45.19 toks/s]
[2025-01-11 02:30:28,049][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.38it/s, est. speed input: 1603.10 toks/s, output: 45.19 toks/s]
WARNING 01-11 02:30:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:28,276][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:29,013][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:00<00:01,  1.36it/s, est. speed input: 1057.27 toks/s, output: 20.36 toks/s]
[2025-01-11 02:30:29,271][root][ERROR] - Processed prompts:  67%|######6   | 2/3 [00:00<00:00,  2.20it/s, est. speed input: 1611.65 toks/s, output: 44.24 toks/s]
[2025-01-11 02:30:29,271][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:00<00:00,  3.02it/s, est. speed input: 2359.07 toks/s, output: 73.37 toks/s]
WARNING 01-11 02:30:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:29,522][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:33,935][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:04<01:59,  4.41s/it, est. speed input: 199.87 toks/s, output: 6.57 toks/s]
[2025-01-11 02:30:33,936][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  6.34it/s, est. speed input: 5588.36 toks/s, output: 183.97 toks/s]
WARNING 01-11 02:30:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:34,308][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:35,336][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.03s/it, est. speed input: 671.02 toks/s, output: 28.20 toks/s]
[2025-01-11 02:30:35,337][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.92it/s, est. speed input: 2426.32 toks/s, output: 84.57 toks/s]
WARNING 01-11 02:30:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:30:35,576][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:30:36,354][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1290.81 toks/s, output: 37.28 toks/s]
[2025-01-11 02:30:36,354][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1290.81 toks/s, output: 37.28 toks/s]
[2025-01-11 02:30:38,953][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:30:39,006][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:30:40,672][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.67s/it]
[2025-01-11 02:30:42,308][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:30:43,893][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 02:30:44,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:30:44,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:31:01,617][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:31:01,669][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:31:03,522][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.85s/it]
[2025-01-11 02:31:05,162][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 02:31:06,752][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:31:07,263][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:31:07,263][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 02:31:26 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:31:26 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:31:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:31:26 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:31:26,757][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:31:28,084][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:31:28,463][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:31:29,771][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:31:31,104][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:31:31,105][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:31:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:31:45 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:31:46 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:31:46 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:32:08 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 02:32:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:09,273][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:13,090][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.38 toks/s, output: 2.62 toks/s]
[2025-01-11 02:32:13,421][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.90it/s, est. speed input: 918.43 toks/s, output: 16.87 toks/s]
[2025-01-11 02:32:13,598][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.14it/s, est. speed input: 2349.44 toks/s, output: 56.19 toks/s]
[2025-01-11 02:32:13,705][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  8.69it/s, est. speed input: 3008.85 toks/s, output: 79.42 toks/s]
[2025-01-11 02:32:13,815][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.80it/s, est. speed input: 3634.96 toks/s, output: 104.80 toks/s]
[2025-01-11 02:32:14,204][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 12.11it/s, est. speed input: 3991.91 toks/s, output: 131.00 toks/s]
[2025-01-11 02:32:14,222][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.47it/s, est. speed input: 4106.31 toks/s, output: 140.24 toks/s]
WARNING 01-11 02:32:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:14,449][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:15,085][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 1146.73 toks/s, output: 31.42 toks/s]
[2025-01-11 02:32:15,086][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 1146.73 toks/s, output: 31.42 toks/s]
WARNING 01-11 02:32:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:15,323][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:18,057][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:22,  2.73s/it, est. speed input: 239.20 toks/s, output: 2.56 toks/s]
[2025-01-11 02:32:18,175][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:02<00:21,  1.33it/s, est. speed input: 684.61 toks/s, output: 8.42 toks/s]
[2025-01-11 02:32:18,278][root][ERROR] - Processed prompts:  29%|##9       | 9/31 [00:02<00:04,  5.17it/s, est. speed input: 1992.59 toks/s, output: 28.43 toks/s]
[2025-01-11 02:32:18,401][root][ERROR] - Processed prompts:  58%|#####8    | 18/31 [00:03<00:01, 12.20it/s, est. speed input: 3817.46 toks/s, output: 63.68 toks/s]
[2025-01-11 02:32:18,518][root][ERROR] - Processed prompts:  77%|#######7  | 24/31 [00:03<00:00, 17.01it/s, est. speed input: 4921.61 toks/s, output: 90.78 toks/s]
[2025-01-11 02:32:18,748][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:03<00:00, 19.33it/s, est. speed input: 5737.56 toks/s, output: 121.45 toks/s]
[2025-01-11 02:32:18,782][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  8.96it/s, est. speed input: 5875.15 toks/s, output: 129.51 toks/s]
WARNING 01-11 02:32:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:19,036][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:22,833][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:53,  3.80s/it, est. speed input: 196.23 toks/s, output: 5.27 toks/s]
[2025-01-11 02:32:23,304][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:53,  1.84s/it, est. speed input: 345.40 toks/s, output: 11.25 toks/s]
[2025-01-11 02:32:23,363][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.17it/s, est. speed input: 5269.34 toks/s, output: 205.49 toks/s]
WARNING 01-11 02:32:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:23,582][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:24,094][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.95it/s, est. speed input: 1278.85 toks/s, output: 25.38 toks/s]
[2025-01-11 02:32:24,094][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.95it/s, est. speed input: 1278.85 toks/s, output: 25.38 toks/s]
WARNING 01-11 02:32:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:24,311][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:25,106][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1042.64 toks/s, output: 37.78 toks/s]
[2025-01-11 02:32:25,106][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1042.64 toks/s, output: 37.78 toks/s]
WARNING 01-11 02:32:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:25,362][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:30,105][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:22,  4.74s/it, est. speed input: 187.47 toks/s, output: 5.90 toks/s]
[2025-01-11 02:32:30,199][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.41it/s, est. speed input: 5604.81 toks/s, output: 186.09 toks/s]
WARNING 01-11 02:32:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:30,582][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:31,517][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 1043.44 toks/s, output: 31.04 toks/s]
[2025-01-11 02:32:31,517][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.14it/s, est. speed input: 1993.04 toks/s, output: 62.05 toks/s]
WARNING 01-11 02:32:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:31,747][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:32,578][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1157.25 toks/s, output: 38.49 toks/s]
[2025-01-11 02:32:32,579][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1157.25 toks/s, output: 38.49 toks/s]
WARNING 01-11 02:32:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:32:32,803][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:32:33,614][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1083.91 toks/s, output: 38.27 toks/s]
[2025-01-11 02:32:33,614][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 1083.91 toks/s, output: 38.27 toks/s]
[2025-01-11 02:32:36,121][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:32:36,173][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:32:37,868][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 02:32:39,546][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 02:32:41,145][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 02:32:41,702][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:32:41,702][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 02:32:58,541][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:32:58,593][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:33:00,471][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.88s/it]
[2025-01-11 02:33:02,091][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 02:33:03,656][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 02:33:04,172][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:33:04,172][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 02:33:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:33:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:33:23 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:33:23 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:33:24,069][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:33:25,501][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.43s/it]
[2025-01-11 02:33:26,022][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
[2025-01-11 02:33:27,591][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.20s/it]
[2025-01-11 02:33:28,959][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.27s/it]
[2025-01-11 02:33:28,959][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.22s/it]
INFO 01-11 02:33:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:33:42 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:33:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:33:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:34:04 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 02:34:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:05,250][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:09,072][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.15 toks/s, output: 2.62 toks/s]
[2025-01-11 02:34:09,397][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:11,  2.23it/s, est. speed input: 1071.74 toks/s, output: 19.53 toks/s]
[2025-01-11 02:34:09,534][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.67it/s, est. speed input: 2223.37 toks/s, output: 51.12 toks/s]
[2025-01-11 02:34:09,648][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  8.26it/s, est. speed input: 2887.76 toks/s, output: 73.67 toks/s]
[2025-01-11 02:34:09,793][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.89it/s, est. speed input: 3633.72 toks/s, output: 103.88 toks/s]
[2025-01-11 02:34:09,925][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 14.20it/s, est. speed input: 4074.91 toks/s, output: 127.06 toks/s]
[2025-01-11 02:34:09,981][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.76it/s, est. speed input: 4295.19 toks/s, output: 140.78 toks/s]
WARNING 01-11 02:34:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:10,196][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:10,855][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 1081.68 toks/s, output: 33.42 toks/s]
[2025-01-11 02:34:10,855][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 1081.68 toks/s, output: 33.42 toks/s]
WARNING 01-11 02:34:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:11,101][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:13,834][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:21,  2.73s/it, est. speed input: 240.40 toks/s, output: 2.56 toks/s]
[2025-01-11 02:34:13,951][root][ERROR] - Processed prompts:  10%|9         | 3/31 [00:02<00:21,  1.33it/s, est. speed input: 698.92 toks/s, output: 8.42 toks/s]
[2025-01-11 02:34:14,089][root][ERROR] - Processed prompts:  48%|####8     | 15/31 [00:02<00:01,  8.91it/s, est. speed input: 3303.85 toks/s, output: 48.87 toks/s]
[2025-01-11 02:34:14,197][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:03<00:00, 16.45it/s, est. speed input: 5292.16 toks/s, output: 88.19 toks/s]
[2025-01-11 02:34:14,523][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  9.06it/s, est. speed input: 5935.37 toks/s, output: 120.39 toks/s]
WARNING 01-11 02:34:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:14,775][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:18,978][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:06,  4.20s/it, est. speed input: 173.43 toks/s, output: 6.42 toks/s]
[2025-01-11 02:34:19,096][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:52,  1.80s/it, est. speed input: 341.09 toks/s, output: 12.96 toks/s]
[2025-01-11 02:34:19,097][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.17it/s, est. speed input: 5263.78 toks/s, output: 207.53 toks/s]
WARNING 01-11 02:34:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:19,309][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:20,090][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 841.41 toks/s, output: 37.14 toks/s]
[2025-01-11 02:34:20,090][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 841.41 toks/s, output: 37.14 toks/s]
WARNING 01-11 02:34:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:20,301][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:21,074][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1072.53 toks/s, output: 37.52 toks/s]
[2025-01-11 02:34:21,075][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1072.53 toks/s, output: 37.52 toks/s]
WARNING 01-11 02:34:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:21,332][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:26,071][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:22,  4.74s/it, est. speed input: 184.86 toks/s, output: 5.91 toks/s]
[2025-01-11 02:34:26,166][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.41it/s, est. speed input: 5634.69 toks/s, output: 186.21 toks/s]
WARNING 01-11 02:34:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:34:26,564][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:34:27,493][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 965.13 toks/s, output: 31.24 toks/s]
[2025-01-11 02:34:27,510][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.11it/s, est. speed input: 1950.79 toks/s, output: 62.38 toks/s]
[2025-01-11 02:34:30,076][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:34:30,129][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:34:31,820][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 02:34:33,518][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 02:34:35,133][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:34:35,672][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:34:35,672][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 02:34:53,559][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:34:53,665][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:34:56,881][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.22s/it]
[2025-01-11 02:35:00,315][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:06,  3.34s/it]
[2025-01-11 02:35:03,466][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:09<00:03,  3.26s/it]
[2025-01-11 02:35:03,997][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.18s/it]
[2025-01-11 02:35:03,997][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.58s/it]
WARNING 01-11 02:35:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:35:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:35:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:35:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:35:24,407][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:35:25,739][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:35:26,123][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 02:35:27,427][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:35:28,763][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:35:28,763][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:35:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:35:43 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:35:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:35:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:36:07 model_runner.py:1335] Graph capturing finished in 24 secs.
WARNING 01-11 02:36:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:08,159][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:12,039][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.88s/it, est. speed input: 163.65 toks/s, output: 2.58 toks/s]
[2025-01-11 02:36:12,371][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.87it/s, est. speed input: 904.56 toks/s, output: 16.62 toks/s]
[2025-01-11 02:36:12,510][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.72it/s, est. speed input: 2189.09 toks/s, output: 51.94 toks/s]
[2025-01-11 02:36:12,621][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  8.87it/s, est. speed input: 2988.23 toks/s, output: 78.66 toks/s]
[2025-01-11 02:36:12,780][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 14.49it/s, est. speed input: 4122.11 toks/s, output: 122.91 toks/s]
[2025-01-11 02:36:12,960][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.66it/s, est. speed input: 4232.24 toks/s, output: 133.92 toks/s]
WARNING 01-11 02:36:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:13,210][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:16,017][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.81s/it, est. speed input: 232.62 toks/s, output: 2.49 toks/s]
[2025-01-11 02:36:16,138][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:02<00:36,  1.23s/it, est. speed input: 447.70 toks/s, output: 5.46 toks/s]
[2025-01-11 02:36:16,251][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:06,  3.94it/s, est. speed input: 1507.62 toks/s, output: 22.04 toks/s]
[2025-01-11 02:36:16,377][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:00, 13.30it/s, est. speed input: 3913.69 toks/s, output: 67.88 toks/s]
[2025-01-11 02:36:16,560][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:03<00:00, 16.73it/s, est. speed input: 4871.69 toks/s, output: 95.52 toks/s]
[2025-01-11 02:36:16,824][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:03<00:00, 17.32it/s, est. speed input: 5432.30 toks/s, output: 128.12 toks/s]
[2025-01-11 02:36:16,943][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.57it/s, est. speed input: 5613.37 toks/s, output: 142.79 toks/s]
WARNING 01-11 02:36:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:17,196][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:21,243][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:01,  4.05s/it, est. speed input: 180.89 toks/s, output: 5.68 toks/s]
[2025-01-11 02:36:21,597][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:54,  1.87s/it, est. speed input: 375.39 toks/s, output: 11.82 toks/s]
[2025-01-11 02:36:21,598][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.04it/s, est. speed input: 5334.17 toks/s, output: 202.89 toks/s]
WARNING 01-11 02:36:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:21,831][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:22,347][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.94it/s, est. speed input: 1392.71 toks/s, output: 27.16 toks/s]
[2025-01-11 02:36:22,347][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.94it/s, est. speed input: 1392.71 toks/s, output: 27.16 toks/s]
WARNING 01-11 02:36:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:22,587][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:23,473][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.13it/s, est. speed input: 886.55 toks/s, output: 32.75 toks/s]
[2025-01-11 02:36:23,473][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.26it/s, est. speed input: 1720.39 toks/s, output: 65.47 toks/s]
WARNING 01-11 02:36:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:23,731][root][ERROR] - Processed prompts:   0%|          | 0/26 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:27,854][root][ERROR] - Processed prompts:   4%|3         | 1/26 [00:04<01:43,  4.12s/it, est. speed input: 213.45 toks/s, output: 7.03 toks/s]
[2025-01-11 02:36:27,872][root][ERROR] - Processed prompts: 100%|##########| 26/26 [00:04<00:00,  6.28it/s, est. speed input: 5512.07 toks/s, output: 182.34 toks/s]
WARNING 01-11 02:36:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:36:28,223][root][ERROR] - Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:36:29,437][root][ERROR] - Processed prompts:  25%|##5       | 1/4 [00:01<00:03,  1.21s/it, est. speed input: 722.50 toks/s, output: 23.89 toks/s]
[2025-01-11 02:36:29,437][root][ERROR] - Processed prompts: 100%|##########| 4/4 [00:01<00:00,  3.29it/s, est. speed input: 3064.48 toks/s, output: 95.53 toks/s]
[2025-01-11 02:36:32,280][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:36:32,333][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:36:34,018][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.68s/it]
[2025-01-11 02:36:35,690][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 02:36:37,335][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:36:37,900][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 02:36:37,900][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 02:36:55,264][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:36:55,316][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:36:57,797][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.48s/it]
[2025-01-11 02:36:59,514][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.03s/it]
[2025-01-11 02:37:01,047][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.80s/it]
[2025-01-11 02:37:01,549][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.29s/it]
[2025-01-11 02:37:01,550][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.56s/it]
WARNING 01-11 02:37:20 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:37:20 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:37:20 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:37:20 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:37:21,102][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:37:22,449][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 02:37:22,846][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 02:37:24,150][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:37:25,477][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:37:25,477][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:37:25 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:37:39 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:37:40 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:37:40 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:38:01 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:38:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:02,037][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:05,758][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.72s/it, est. speed input: 170.67 toks/s, output: 2.69 toks/s]
[2025-01-11 02:38:06,098][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.60it/s, est. speed input: 781.72 toks/s, output: 14.53 toks/s]
[2025-01-11 02:38:06,203][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.37it/s, est. speed input: 1371.90 toks/s, output: 30.49 toks/s]
[2025-01-11 02:38:06,339][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  5.51it/s, est. speed input: 1918.66 toks/s, output: 47.41 toks/s]
[2025-01-11 02:38:06,451][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.33it/s, est. speed input: 3021.29 toks/s, output: 86.78 toks/s]
[2025-01-11 02:38:06,594][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.61it/s, est. speed input: 3623.10 toks/s, output: 113.67 toks/s]
[2025-01-11 02:38:06,748][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.74it/s, est. speed input: 4178.07 toks/s, output: 143.27 toks/s]
[2025-01-11 02:38:06,899][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.58it/s, est. speed input: 4179.46 toks/s, output: 148.30 toks/s]
WARNING 01-11 02:38:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:07,159][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:10,003][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:28,  2.84s/it, est. speed input: 231.02 toks/s, output: 2.46 toks/s]
[2025-01-11 02:38:10,120][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:02<00:15,  1.76it/s, est. speed input: 891.47 toks/s, output: 10.47 toks/s]
[2025-01-11 02:38:10,225][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:04,  4.80it/s, est. speed input: 1935.39 toks/s, output: 26.41 toks/s]
[2025-01-11 02:38:10,361][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  8.30it/s, est. speed input: 2871.75 toks/s, output: 44.65 toks/s]
[2025-01-11 02:38:10,478][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01, 12.39it/s, est. speed input: 3762.98 toks/s, output: 66.89 toks/s]
[2025-01-11 02:38:10,649][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:03<00:00, 20.38it/s, est. speed input: 5274.95 toks/s, output: 111.76 toks/s]
[2025-01-11 02:38:10,782][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.83it/s, est. speed input: 5804.45 toks/s, output: 138.82 toks/s]
WARNING 01-11 02:38:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:11,031][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:15,165][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<01:59,  4.13s/it, est. speed input: 175.88 toks/s, output: 6.53 toks/s]
[2025-01-11 02:38:15,281][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:04<00:49,  1.77s/it, est. speed input: 346.38 toks/s, output: 13.18 toks/s]
[2025-01-11 02:38:15,282][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  7.06it/s, est. speed input: 5248.02 toks/s, output: 204.22 toks/s]
WARNING 01-11 02:38:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:15,501][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:16,106][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.65it/s, est. speed input: 1165.02 toks/s, output: 19.86 toks/s]
[2025-01-11 02:38:16,106][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  3.31it/s, est. speed input: 2336.96 toks/s, output: 39.69 toks/s]
WARNING 01-11 02:38:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:16,326][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:17,214][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.13it/s, est. speed input: 825.71 toks/s, output: 32.67 toks/s]
[2025-01-11 02:38:17,214][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.25it/s, est. speed input: 1646.28 toks/s, output: 65.31 toks/s]
WARNING 01-11 02:38:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:17,471][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:22,080][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:04<02:09,  4.61s/it, est. speed input: 197.66 toks/s, output: 6.29 toks/s]
[2025-01-11 02:38:22,082][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.29it/s, est. speed input: 5513.30 toks/s, output: 182.37 toks/s]
WARNING 01-11 02:38:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:22,746][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:23,780][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 932.18 toks/s, output: 28.04 toks/s]
[2025-01-11 02:38:23,781][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 932.18 toks/s, output: 28.04 toks/s]
WARNING 01-11 02:38:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:38:24,185][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:38:25,231][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.05s/it, est. speed input: 885.54 toks/s, output: 27.73 toks/s]
[2025-01-11 02:38:25,232][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.91it/s, est. speed input: 1774.74 toks/s, output: 55.43 toks/s]
[2025-01-11 02:38:29,665][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:38:29,719][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:38:31,390][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 02:38:33,058][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 02:38:34,683][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 02:38:35,219][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:38:35,219][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 02:38:52,398][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:38:52,451][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:38:54,363][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 02:38:55,996][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 02:38:57,618][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 02:38:58,139][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 02:38:58,140][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 02:39:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:39:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:39:18 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:39:18 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:39:18,308][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:39:19,659][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 02:39:20,043][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 02:39:21,354][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:39:22,687][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:39:22,687][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:39:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:39:36 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:39:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:39:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:40:04 model_runner.py:1335] Graph capturing finished in 28 secs.
WARNING 01-11 02:40:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:40:05,139][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:40:08,925][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.73 toks/s, output: 2.64 toks/s]
[2025-01-11 02:40:09,265][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.57it/s, est. speed input: 769.40 toks/s, output: 14.30 toks/s]
[2025-01-11 02:40:09,367][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.23it/s, est. speed input: 1651.95 toks/s, output: 38.08 toks/s]
[2025-01-11 02:40:09,497][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.30it/s, est. speed input: 2185.36 toks/s, output: 54.83 toks/s]
[2025-01-11 02:40:09,697][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.51it/s, est. speed input: 3065.05 toks/s, output: 87.76 toks/s]
[2025-01-11 02:40:09,807][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 13.16it/s, est. speed input: 3536.31 toks/s, output: 110.74 toks/s]
[2025-01-11 02:40:09,912][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.43it/s, est. speed input: 4124.35 toks/s, output: 143.31 toks/s]
[2025-01-11 02:40:10,012][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.57it/s, est. speed input: 4169.88 toks/s, output: 148.98 toks/s]
WARNING 01-11 02:40:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:40:10,262][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:40:13,097][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.83s/it, est. speed input: 227.54 toks/s, output: 2.47 toks/s]
[2025-01-11 02:40:13,216][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:02<00:15,  1.76it/s, est. speed input: 882.87 toks/s, output: 10.83 toks/s]
[2025-01-11 02:40:13,321][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:03<00:04,  5.44it/s, est. speed input: 2149.16 toks/s, output: 30.40 toks/s]
[2025-01-11 02:40:13,452][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:03<00:01,  9.72it/s, est. speed input: 3286.69 toks/s, output: 52.66 toks/s]
[2025-01-11 02:40:13,593][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:00, 12.33it/s, est. speed input: 3935.79 toks/s, output: 69.63 toks/s]
[2025-01-11 02:40:13,774][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:03<00:00, 14.34it/s, est. speed input: 4483.15 toks/s, output: 90.25 toks/s]
[2025-01-11 02:40:13,942][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:03<00:00, 16.35it/s, est. speed input: 4986.32 toks/s, output: 116.02 toks/s]
[2025-01-11 02:40:14,053][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:03<00:00, 18.14it/s, est. speed input: 5369.72 toks/s, output: 138.73 toks/s]
[2025-01-11 02:40:14,170][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.19it/s, est. speed input: 5380.75 toks/s, output: 145.83 toks/s]
WARNING 01-11 02:40:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:40:14,456][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:40:18,180][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.72s/it, est. speed input: 199.24 toks/s, output: 4.30 toks/s]
[2025-01-11 02:40:18,966][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:59,  2.00s/it, est. speed input: 335.45 toks/s, output: 9.98 toks/s]
[2025-01-11 02:40:19,004][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.04it/s, est. speed input: 5318.87 toks/s, output: 202.05 toks/s]
WARNING 01-11 02:40:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:40:19,297][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:40:23,858][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:04<02:07,  4.56s/it, est. speed input: 194.27 toks/s, output: 6.36 toks/s]
[2025-01-11 02:40:23,895][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.31it/s, est. speed input: 5530.58 toks/s, output: 183.58 toks/s]
WARNING 01-11 02:40:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:40:24,265][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:40:25,065][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1211.04 toks/s, output: 36.28 toks/s]
[2025-01-11 02:40:25,065][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1211.04 toks/s, output: 36.28 toks/s]
[2025-01-11 02:40:27,742][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:40:27,796][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:40:29,417][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 02:40:31,096][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 02:40:32,716][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:40:33,243][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:40:33,244][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:40:50,826][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:40:50,877][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:40:52,697][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.82s/it]
[2025-01-11 02:40:54,314][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 02:40:55,874][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:40:56,377][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:40:56,378][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 02:41:15 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:41:15 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:41:15 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:41:15 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:41:15,856][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:41:17,185][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:41:17,730][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
[2025-01-11 02:41:19,754][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.40s/it]
[2025-01-11 02:41:21,149][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 02:41:21,149][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.32s/it]
INFO 01-11 02:41:21 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:41:35 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:41:35 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:41:35 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:41:57 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:41:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:41:57,832][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:01,625][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.45 toks/s, output: 2.64 toks/s]
[2025-01-11 02:42:01,966][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.57it/s, est. speed input: 768.17 toks/s, output: 14.27 toks/s]
[2025-01-11 02:42:02,186][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.73it/s, est. speed input: 2187.64 toks/s, output: 53.28 toks/s]
[2025-01-11 02:42:02,301][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  7.08it/s, est. speed input: 2557.85 toks/s, output: 67.13 toks/s]
[2025-01-11 02:42:02,402][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  8.81it/s, est. speed input: 2917.92 toks/s, output: 82.49 toks/s]
[2025-01-11 02:42:02,515][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.72it/s, est. speed input: 3390.12 toks/s, output: 105.07 toks/s]
[2025-01-11 02:42:02,630][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.28it/s, est. speed input: 4103.54 toks/s, output: 145.30 toks/s]
[2025-01-11 02:42:02,963][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.24it/s, est. speed input: 3960.51 toks/s, output: 146.76 toks/s]
WARNING 01-11 02:42:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:03,184][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:03,954][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 941.61 toks/s, output: 37.66 toks/s]
[2025-01-11 02:42:03,955][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 941.61 toks/s, output: 37.66 toks/s]
WARNING 01-11 02:42:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:04,200][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:06,995][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:02<01:23,  2.79s/it, est. speed input: 234.35 toks/s, output: 2.86 toks/s]
[2025-01-11 02:42:07,109][root][ERROR] - Processed prompts:  16%|#6        | 5/31 [00:02<00:11,  2.27it/s, est. speed input: 1123.52 toks/s, output: 15.47 toks/s]
[2025-01-11 02:42:07,248][root][ERROR] - Processed prompts:  42%|####1     | 13/31 [00:03<00:02,  7.11it/s, est. speed input: 2781.57 toks/s, output: 43.97 toks/s]
[2025-01-11 02:42:07,355][root][ERROR] - Processed prompts:  65%|######4   | 20/31 [00:03<00:00, 12.23it/s, est. speed input: 4148.43 toks/s, output: 74.17 toks/s]
[2025-01-11 02:42:07,497][root][ERROR] - Processed prompts:  81%|########  | 25/31 [00:03<00:00, 15.51it/s, est. speed input: 4974.00 toks/s, output: 99.80 toks/s]
[2025-01-11 02:42:07,620][root][ERROR] - Processed prompts:  97%|#########6| 30/31 [00:03<00:00, 19.28it/s, est. speed input: 5763.85 toks/s, output: 131.01 toks/s]
[2025-01-11 02:42:07,671][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:03<00:00,  8.93it/s, est. speed input: 5870.01 toks/s, output: 137.75 toks/s]
WARNING 01-11 02:42:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:07,953][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:12,164][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:02,  4.21s/it, est. speed input: 176.69 toks/s, output: 6.89 toks/s]
[2025-01-11 02:42:12,305][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  9.62it/s, est. speed input: 5078.97 toks/s, output: 202.66 toks/s]
[2025-01-11 02:42:12,305][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.89it/s, est. speed input: 5078.97 toks/s, output: 202.66 toks/s]
WARNING 01-11 02:42:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:12,534][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:13,082][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.82it/s, est. speed input: 1323.81 toks/s, output: 16.41 toks/s]
[2025-01-11 02:42:13,417][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.37it/s, est. speed input: 1760.88 toks/s, output: 43.03 toks/s]
[2025-01-11 02:42:13,417][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.26it/s, est. speed input: 1760.88 toks/s, output: 43.03 toks/s]
WARNING 01-11 02:42:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:13,638][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:14,390][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 989.16 toks/s, output: 38.55 toks/s]
[2025-01-11 02:42:14,390][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 989.16 toks/s, output: 38.55 toks/s]
WARNING 01-11 02:42:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:14,652][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:19,325][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:15,  4.67s/it, est. speed input: 189.38 toks/s, output: 6.21 toks/s]
[2025-01-11 02:42:19,326][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.42it/s, est. speed input: 5659.06 toks/s, output: 186.13 toks/s]
WARNING 01-11 02:42:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:42:19,715][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:42:20,503][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1199.85 toks/s, output: 36.82 toks/s]
[2025-01-11 02:42:20,503][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1199.85 toks/s, output: 36.82 toks/s]
[2025-01-11 02:42:23,201][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:42:23,255][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:42:25,035][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.78s/it]
[2025-01-11 02:42:26,731][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 02:42:28,415][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 02:42:29,242][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:42:29,242][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.50s/it]
[2025-01-11 02:42:49,010][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:42:49,062][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:42:50,899][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.84s/it]
[2025-01-11 02:42:52,483][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.69s/it]
[2025-01-11 02:42:54,044][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 02:42:54,557][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:42:54,557][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
WARNING 01-11 02:43:13 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:43:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:43:14 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:43:14 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:43:14,494][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:43:15,842][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 02:43:16,248][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 02:43:17,578][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 02:43:18,935][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 02:43:18,935][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 02:43:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:43:33 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:43:33 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:43:33 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:43:55 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:43:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:43:55,810][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:43:59,560][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.75s/it, est. speed input: 169.33 toks/s, output: 2.67 toks/s]
[2025-01-11 02:43:59,833][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.31it/s, est. speed input: 1104.98 toks/s, output: 19.89 toks/s]
[2025-01-11 02:43:59,980][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:05,  3.99it/s, est. speed input: 1675.20 toks/s, output: 35.01 toks/s]
[2025-01-11 02:44:00,109][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.71it/s, est. speed input: 2363.48 toks/s, output: 56.76 toks/s]
[2025-01-11 02:44:00,247][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 14.49it/s, est. speed input: 3863.89 toks/s, output: 110.65 toks/s]
[2025-01-11 02:44:00,430][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 16.64it/s, est. speed input: 4397.94 toks/s, output: 140.68 toks/s]
[2025-01-11 02:44:00,430][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.93it/s, est. speed input: 4397.94 toks/s, output: 140.68 toks/s]
WARNING 01-11 02:44:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:44:00,709][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:44:03,591][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.88s/it, est. speed input: 223.86 toks/s, output: 2.78 toks/s]
[2025-01-11 02:44:03,702][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:02<00:07,  3.15it/s, est. speed input: 1530.33 toks/s, output: 20.38 toks/s]
[2025-01-11 02:44:03,827][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:01, 10.95it/s, est. speed input: 4200.56 toks/s, output: 67.03 toks/s]
[2025-01-11 02:44:03,968][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:03<00:00, 15.25it/s, est. speed input: 5428.79 toks/s, output: 97.59 toks/s]
[2025-01-11 02:44:04,549][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.33it/s, est. speed input: 5457.37 toks/s, output: 117.47 toks/s]
WARNING 01-11 02:44:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:44:04,812][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:44:09,276][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:18,  4.46s/it, est. speed input: 165.77 toks/s, output: 6.50 toks/s]
[2025-01-11 02:44:09,311][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.11it/s, est. speed input: 5256.28 toks/s, output: 206.72 toks/s]
WARNING 01-11 02:44:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:44:09,598][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:44:14,418][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:24,  4.82s/it, est. speed input: 182.39 toks/s, output: 6.02 toks/s]
[2025-01-11 02:44:14,453][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.39it/s, est. speed input: 5602.43 toks/s, output: 185.62 toks/s]
[2025-01-11 02:44:17,257][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:44:17,309][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:44:18,974][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.67s/it]
[2025-01-11 02:44:20,706][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 02:44:22,312][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:44:22,870][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:44:22,870][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 02:44:41,136][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:44:41,188][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:44:43,038][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.85s/it]
[2025-01-11 02:44:46,156][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:05,  2.60s/it]
[2025-01-11 02:44:49,290][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:08<00:02,  2.84s/it]
[2025-01-11 02:44:50,347][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:09<00:00,  2.14s/it]
[2025-01-11 02:44:50,348][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:09<00:00,  2.29s/it]
WARNING 01-11 02:45:10 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:45:10 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:45:11 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:45:11 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:45:11,730][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:45:13,065][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.33s/it]
[2025-01-11 02:45:13,487][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.25it/s]
[2025-01-11 02:45:14,800][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:45:16,127][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:45:16,127][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:45:16 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:45:30 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:45:30 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:45:30 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:45:52 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 02:45:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:45:52,334][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:45:56,100][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.77s/it, est. speed input: 168.62 toks/s, output: 2.66 toks/s]
[2025-01-11 02:45:56,442][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.58it/s, est. speed input: 773.07 toks/s, output: 14.37 toks/s]
[2025-01-11 02:45:56,581][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.53it/s, est. speed input: 2093.66 toks/s, output: 49.93 toks/s]
[2025-01-11 02:45:56,701][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  7.52it/s, est. speed input: 2617.70 toks/s, output: 67.79 toks/s]
[2025-01-11 02:45:56,831][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:01,  9.84it/s, est. speed input: 3107.16 toks/s, output: 86.96 toks/s]
[2025-01-11 02:45:56,949][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 16.46it/s, est. speed input: 4127.89 toks/s, output: 133.48 toks/s]
[2025-01-11 02:45:57,033][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.81it/s, est. speed input: 4324.47 toks/s, output: 145.35 toks/s]
WARNING 01-11 02:45:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:45:57,306][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:00,130][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.82s/it, est. speed input: 231.62 toks/s, output: 2.48 toks/s]
[2025-01-11 02:46:00,244][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:02<00:12,  2.25it/s, est. speed input: 1113.61 toks/s, output: 12.93 toks/s]
[2025-01-11 02:46:00,379][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:03<00:01,  9.57it/s, est. speed input: 3621.97 toks/s, output: 52.72 toks/s]
[2025-01-11 02:46:00,513][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00, 13.33it/s, est. speed input: 4701.63 toks/s, output: 77.64 toks/s]
[2025-01-11 02:46:00,701][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:03<00:00, 17.54it/s, est. speed input: 5792.65 toks/s, output: 110.74 toks/s]
[2025-01-11 02:46:00,846][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  9.04it/s, est. speed input: 5928.14 toks/s, output: 124.29 toks/s]
WARNING 01-11 02:46:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:46:01,115][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:05,502][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:11,  4.39s/it, est. speed input: 171.66 toks/s, output: 6.61 toks/s]
[2025-01-11 02:46:05,537][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.01it/s, est. speed input: 5230.82 toks/s, output: 203.79 toks/s]
WARNING 01-11 02:46:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:46:05,748][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:06,188][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.27it/s, est. speed input: 1577.31 toks/s, output: 20.45 toks/s]
[2025-01-11 02:46:06,189][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.27it/s, est. speed input: 1577.31 toks/s, output: 20.45 toks/s]
WARNING 01-11 02:46:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:46:06,412][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:07,169][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 959.04 toks/s, output: 38.31 toks/s]
[2025-01-11 02:46:07,169][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 959.04 toks/s, output: 38.31 toks/s]
WARNING 01-11 02:46:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:46:07,421][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:11,847][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:04<02:03,  4.43s/it, est. speed input: 196.38 toks/s, output: 6.10 toks/s]
[2025-01-11 02:46:11,961][root][ERROR] - Processed prompts:   7%|6         | 2/29 [00:04<00:51,  1.89s/it, est. speed input: 383.79 toks/s, output: 12.34 toks/s]
[2025-01-11 02:46:11,961][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.39it/s, est. speed input: 5611.22 toks/s, output: 184.82 toks/s]
WARNING 01-11 02:46:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:46:12,319][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:46:13,206][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.13it/s, est. speed input: 1061.18 toks/s, output: 30.45 toks/s]
[2025-01-11 02:46:13,240][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.17it/s, est. speed input: 2013.31 toks/s, output: 60.81 toks/s]
[2025-01-11 02:46:15,870][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:46:15,924][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:46:17,557][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 02:46:19,187][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 02:46:20,818][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 02:46:21,351][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:46:21,351][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:46:38,246][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:46:38,298][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:46:40,192][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 02:46:41,852][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 02:46:43,419][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 02:46:43,927][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:46:43,927][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 02:47:02 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:47:02 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:47:03 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:47:03 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:47:03,768][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:47:05,079][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 02:47:05,456][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 02:47:06,763][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 02:47:08,100][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:47:08,100][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 02:47:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:47:22 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:47:22 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:47:22 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:47:48 model_runner.py:1335] Graph capturing finished in 25 secs.
WARNING 01-11 02:47:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:47:48,349][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:47:52,165][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.41 toks/s, output: 2.62 toks/s]
[2025-01-11 02:47:52,496][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.90it/s, est. speed input: 918.71 toks/s, output: 16.88 toks/s]
[2025-01-11 02:47:52,599][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:06,  3.62it/s, est. speed input: 1494.23 toks/s, output: 32.71 toks/s]
[2025-01-11 02:47:52,728][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.34it/s, est. speed input: 2175.37 toks/s, output: 53.44 toks/s]
[2025-01-11 02:47:52,851][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 14.24it/s, est. speed input: 3667.77 toks/s, output: 107.08 toks/s]
[2025-01-11 02:47:53,047][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 17.16it/s, est. speed input: 4325.99 toks/s, output: 140.30 toks/s]
[2025-01-11 02:47:53,047][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.81it/s, est. speed input: 4325.99 toks/s, output: 140.30 toks/s]
WARNING 01-11 02:47:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:47:53,323][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:47:56,136][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.81s/it, est. speed input: 234.31 toks/s, output: 2.49 toks/s]
[2025-01-11 02:47:56,257][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:02<00:36,  1.23s/it, est. speed input: 444.49 toks/s, output: 5.45 toks/s]
[2025-01-11 02:47:56,362][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:04,  5.22it/s, est. speed input: 1943.19 toks/s, output: 27.64 toks/s]
[2025-01-11 02:47:56,482][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:03<00:00, 13.74it/s, est. speed input: 4138.73 toks/s, output: 69.33 toks/s]
[2025-01-11 02:47:56,593][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:03<00:00, 19.42it/s, est. speed input: 5403.07 toks/s, output: 101.22 toks/s]
[2025-01-11 02:47:56,942][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.84it/s, est. speed input: 5792.35 toks/s, output: 132.34 toks/s]
WARNING 01-11 02:47:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:47:57,197][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:48:01,558][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:10,  4.36s/it, est. speed input: 174.30 toks/s, output: 6.65 toks/s]
[2025-01-11 02:48:01,559][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.11it/s, est. speed input: 5261.83 toks/s, output: 206.13 toks/s]
WARNING 01-11 02:48:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:48:01,771][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:48:02,226][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 1590.08 toks/s, output: 21.99 toks/s]
[2025-01-11 02:48:02,226][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 1590.08 toks/s, output: 21.99 toks/s]
WARNING 01-11 02:48:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:48:02,453][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:48:03,402][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 775.09 toks/s, output: 30.58 toks/s]
[2025-01-11 02:48:03,403][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 775.09 toks/s, output: 30.58 toks/s]
WARNING 01-11 02:48:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:48:03,894][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:48:09,965][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:06<02:56,  6.07s/it, est. speed input: 145.14 toks/s, output: 4.78 toks/s]
[2025-01-11 02:48:10,003][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:06<00:00,  4.91it/s, est. speed input: 4316.13 toks/s, output: 142.74 toks/s]
WARNING 01-11 02:48:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:48:10,727][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:48:11,743][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 928.64 toks/s, output: 28.56 toks/s]
[2025-01-11 02:48:11,744][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 928.64 toks/s, output: 28.56 toks/s]
[2025-01-11 02:48:15,722][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:48:15,775][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:48:17,414][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 02:48:19,077][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:48:20,696][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:48:21,236][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:48:21,237][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:48:38,072][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:48:38,124][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:48:40,069][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 02:48:41,673][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 02:48:43,271][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 02:48:43,787][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:48:43,788][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 02:49:02 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:49:02 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:49:03 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:49:03 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:49:03,614][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:49:04,934][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 02:49:05,315][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:49:06,615][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 02:49:07,945][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 02:49:07,945][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 02:49:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:49:21 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:49:22 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:49:22 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:49:44 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:49:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:49:44,949][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:49:48,319][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:44,  3.37s/it, est. speed input: 188.44 toks/s, output: 2.97 toks/s]
[2025-01-11 02:49:48,651][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:12,  2.12it/s, est. speed input: 1029.26 toks/s, output: 18.91 toks/s]
[2025-01-11 02:49:48,859][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:03<00:01,  7.64it/s, est. speed input: 2923.20 toks/s, output: 72.38 toks/s]
[2025-01-11 02:49:48,973][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 12.16it/s, est. speed input: 4102.84 toks/s, output: 114.81 toks/s]
[2025-01-11 02:49:49,181][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.04it/s, est. speed input: 4651.56 toks/s, output: 144.61 toks/s]
[2025-01-11 02:49:49,198][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.53it/s, est. speed input: 4782.03 toks/s, output: 152.50 toks/s]
WARNING 01-11 02:49:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:49:49,449][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:49:52,269][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.82s/it, est. speed input: 237.19 toks/s, output: 2.48 toks/s]
[2025-01-11 02:49:52,388][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:02<00:15,  1.77it/s, est. speed input: 893.02 toks/s, output: 10.89 toks/s]
[2025-01-11 02:49:52,526][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  7.86it/s, est. speed input: 2987.28 toks/s, output: 42.25 toks/s]
[2025-01-11 02:49:52,682][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00, 13.93it/s, est. speed input: 4659.96 toks/s, output: 77.01 toks/s]
[2025-01-11 02:49:53,025][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:03<00:00, 14.11it/s, est. speed input: 5128.79 toks/s, output: 102.34 toks/s]
[2025-01-11 02:49:53,078][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.82it/s, est. speed input: 5774.89 toks/s, output: 136.13 toks/s]
WARNING 01-11 02:49:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:49:53,327][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:49:57,585][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:03,  4.26s/it, est. speed input: 172.39 toks/s, output: 6.81 toks/s]
[2025-01-11 02:49:57,620][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.99it/s, est. speed input: 5218.20 toks/s, output: 203.15 toks/s]
WARNING 01-11 02:49:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:49:57,839][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:49:58,468][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.59it/s, est. speed input: 1104.20 toks/s, output: 22.27 toks/s]
[2025-01-11 02:49:58,734][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.41it/s, est. speed input: 1562.88 toks/s, output: 49.15 toks/s]
[2025-01-11 02:49:58,734][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.23it/s, est. speed input: 1562.88 toks/s, output: 49.15 toks/s]
WARNING 01-11 02:49:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:49:58,971][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:49:59,881][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 804.67 toks/s, output: 31.88 toks/s]
[2025-01-11 02:49:59,881][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.20it/s, est. speed input: 1806.54 toks/s, output: 63.73 toks/s]
WARNING 01-11 02:50:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:50:00,133][root][ERROR] - Processed prompts:   0%|          | 0/28 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:50:04,554][root][ERROR] - Processed prompts:   4%|3         | 1/28 [00:04<01:59,  4.42s/it, est. speed input: 196.82 toks/s, output: 6.56 toks/s]
[2025-01-11 02:50:04,589][root][ERROR] - Processed prompts: 100%|##########| 28/28 [00:04<00:00,  6.28it/s, est. speed input: 5524.62 toks/s, output: 182.71 toks/s]
WARNING 01-11 02:50:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:50:04,987][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:50:05,784][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1153.13 toks/s, output: 36.43 toks/s]
[2025-01-11 02:50:05,784][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1153.13 toks/s, output: 36.43 toks/s]
[2025-01-11 02:50:08,478][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:50:08,531][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:50:10,305][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.77s/it]
[2025-01-11 02:50:12,424][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.98s/it]
[2025-01-11 02:50:14,252][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.91s/it]
[2025-01-11 02:50:14,900][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.41s/it]
[2025-01-11 02:50:14,901][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.59s/it]
[2025-01-11 02:50:33,239][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:50:33,349][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:50:36,601][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.25s/it]
[2025-01-11 02:50:40,029][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:06,  3.36s/it]
[2025-01-11 02:50:43,208][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:09<00:03,  3.27s/it]
[2025-01-11 02:50:44,244][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.39s/it]
[2025-01-11 02:50:44,245][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.72s/it]
WARNING 01-11 02:51:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:51:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:51:03 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:51:03 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:51:03,812][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:51:05,141][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 02:51:05,519][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 02:51:06,827][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:51:08,172][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:51:08,172][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:51:08 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:51:22 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:51:22 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:51:22 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:51:44 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:51:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:51:44,763][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:51:48,494][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.73s/it, est. speed input: 170.19 toks/s, output: 2.95 toks/s]
[2025-01-11 02:51:48,796][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.71s/it, est. speed input: 314.93 toks/s, output: 6.70 toks/s]
[2025-01-11 02:51:48,905][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  4.28it/s, est. speed input: 1533.33 toks/s, output: 39.60 toks/s]
[2025-01-11 02:51:49,037][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  6.38it/s, est. speed input: 2080.08 toks/s, output: 56.39 toks/s]
[2025-01-11 02:51:49,174][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 15.84it/s, est. speed input: 3887.27 toks/s, output: 119.94 toks/s]
[2025-01-11 02:51:49,270][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.10it/s, est. speed input: 4509.19 toks/s, output: 149.12 toks/s]
WARNING 01-11 02:51:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:51:49,520][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:51:52,477][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:31,  2.96s/it, est. speed input: 222.26 toks/s, output: 3.04 toks/s]
[2025-01-11 02:51:52,584][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:03<00:05,  4.00it/s, est. speed input: 1931.05 toks/s, output: 28.40 toks/s]
[2025-01-11 02:51:52,722][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:02,  6.67it/s, est. speed input: 2869.55 toks/s, output: 47.16 toks/s]
[2025-01-11 02:51:52,926][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:03<00:01,  9.34it/s, est. speed input: 3658.14 toks/s, output: 71.06 toks/s]
[2025-01-11 02:51:53,321][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:03<00:00,  9.57it/s, est. speed input: 3971.00 toks/s, output: 93.67 toks/s]
[2025-01-11 02:51:53,372][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.31it/s, est. speed input: 5448.29 toks/s, output: 165.67 toks/s]
WARNING 01-11 02:51:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:51:53,637][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:51:58,284][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:24,  4.65s/it, est. speed input: 158.38 toks/s, output: 6.24 toks/s]
[2025-01-11 02:51:58,285][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.88it/s, est. speed input: 5457.00 toks/s, output: 199.66 toks/s]
WARNING 01-11 02:51:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:51:58,617][root][ERROR] - Processed prompts:   0%|          | 0/22 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:52:02,231][root][ERROR] - Processed prompts:   5%|4         | 1/22 [00:03<01:15,  3.61s/it, est. speed input: 246.25 toks/s, output: 8.02 toks/s]
[2025-01-11 02:52:02,232][root][ERROR] - Processed prompts: 100%|##########| 22/22 [00:03<00:00,  6.09it/s, est. speed input: 5357.61 toks/s, output: 176.49 toks/s]
[2025-01-11 02:52:05,022][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:52:05,076][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:52:06,702][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 02:52:08,375][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:52:09,997][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 02:52:10,541][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:52:10,541][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:52:28,468][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:52:28,521][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:52:30,474][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 02:52:32,079][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 02:52:33,620][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 02:52:34,146][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 02:52:34,146][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 02:52:52 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:52:52 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:52:53 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:52:53 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:52:53,298][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:52:54,614][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 02:52:54,994][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 02:52:56,291][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 02:52:57,620][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 02:52:57,620][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 02:52:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:53:11 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:53:12 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:53:12 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:53:33 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 02:53:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:53:33,906][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:53:37,730][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.07 toks/s, output: 2.62 toks/s]
[2025-01-11 02:53:38,056][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:11,  2.23it/s, est. speed input: 1071.31 toks/s, output: 19.52 toks/s]
[2025-01-11 02:53:38,188][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:02,  6.58it/s, est. speed input: 2521.19 toks/s, output: 59.56 toks/s]
[2025-01-11 02:53:38,294][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:01,  9.15it/s, est. speed input: 3184.08 toks/s, output: 82.28 toks/s]
[2025-01-11 02:53:38,428][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.05it/s, est. speed input: 3791.62 toks/s, output: 107.04 toks/s]
[2025-01-11 02:53:38,555][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 15.40it/s, est. speed input: 4371.57 toks/s, output: 137.90 toks/s]
[2025-01-11 02:53:38,555][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.88it/s, est. speed input: 4371.57 toks/s, output: 137.90 toks/s]
WARNING 01-11 02:53:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:53:38,810][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:53:41,687][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.88s/it, est. speed input: 226.63 toks/s, output: 2.78 toks/s]
[2025-01-11 02:53:41,795][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:02<00:07,  3.16it/s, est. speed input: 1538.99 toks/s, output: 19.76 toks/s]
[2025-01-11 02:53:41,981][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:03<00:03,  5.80it/s, est. speed input: 2487.46 toks/s, output: 35.95 toks/s]
[2025-01-11 02:53:42,646][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:03<00:02,  5.88it/s, est. speed input: 2738.43 toks/s, output: 51.61 toks/s]
[2025-01-11 02:53:42,717][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:03<00:00,  8.19it/s, est. speed input: 5363.35 toks/s, output: 177.65 toks/s]
WARNING 01-11 02:53:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:53:42,982][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:53:47,707][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:26,  4.72s/it, est. speed input: 153.87 toks/s, output: 6.14 toks/s]
[2025-01-11 02:53:47,707][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.77it/s, est. speed input: 5512.01 toks/s, output: 196.38 toks/s]
WARNING 01-11 02:53:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:53:48,021][root][ERROR] - Processed prompts:   0%|          | 0/18 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:53:51,134][root][ERROR] - Processed prompts:   6%|5         | 1/18 [00:03<00:52,  3.11s/it, est. speed input: 284.32 toks/s, output: 9.32 toks/s]
[2025-01-11 02:53:51,172][root][ERROR] - Processed prompts: 100%|##########| 18/18 [00:03<00:00,  5.71it/s, est. speed input: 5031.02 toks/s, output: 166.95 toks/s]
[2025-01-11 02:53:54,012][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:53:54,065][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:53:55,786][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 02:53:57,386][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:53:59,000][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 02:53:59,527][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:53:59,528][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 02:54:17,848][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:54:17,900][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:54:19,805][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 02:54:21,418][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 02:54:22,988][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 02:54:23,489][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 02:54:23,489][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 02:54:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:54:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:54:42 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:54:42 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:54:42,759][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:54:44,081][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 02:54:44,488][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 02:54:45,783][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:54:47,137][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:54:47,137][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 02:54:47 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:55:01 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:55:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:55:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:55:29 model_runner.py:1335] Graph capturing finished in 28 secs.
WARNING 01-11 02:55:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:55:29,833][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:55:33,668][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.84s/it, est. speed input: 165.57 toks/s, output: 2.61 toks/s]
[2025-01-11 02:55:34,000][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.89it/s, est. speed input: 914.46 toks/s, output: 16.80 toks/s]
[2025-01-11 02:55:34,100][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  5.87it/s, est. speed input: 2232.55 toks/s, output: 53.21 toks/s]
[2025-01-11 02:55:34,212][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  8.49it/s, est. speed input: 2900.48 toks/s, output: 74.68 toks/s]
[2025-01-11 02:55:34,354][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 12.16it/s, est. speed input: 3651.99 toks/s, output: 103.30 toks/s]
[2025-01-11 02:55:34,524][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 14.87it/s, est. speed input: 4196.35 toks/s, output: 132.17 toks/s]
[2025-01-11 02:55:34,541][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.80it/s, est. speed input: 4315.82 toks/s, output: 139.12 toks/s]
WARNING 01-11 02:55:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:55:34,788][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:55:37,680][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.89s/it, est. speed input: 226.19 toks/s, output: 2.77 toks/s]
[2025-01-11 02:55:37,798][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:23,  1.26it/s, est. speed input: 651.65 toks/s, output: 8.64 toks/s]
[2025-01-11 02:55:37,901][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:05,  4.30it/s, est. speed input: 1687.95 toks/s, output: 25.06 toks/s]
[2025-01-11 02:55:38,243][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:03<00:03,  5.34it/s, est. speed input: 2094.37 toks/s, output: 37.05 toks/s]
[2025-01-11 02:55:38,744][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:03<00:03,  5.55it/s, est. speed input: 2330.68 toks/s, output: 54.35 toks/s]
[2025-01-11 02:55:38,804][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.97it/s, est. speed input: 5221.05 toks/s, output: 192.76 toks/s]
WARNING 01-11 02:55:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:55:39,068][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:55:43,448][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:15,  4.38s/it, est. speed input: 167.37 toks/s, output: 4.80 toks/s]
[2025-01-11 02:55:43,935][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<01:02,  2.09s/it, est. speed input: 338.24 toks/s, output: 10.27 toks/s]
[2025-01-11 02:55:43,935][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.57it/s, est. speed input: 5574.63 toks/s, output: 189.03 toks/s]
WARNING 01-11 02:55:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:55:44,272][root][ERROR] - Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:55:46,523][root][ERROR] - Processed prompts:   8%|8         | 1/12 [00:02<00:24,  2.25s/it, est. speed input: 393.58 toks/s, output: 12.88 toks/s]
[2025-01-11 02:55:46,524][root][ERROR] - Processed prompts: 100%|##########| 12/12 [00:02<00:00,  5.33it/s, est. speed input: 4620.11 toks/s, output: 154.55 toks/s]
WARNING 01-11 02:55:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:55:46,806][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:55:47,599][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1216.49 toks/s, output: 36.59 toks/s]
[2025-01-11 02:55:47,599][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1216.49 toks/s, output: 36.59 toks/s]
[2025-01-11 02:55:50,339][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:55:50,393][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:55:52,035][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 02:55:53,695][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 02:55:55,293][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 02:55:55,823][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:55:55,823][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 02:56:14,912][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:56:14,964][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:56:16,835][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.87s/it]
[2025-01-11 02:56:18,457][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 02:56:19,990][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 02:56:20,491][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 02:56:20,491][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 02:56:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:56:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:56:37 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:56:37 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:56:38,110][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:56:39,455][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 02:56:39,835][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 02:56:41,158][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 02:56:42,494][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:56:42,494][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:56:42 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:56:56 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:56:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:56:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:57:18 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:57:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:57:19,066][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:57:22,860][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.39 toks/s, output: 2.64 toks/s]
[2025-01-11 02:57:23,201][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.57it/s, est. speed input: 767.97 toks/s, output: 14.27 toks/s]
[2025-01-11 02:57:23,305][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.31it/s, est. speed input: 1348.36 toks/s, output: 29.96 toks/s]
[2025-01-11 02:57:23,417][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  9.14it/s, est. speed input: 2773.14 toks/s, output: 72.17 toks/s]
[2025-01-11 02:57:23,572][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 11.87it/s, est. speed input: 3382.44 toks/s, output: 96.10 toks/s]
[2025-01-11 02:57:23,694][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 15.28it/s, est. speed input: 3979.30 toks/s, output: 124.90 toks/s]
[2025-01-11 02:57:23,878][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4223.38 toks/s, output: 141.75 toks/s]
WARNING 01-11 02:57:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:57:24,130][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:57:27,144][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.01s/it, est. speed input: 220.93 toks/s, output: 3.32 toks/s]
[2025-01-11 02:57:28,096][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:17,  1.53it/s, est. speed input: 839.56 toks/s, output: 16.89 toks/s]
[2025-01-11 02:57:28,259][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.86it/s, est. speed input: 964.30 toks/s, output: 23.49 toks/s]
[2025-01-11 02:57:28,352][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.58it/s, est. speed input: 4969.79 toks/s, output: 215.03 toks/s]
WARNING 01-11 02:57:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:57:28,613][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:57:33,146][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:20,  4.53s/it, est. speed input: 161.04 toks/s, output: 4.63 toks/s]
[2025-01-11 02:57:33,635][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:05<01:04,  2.15s/it, est. speed input: 292.92 toks/s, output: 9.96 toks/s]
[2025-01-11 02:57:33,636][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.37it/s, est. speed input: 5662.60 toks/s, output: 183.17 toks/s]
WARNING 01-11 02:57:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:57:34,031][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:57:35,354][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.32s/it, est. speed input: 690.75 toks/s, output: 21.92 toks/s]
[2025-01-11 02:57:35,371][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.73it/s, est. speed input: 3187.92 toks/s, output: 108.90 toks/s]
WARNING 01-11 02:57:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:57:35,613][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:57:36,408][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1208.62 toks/s, output: 36.47 toks/s]
[2025-01-11 02:57:36,408][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1208.62 toks/s, output: 36.47 toks/s]
[2025-01-11 02:57:39,174][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:57:39,227][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:57:40,883][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 02:57:42,619][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 02:57:44,425][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.75s/it]
[2025-01-11 02:57:44,959][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.27s/it]
[2025-01-11 02:57:44,959][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
[2025-01-11 02:58:04,932][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:58:04,984][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:58:06,937][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 02:58:08,566][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 02:58:10,166][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 02:58:10,671][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 02:58:10,672][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 02:58:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 02:58:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 02:58:30 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 02:58:30 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 02:58:30,434][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:58:31,815][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.38s/it]
[2025-01-11 02:58:32,195][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 02:58:33,493][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 02:58:34,845][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 02:58:34,845][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 02:58:35 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 02:58:48 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 02:58:49 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 02:58:49 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 02:59:11 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 02:59:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:59:11,388][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:59:15,207][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.25 toks/s, output: 2.62 toks/s]
[2025-01-11 02:59:15,503][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.10s/it, est. speed input: 462.94 toks/s, output: 8.75 toks/s]
[2025-01-11 02:59:15,617][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.77it/s, est. speed input: 750.70 toks/s, output: 16.31 toks/s]
[2025-01-11 02:59:15,755][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  7.61it/s, est. speed input: 2181.05 toks/s, output: 57.25 toks/s]
[2025-01-11 02:59:15,866][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 10.06it/s, est. speed input: 2693.92 toks/s, output: 75.02 toks/s]
[2025-01-11 02:59:15,995][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 12.70it/s, est. speed input: 3170.10 toks/s, output: 95.07 toks/s]
[2025-01-11 02:59:16,154][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 18.37it/s, est. speed input: 3996.68 toks/s, output: 135.74 toks/s]
[2025-01-11 02:59:16,356][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.44it/s, est. speed input: 4090.14 toks/s, output: 148.15 toks/s]
WARNING 01-11 02:59:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:59:16,620][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:59:19,436][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:27,  2.82s/it, est. speed input: 232.62 toks/s, output: 2.49 toks/s]
[2025-01-11 02:59:19,615][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:02<00:23,  1.26it/s, est. speed input: 658.20 toks/s, output: 8.35 toks/s]
[2025-01-11 02:59:19,784][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:03<00:11,  2.32it/s, est. speed input: 1040.74 toks/s, output: 15.17 toks/s]
[2025-01-11 02:59:20,759][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:14,  1.73it/s, est. speed input: 953.01 toks/s, output: 19.09 toks/s]
[2025-01-11 02:59:20,841][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.58it/s, est. speed input: 4985.42 toks/s, output: 212.78 toks/s]
WARNING 01-11 02:59:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:59:21,108][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:59:25,981][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:31,  4.87s/it, est. speed input: 148.38 toks/s, output: 5.54 toks/s]
[2025-01-11 02:59:26,104][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<01:02,  2.08s/it, est. speed input: 293.07 toks/s, output: 11.21 toks/s]
[2025-01-11 02:59:26,104][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.40it/s, est. speed input: 5699.77 toks/s, output: 185.34 toks/s]
WARNING 01-11 02:59:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 02:59:26,477][root][ERROR] - Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 02:59:27,818][root][ERROR] - Processed prompts:  20%|##        | 1/5 [00:01<00:05,  1.34s/it, est. speed input: 650.16 toks/s, output: 21.62 toks/s]
[2025-01-11 02:59:27,819][root][ERROR] - Processed prompts: 100%|##########| 5/5 [00:01<00:00,  3.73it/s, est. speed input: 3272.08 toks/s, output: 108.07 toks/s]
[2025-01-11 02:59:30,647][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:59:30,700][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:59:32,334][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 02:59:33,953][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 02:59:35,530][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.60s/it]
[2025-01-11 02:59:36,057][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 02:59:36,058][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 02:59:55,747][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 02:59:55,800][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 02:59:57,750][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 02:59:59,339][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 03:00:00,945][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:00:01,479][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:00:01,479][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:00:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:00:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:00:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:00:17 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:00:18,100][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:00:19,448][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:00:19,826][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 03:00:21,114][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:00:22,436][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 03:00:22,436][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:00:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:00:36 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:00:37 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:00:37 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:00:58 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:00:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:00:59,113][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:02,767][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:53,  3.65s/it, est. speed input: 173.81 toks/s, output: 2.74 toks/s]
[2025-01-11 03:01:03,043][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:12,  2.01it/s, est. speed input: 969.61 toks/s, output: 17.31 toks/s]
[2025-01-11 03:01:03,189][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.69it/s, est. speed input: 1869.85 toks/s, output: 40.73 toks/s]
[2025-01-11 03:01:03,323][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  9.85it/s, est. speed input: 3168.15 toks/s, output: 79.35 toks/s]
[2025-01-11 03:01:03,434][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 13.77it/s, est. speed input: 3968.34 toks/s, output: 111.10 toks/s]
[2025-01-11 03:01:03,599][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 16.40it/s, est. speed input: 4530.19 toks/s, output: 142.01 toks/s]
[2025-01-11 03:01:03,599][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.13it/s, est. speed input: 4530.19 toks/s, output: 142.01 toks/s]
WARNING 01-11 03:01:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:03,878][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:06,741][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:28,  2.86s/it, est. speed input: 228.45 toks/s, output: 2.79 toks/s]
[2025-01-11 03:01:06,859][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:02<00:22,  1.27it/s, est. speed input: 660.90 toks/s, output: 8.72 toks/s]
[2025-01-11 03:01:07,813][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:03<00:10,  2.41it/s, est. speed input: 1172.58 toks/s, output: 21.60 toks/s]
[2025-01-11 03:01:07,967][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:08,  2.75it/s, est. speed input: 1289.41 toks/s, output: 28.37 toks/s]
[2025-01-11 03:01:08,005][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.75it/s, est. speed input: 5073.37 toks/s, output: 209.34 toks/s]
WARNING 01-11 03:01:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:08,285][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:12,966][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:20,  4.68s/it, est. speed input: 161.10 toks/s, output: 5.77 toks/s]
[2025-01-11 03:01:13,085][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:57,  2.00s/it, est. speed input: 311.48 toks/s, output: 11.67 toks/s]
[2025-01-11 03:01:13,086][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.46it/s, est. speed input: 5647.42 toks/s, output: 186.86 toks/s]
WARNING 01-11 03:01:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:13,293][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:14,120][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 865.40 toks/s, output: 39.94 toks/s]
[2025-01-11 03:01:14,120][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 865.40 toks/s, output: 39.94 toks/s]
WARNING 01-11 03:01:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:14,464][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:15,206][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 977.67 toks/s, output: 39.11 toks/s]
[2025-01-11 03:01:15,207][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 977.67 toks/s, output: 39.11 toks/s]
WARNING 01-11 03:01:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:15,424][root][ERROR] - Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:16,976][root][ERROR] - Processed prompts:  14%|#4        | 1/7 [00:01<00:09,  1.55s/it, est. speed input: 578.73 toks/s, output: 17.40 toks/s]
[2025-01-11 03:01:17,026][root][ERROR] - Processed prompts: 100%|##########| 7/7 [00:01<00:00,  4.37it/s, est. speed input: 3852.70 toks/s, output: 125.53 toks/s]
WARNING 01-11 03:01:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:01:17,279][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:01:18,057][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1231.47 toks/s, output: 37.28 toks/s]
[2025-01-11 03:01:18,057][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1231.47 toks/s, output: 37.28 toks/s]
[2025-01-11 03:01:20,797][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:01:20,849][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:01:22,452][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 03:01:24,098][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:01:25,749][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 03:01:26,328][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:01:26,329][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:01:45,444][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:01:45,496][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:01:47,419][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 03:01:49,057][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:01:50,657][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:01:51,170][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:01:51,171][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:02:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:02:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:02:08 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:02:08 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:02:08,359][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:02:09,682][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:02:10,061][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:02:11,360][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:02:12,706][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:02:12,707][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:02:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:02:26 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:02:27 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:02:27 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:02:49 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:02:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:02:49,577][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:02:53,311][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.73s/it, est. speed input: 170.08 toks/s, output: 2.68 toks/s]
[2025-01-11 03:02:53,673][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.75s/it, est. speed input: 310.12 toks/s, output: 6.35 toks/s]
[2025-01-11 03:02:53,784][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:10,  2.37it/s, est. speed input: 905.83 toks/s, output: 22.11 toks/s]
[2025-01-11 03:02:53,915][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  6.98it/s, est. speed input: 2049.78 toks/s, output: 55.34 toks/s]
[2025-01-11 03:02:54,030][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 11.03it/s, est. speed input: 2852.44 toks/s, output: 84.67 toks/s]
[2025-01-11 03:02:54,150][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 15.63it/s, est. speed input: 3610.94 toks/s, output: 117.23 toks/s]
[2025-01-11 03:02:54,326][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 18.16it/s, est. speed input: 4145.62 toks/s, output: 145.52 toks/s]
[2025-01-11 03:02:54,410][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.62it/s, est. speed input: 4205.21 toks/s, output: 151.69 toks/s]
WARNING 01-11 03:02:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:02:54,667][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:02:57,670][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.00s/it, est. speed input: 221.82 toks/s, output: 3.33 toks/s]
[2025-01-11 03:02:58,092][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:44,  1.48s/it, est. speed input: 384.83 toks/s, output: 7.88 toks/s]
[2025-01-11 03:02:58,900][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.24it/s, est. speed input: 621.84 toks/s, output: 17.96 toks/s]
[2025-01-11 03:02:58,942][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.48it/s, est. speed input: 4918.55 toks/s, output: 222.44 toks/s]
WARNING 01-11 03:02:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:02:59,233][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:03:04,274][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:36,  5.04s/it, est. speed input: 182.30 toks/s, output: 5.75 toks/s]
[2025-01-11 03:03:04,275][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.35it/s, est. speed input: 5724.82 toks/s, output: 184.06 toks/s]
WARNING 01-11 03:03:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:03:04,651][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:03:05,710][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 836.73 toks/s, output: 27.39 toks/s]
[2025-01-11 03:03:05,711][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.83it/s, est. speed input: 2501.73 toks/s, output: 82.13 toks/s]
[2025-01-11 03:03:08,492][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:03:08,545][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:03:10,214][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 03:03:11,822][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:03:13,439][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:03:13,961][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:03:13,961][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:03:33,938][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:03:33,990][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:03:36,507][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.52s/it]
[2025-01-11 03:03:38,614][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.28s/it]
[2025-01-11 03:03:40,643][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.16s/it]
[2025-01-11 03:03:41,262][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.55s/it]
[2025-01-11 03:03:41,262][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.82s/it]
WARNING 01-11 03:03:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:03:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:03:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:03:57 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:03:57,914][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:03:59,231][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:03:59,608][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 03:04:00,903][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 03:04:02,238][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:04:02,238][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:04:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:04:16 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:04:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:04:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:04:38 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:04:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:39,288][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:43,045][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.76s/it, est. speed input: 169.03 toks/s, output: 2.66 toks/s]
[2025-01-11 03:04:43,407][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 308.35 toks/s, output: 6.31 toks/s]
[2025-01-11 03:04:43,518][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.35it/s, est. speed input: 900.80 toks/s, output: 21.99 toks/s]
[2025-01-11 03:04:43,662][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  5.11it/s, est. speed input: 1596.92 toks/s, output: 42.75 toks/s]
[2025-01-11 03:04:43,805][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01, 11.35it/s, est. speed input: 2811.40 toks/s, output: 85.23 toks/s]
[2025-01-11 03:04:43,947][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 17.34it/s, est. speed input: 3816.54 toks/s, output: 130.72 toks/s]
[2025-01-11 03:04:44,444][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.21it/s, est. speed input: 3940.94 toks/s, output: 148.56 toks/s]
WARNING 01-11 03:04:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:44,700][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:47,706][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.01s/it, est. speed input: 221.58 toks/s, output: 3.33 toks/s]
[2025-01-11 03:04:48,068][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:43,  1.45s/it, est. speed input: 395.50 toks/s, output: 7.72 toks/s]
[2025-01-11 03:04:48,949][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:34,  1.19s/it, est. speed input: 468.63 toks/s, output: 13.42 toks/s]
[2025-01-11 03:04:49,028][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.39it/s, est. speed input: 4866.55 toks/s, output: 223.44 toks/s]
WARNING 01-11 03:04:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:49,329][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:54,252][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:27,  4.92s/it, est. speed input: 151.77 toks/s, output: 5.89 toks/s]
[2025-01-11 03:04:54,252][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.30it/s, est. speed input: 5717.55 toks/s, output: 182.62 toks/s]
WARNING 01-11 03:04:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:54,475][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:55,235][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 960.77 toks/s, output: 38.17 toks/s]
[2025-01-11 03:04:55,235][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 960.77 toks/s, output: 38.17 toks/s]
WARNING 01-11 03:04:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:55,600][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:56,371][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1198.73 toks/s, output: 37.62 toks/s]
[2025-01-11 03:04:56,371][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1198.73 toks/s, output: 37.62 toks/s]
WARNING 01-11 03:04:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:04:56,576][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:04:57,486][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 979.89 toks/s, output: 31.86 toks/s]
[2025-01-11 03:04:57,486][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.20it/s, est. speed input: 1952.50 toks/s, output: 63.69 toks/s]
[2025-01-11 03:05:00,277][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:05:00,331][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:05:01,984][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 03:05:03,606][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:05:05,256][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 03:05:05,797][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:05:05,797][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:05:26,096][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:05:26,147][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:05:28,098][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 03:05:29,761][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 03:05:31,375][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 03:05:31,884][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:05:31,884][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:05:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:05:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:05:47 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:05:48 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:05:48,184][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:05:49,530][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:05:49,912][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:05:51,223][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:05:52,554][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:05:52,554][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:05:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:06:06 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:06:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:06:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:06:29 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:06:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:06:29,333][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:06:33,105][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.77s/it, est. speed input: 168.36 toks/s, output: 2.65 toks/s]
[2025-01-11 03:06:33,466][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.77s/it, est. speed input: 307.25 toks/s, output: 6.29 toks/s]
[2025-01-11 03:06:33,574][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.80it/s, est. speed input: 1048.01 toks/s, output: 25.70 toks/s]
[2025-01-11 03:06:33,714][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  6.13it/s, est. speed input: 1883.98 toks/s, output: 51.12 toks/s]
[2025-01-11 03:06:33,839][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 13.97it/s, est. speed input: 3381.83 toks/s, output: 103.41 toks/s]
[2025-01-11 03:06:34,055][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 16.61it/s, est. speed input: 4034.26 toks/s, output: 134.69 toks/s]
[2025-01-11 03:06:34,146][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4221.66 toks/s, output: 148.55 toks/s]
WARNING 01-11 03:06:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:06:34,407][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:06:37,422][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.01s/it, est. speed input: 220.92 toks/s, output: 3.32 toks/s]
[2025-01-11 03:06:38,567][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.91s/it, est. speed input: 317.60 toks/s, output: 9.38 toks/s]
[2025-01-11 03:06:38,682][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.75it/s, est. speed input: 766.79 toks/s, output: 30.41 toks/s]
[2025-01-11 03:06:38,726][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.41it/s, est. speed input: 4863.93 toks/s, output: 225.75 toks/s]
WARNING 01-11 03:06:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:06:38,987][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:06:44,089][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 179.74 toks/s, output: 5.68 toks/s]
[2025-01-11 03:06:44,090][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5722.60 toks/s, output: 181.86 toks/s]
WARNING 01-11 03:06:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:06:44,488][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:06:45,285][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1111.82 toks/s, output: 36.39 toks/s]
[2025-01-11 03:06:45,285][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1111.82 toks/s, output: 36.39 toks/s]
[2025-01-11 03:06:48,057][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:06:48,110][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:06:49,783][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 03:06:51,424][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:06:53,014][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 03:06:53,561][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:06:53,562][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:07:13,594][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:07:13,646][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:07:15,569][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 03:07:17,178][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 03:07:18,713][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 03:07:19,219][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:07:19,219][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 03:07:34 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:07:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:07:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:07:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:07:35,309][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:07:36,635][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:07:37,015][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:07:38,321][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:07:39,649][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:07:39,649][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:07:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:07:53 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:07:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:07:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:08:15 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:08:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:16,033][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:19,757][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.72s/it, est. speed input: 170.52 toks/s, output: 2.69 toks/s]
[2025-01-11 03:08:20,119][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.75s/it, est. speed input: 310.85 toks/s, output: 6.36 toks/s]
[2025-01-11 03:08:20,227][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.83it/s, est. speed input: 1059.99 toks/s, output: 25.99 toks/s]
[2025-01-11 03:08:20,356][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  8.03it/s, est. speed input: 2350.20 toks/s, output: 64.54 toks/s]
[2025-01-11 03:08:20,460][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.33it/s, est. speed input: 3012.51 toks/s, output: 88.10 toks/s]
[2025-01-11 03:08:20,642][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 15.01it/s, est. speed input: 3719.96 toks/s, output: 119.33 toks/s]
[2025-01-11 03:08:20,758][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 18.79it/s, est. speed input: 4300.92 toks/s, output: 152.39 toks/s]
[2025-01-11 03:08:20,758][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.77it/s, est. speed input: 4300.92 toks/s, output: 152.39 toks/s]
WARNING 01-11 03:08:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:21,027][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:24,055][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.03s/it, est. speed input: 219.94 toks/s, output: 3.30 toks/s]
[2025-01-11 03:08:24,878][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:03<00:31,  1.09s/it, est. speed input: 514.99 toks/s, output: 11.43 toks/s]
[2025-01-11 03:08:25,166][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.23it/s, est. speed input: 637.10 toks/s, output: 17.64 toks/s]
[2025-01-11 03:08:25,276][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.86it/s, est. speed input: 1083.79 toks/s, output: 38.60 toks/s]
[2025-01-11 03:08:25,352][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.40it/s, est. speed input: 4858.23 toks/s, output: 219.19 toks/s]
WARNING 01-11 03:08:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:25,642][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:30,551][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:27,  4.91s/it, est. speed input: 151.17 toks/s, output: 5.91 toks/s]
[2025-01-11 03:08:30,585][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.27it/s, est. speed input: 5652.81 toks/s, output: 182.27 toks/s]
WARNING 01-11 03:08:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:30,807][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:31,590][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 900.36 toks/s, output: 38.31 toks/s]
[2025-01-11 03:08:31,590][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 900.36 toks/s, output: 38.31 toks/s]
WARNING 01-11 03:08:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:31,958][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:32,739][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1168.18 toks/s, output: 37.10 toks/s]
[2025-01-11 03:08:32,740][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1168.18 toks/s, output: 37.10 toks/s]
WARNING 01-11 03:08:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:08:32,947][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:08:34,004][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 838.53 toks/s, output: 27.45 toks/s]
[2025-01-11 03:08:34,021][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.79it/s, est. speed input: 2482.18 toks/s, output: 81.93 toks/s]
[2025-01-11 03:08:36,826][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:08:36,879][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:08:38,540][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 03:08:40,174][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:08:41,756][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 03:08:42,284][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:08:42,285][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:09:02,104][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:09:02,156][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:09:04,089][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 03:09:05,752][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 03:09:07,377][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 03:09:07,887][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:09:07,887][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:09:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:09:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:09:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:09:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:09:24,571][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:09:25,888][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:09:26,314][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 03:09:27,657][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 03:09:28,982][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:09:28,982][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 03:09:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:09:43 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:09:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:09:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:10:05 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:10:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:10:05,466][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:10:08,920][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:47,  3.45s/it, est. speed input: 183.88 toks/s, output: 2.90 toks/s]
[2025-01-11 03:10:09,281][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:49,  1.63s/it, est. speed input: 332.90 toks/s, output: 6.82 toks/s]
[2025-01-11 03:10:09,387][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:06,  3.51it/s, est. speed input: 1295.87 toks/s, output: 31.89 toks/s]
[2025-01-11 03:10:09,531][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  6.35it/s, est. speed input: 2031.00 toks/s, output: 55.36 toks/s]
[2025-01-11 03:10:09,636][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 12.41it/s, est. speed input: 3198.49 toks/s, output: 95.94 toks/s]
[2025-01-11 03:10:09,749][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 16.15it/s, est. speed input: 3854.92 toks/s, output: 123.98 toks/s]
[2025-01-11 03:10:09,900][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 19.27it/s, est. speed input: 4440.10 toks/s, output: 156.09 toks/s]
[2025-01-11 03:10:09,900][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.22it/s, est. speed input: 4583.11 toks/s, output: 163.97 toks/s]
WARNING 01-11 03:10:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:10:10,145][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:10:13,146][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.00s/it, est. speed input: 221.88 toks/s, output: 3.33 toks/s]
[2025-01-11 03:10:14,171][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.84s/it, est. speed input: 327.09 toks/s, output: 9.19 toks/s]
[2025-01-11 03:10:14,289][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:30,  1.05s/it, est. speed input: 475.60 toks/s, output: 15.93 toks/s]
[2025-01-11 03:10:14,404][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:13,  2.04it/s, est. speed input: 773.61 toks/s, output: 29.82 toks/s]
[2025-01-11 03:10:14,491][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.36it/s, est. speed input: 4836.22 toks/s, output: 225.50 toks/s]
WARNING 01-11 03:10:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:10:14,782][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:10:19,839][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:36,  5.06s/it, est. speed input: 183.92 toks/s, output: 5.74 toks/s]
[2025-01-11 03:10:19,840][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.33it/s, est. speed input: 5743.69 toks/s, output: 183.49 toks/s]
WARNING 01-11 03:10:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:10:20,236][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:10:21,168][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.07it/s, est. speed input: 950.85 toks/s, output: 31.12 toks/s]
[2025-01-11 03:10:21,185][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.11it/s, est. speed input: 1874.25 toks/s, output: 62.16 toks/s]
[2025-01-11 03:10:23,984][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:10:24,038][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:10:25,736][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-01-11 03:10:27,362][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 03:10:29,030][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 03:10:29,558][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:10:29,558][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 03:10:49,513][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:10:49,565][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:10:51,496][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 03:10:53,131][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:10:54,753][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 03:10:55,261][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:10:55,262][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:11:10 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:11:10 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:11:11 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:11:11 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:11:11,623][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:11:12,943][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:11:13,332][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:11:14,635][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:11:15,988][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:11:15,989][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:11:16 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:11:30 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:11:30 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:11:30 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:11:52 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:11:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:11:53,041][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:11:56,708][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:53,  3.67s/it, est. speed input: 173.20 toks/s, output: 2.73 toks/s]
[2025-01-11 03:11:57,069][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.72s/it, est. speed input: 315.30 toks/s, output: 6.45 toks/s]
[2025-01-11 03:11:57,184][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:13,  1.94it/s, est. speed input: 766.49 toks/s, output: 18.59 toks/s]
[2025-01-11 03:11:57,323][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  6.61it/s, est. speed input: 1928.02 toks/s, output: 53.02 toks/s]
[2025-01-11 03:11:57,457][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 12.89it/s, est. speed input: 3163.83 toks/s, output: 96.48 toks/s]
[2025-01-11 03:11:57,611][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 15.73it/s, est. speed input: 3751.54 toks/s, output: 123.19 toks/s]
[2025-01-11 03:11:57,792][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 16.99it/s, est. speed input: 4143.75 toks/s, output: 148.40 toks/s]
[2025-01-11 03:11:57,975][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.49it/s, est. speed input: 4118.62 toks/s, output: 153.23 toks/s]
WARNING 01-11 03:11:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:11:58,240][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:12:02,545][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:13,  4.30s/it, est. speed input: 154.48 toks/s, output: 7.20 toks/s]
[2025-01-11 03:12:02,601][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4825.93 toks/s, output: 231.36 toks/s]
WARNING 01-11 03:12:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:12:02,860][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:12:07,954][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.09s/it, est. speed input: 181.98 toks/s, output: 5.69 toks/s]
[2025-01-11 03:12:07,955][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5775.92 toks/s, output: 182.14 toks/s]
[2025-01-11 03:12:10,995][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:12:11,048][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:12:12,740][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 03:12:14,378][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 03:12:16,046][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 03:12:16,577][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:12:16,577][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 03:12:36,809][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:12:36,861][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:12:38,803][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 03:12:40,419][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 03:12:41,954][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 03:12:42,466][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:12:42,467][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 03:12:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:12:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:12:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:12:58 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:12:58,690][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:13:00,032][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 03:13:00,424][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:13:01,729][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:13:03,085][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:13:03,086][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 03:13:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:13:17 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:13:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:13:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:13:45 model_runner.py:1335] Graph capturing finished in 28 secs.
WARNING 01-11 03:13:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:13:46,033][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:13:49,826][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.79s/it, est. speed input: 167.42 toks/s, output: 2.64 toks/s]
[2025-01-11 03:13:50,187][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.77s/it, est. speed input: 305.69 toks/s, output: 6.26 toks/s]
[2025-01-11 03:13:50,295][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.79it/s, est. speed input: 1042.85 toks/s, output: 25.57 toks/s]
[2025-01-11 03:13:50,398][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:04,  4.43it/s, est. speed input: 1454.72 toks/s, output: 38.49 toks/s]
[2025-01-11 03:13:50,523][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01, 11.00it/s, est. speed input: 2686.99 toks/s, output: 80.62 toks/s]
[2025-01-11 03:13:50,648][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 16.49it/s, est. speed input: 3577.23 toks/s, output: 117.44 toks/s]
[2025-01-11 03:13:50,820][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 18.93it/s, est. speed input: 4112.25 toks/s, output: 146.44 toks/s]
[2025-01-11 03:13:50,954][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.50it/s, est. speed input: 4129.44 toks/s, output: 151.20 toks/s]
WARNING 01-11 03:13:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:13:51,198][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:13:54,202][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:33,  3.00s/it, est. speed input: 221.76 toks/s, output: 3.33 toks/s]
[2025-01-11 03:13:55,408][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:58,  1.95s/it, est. speed input: 312.84 toks/s, output: 9.50 toks/s]
[2025-01-11 03:13:55,511][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 12.07it/s, est. speed input: 4573.63 toks/s, output: 211.48 toks/s]
[2025-01-11 03:13:55,511][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.42it/s, est. speed input: 4877.32 toks/s, output: 226.77 toks/s]
WARNING 01-11 03:13:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:13:55,775][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:14:00,852][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.08s/it, est. speed input: 182.20 toks/s, output: 5.71 toks/s]
[2025-01-11 03:14:00,853][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.30it/s, est. speed input: 5756.71 toks/s, output: 182.76 toks/s]
WARNING 01-11 03:14:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:14:01,245][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:14:02,032][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1126.77 toks/s, output: 36.88 toks/s]
[2025-01-11 03:14:02,032][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1126.77 toks/s, output: 36.88 toks/s]
[2025-01-11 03:14:04,885][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:14:04,939][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:14:06,660][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 03:14:08,260][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:14:09,820][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 03:14:10,343][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 03:14:10,344][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:14:30,585][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:14:30,636][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:14:32,731][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.09s/it]
[2025-01-11 03:14:34,324][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 03:14:35,927][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 03:14:36,447][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 03:14:36,447][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 03:14:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:14:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:14:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:14:52 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:14:52,612][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:14:53,941][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:14:54,321][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:14:55,659][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:14:56,988][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:14:56,989][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:14:57 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:15:11 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:15:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:15:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:15:39 model_runner.py:1335] Graph capturing finished in 27 secs.
WARNING 01-11 03:15:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:39,432][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:42,904][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:47,  3.47s/it, est. speed input: 182.92 toks/s, output: 2.88 toks/s]
[2025-01-11 03:15:43,266][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:49,  1.64s/it, est. speed input: 331.29 toks/s, output: 6.78 toks/s]
[2025-01-11 03:15:43,371][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:06,  3.49it/s, est. speed input: 1289.76 toks/s, output: 31.74 toks/s]
[2025-01-11 03:15:43,502][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  7.65it/s, est. speed input: 2340.58 toks/s, output: 63.15 toks/s]
[2025-01-11 03:15:43,624][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 15.07it/s, est. speed input: 3787.43 toks/s, output: 114.52 toks/s]
[2025-01-11 03:15:43,846][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.52it/s, est. speed input: 4459.75 toks/s, output: 150.89 toks/s]
[2025-01-11 03:15:43,897][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.17it/s, est. speed input: 4551.57 toks/s, output: 157.69 toks/s]
WARNING 01-11 03:15:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:44,162][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:47,472][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:42,  3.31s/it, est. speed input: 196.38 toks/s, output: 4.53 toks/s]
[2025-01-11 03:15:48,317][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.86s/it, est. speed input: 313.41 toks/s, output: 10.59 toks/s]
[2025-01-11 03:15:48,432][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.80it/s, est. speed input: 772.84 toks/s, output: 31.62 toks/s]
[2025-01-11 03:15:48,478][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.41it/s, est. speed input: 4864.65 toks/s, output: 227.52 toks/s]
WARNING 01-11 03:15:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:48,749][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:53,731][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.98s/it, est. speed input: 184.47 toks/s, output: 5.82 toks/s]
[2025-01-11 03:15:53,732][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.22it/s, est. speed input: 5712.44 toks/s, output: 180.42 toks/s]
WARNING 01-11 03:15:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:53,952][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:54,482][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 1316.80 toks/s, output: 26.41 toks/s]
[2025-01-11 03:15:54,483][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 1316.80 toks/s, output: 26.41 toks/s]
WARNING 01-11 03:15:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:54,873][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:55,623][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 972.96 toks/s, output: 38.65 toks/s]
[2025-01-11 03:15:55,624][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 972.96 toks/s, output: 38.65 toks/s]
WARNING 01-11 03:15:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:15:55,898][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:15:56,887][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 932.79 toks/s, output: 29.34 toks/s]
[2025-01-11 03:15:56,888][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 932.79 toks/s, output: 29.34 toks/s]
[2025-01-11 03:16:01,587][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:16:01,688][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:16:04,349][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.66s/it]
[2025-01-11 03:16:06,035][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.09s/it]
[2025-01-11 03:16:07,690][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:01,  1.89s/it]
[2025-01-11 03:16:08,236][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.36s/it]
[2025-01-11 03:16:08,237][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.64s/it]
[2025-01-11 03:16:28,204][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:16:28,256][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:16:30,152][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 03:16:31,758][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 03:16:33,309][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 03:16:33,815][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:16:33,815][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 03:16:49 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:16:49 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:16:50 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:16:50 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:16:50,393][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:16:51,736][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 03:16:52,176][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.23it/s]
[2025-01-11 03:16:53,556][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.07s/it]
[2025-01-11 03:16:55,187][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.29s/it]
[2025-01-11 03:16:55,187][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
INFO 01-11 03:16:55 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:17:09 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:17:09 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:17:09 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:17:31 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:17:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:31,712][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:35,490][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.78s/it, est. speed input: 168.12 toks/s, output: 2.65 toks/s]
[2025-01-11 03:17:35,852][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.77s/it, est. speed input: 306.84 toks/s, output: 6.28 toks/s]
[2025-01-11 03:17:35,966][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:14,  1.89it/s, est. speed input: 746.46 toks/s, output: 18.10 toks/s]
[2025-01-11 03:17:36,067][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:04,  4.80it/s, est. speed input: 1458.22 toks/s, output: 39.27 toks/s]
[2025-01-11 03:17:36,184][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 12.99it/s, est. speed input: 2982.40 toks/s, output: 90.80 toks/s]
[2025-01-11 03:17:36,319][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 17.13it/s, est. speed input: 3722.36 toks/s, output: 121.15 toks/s]
[2025-01-11 03:17:36,526][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 18.70it/s, est. speed input: 4221.89 toks/s, output: 152.50 toks/s]
[2025-01-11 03:17:36,526][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4221.89 toks/s, output: 152.50 toks/s]
WARNING 01-11 03:17:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:36,819][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:40,989][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.17s/it, est. speed input: 160.44 toks/s, output: 6.95 toks/s]
[2025-01-11 03:17:41,109][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.13s/it, est. speed input: 464.39 toks/s, output: 20.98 toks/s]
[2025-01-11 03:17:41,152][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.39it/s, est. speed input: 4852.97 toks/s, output: 230.09 toks/s]
WARNING 01-11 03:17:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:41,414][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:46,384][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.97s/it, est. speed input: 187.92 toks/s, output: 5.83 toks/s]
[2025-01-11 03:17:46,385][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.24it/s, est. speed input: 5731.57 toks/s, output: 180.85 toks/s]
WARNING 01-11 03:17:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:46,600][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:47,412][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 881.39 toks/s, output: 39.45 toks/s]
[2025-01-11 03:17:47,412][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 881.39 toks/s, output: 39.45 toks/s]
WARNING 01-11 03:17:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:47,797][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:48,543][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 972.15 toks/s, output: 38.89 toks/s]
[2025-01-11 03:17:48,544][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 972.15 toks/s, output: 38.89 toks/s]
WARNING 01-11 03:17:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:17:48,758][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:17:49,533][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1235.38 toks/s, output: 37.43 toks/s]
[2025-01-11 03:17:49,533][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1235.38 toks/s, output: 37.43 toks/s]
[2025-01-11 03:17:52,409][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:17:52,462][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:17:54,106][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 03:17:55,724][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:17:57,395][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 03:17:57,932][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:17:57,933][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:18:18,121][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:18:18,173][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:18:20,055][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.88s/it]
[2025-01-11 03:18:21,636][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 03:18:23,178][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.63s/it]
[2025-01-11 03:18:23,693][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:18:23,693][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 03:18:39 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:18:39 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:18:39 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:18:39 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:18:40,039][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:18:41,364][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:18:41,777][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 03:18:43,081][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:18:44,417][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:18:44,417][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:18:44 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:18:58 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:18:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:18:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:19:20 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:19:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:19:20,939][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:19:24,675][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.74s/it, est. speed input: 170.01 toks/s, output: 2.68 toks/s]
[2025-01-11 03:19:25,036][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.75s/it, est. speed input: 310.01 toks/s, output: 6.35 toks/s]
[2025-01-11 03:19:25,144][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:08,  2.83it/s, est. speed input: 1057.22 toks/s, output: 25.92 toks/s]
[2025-01-11 03:19:25,271][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:01,  8.03it/s, est. speed input: 2345.69 toks/s, output: 63.95 toks/s]
[2025-01-11 03:19:25,396][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 15.02it/s, est. speed input: 3704.98 toks/s, output: 113.55 toks/s]
[2025-01-11 03:19:25,598][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 17.70it/s, est. speed input: 4361.68 toks/s, output: 147.68 toks/s]
[2025-01-11 03:19:25,598][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.87it/s, est. speed input: 4361.68 toks/s, output: 147.68 toks/s]
WARNING 01-11 03:19:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:19:25,860][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:19:29,981][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.12s/it, est. speed input: 158.22 toks/s, output: 6.79 toks/s]
[2025-01-11 03:19:30,100][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.23it/s, est. speed input: 618.22 toks/s, output: 27.36 toks/s]
[2025-01-11 03:19:30,202][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 12.06it/s, est. speed input: 4224.73 toks/s, output: 198.53 toks/s]
[2025-01-11 03:19:30,202][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.37it/s, est. speed input: 4831.55 toks/s, output: 228.92 toks/s]
WARNING 01-11 03:19:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:19:30,476][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:19:35,446][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.97s/it, est. speed input: 184.92 toks/s, output: 5.84 toks/s]
[2025-01-11 03:19:35,866][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  7.87it/s, est. speed input: 5277.52 toks/s, output: 171.43 toks/s]
[2025-01-11 03:19:35,866][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  5.75it/s, est. speed input: 5277.52 toks/s, output: 171.43 toks/s]
WARNING 01-11 03:19:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:19:36,084][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:19:36,884][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 887.12 toks/s, output: 38.73 toks/s]
[2025-01-11 03:19:36,884][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 887.12 toks/s, output: 38.73 toks/s]
WARNING 01-11 03:19:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:19:37,271][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:19:38,049][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1174.62 toks/s, output: 37.27 toks/s]
[2025-01-11 03:19:38,050][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1174.62 toks/s, output: 37.27 toks/s]
[2025-01-11 03:19:41,098][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:19:41,151][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:19:42,773][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 03:19:44,427][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 03:19:46,044][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:19:46,568][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:19:46,569][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:20:06,632][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:20:06,685][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:20:08,625][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 03:20:10,241][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 03:20:11,825][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 03:20:12,330][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:20:12,331][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 03:20:28 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:20:28 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:20:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:20:28 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:20:28,914][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:20:30,281][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.37s/it]
[2025-01-11 03:20:30,666][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 03:20:31,974][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:20:33,701][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.30s/it]
[2025-01-11 03:20:33,702][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
INFO 01-11 03:20:34 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:20:48 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:20:48 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:20:48 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:21:10 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:21:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:21:11,043][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:21:14,840][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 167.24 toks/s, output: 2.63 toks/s]
[2025-01-11 03:21:15,202][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.78s/it, est. speed input: 305.38 toks/s, output: 6.25 toks/s]
[2025-01-11 03:21:15,313][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.33it/s, est. speed input: 892.33 toks/s, output: 21.78 toks/s]
[2025-01-11 03:21:15,449][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:02,  6.87it/s, est. speed input: 2017.63 toks/s, output: 55.38 toks/s]
[2025-01-11 03:21:15,568][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 14.67it/s, est. speed input: 3508.01 toks/s, output: 107.39 toks/s]
[2025-01-11 03:21:15,739][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.89it/s, est. speed input: 4191.64 toks/s, output: 140.32 toks/s]
[2025-01-11 03:21:15,773][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.77it/s, est. speed input: 4295.87 toks/s, output: 146.72 toks/s]
WARNING 01-11 03:21:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:21:16,051][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:21:20,148][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.10s/it, est. speed input: 158.90 toks/s, output: 6.83 toks/s]
[2025-01-11 03:21:20,325][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:23,  1.22it/s, est. speed input: 614.49 toks/s, output: 27.38 toks/s]
[2025-01-11 03:21:20,410][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4815.57 toks/s, output: 229.44 toks/s]
WARNING 01-11 03:21:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:21:20,676][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:21:25,820][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:39,  5.14s/it, est. speed input: 180.04 toks/s, output: 5.64 toks/s]
[2025-01-11 03:21:25,821][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.22it/s, est. speed input: 5675.99 toks/s, output: 180.39 toks/s]
WARNING 01-11 03:21:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:21:26,421][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:21:27,381][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 927.14 toks/s, output: 30.24 toks/s]
[2025-01-11 03:21:27,381][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 927.14 toks/s, output: 30.24 toks/s]
[2025-01-11 03:21:31,666][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:21:31,774][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:21:35,102][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.33s/it]
[2025-01-11 03:21:37,704][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:05<00:05,  2.90s/it]
[2025-01-11 03:21:39,387][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:07<00:02,  2.34s/it]
[2025-01-11 03:21:39,954][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:08<00:00,  1.64s/it]
[2025-01-11 03:21:39,955][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:08<00:00,  2.05s/it]
[2025-01-11 03:21:59,891][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:21:59,942][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:22:01,852][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 03:22:03,438][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 03:22:05,033][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:22:05,560][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:22:05,560][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 03:22:21 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:22:21 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:22:21 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:22:21 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:22:21,739][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:22:23,073][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:22:23,470][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:22:24,763][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:22:26,082][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:22:26,083][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:22:26 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:22:40 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:22:40 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:22:40 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:23:02 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:23:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:02,574][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:06,394][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.22 toks/s, output: 2.62 toks/s]
[2025-01-11 03:23:06,748][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.12s/it, est. speed input: 456.36 toks/s, output: 8.86 toks/s]
[2025-01-11 03:23:06,856][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:07,  3.09it/s, est. speed input: 1186.26 toks/s, output: 28.49 toks/s]
[2025-01-11 03:23:06,979][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.98it/s, est. speed input: 3604.02 toks/s, output: 100.12 toks/s]
[2025-01-11 03:23:07,231][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 15.59it/s, est. speed input: 4363.61 toks/s, output: 135.07 toks/s]
[2025-01-11 03:23:07,231][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.87it/s, est. speed input: 4363.61 toks/s, output: 135.07 toks/s]
WARNING 01-11 03:23:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:07,481][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:11,575][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.09s/it, est. speed input: 159.77 toks/s, output: 6.84 toks/s]
[2025-01-11 03:23:11,696][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 312.96 toks/s, output: 13.76 toks/s]
[2025-01-11 03:23:11,810][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.42it/s, est. speed input: 3928.37 toks/s, output: 186.18 toks/s]
[2025-01-11 03:23:11,831][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.36it/s, est. speed input: 4809.97 toks/s, output: 231.52 toks/s]
WARNING 01-11 03:23:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:12,090][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:17,037][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 184.80 toks/s, output: 5.86 toks/s]
[2025-01-11 03:23:17,037][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.27it/s, est. speed input: 5704.88 toks/s, output: 181.73 toks/s]
WARNING 01-11 03:23:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:17,250][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:18,043][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 896.74 toks/s, output: 39.10 toks/s]
[2025-01-11 03:23:18,044][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 896.74 toks/s, output: 39.10 toks/s]
WARNING 01-11 03:23:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:18,426][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:19,197][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1185.83 toks/s, output: 37.62 toks/s]
[2025-01-11 03:23:19,197][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1185.83 toks/s, output: 37.62 toks/s]
WARNING 01-11 03:23:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:23:19,412][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:23:20,191][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1144.58 toks/s, output: 37.21 toks/s]
[2025-01-11 03:23:20,191][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1144.58 toks/s, output: 37.21 toks/s]
[2025-01-11 03:23:22,991][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:23:23,098][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:23:26,578][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:10,  3.48s/it]
[2025-01-11 03:23:29,833][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:06,  3.35s/it]
[2025-01-11 03:23:33,146][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:10<00:03,  3.33s/it]
[2025-01-11 03:23:34,216][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:11<00:00,  2.44s/it]
[2025-01-11 03:23:34,216][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:11<00:00,  2.78s/it]
[2025-01-11 03:23:56,111][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:23:56,163][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:23:58,104][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 03:23:59,698][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 03:24:01,286][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 03:24:01,789][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:24:01,790][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 03:24:17 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:24:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:24:17 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:24:17 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:24:17,935][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:24:19,292][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 03:24:19,671][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:24:20,968][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:24:22,324][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:24:22,324][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 03:24:22 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:24:36 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:24:36 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:24:36 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:24:58 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:24:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:24:59,178][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:25:02,913][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:55,  3.74s/it, est. speed input: 170.01 toks/s, output: 2.68 toks/s]
[2025-01-11 03:25:03,208][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.08s/it, est. speed input: 472.65 toks/s, output: 8.93 toks/s]
[2025-01-11 03:25:03,321][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.28it/s, est. speed input: 919.66 toks/s, output: 20.52 toks/s]
[2025-01-11 03:25:03,421][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  5.25it/s, est. speed input: 1646.16 toks/s, output: 41.24 toks/s]
[2025-01-11 03:25:03,551][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01, 10.29it/s, est. speed input: 2613.84 toks/s, output: 73.86 toks/s]
[2025-01-11 03:25:03,670][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 17.08it/s, est. speed input: 3675.05 toks/s, output: 114.41 toks/s]
[2025-01-11 03:25:03,866][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 18.94it/s, est. speed input: 4199.10 toks/s, output: 143.35 toks/s]
[2025-01-11 03:25:03,866][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.83it/s, est. speed input: 4334.35 toks/s, output: 150.81 toks/s]
WARNING 01-11 03:25:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:25:04,111][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:25:08,149][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.04s/it, est. speed input: 162.24 toks/s, output: 6.69 toks/s]
[2025-01-11 03:25:08,330][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.77s/it, est. speed input: 310.78 toks/s, output: 13.51 toks/s]
[2025-01-11 03:25:08,440][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 11.82it/s, est. speed input: 4094.15 toks/s, output: 193.11 toks/s]
[2025-01-11 03:25:08,474][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.33it/s, est. speed input: 4813.39 toks/s, output: 229.89 toks/s]
WARNING 01-11 03:25:08 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:25:08,726][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:25:13,693][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.97s/it, est. speed input: 185.07 toks/s, output: 5.84 toks/s]
[2025-01-11 03:25:13,693][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.24it/s, est. speed input: 5730.19 toks/s, output: 181.01 toks/s]
WARNING 01-11 03:25:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:25:13,927][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:25:14,752][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 865.48 toks/s, output: 40.06 toks/s]
[2025-01-11 03:25:14,752][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 865.48 toks/s, output: 40.06 toks/s]
WARNING 01-11 03:25:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:25:15,150][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:25:15,920][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1193.58 toks/s, output: 37.66 toks/s]
[2025-01-11 03:25:15,920][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1193.58 toks/s, output: 37.66 toks/s]
[2025-01-11 03:25:18,605][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:25:18,658][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:25:20,265][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 03:25:21,899][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 03:25:23,519][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 03:25:24,080][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:25:24,081][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:25:44,277][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:25:44,329][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:25:46,328][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  2.00s/it]
[2025-01-11 03:25:47,918][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:25:49,517][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 03:25:50,041][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:25:50,041][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:26:05 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:26:05 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:26:05 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:26:05 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:26:06,204][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:26:07,536][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:26:07,914][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:26:09,319][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.06s/it]
[2025-01-11 03:26:10,641][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 03:26:10,641][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 03:26:10 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:26:24 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:26:25 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:26:25 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:26:46 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:26:46 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:26:46,987][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:26:51,167][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 151.93 toks/s, output: 3.83 toks/s]
[2025-01-11 03:26:51,285][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.54it/s, est. speed input: 738.72 toks/s, output: 19.78 toks/s]
[2025-01-11 03:26:51,416][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 12.06it/s, est. speed input: 4157.74 toks/s, output: 121.24 toks/s]
[2025-01-11 03:26:51,636][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.88it/s, est. speed input: 4371.05 toks/s, output: 135.30 toks/s]
WARNING 01-11 03:26:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:26:51,886][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:26:55,986][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.10s/it, est. speed input: 159.52 toks/s, output: 6.83 toks/s]
[2025-01-11 03:26:56,107][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 310.14 toks/s, output: 13.74 toks/s]
[2025-01-11 03:26:56,225][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 10.48it/s, est. speed input: 3618.33 toks/s, output: 171.24 toks/s]
[2025-01-11 03:26:56,265][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.31it/s, est. speed input: 4777.89 toks/s, output: 231.15 toks/s]
WARNING 01-11 03:26:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:26:56,519][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:27:01,466][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 184.78 toks/s, output: 5.86 toks/s]
[2025-01-11 03:27:01,467][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.27it/s, est. speed input: 5704.35 toks/s, output: 181.72 toks/s]
WARNING 01-11 03:27:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:27:01,674][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:27:02,500][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 860.69 toks/s, output: 39.95 toks/s]
[2025-01-11 03:27:02,500][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 860.69 toks/s, output: 39.95 toks/s]
WARNING 01-11 03:27:02 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:27:02,877][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:27:03,658][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1171.67 toks/s, output: 37.17 toks/s]
[2025-01-11 03:27:03,658][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1171.67 toks/s, output: 37.17 toks/s]
WARNING 01-11 03:27:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:27:03,874][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:27:04,641][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1162.46 toks/s, output: 37.79 toks/s]
[2025-01-11 03:27:04,642][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 1162.46 toks/s, output: 37.79 toks/s]
[2025-01-11 03:27:07,621][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:27:07,675][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:27:09,380][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-01-11 03:27:11,007][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 03:27:12,623][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 03:27:13,157][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:27:13,158][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:27:33,189][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:27:33,240][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:27:35,184][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 03:27:36,822][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:27:38,365][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:27:38,872][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:27:38,873][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 03:27:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:27:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:27:54 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:27:54 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:27:55,109][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:27:56,439][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:27:56,852][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 03:27:58,156][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:27:59,520][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 03:27:59,520][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 03:27:59 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:28:13 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:28:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:28:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:28:36 model_runner.py:1335] Graph capturing finished in 23 secs.
WARNING 01-11 03:28:37 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:37,247][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:41,323][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.08s/it, est. speed input: 155.81 toks/s, output: 3.93 toks/s]
[2025-01-11 03:28:41,447][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.24it/s, est. speed input: 604.91 toks/s, output: 15.96 toks/s]
[2025-01-11 03:28:41,594][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  5.20it/s, est. speed input: 1899.51 toks/s, output: 54.07 toks/s]
[2025-01-11 03:28:41,733][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.06it/s, est. speed input: 3114.80 toks/s, output: 96.54 toks/s]
[2025-01-11 03:28:41,907][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.44it/s, est. speed input: 3679.40 toks/s, output: 122.97 toks/s]
[2025-01-11 03:28:42,335][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 12.21it/s, est. speed input: 3993.97 toks/s, output: 151.93 toks/s]
[2025-01-11 03:28:42,336][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.29it/s, est. speed input: 3993.97 toks/s, output: 151.93 toks/s]
WARNING 01-11 03:28:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:42,839][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:47,132][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:13,  4.29s/it, est. speed input: 157.03 toks/s, output: 6.52 toks/s]
[2025-01-11 03:28:47,258][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.84s/it, est. speed input: 301.90 toks/s, output: 13.13 toks/s]
[2025-01-11 03:28:47,390][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:01,  9.12it/s, est. speed input: 3179.83 toks/s, output: 149.88 toks/s]
[2025-01-11 03:28:47,432][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.97it/s, est. speed input: 4586.73 toks/s, output: 221.45 toks/s]
WARNING 01-11 03:28:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:47,959][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:53,048][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:05<02:32,  5.09s/it, est. speed input: 182.17 toks/s, output: 5.70 toks/s]
[2025-01-11 03:28:53,048][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:05<00:00,  6.09it/s, est. speed input: 5572.83 toks/s, output: 176.64 toks/s]
WARNING 01-11 03:28:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:53,267][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:54,019][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 961.68 toks/s, output: 37.24 toks/s]
[2025-01-11 03:28:54,019][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 961.68 toks/s, output: 37.24 toks/s]
WARNING 01-11 03:28:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:54,411][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:55,160][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 972.25 toks/s, output: 38.73 toks/s]
[2025-01-11 03:28:55,160][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 972.25 toks/s, output: 38.73 toks/s]
WARNING 01-11 03:28:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:55,373][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:56,156][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1164.14 toks/s, output: 37.02 toks/s]
[2025-01-11 03:28:56,156][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1164.14 toks/s, output: 37.02 toks/s]
WARNING 01-11 03:28:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:56,381][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:57,122][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1298.26 toks/s, output: 36.47 toks/s]
[2025-01-11 03:28:57,122][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1298.26 toks/s, output: 36.47 toks/s]
WARNING 01-11 03:28:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:28:57,335][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:28:58,093][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1363.36 toks/s, output: 35.63 toks/s]
[2025-01-11 03:28:58,094][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 1363.36 toks/s, output: 35.63 toks/s]
[2025-01-11 03:29:00,817][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:29:00,870][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:29:02,476][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 03:29:04,153][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:29:05,780][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 03:29:06,327][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:29:06,328][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:29:26,205][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:29:26,257][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:29:28,270][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.01s/it]
[2025-01-11 03:29:29,912][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 03:29:31,494][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 03:29:32,073][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 03:29:32,073][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 03:29:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:29:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:29:48 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:29:48 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:29:48,449][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:29:49,774][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:29:50,153][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:29:51,484][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:29:52,818][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:29:52,818][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:29:53 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:30:06 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:30:07 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:30:07 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:30:29 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:30:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:30:29,325][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:30:33,150][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:58,  3.82s/it, est. speed input: 166.02 toks/s, output: 4.18 toks/s]
[2025-01-11 03:30:33,264][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:12,  2.04it/s, est. speed input: 967.28 toks/s, output: 25.64 toks/s]
[2025-01-11 03:30:33,407][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.76it/s, est. speed input: 1866.52 toks/s, output: 52.66 toks/s]
[2025-01-11 03:30:33,589][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  8.48it/s, est. speed input: 2829.37 toks/s, output: 86.77 toks/s]
[2025-01-11 03:30:33,725][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.23it/s, est. speed input: 3607.72 toks/s, output: 120.90 toks/s]
[2025-01-11 03:30:33,842][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 15.73it/s, est. speed input: 4217.11 toks/s, output: 155.84 toks/s]
[2025-01-11 03:30:34,210][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.55it/s, est. speed input: 4159.64 toks/s, output: 163.56 toks/s]
WARNING 01-11 03:30:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:30:34,453][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:30:38,691][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 155.76 toks/s, output: 7.08 toks/s]
[2025-01-11 03:30:38,809][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00,  8.01it/s, est. speed input: 3789.54 toks/s, output: 178.63 toks/s]
[2025-01-11 03:30:38,846][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.28it/s, est. speed input: 4801.76 toks/s, output: 230.61 toks/s]
WARNING 01-11 03:30:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:30:39,131][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:30:44,248][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.12s/it, est. speed input: 181.17 toks/s, output: 5.67 toks/s]
[2025-01-11 03:30:44,285][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.21it/s, est. speed input: 5687.00 toks/s, output: 180.66 toks/s]
WARNING 01-11 03:30:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:30:44,682][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:30:45,472][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1257.38 toks/s, output: 36.72 toks/s]
[2025-01-11 03:30:45,473][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1257.38 toks/s, output: 36.72 toks/s]
WARNING 01-11 03:30:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:30:45,681][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:30:46,505][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1085.18 toks/s, output: 37.63 toks/s]
[2025-01-11 03:30:46,505][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 1085.18 toks/s, output: 37.63 toks/s]
[2025-01-11 03:30:49,225][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:30:49,278][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:30:50,940][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 03:30:52,591][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 03:30:54,189][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:30:54,748][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:30:54,749][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:31:16,064][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:31:16,170][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:31:19,393][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:03<00:09,  3.22s/it]
[2025-01-11 03:31:22,782][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:06<00:06,  3.32s/it]
[2025-01-11 03:31:26,004][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:09<00:03,  3.28s/it]
[2025-01-11 03:31:26,956][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.36s/it]
[2025-01-11 03:31:26,957][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:10<00:00,  2.70s/it]
WARNING 01-11 03:31:44 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:31:44 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:31:45 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:31:45 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:31:45,274][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:31:46,600][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:31:46,981][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:31:48,283][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:31:49,640][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:31:49,640][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:31:49 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:32:03 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:32:04 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:32:04 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:32:25 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:32:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:26,294][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:30,159][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.86s/it, est. speed input: 164.31 toks/s, output: 2.85 toks/s]
[2025-01-11 03:32:30,462][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.77s/it, est. speed input: 304.75 toks/s, output: 6.48 toks/s]
[2025-01-11 03:32:30,574][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.34it/s, est. speed input: 890.21 toks/s, output: 22.20 toks/s]
[2025-01-11 03:32:30,713][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  6.29it/s, est. speed input: 1868.08 toks/s, output: 51.37 toks/s]
[2025-01-11 03:32:30,820][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 17.38it/s, est. speed input: 3928.69 toks/s, output: 122.85 toks/s]
[2025-01-11 03:32:31,018][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.77it/s, est. speed input: 4301.69 toks/s, output: 145.01 toks/s]
WARNING 01-11 03:32:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:31,275][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:34,018][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:25,  2.74s/it, est. speed input: 238.02 toks/s, output: 2.19 toks/s]
[2025-01-11 03:32:35,285][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:56,  1.87s/it, est. speed input: 326.19 toks/s, output: 8.23 toks/s]
[2025-01-11 03:32:35,399][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.79it/s, est. speed input: 794.30 toks/s, output: 28.37 toks/s]
[2025-01-11 03:32:35,505][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:07,  3.39it/s, est. speed input: 1239.08 toks/s, output: 49.17 toks/s]
[2025-01-11 03:32:35,605][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.39it/s, est. speed input: 4844.65 toks/s, output: 225.87 toks/s]
WARNING 01-11 03:32:35 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:35,886][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:40,807][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:27,  4.92s/it, est. speed input: 186.34 toks/s, output: 5.89 toks/s]
[2025-01-11 03:32:40,808][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.30it/s, est. speed input: 5706.13 toks/s, output: 182.65 toks/s]
WARNING 01-11 03:32:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:41,031][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:41,848][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 874.84 toks/s, output: 39.21 toks/s]
[2025-01-11 03:32:41,848][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 874.84 toks/s, output: 39.21 toks/s]
WARNING 01-11 03:32:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:42,234][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:43,015][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1174.16 toks/s, output: 37.13 toks/s]
[2025-01-11 03:32:43,015][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1174.16 toks/s, output: 37.13 toks/s]
WARNING 01-11 03:32:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:32:43,225][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:32:44,139][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.09it/s, est. speed input: 950.84 toks/s, output: 31.73 toks/s]
[2025-01-11 03:32:44,139][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.19it/s, est. speed input: 1928.31 toks/s, output: 63.44 toks/s]
[2025-01-11 03:32:46,847][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:32:46,900][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:32:48,545][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 03:32:50,162][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:32:51,784][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:32:52,330][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:32:52,331][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:33:12,177][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:33:12,229][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:33:14,148][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 03:33:15,777][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 03:33:17,455][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 03:33:17,955][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 03:33:17,956][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:33:33 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:33:33 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:33:33 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:33:34 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:33:34,188][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:33:35,511][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:33:35,916][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:33:37,240][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:33:38,564][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:33:38,564][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:33:38 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:33:52 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:33:53 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:33:53 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:34:14 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:34:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:15,039][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:19,230][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.19s/it, est. speed input: 151.53 toks/s, output: 3.82 toks/s]
[2025-01-11 03:34:19,346][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.54it/s, est. speed input: 737.27 toks/s, output: 19.51 toks/s]
[2025-01-11 03:34:19,463][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  7.24it/s, est. speed input: 2583.97 toks/s, output: 73.25 toks/s]
[2025-01-11 03:34:19,590][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 12.57it/s, est. speed input: 3907.09 toks/s, output: 120.42 toks/s]
[2025-01-11 03:34:19,792][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.73it/s, est. speed input: 4275.71 toks/s, output: 143.72 toks/s]
WARNING 01-11 03:34:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:20,010][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:21,067][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:01<00:01,  1.06s/it, est. speed input: 663.05 toks/s, output: 35.94 toks/s]
[2025-01-11 03:34:21,068][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:01<00:00,  1.89it/s, est. speed input: 1325.64 toks/s, output: 71.86 toks/s]
WARNING 01-11 03:34:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:21,337][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:24,228][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:02<01:23,  2.89s/it, est. speed input: 227.63 toks/s, output: 3.81 toks/s]
[2025-01-11 03:34:25,322][root][ERROR] - Processed prompts:   7%|6         | 2/30 [00:03<00:51,  1.83s/it, est. speed input: 328.75 toks/s, output: 10.29 toks/s]
[2025-01-11 03:34:25,441][root][ERROR] - Processed prompts:  67%|######6   | 20/30 [00:04<00:01,  8.34it/s, est. speed input: 3189.26 toks/s, output: 146.44 toks/s]
[2025-01-11 03:34:25,646][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00, 12.90it/s, est. speed input: 4556.70 toks/s, output: 219.32 toks/s]
[2025-01-11 03:34:25,646][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.96it/s, est. speed input: 4556.70 toks/s, output: 219.32 toks/s]
WARNING 01-11 03:34:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:25,914][root][ERROR] - Processed prompts:   0%|          | 0/29 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:30,517][root][ERROR] - Processed prompts:   3%|3         | 1/29 [00:04<02:08,  4.60s/it, est. speed input: 198.57 toks/s, output: 6.30 toks/s]
[2025-01-11 03:34:30,518][root][ERROR] - Processed prompts: 100%|##########| 29/29 [00:04<00:00,  6.30it/s, est. speed input: 5661.08 toks/s, output: 182.67 toks/s]
WARNING 01-11 03:34:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:30,738][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:31,784][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.05s/it, est. speed input: 642.66 toks/s, output: 29.65 toks/s]
[2025-01-11 03:34:31,801][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.82it/s, est. speed input: 1952.02 toks/s, output: 88.43 toks/s]
WARNING 01-11 03:34:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:32,171][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:33,259][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.09s/it, est. speed input: 919.46 toks/s, output: 26.66 toks/s]
[2025-01-11 03:34:33,259][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.76it/s, est. speed input: 2678.17 toks/s, output: 79.96 toks/s]
WARNING 01-11 03:34:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:34:33,491][root][ERROR] - Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:34:34,552][root][ERROR] - Processed prompts:  33%|###3      | 1/3 [00:01<00:02,  1.06s/it, est. speed input: 828.58 toks/s, output: 27.34 toks/s]
[2025-01-11 03:34:34,569][root][ERROR] - Processed prompts: 100%|##########| 3/3 [00:01<00:00,  2.78it/s, est. speed input: 2474.19 toks/s, output: 81.61 toks/s]
[2025-01-11 03:34:37,319][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:34:37,373][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:34:39,097][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 03:34:40,784][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 03:34:42,397][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:34:42,942][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:34:42,943][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 03:35:02,821][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:35:02,873][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:35:04,786][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 03:35:06,393][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 03:35:07,932][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 03:35:08,442][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:35:08,443][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 03:35:24 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:35:24 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:35:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:35:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:35:25,165][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:35:26,514][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:35:26,894][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:35:28,190][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:35:29,516][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:35:29,516][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:35:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:35:43 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:35:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:35:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:36:05 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:36:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:36:06,048][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:36:09,898][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.85s/it, est. speed input: 164.94 toks/s, output: 4.16 toks/s]
[2025-01-11 03:36:10,016][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.32it/s, est. speed input: 640.24 toks/s, output: 16.89 toks/s]
[2025-01-11 03:36:10,116][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  4.15it/s, est. speed input: 1561.22 toks/s, output: 43.76 toks/s]
[2025-01-11 03:36:10,230][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 11.53it/s, est. speed input: 3341.12 toks/s, output: 103.80 toks/s]
[2025-01-11 03:36:10,355][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.09it/s, est. speed input: 4275.88 toks/s, output: 142.10 toks/s]
[2025-01-11 03:36:10,548][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.11it/s, est. speed input: 4516.13 toks/s, output: 160.46 toks/s]
WARNING 01-11 03:36:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:36:10,826][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:36:15,001][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.17s/it, est. speed input: 156.90 toks/s, output: 6.95 toks/s]
[2025-01-11 03:36:15,120][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:23,  1.22it/s, est. speed input: 613.03 toks/s, output: 27.95 toks/s]
[2025-01-11 03:36:15,226][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 13.24it/s, est. speed input: 4627.36 toks/s, output: 223.43 toks/s]
[2025-01-11 03:36:15,226][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.27it/s, est. speed input: 4776.23 toks/s, output: 231.38 toks/s]
WARNING 01-11 03:36:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:36:15,505][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:36:20,604][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 183.18 toks/s, output: 5.69 toks/s]
[2025-01-11 03:36:20,605][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5763.84 toks/s, output: 181.97 toks/s]
[2025-01-11 03:36:23,480][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:36:23,533][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:36:25,135][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 03:36:26,811][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:36:28,387][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 03:36:28,922][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:36:28,922][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:36:49,040][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:36:49,091][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:36:51,012][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 03:36:52,628][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 03:36:54,194][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:36:54,700][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:36:54,700][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 03:37:10 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:37:10 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:37:10 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:37:10 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:37:10,929][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:37:12,252][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:37:12,630][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 03:37:13,929][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 03:37:15,254][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 03:37:15,254][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:37:15 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:37:29 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:37:29 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:37:29 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:37:51 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:37:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:37:51,711][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:37:55,782][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.07s/it, est. speed input: 156.00 toks/s, output: 3.93 toks/s]
[2025-01-11 03:37:55,900][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.25it/s, est. speed input: 606.30 toks/s, output: 16.23 toks/s]
[2025-01-11 03:37:56,012][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  8.03it/s, est. speed input: 2805.04 toks/s, output: 79.98 toks/s]
[2025-01-11 03:37:56,144][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.20it/s, est. speed input: 3867.27 toks/s, output: 118.19 toks/s]
[2025-01-11 03:37:56,358][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.89it/s, est. speed input: 4372.57 toks/s, output: 146.97 toks/s]
WARNING 01-11 03:37:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:37:56,609][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:37:59,489][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.88s/it, est. speed input: 227.46 toks/s, output: 2.78 toks/s]
[2025-01-11 03:38:00,754][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.93s/it, est. speed input: 315.29 toks/s, output: 8.93 toks/s]
[2025-01-11 03:38:00,867][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:12,  2.15it/s, est. speed input: 919.93 toks/s, output: 36.87 toks/s]
[2025-01-11 03:38:00,963][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.35it/s, est. speed input: 4817.19 toks/s, output: 226.46 toks/s]
WARNING 01-11 03:38:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:38:01,219][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:38:06,290][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.07s/it, est. speed input: 180.24 toks/s, output: 5.72 toks/s]
[2025-01-11 03:38:06,291][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.31it/s, est. speed input: 5750.66 toks/s, output: 182.97 toks/s]
WARNING 01-11 03:38:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:38:06,692][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:38:07,481][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1106.30 toks/s, output: 36.75 toks/s]
[2025-01-11 03:38:07,481][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1106.30 toks/s, output: 36.75 toks/s]
[2025-01-11 03:38:10,179][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:38:10,232][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:38:11,836][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 03:38:13,471][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.62s/it]
[2025-01-11 03:38:15,060][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 03:38:15,583][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 03:38:15,584][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 03:38:35,710][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:38:35,762][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:38:37,709][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 03:38:39,381][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 03:38:41,098][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.75s/it]
[2025-01-11 03:38:41,599][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 03:38:41,600][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.46s/it]
WARNING 01-11 03:38:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:38:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:38:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:38:57 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:38:57,899][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:38:59,221][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:38:59,626][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:39:00,917][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:39:02,259][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:39:02,259][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:39:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:39:16 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:39:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:39:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:39:38 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:39:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:38,771][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:42,828][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.06s/it, est. speed input: 156.56 toks/s, output: 3.94 toks/s]
[2025-01-11 03:39:42,943][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.59it/s, est. speed input: 761.07 toks/s, output: 20.14 toks/s]
[2025-01-11 03:39:43,088][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:05,  4.20it/s, est. speed input: 1618.32 toks/s, output: 45.41 toks/s]
[2025-01-11 03:39:43,210][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.27it/s, est. speed input: 3576.53 toks/s, output: 113.32 toks/s]
[2025-01-11 03:39:43,461][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 15.03it/s, est. speed input: 4332.90 toks/s, output: 153.74 toks/s]
[2025-01-11 03:39:43,462][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.82it/s, est. speed input: 4332.90 toks/s, output: 153.74 toks/s]
WARNING 01-11 03:39:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:43,724][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:47,155][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:46,  3.43s/it, est. speed input: 190.04 toks/s, output: 4.95 toks/s]
[2025-01-11 03:39:47,878][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.84s/it, est. speed input: 314.65 toks/s, output: 11.07 toks/s]
[2025-01-11 03:39:47,990][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.26it/s, est. speed input: 921.18 toks/s, output: 38.91 toks/s]
[2025-01-11 03:39:48,095][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 14.95it/s, est. speed input: 4206.69 toks/s, output: 196.99 toks/s]
[2025-01-11 03:39:48,095][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.32it/s, est. speed input: 4807.22 toks/s, output: 229.01 toks/s]
WARNING 01-11 03:39:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:48,360][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:53,314][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 185.11 toks/s, output: 5.85 toks/s]
[2025-01-11 03:39:53,314][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.26it/s, est. speed input: 5712.10 toks/s, output: 181.45 toks/s]
WARNING 01-11 03:39:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:53,523][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:54,355][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 852.74 toks/s, output: 39.69 toks/s]
[2025-01-11 03:39:54,355][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 852.74 toks/s, output: 39.69 toks/s]
WARNING 01-11 03:39:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:54,730][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:55,515][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1163.46 toks/s, output: 36.95 toks/s]
[2025-01-11 03:39:55,515][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1163.46 toks/s, output: 36.95 toks/s]
WARNING 01-11 03:39:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:39:55,723][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:39:56,516][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1109.23 toks/s, output: 37.86 toks/s]
[2025-01-11 03:39:56,516][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1109.23 toks/s, output: 37.86 toks/s]
[2025-01-11 03:39:59,249][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:39:59,303][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:40:00,959][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 03:40:02,572][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:40:04,245][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 03:40:04,769][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:40:04,769][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:40:24,709][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:40:24,760][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:40:26,759][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  2.00s/it]
[2025-01-11 03:40:28,336][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 03:40:29,911][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 03:40:30,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:40:30,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:40:45 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:40:45 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:40:46 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:40:46 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:40:46,834][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:40:48,188][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:40:48,569][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:40:49,927][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[2025-01-11 03:40:51,390][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
[2025-01-11 03:40:51,390][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
INFO 01-11 03:40:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:41:05 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:41:05 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:41:05 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:41:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:41:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:41:28,278][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:41:32,454][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 152.06 toks/s, output: 3.83 toks/s]
[2025-01-11 03:41:32,571][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:23,  1.22it/s, est. speed input: 591.63 toks/s, output: 15.61 toks/s]
[2025-01-11 03:41:32,721][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.77it/s, est. speed input: 1429.19 toks/s, output: 40.51 toks/s]
[2025-01-11 03:41:32,862][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.38it/s, est. speed input: 3047.65 toks/s, output: 97.30 toks/s]
[2025-01-11 03:41:32,977][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 14.74it/s, est. speed input: 3918.80 toks/s, output: 134.28 toks/s]
[2025-01-11 03:41:33,164][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.55it/s, est. speed input: 4158.89 toks/s, output: 152.07 toks/s]
WARNING 01-11 03:41:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:41:33,419][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:41:36,482][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:34,  3.06s/it, est. speed input: 212.87 toks/s, output: 3.59 toks/s]
[2025-01-11 03:41:37,446][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:54,  1.83s/it, est. speed input: 325.79 toks/s, output: 9.44 toks/s]
[2025-01-11 03:41:37,622][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.07s/it, est. speed input: 467.21 toks/s, output: 16.18 toks/s]
[2025-01-11 03:41:37,739][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 12.78it/s, est. speed input: 3654.75 toks/s, output: 167.12 toks/s]
[2025-01-11 03:41:37,776][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4827.03 toks/s, output: 227.19 toks/s]
WARNING 01-11 03:41:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:41:38,059][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:41:43,111][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:36,  5.05s/it, est. speed input: 182.32 toks/s, output: 5.74 toks/s]
[2025-01-11 03:41:43,157][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5702.74 toks/s, output: 182.46 toks/s]
WARNING 01-11 03:41:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:41:43,548][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:41:44,521][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.03it/s, est. speed input: 897.16 toks/s, output: 31.86 toks/s]
[2025-01-11 03:41:44,521][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.05it/s, est. speed input: 1816.28 toks/s, output: 63.69 toks/s]
[2025-01-11 03:41:47,279][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:41:47,332][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:41:48,966][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.63s/it]
[2025-01-11 03:41:50,596][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:41:52,262][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 03:41:52,784][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:41:52,785][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:42:12,859][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:42:12,911][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:42:14,801][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 03:42:16,478][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:42:18,106][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 03:42:18,613][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:42:18,614][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:42:34 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:42:34 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:42:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:42:34 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:42:35,113][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:42:36,461][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:42:36,840][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 03:42:38,137][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:42:39,483][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:42:39,483][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:42:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:42:53 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:42:54 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:42:54 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:43:15 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:43:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:43:15,840][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:43:19,898][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.06s/it, est. speed input: 156.50 toks/s, output: 3.94 toks/s]
[2025-01-11 03:43:20,015][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.25it/s, est. speed input: 608.46 toks/s, output: 16.05 toks/s]
[2025-01-11 03:43:20,150][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.71it/s, est. speed input: 2062.86 toks/s, output: 58.71 toks/s]
[2025-01-11 03:43:20,258][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 14.42it/s, est. speed input: 4168.48 toks/s, output: 132.65 toks/s]
[2025-01-11 03:43:20,479][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.90it/s, est. speed input: 4381.41 toks/s, output: 147.48 toks/s]
WARNING 01-11 03:43:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:43:20,726][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:43:23,603][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.88s/it, est. speed input: 227.66 toks/s, output: 2.78 toks/s]
[2025-01-11 03:43:24,867][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.93s/it, est. speed input: 315.14 toks/s, output: 8.93 toks/s]
[2025-01-11 03:43:24,983][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:15,  1.74it/s, est. speed input: 768.03 toks/s, output: 30.07 toks/s]
[2025-01-11 03:43:25,090][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 15.01it/s, est. speed input: 4354.84 toks/s, output: 204.16 toks/s]
[2025-01-11 03:43:25,091][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.33it/s, est. speed input: 4805.74 toks/s, output: 228.21 toks/s]
WARNING 01-11 03:43:25 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:43:25,355][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:43:30,421][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.07s/it, est. speed input: 181.19 toks/s, output: 5.72 toks/s]
[2025-01-11 03:43:30,422][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.31it/s, est. speed input: 5756.02 toks/s, output: 183.13 toks/s]
WARNING 01-11 03:43:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:43:30,829][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:43:31,631][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1088.85 toks/s, output: 36.17 toks/s]
[2025-01-11 03:43:31,631][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1088.85 toks/s, output: 36.17 toks/s]
[2025-01-11 03:43:34,400][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:43:34,453][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:43:36,103][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 03:43:37,755][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:43:39,318][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 03:43:39,851][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:43:39,851][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:43:59,931][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:43:59,983][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:44:01,971][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.99s/it]
[2025-01-11 03:44:03,604][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 03:44:05,186][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 03:44:05,687][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:44:05,688][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:44:21 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:44:21 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:44:22 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:44:22 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:44:22,230][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:44:23,572][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 03:44:23,980][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 03:44:25,304][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 03:44:26,658][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
[2025-01-11 03:44:26,658][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 03:44:26 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:44:40 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:44:41 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:44:41 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:45:02 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:45:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:03,292][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:06,767][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:47,  3.48s/it, est. speed input: 182.72 toks/s, output: 2.88 toks/s]
[2025-01-11 03:45:07,128][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:03<00:49,  1.64s/it, est. speed input: 331.05 toks/s, output: 6.78 toks/s]
[2025-01-11 03:45:07,239][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:03<00:10,  2.51it/s, est. speed input: 965.31 toks/s, output: 23.56 toks/s]
[2025-01-11 03:45:07,374][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:02,  6.72it/s, est. speed input: 2022.12 toks/s, output: 54.63 toks/s]
[2025-01-11 03:45:07,484][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 17.62it/s, est. speed input: 4089.71 toks/s, output: 126.19 toks/s]
[2025-01-11 03:45:07,683][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.29it/s, est. speed input: 4627.47 toks/s, output: 155.77 toks/s]
WARNING 01-11 03:45:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:07,930][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:10,818][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.89s/it, est. speed input: 226.80 toks/s, output: 2.77 toks/s]
[2025-01-11 03:45:11,961][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.86s/it, est. speed input: 324.96 toks/s, output: 8.68 toks/s]
[2025-01-11 03:45:12,138][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.09s/it, est. speed input: 468.20 toks/s, output: 15.45 toks/s]
[2025-01-11 03:45:12,254][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 12.58it/s, est. speed input: 3641.08 toks/s, output: 166.28 toks/s]
[2025-01-11 03:45:12,291][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4809.56 toks/s, output: 226.30 toks/s]
WARNING 01-11 03:45:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:12,548][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:17,503][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 185.07 toks/s, output: 5.85 toks/s]
[2025-01-11 03:45:17,504][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.26it/s, est. speed input: 5700.57 toks/s, output: 181.41 toks/s]
WARNING 01-11 03:45:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:17,709][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:18,520][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 880.15 toks/s, output: 38.21 toks/s]
[2025-01-11 03:45:18,521][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 880.15 toks/s, output: 38.21 toks/s]
WARNING 01-11 03:45:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:18,894][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:19,693][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1147.90 toks/s, output: 36.30 toks/s]
[2025-01-11 03:45:19,693][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 1147.90 toks/s, output: 36.30 toks/s]
WARNING 01-11 03:45:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:45:19,898][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:45:20,681][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1115.33 toks/s, output: 37.05 toks/s]
[2025-01-11 03:45:20,682][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1115.33 toks/s, output: 37.05 toks/s]
[2025-01-11 03:45:23,536][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:45:23,589][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:45:25,329][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.74s/it]
[2025-01-11 03:45:26,996][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 03:45:28,654][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:45:29,231][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 03:45:29,231][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-11 03:45:49,285][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:45:49,337][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:45:51,225][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 03:45:52,841][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 03:45:54,382][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 03:45:54,884][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:45:54,884][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 03:46:10 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:46:10 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:46:10 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:46:10 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:46:11,028][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:46:12,350][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:46:12,752][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:46:14,040][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:46:15,362][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 03:46:15,362][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:46:15 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:46:29 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:46:29 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:46:29 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:46:51 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:46:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:46:51,646][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:46:55,412][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.77s/it, est. speed input: 168.62 toks/s, output: 2.66 toks/s]
[2025-01-11 03:46:55,774][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 307.69 toks/s, output: 6.30 toks/s]
[2025-01-11 03:46:55,885][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:11,  2.35it/s, est. speed input: 898.88 toks/s, output: 21.94 toks/s]
[2025-01-11 03:46:56,022][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:03,  6.31it/s, est. speed input: 1886.51 toks/s, output: 51.19 toks/s]
[2025-01-11 03:46:56,140][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 15.00it/s, est. speed input: 3532.83 toks/s, output: 108.15 toks/s]
[2025-01-11 03:46:56,322][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 17.99it/s, est. speed input: 4209.95 toks/s, output: 140.30 toks/s]
[2025-01-11 03:46:56,339][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.82it/s, est. speed input: 4329.72 toks/s, output: 147.24 toks/s]
WARNING 01-11 03:46:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:46:56,625][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:46:59,503][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.88s/it, est. speed input: 224.20 toks/s, output: 2.78 toks/s]
[2025-01-11 03:47:00,651][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.86s/it, est. speed input: 323.74 toks/s, output: 8.70 toks/s]
[2025-01-11 03:47:00,827][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.09s/it, est. speed input: 466.06 toks/s, output: 15.47 toks/s]
[2025-01-11 03:47:00,947][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 12.57it/s, est. speed input: 3637.20 toks/s, output: 167.06 toks/s]
[2025-01-11 03:47:00,989][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.33it/s, est. speed input: 4809.79 toks/s, output: 227.56 toks/s]
WARNING 01-11 03:47:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:47:01,272][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:47:06,343][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.07s/it, est. speed input: 180.43 toks/s, output: 5.72 toks/s]
[2025-01-11 03:47:06,344][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.31it/s, est. speed input: 5752.15 toks/s, output: 182.96 toks/s]
WARNING 01-11 03:47:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:47:06,738][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:47:07,529][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1090.72 toks/s, output: 36.65 toks/s]
[2025-01-11 03:47:07,530][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1090.72 toks/s, output: 36.65 toks/s]
[2025-01-11 03:47:10,379][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:47:10,433][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:47:12,038][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.60s/it]
[2025-01-11 03:47:13,697][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 03:47:15,314][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:47:15,842][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 03:47:15,842][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 03:47:35,979][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:47:36,031][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:47:37,943][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 03:47:39,554][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 03:47:41,132][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:47:41,633][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:47:41,633][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 03:47:57 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:47:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:47:57 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:47:57 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:47:57,769][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:47:59,092][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 03:47:59,467][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 03:48:00,762][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 03:48:02,093][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:48:02,094][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:48:02 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:48:16 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:48:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:48:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:48:38 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:48:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:48:38,485][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:48:42,194][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:54,  3.71s/it, est. speed input: 171.21 toks/s, output: 2.70 toks/s]
[2025-01-11 03:48:42,490][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:31,  1.07s/it, est. speed input: 475.72 toks/s, output: 8.99 toks/s]
[2025-01-11 03:48:42,649][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.61it/s, est. speed input: 1372.59 toks/s, output: 32.18 toks/s]
[2025-01-11 03:48:42,781][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  7.54it/s, est. speed input: 2365.05 toks/s, output: 62.85 toks/s]
[2025-01-11 03:48:42,900][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 16.99it/s, est. speed input: 4171.45 toks/s, output: 125.72 toks/s]
[2025-01-11 03:48:43,058][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.00it/s, est. speed input: 4443.92 toks/s, output: 141.28 toks/s]
WARNING 01-11 03:48:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:48:43,309][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:48:47,171][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.86s/it, est. speed input: 169.83 toks/s, output: 6.21 toks/s]
[2025-01-11 03:48:47,534][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:54,  1.80s/it, est. speed input: 309.32 toks/s, output: 12.78 toks/s]
[2025-01-11 03:48:47,667][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01,  8.86it/s, est. speed input: 3148.89 toks/s, output: 148.93 toks/s]
[2025-01-11 03:48:47,721][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.25it/s, est. speed input: 4746.82 toks/s, output: 230.52 toks/s]
WARNING 01-11 03:48:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:48:48,003][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:48:53,128][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.12s/it, est. speed input: 179.12 toks/s, output: 5.66 toks/s]
[2025-01-11 03:48:53,129][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.24it/s, est. speed input: 5719.51 toks/s, output: 181.04 toks/s]
[2025-01-11 03:48:56,190][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:48:56,243][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:48:57,894][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 03:48:59,550][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 03:49:01,160][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 03:49:01,686][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 03:49:01,687][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 03:49:21,804][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:49:21,855][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:49:23,768][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 03:49:25,403][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 03:49:27,008][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:49:27,528][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:49:27,528][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:49:42 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:49:42 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:49:43 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:49:43 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:49:43,638][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:49:44,986][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:49:45,364][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 03:49:46,662][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:49:47,992][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:49:47,993][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:49:48 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:50:02 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:50:02 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:50:02 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:50:24 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:50:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:50:24,991][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:50:28,789][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 167.17 toks/s, output: 2.63 toks/s]
[2025-01-11 03:50:29,144][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.12s/it, est. speed input: 458.72 toks/s, output: 8.91 toks/s]
[2025-01-11 03:50:29,285][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.17it/s, est. speed input: 2218.27 toks/s, output: 56.36 toks/s]
[2025-01-11 03:50:29,392][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.72it/s, est. speed input: 3606.87 toks/s, output: 102.70 toks/s]
[2025-01-11 03:50:29,630][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 14.70it/s, est. speed input: 4380.46 toks/s, output: 136.24 toks/s]
[2025-01-11 03:50:29,630][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.90it/s, est. speed input: 4380.46 toks/s, output: 136.24 toks/s]
WARNING 01-11 03:50:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:50:29,880][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:50:32,754][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.87s/it, est. speed input: 227.88 toks/s, output: 2.78 toks/s]
[2025-01-11 03:50:33,959][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:56,  1.89s/it, est. speed input: 320.39 toks/s, output: 8.82 toks/s]
[2025-01-11 03:50:34,075][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:20,  1.35it/s, est. speed input: 620.26 toks/s, output: 22.41 toks/s]
[2025-01-11 03:50:34,189][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 13.68it/s, est. speed input: 3941.90 toks/s, output: 181.49 toks/s]
[2025-01-11 03:50:34,228][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.36it/s, est. speed input: 4813.02 toks/s, output: 226.53 toks/s]
WARNING 01-11 03:50:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:50:34,511][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:50:39,588][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.08s/it, est. speed input: 180.63 toks/s, output: 5.71 toks/s]
[2025-01-11 03:50:39,773][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  8.48it/s, est. speed input: 5532.87 toks/s, output: 178.44 toks/s]
[2025-01-11 03:50:39,774][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.08it/s, est. speed input: 5532.87 toks/s, output: 178.44 toks/s]
WARNING 01-11 03:50:40 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:50:40,200][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:50:40,941][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1177.48 toks/s, output: 33.72 toks/s]
[2025-01-11 03:50:40,941][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1177.48 toks/s, output: 33.72 toks/s]
WARNING 01-11 03:50:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:50:41,154][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:50:41,884][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 1292.07 toks/s, output: 34.25 toks/s]
[2025-01-11 03:50:41,884][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 1292.07 toks/s, output: 34.25 toks/s]
[2025-01-11 03:50:44,743][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:50:44,796][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:50:46,529][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.73s/it]
[2025-01-11 03:50:48,198][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 03:50:49,891][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 03:50:50,426][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 03:50:50,427][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
[2025-01-11 03:51:10,386][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:51:10,438][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:51:12,358][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 03:51:14,042][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 03:51:15,603][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:51:16,136][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:51:16,136][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:51:31 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:51:31 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:51:32 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:51:32 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:51:32,524][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:51:33,839][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 03:51:34,245][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 03:51:35,535][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 03:51:36,882][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:51:36,882][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:51:37 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:51:50 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:51:51 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:51:51 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:52:13 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:52:13 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:52:13,555][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:52:17,740][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.19s/it, est. speed input: 151.72 toks/s, output: 3.82 toks/s]
[2025-01-11 03:52:17,857][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:23,  1.21it/s, est. speed input: 590.37 toks/s, output: 15.57 toks/s]
[2025-01-11 03:52:17,997][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.54it/s, est. speed input: 2001.42 toks/s, output: 57.63 toks/s]
[2025-01-11 03:52:18,118][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.58it/s, est. speed input: 3479.45 toks/s, output: 109.37 toks/s]
[2025-01-11 03:52:18,370][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 14.39it/s, est. speed input: 4220.28 toks/s, output: 149.12 toks/s]
[2025-01-11 03:52:18,370][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.65it/s, est. speed input: 4220.28 toks/s, output: 149.12 toks/s]
WARNING 01-11 03:52:18 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:52:18,624][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:52:22,806][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 157.82 toks/s, output: 6.93 toks/s]
[2025-01-11 03:52:22,926][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.13s/it, est. speed input: 457.25 toks/s, output: 20.92 toks/s]
[2025-01-11 03:52:23,008][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.30it/s, est. speed input: 4793.24 toks/s, output: 231.08 toks/s]
WARNING 01-11 03:52:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:52:23,271][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:52:28,384][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.11s/it, est. speed input: 178.96 toks/s, output: 5.67 toks/s]
[2025-01-11 03:52:28,384][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.26it/s, est. speed input: 5747.42 toks/s, output: 181.48 toks/s]
[2025-01-11 03:52:31,470][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:52:31,524][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:52:33,212][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 03:52:34,916][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 03:52:36,528][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 03:52:37,077][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:52:37,078][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 03:52:57,185][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:52:57,236][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:52:59,175][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 03:53:00,832][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 03:53:02,438][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 03:53:02,958][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:53:02,959][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:53:18 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:53:18 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:53:18 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:53:18 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:53:19,081][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:53:20,395][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 03:53:20,770][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 03:53:22,055][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-01-11 03:53:23,382][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 03:53:23,383][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 03:53:23 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:53:37 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:53:38 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:53:38 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:53:59 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:53:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:53:59,703][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:03,642][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:02,  3.94s/it, est. speed input: 161.22 toks/s, output: 4.06 toks/s]
[2025-01-11 03:54:03,757][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:16,  1.64it/s, est. speed input: 783.32 toks/s, output: 20.48 toks/s]
[2025-01-11 03:54:03,891][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.73it/s, est. speed input: 2122.69 toks/s, output: 59.93 toks/s]
[2025-01-11 03:54:04,015][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 14.52it/s, est. speed input: 4270.71 toks/s, output: 134.74 toks/s]
[2025-01-11 03:54:04,216][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.09it/s, est. speed input: 4503.48 toks/s, output: 150.48 toks/s]
WARNING 01-11 03:54:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:54:04,487][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:07,358][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.87s/it, est. speed input: 228.13 toks/s, output: 2.79 toks/s]
[2025-01-11 03:54:08,623][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.93s/it, est. speed input: 316.23 toks/s, output: 8.95 toks/s]
[2025-01-11 03:54:08,736][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:12,  2.16it/s, est. speed input: 924.49 toks/s, output: 36.95 toks/s]
[2025-01-11 03:54:08,835][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.36it/s, est. speed input: 4823.02 toks/s, output: 227.22 toks/s]
WARNING 01-11 03:54:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:54:09,099][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:14,063][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.96s/it, est. speed input: 184.74 toks/s, output: 5.84 toks/s]
[2025-01-11 03:54:14,064][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.24it/s, est. speed input: 5690.41 toks/s, output: 181.09 toks/s]
WARNING 01-11 03:54:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:54:14,274][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:15,078][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 882.85 toks/s, output: 38.55 toks/s]
[2025-01-11 03:54:15,078][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 882.85 toks/s, output: 38.55 toks/s]
WARNING 01-11 03:54:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:54:15,460][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:16,248][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1158.74 toks/s, output: 36.80 toks/s]
[2025-01-11 03:54:16,248][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1158.74 toks/s, output: 36.80 toks/s]
WARNING 01-11 03:54:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:54:16,457][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:54:17,292][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1045.99 toks/s, output: 38.34 toks/s]
[2025-01-11 03:54:17,292][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 1045.99 toks/s, output: 38.34 toks/s]
[2025-01-11 03:54:20,185][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:54:20,238][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:54:21,853][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 03:54:23,554][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 03:54:25,159][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 03:54:25,713][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:54:25,714][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 03:54:45,784][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:54:45,836][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:54:47,784][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 03:54:49,410][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 03:54:51,008][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:54:51,537][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 03:54:51,538][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 03:55:07 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:55:07 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:55:07 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:55:07 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:55:07,989][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:55:09,322][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 03:55:09,725][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 03:55:11,028][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:55:12,381][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 03:55:12,381][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 03:55:12 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:55:26 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:55:26 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:55:26 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:55:49 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:55:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:55:49,350][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:55:53,152][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:57,  3.80s/it, est. speed input: 167.03 toks/s, output: 4.21 toks/s]
[2025-01-11 03:55:53,269][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:03<00:21,  1.33it/s, est. speed input: 648.13 toks/s, output: 17.10 toks/s]
[2025-01-11 03:55:53,403][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.54it/s, est. speed input: 2350.38 toks/s, output: 67.36 toks/s]
[2025-01-11 03:55:53,543][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 13.49it/s, est. speed input: 4089.23 toks/s, output: 129.27 toks/s]
[2025-01-11 03:55:53,833][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.14it/s, est. speed input: 4533.10 toks/s, output: 161.07 toks/s]
WARNING 01-11 03:55:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:55:54,104][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:55:58,360][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.26s/it, est. speed input: 153.20 toks/s, output: 7.05 toks/s]
[2025-01-11 03:55:58,462][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:03,  5.10it/s, est. speed input: 2407.33 toks/s, output: 113.81 toks/s]
[2025-01-11 03:55:58,504][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.27it/s, est. speed input: 4775.17 toks/s, output: 230.67 toks/s]
WARNING 01-11 03:55:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:55:58,789][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:56:03,906][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.12s/it, est. speed input: 179.43 toks/s, output: 5.67 toks/s]
[2025-01-11 03:56:03,906][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.25it/s, est. speed input: 5744.17 toks/s, output: 181.35 toks/s]
[2025-01-11 03:56:06,972][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:56:07,025][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:56:08,800][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.77s/it]
[2025-01-11 03:56:10,991][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:04,  2.02s/it]
[2025-01-11 03:56:12,701][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.88s/it]
[2025-01-11 03:56:13,230][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.35s/it]
[2025-01-11 03:56:13,230][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.55s/it]
[2025-01-11 03:56:33,549][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:56:33,603][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:56:35,533][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 03:56:37,201][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.78s/it]
[2025-01-11 03:56:38,767][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 03:56:39,284][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 03:56:39,285][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 03:56:54 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:56:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:56:55 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:56:55 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:56:55,511][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:56:56,861][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 03:56:57,329][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
[2025-01-11 03:56:58,645][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.05s/it]
[2025-01-11 03:57:00,058][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
[2025-01-11 03:57:00,058][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
INFO 01-11 03:57:00 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:57:14 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:57:14 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:57:14 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:57:36 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 03:57:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:57:36,419][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:57:40,424][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:04,  4.00s/it, est. speed input: 158.58 toks/s, output: 4.00 toks/s]
[2025-01-11 03:57:40,541][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.27it/s, est. speed input: 616.29 toks/s, output: 16.26 toks/s]
[2025-01-11 03:57:40,641][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  4.01it/s, est. speed input: 1504.18 toks/s, output: 42.16 toks/s]
[2025-01-11 03:57:40,748][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 12.45it/s, est. speed input: 3520.75 toks/s, output: 109.73 toks/s]
[2025-01-11 03:57:41,105][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 14.85it/s, est. speed input: 4336.89 toks/s, output: 158.79 toks/s]
[2025-01-11 03:57:41,105][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.83it/s, est. speed input: 4336.89 toks/s, output: 158.79 toks/s]
WARNING 01-11 03:57:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:57:41,351][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:57:45,595][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 153.40 toks/s, output: 7.07 toks/s]
[2025-01-11 03:57:45,721][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00,  8.30it/s, est. speed input: 3912.70 toks/s, output: 186.51 toks/s]
[2025-01-11 03:57:45,758][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.26it/s, est. speed input: 4773.14 toks/s, output: 230.77 toks/s]
WARNING 01-11 03:57:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:57:46,024][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:57:51,133][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.11s/it, est. speed input: 179.49 toks/s, output: 5.68 toks/s]
[2025-01-11 03:57:51,134][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.26it/s, est. speed input: 5756.69 toks/s, output: 181.61 toks/s]
[2025-01-11 03:57:54,206][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:57:54,260][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:57:56,139][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.88s/it]
[2025-01-11 03:57:57,731][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-01-11 03:57:59,392][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 03:57:59,942][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 03:57:59,942][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-11 03:58:19,978][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:58:20,030][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:58:21,918][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 03:58:23,545][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 03:58:25,084][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 03:58:25,619][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 03:58:25,619][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 03:58:41 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 03:58:41 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 03:58:41 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 03:58:41 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 03:58:41,720][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:58:43,056][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 03:58:43,461][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 03:58:44,755][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 03:58:46,082][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 03:58:46,083][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 03:58:46 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 03:59:00 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 03:59:00 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 03:59:00 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 03:59:22 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 03:59:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:59:22,835][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:59:26,921][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.09s/it, est. speed input: 155.43 toks/s, output: 3.92 toks/s]
[2025-01-11 03:59:27,036][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.58it/s, est. speed input: 755.76 toks/s, output: 19.99 toks/s]
[2025-01-11 03:59:27,167][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:02,  6.00it/s, est. speed input: 2198.78 toks/s, output: 62.33 toks/s]
[2025-01-11 03:59:27,295][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 13.29it/s, est. speed input: 3986.33 toks/s, output: 124.88 toks/s]
[2025-01-11 03:59:27,555][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.78it/s, est. speed input: 4304.78 toks/s, output: 146.39 toks/s]
WARNING 01-11 03:59:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:59:27,813][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:59:31,861][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.05s/it, est. speed input: 161.82 toks/s, output: 6.67 toks/s]
[2025-01-11 03:59:32,043][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:53,  1.77s/it, est. speed input: 309.74 toks/s, output: 13.48 toks/s]
[2025-01-11 03:59:32,175][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.02it/s, est. speed input: 4511.01 toks/s, output: 214.83 toks/s]
[2025-01-11 03:59:32,175][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4810.40 toks/s, output: 230.41 toks/s]
WARNING 01-11 03:59:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 03:59:32,472][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 03:59:37,572][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 179.22 toks/s, output: 5.69 toks/s]
[2025-01-11 03:59:37,573][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5756.67 toks/s, output: 181.94 toks/s]
[2025-01-11 03:59:40,649][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 03:59:40,701][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 03:59:42,356][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 03:59:43,967][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 03:59:45,566][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 03:59:46,088][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 03:59:46,088][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 04:00:06,414][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:00:06,467][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:00:08,402][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 04:00:10,028][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 04:00:11,598][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 04:00:12,122][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:00:12,122][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:00:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:00:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:00:28 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:00:28 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:00:28,725][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:00:30,044][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 04:00:30,419][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 04:00:31,741][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:00:33,183][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
[2025-01-11 04:00:33,183][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
INFO 01-11 04:00:33 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:00:47 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:00:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:00:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:01:09 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:01:09 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:01:09,464][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:01:13,631][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.17s/it, est. speed input: 152.41 toks/s, output: 3.84 toks/s]
[2025-01-11 04:01:13,748][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.22it/s, est. speed input: 592.94 toks/s, output: 15.64 toks/s]
[2025-01-11 04:01:13,848][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.86it/s, est. speed input: 1448.44 toks/s, output: 40.60 toks/s]
[2025-01-11 04:01:13,961][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.79it/s, est. speed input: 3106.36 toks/s, output: 96.28 toks/s]
[2025-01-11 04:01:14,263][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 13.34it/s, est. speed input: 3837.55 toks/s, output: 132.95 toks/s]
[2025-01-11 04:01:14,572][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.26it/s, est. speed input: 3977.92 toks/s, output: 149.95 toks/s]
WARNING 01-11 04:01:14 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:01:14,824][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:01:19,059][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 153.95 toks/s, output: 7.08 toks/s]
[2025-01-11 04:01:19,175][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00,  8.99it/s, est. speed input: 4236.89 toks/s, output: 201.12 toks/s]
[2025-01-11 04:01:19,197][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.32it/s, est. speed input: 4815.57 toks/s, output: 231.21 toks/s]
WARNING 01-11 04:01:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:01:19,462][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:01:24,568][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.11s/it, est. speed input: 179.60 toks/s, output: 5.68 toks/s]
[2025-01-11 04:01:24,569][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5764.62 toks/s, output: 181.72 toks/s]
[2025-01-11 04:01:27,623][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:01:27,676][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:01:29,331][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 04:01:31,018][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 04:01:32,640][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:01:33,195][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:01:33,195][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:01:53,371][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:01:53,423][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:01:55,449][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.02s/it]
[2025-01-11 04:01:57,139][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.83s/it]
[2025-01-11 04:01:58,777][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.74s/it]
[2025-01-11 04:01:59,303][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.26s/it]
[2025-01-11 04:01:59,303][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.47s/it]
WARNING 01-11 04:02:14 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:02:14 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:02:15 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:02:15 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:02:15,667][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:02:17,011][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 04:02:17,383][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 04:02:18,676][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 04:02:20,000][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:02:20,000][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:02:20 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:02:34 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:02:34 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:02:34 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:02:55 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:02:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:02:56,223][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:03:00,318][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.09s/it, est. speed input: 155.08 toks/s, output: 3.91 toks/s]
[2025-01-11 04:03:00,438][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.11s/it, est. speed input: 451.98 toks/s, output: 12.10 toks/s]
[2025-01-11 04:03:00,579][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.88it/s, est. speed input: 1749.61 toks/s, output: 49.82 toks/s]
[2025-01-11 04:03:00,684][root][ERROR] - Processed prompts:  62%|######2   | 20/32 [00:04<00:01,  9.37it/s, est. speed input: 2847.28 toks/s, output: 87.44 toks/s]
[2025-01-11 04:03:01,020][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 11.27it/s, est. speed input: 3442.10 toks/s, output: 121.76 toks/s]
[2025-01-11 04:03:01,126][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.69it/s, est. speed input: 3885.90 toks/s, output: 149.52 toks/s]
[2025-01-11 04:03:01,546][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.01it/s, est. speed input: 3817.88 toks/s, output: 157.83 toks/s]
WARNING 01-11 04:03:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:03:01,809][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:03:05,916][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.11s/it, est. speed input: 158.77 toks/s, output: 6.82 toks/s]
[2025-01-11 04:03:06,037][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 309.15 toks/s, output: 13.72 toks/s]
[2025-01-11 04:03:06,150][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.20it/s, est. speed input: 4562.99 toks/s, output: 216.07 toks/s]
[2025-01-11 04:03:06,169][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.34it/s, est. speed input: 4845.90 toks/s, output: 230.72 toks/s]
WARNING 01-11 04:03:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:03:06,436][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:03:11,542][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.11s/it, est. speed input: 182.34 toks/s, output: 5.68 toks/s]
[2025-01-11 04:03:11,543][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5779.03 toks/s, output: 181.72 toks/s]
[2025-01-11 04:03:14,628][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:03:14,681][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:03:16,305][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 04:03:17,895][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.60s/it]
[2025-01-11 04:03:19,519][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 04:03:20,049][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:03:20,049][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.34s/it]
[2025-01-11 04:03:40,216][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:03:40,267][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:03:42,187][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 04:03:43,824][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 04:03:45,410][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 04:03:45,915][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:03:45,915][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:04:01 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:04:01 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:04:01 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:04:02 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:04:02,192][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:04:03,505][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:04:03,900][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:04:05,216][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:04:06,532][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:04:06,533][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:04:06 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:04:20 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:04:21 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:04:21 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:04:42 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:04:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:04:43,027][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:04:47,191][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.16s/it, est. speed input: 152.51 toks/s, output: 3.84 toks/s]
[2025-01-11 04:04:47,308][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.22it/s, est. speed input: 593.31 toks/s, output: 15.65 toks/s]
[2025-01-11 04:04:47,408][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.86it/s, est. speed input: 1449.35 toks/s, output: 40.63 toks/s]
[2025-01-11 04:04:47,522][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:00, 10.79it/s, est. speed input: 3107.67 toks/s, output: 96.54 toks/s]
[2025-01-11 04:04:47,718][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 14.37it/s, est. speed input: 3925.24 toks/s, output: 132.16 toks/s]
[2025-01-11 04:04:47,835][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.66it/s, est. speed input: 4226.17 toks/s, output: 151.62 toks/s]
WARNING 01-11 04:04:48 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:04:48,107][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:04:51,541][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:46,  3.43s/it, est. speed input: 189.87 toks/s, output: 4.95 toks/s]
[2025-01-11 04:04:52,265][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:55,  1.84s/it, est. speed input: 314.33 toks/s, output: 11.06 toks/s]
[2025-01-11 04:04:52,382][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:20,  1.38it/s, est. speed input: 612.13 toks/s, output: 25.03 toks/s]
[2025-01-11 04:04:52,478][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.32it/s, est. speed input: 4809.38 toks/s, output: 228.12 toks/s]
WARNING 01-11 04:04:52 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:04:52,749][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:04:57,696][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 185.38 toks/s, output: 5.86 toks/s]
[2025-01-11 04:04:57,697][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.27it/s, est. speed input: 5721.84 toks/s, output: 181.71 toks/s]
WARNING 01-11 04:04:57 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:04:57,977][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:04:58,801][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 860.13 toks/s, output: 38.82 toks/s]
[2025-01-11 04:04:58,802][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 860.13 toks/s, output: 38.82 toks/s]
WARNING 01-11 04:04:59 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:04:59,207][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:04:59,985][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1173.99 toks/s, output: 37.29 toks/s]
[2025-01-11 04:04:59,986][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1173.99 toks/s, output: 37.29 toks/s]
WARNING 01-11 04:05:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:05:00,200][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:05:00,988][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1115.24 toks/s, output: 38.06 toks/s]
[2025-01-11 04:05:00,989][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1115.24 toks/s, output: 38.06 toks/s]
[2025-01-11 04:05:03,948][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:05:04,000][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:05:05,993][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.99s/it]
[2025-01-11 04:05:07,647][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 04:05:09,247][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 04:05:09,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 04:05:09,778][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
[2025-01-11 04:05:30,044][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:05:30,096][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:05:32,063][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.97s/it]
[2025-01-11 04:05:33,658][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 04:05:35,199][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.65s/it]
[2025-01-11 04:05:35,731][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:05:35,731][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:05:51 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:05:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:05:52 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:05:52 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:05:52,302][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:05:53,662][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 04:05:54,037][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 04:05:55,326][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 04:05:56,674][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 04:05:56,675][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:05:56 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:06:10 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:06:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:06:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:06:32 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:06:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:06:33,093][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:06:37,254][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:08,  4.16s/it, est. speed input: 152.61 toks/s, output: 3.85 toks/s]
[2025-01-11 04:06:37,373][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.22it/s, est. speed input: 593.50 toks/s, output: 15.89 toks/s]
[2025-01-11 04:06:37,473][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.87it/s, est. speed input: 1449.87 toks/s, output: 40.87 toks/s]
[2025-01-11 04:06:37,588][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.18it/s, est. speed input: 2966.43 toks/s, output: 91.87 toks/s]
[2025-01-11 04:06:37,907][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.03it/s, est. speed input: 3561.64 toks/s, output: 120.69 toks/s]
[2025-01-11 04:06:37,952][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.59it/s, est. speed input: 4181.64 toks/s, output: 157.63 toks/s]
WARNING 01-11 04:06:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:06:38,214][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:06:42,443][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.23s/it, est. speed input: 154.89 toks/s, output: 7.09 toks/s]
[2025-01-11 04:06:42,556][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00,  9.99it/s, est. speed input: 4694.45 toks/s, output: 223.83 toks/s]
[2025-01-11 04:06:42,590][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.31it/s, est. speed input: 4811.37 toks/s, output: 230.09 toks/s]
WARNING 01-11 04:06:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:06:42,865][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:06:47,960][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.09s/it, est. speed input: 179.98 toks/s, output: 5.69 toks/s]
[2025-01-11 04:06:47,961][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5776.85 toks/s, output: 182.11 toks/s]
[2025-01-11 04:06:51,062][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:06:51,116][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:06:52,784][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 04:06:54,468][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 04:06:56,115][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 04:06:56,663][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:06:56,663][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 04:07:16,987][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:07:17,039][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:07:18,842][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.80s/it]
[2025-01-11 04:07:20,452][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.69s/it]
[2025-01-11 04:07:22,016][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 04:07:22,523][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:07:22,523][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
WARNING 01-11 04:07:37 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:07:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:07:38 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:07:38 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:07:38,873][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:07:40,183][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:07:40,557][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[2025-01-11 04:07:41,845][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-01-11 04:07:43,192][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:07:43,192][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:07:43 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:07:57 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:07:57 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:07:57 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:08:19 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:08:19 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:08:19,557][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:08:23,546][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.99s/it, est. speed input: 159.19 toks/s, output: 4.01 toks/s]
[2025-01-11 04:08:23,667][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:51,  1.71s/it, est. speed input: 308.99 toks/s, output: 8.27 toks/s]
[2025-01-11 04:08:23,778][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:10,  2.41it/s, est. speed input: 902.60 toks/s, output: 25.82 toks/s]
[2025-01-11 04:08:23,898][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:00, 11.47it/s, est. speed input: 3071.87 toks/s, output: 98.13 toks/s]
[2025-01-11 04:08:24,267][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 13.30it/s, est. speed input: 3774.70 toks/s, output: 135.24 toks/s]
[2025-01-11 04:08:24,268][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.79it/s, est. speed input: 4313.66 toks/s, output: 166.64 toks/s]
WARNING 01-11 04:08:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:08:24,545][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:08:27,421][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:02<01:29,  2.87s/it, est. speed input: 227.84 toks/s, output: 2.78 toks/s]
[2025-01-11 04:08:28,686][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.93s/it, est. speed input: 316.39 toks/s, output: 8.94 toks/s]
[2025-01-11 04:08:28,802][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:21,  1.32it/s, est. speed input: 617.92 toks/s, output: 22.79 toks/s]
[2025-01-11 04:08:28,871][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.40it/s, est. speed input: 4872.56 toks/s, output: 226.36 toks/s]
WARNING 01-11 04:08:29 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:08:29,155][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:08:34,237][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.08s/it, est. speed input: 180.49 toks/s, output: 5.71 toks/s]
[2025-01-11 04:08:34,237][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.30it/s, est. speed input: 5759.83 toks/s, output: 182.62 toks/s]
WARNING 01-11 04:08:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:08:34,627][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:08:35,418][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1103.94 toks/s, output: 36.67 toks/s]
[2025-01-11 04:08:35,418][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 1103.94 toks/s, output: 36.67 toks/s]
[2025-01-11 04:08:38,344][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:08:38,398][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:08:40,066][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.67s/it]
[2025-01-11 04:08:41,773][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.69s/it]
[2025-01-11 04:08:43,369][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:08:43,914][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:08:43,915][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:09:04,025][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:09:04,077][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:09:06,030][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 04:09:07,793][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.84s/it]
[2025-01-11 04:09:09,599][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.83s/it]
[2025-01-11 04:09:10,220][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.35s/it]
[2025-01-11 04:09:10,220][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.54s/it]
WARNING 01-11 04:09:25 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:09:25 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:09:26 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:09:26 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:09:26,792][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:09:28,117][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 04:09:28,495][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 04:09:29,783][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:09:31,123][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:09:31,124][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:09:31 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:09:45 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:09:45 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:09:45 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:10:07 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:10:07 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:10:07,592][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:10:11,731][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:08,  4.14s/it, est. speed input: 153.42 toks/s, output: 3.87 toks/s]
[2025-01-11 04:10:11,848][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.23it/s, est. speed input: 596.79 toks/s, output: 15.74 toks/s]
[2025-01-11 04:10:11,950][root][ERROR] - Processed prompts:  34%|###4      | 11/32 [00:04<00:04,  4.34it/s, est. speed input: 1602.93 toks/s, output: 45.67 toks/s]
[2025-01-11 04:10:12,060][root][ERROR] - Processed prompts:  66%|######5   | 21/32 [00:04<00:01, 10.08it/s, est. speed input: 2984.74 toks/s, output: 91.55 toks/s]
[2025-01-11 04:10:12,254][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 13.80it/s, est. speed input: 3814.24 toks/s, output: 125.71 toks/s]
[2025-01-11 04:10:12,373][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.69it/s, est. speed input: 4250.25 toks/s, output: 152.06 toks/s]
WARNING 01-11 04:10:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:10:12,644][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:10:16,891][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.25s/it, est. speed input: 154.22 toks/s, output: 7.06 toks/s]
[2025-01-11 04:10:16,998][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 10.29it/s, est. speed input: 4827.98 toks/s, output: 229.91 toks/s]
[2025-01-11 04:10:16,998][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.35it/s, est. speed input: 4827.98 toks/s, output: 229.91 toks/s]
WARNING 01-11 04:10:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:10:17,292][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:10:22,391][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 179.46 toks/s, output: 5.69 toks/s]
[2025-01-11 04:10:22,392][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5765.05 toks/s, output: 181.98 toks/s]
[2025-01-11 04:10:25,504][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:10:25,558][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:10:27,269][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.71s/it]
[2025-01-11 04:10:28,929][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 04:10:30,537][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:10:31,065][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:10:31,066][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:10:51,111][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:10:51,163][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:10:53,115][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 04:10:54,760][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 04:10:56,429][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.72s/it]
[2025-01-11 04:10:56,939][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 04:10:56,939][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 04:11:12 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:11:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:11:13 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:11:13 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:11:13,367][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:11:14,679][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:11:15,054][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 04:11:16,346][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
[2025-01-11 04:11:17,672][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:11:17,672][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:11:17 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:11:31 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:11:32 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:11:32 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:11:53 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:11:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:11:54,191][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:11:58,114][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:01,  3.92s/it, est. speed input: 161.89 toks/s, output: 4.08 toks/s]
[2025-01-11 04:11:58,224][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.35it/s, est. speed input: 1102.28 toks/s, output: 28.77 toks/s]
[2025-01-11 04:11:58,356][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.42it/s, est. speed input: 2439.14 toks/s, output: 69.14 toks/s]
[2025-01-11 04:11:58,457][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 11.67it/s, est. speed input: 3721.44 toks/s, output: 115.57 toks/s]
[2025-01-11 04:11:58,760][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 14.11it/s, est. speed input: 4447.04 toks/s, output: 157.57 toks/s]
[2025-01-11 04:11:58,761][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.00it/s, est. speed input: 4447.04 toks/s, output: 157.57 toks/s]
WARNING 01-11 04:11:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:11:59,021][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:12:03,243][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:10,  4.22s/it, est. speed input: 158.95 toks/s, output: 7.11 toks/s]
[2025-01-11 04:12:03,321][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.44it/s, est. speed input: 4888.44 toks/s, output: 230.52 toks/s]
WARNING 01-11 04:12:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:12:03,578][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:12:08,675][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.10s/it, est. speed input: 180.13 toks/s, output: 5.69 toks/s]
[2025-01-11 04:12:08,676][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5766.45 toks/s, output: 182.07 toks/s]
[2025-01-11 04:12:11,780][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:12:11,834][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:12:13,488][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 04:12:15,132][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 04:12:16,789][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:12:17,323][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:12:17,323][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 04:12:37,359][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:12:37,411][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:12:39,411][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  2.00s/it]
[2025-01-11 04:12:41,054][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 04:12:42,663][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 04:12:43,162][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 04:12:43,163][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.44s/it]
WARNING 01-11 04:12:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:12:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:12:59 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:12:59 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:12:59,357][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:13:00,666][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:13:01,043][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[2025-01-11 04:13:02,351][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:13:03,666][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:13:03,666][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:13:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:13:17 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:13:18 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:13:18 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:13:39 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:13:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:13:39,992][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:13:44,228][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 149.93 toks/s, output: 3.78 toks/s]
[2025-01-11 04:13:44,342][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.53it/s, est. speed input: 729.93 toks/s, output: 19.08 toks/s]
[2025-01-11 04:13:44,486][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.48it/s, est. speed input: 1695.90 toks/s, output: 47.85 toks/s]
[2025-01-11 04:13:44,606][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  7.63it/s, est. speed input: 2477.51 toks/s, output: 75.65 toks/s]
[2025-01-11 04:13:44,746][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 12.03it/s, est. speed input: 3339.67 toks/s, output: 110.66 toks/s]
[2025-01-11 04:13:44,932][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 14.47it/s, est. speed input: 3856.81 toks/s, output: 141.11 toks/s]
[2025-01-11 04:13:45,165][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.19it/s, est. speed input: 3928.40 toks/s, output: 151.76 toks/s]
WARNING 01-11 04:13:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:13:45,415][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:13:49,579][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.16s/it, est. speed input: 156.09 toks/s, output: 6.96 toks/s]
[2025-01-11 04:13:49,699][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.12s/it, est. speed input: 458.42 toks/s, output: 21.01 toks/s]
[2025-01-11 04:13:49,753][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.38it/s, est. speed input: 4857.68 toks/s, output: 229.11 toks/s]
WARNING 01-11 04:13:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:13:50,037][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:13:55,148][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.11s/it, est. speed input: 179.62 toks/s, output: 5.67 toks/s]
[2025-01-11 04:13:55,148][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.26it/s, est. speed input: 5762.81 toks/s, output: 181.55 toks/s]
[2025-01-11 04:13:58,308][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:13:58,362][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:13:59,939][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.58s/it]
[2025-01-11 04:14:01,605][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 04:14:03,164][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.60s/it]
[2025-01-11 04:14:03,691][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.17s/it]
[2025-01-11 04:14:03,691][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.33s/it]
[2025-01-11 04:14:23,791][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:14:23,843][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:14:25,822][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.98s/it]
[2025-01-11 04:14:27,452][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 04:14:29,059][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 04:14:29,565][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 04:14:29,566][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 04:14:44 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:14:45 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:14:45 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:14:45 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:14:45,696][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:14:47,034][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 04:14:47,436][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[2025-01-11 04:14:48,725][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:14:50,051][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:14:50,051][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:14:50 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:15:04 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:15:04 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:15:04 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:15:26 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:15:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:15:26,947][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:15:31,128][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 151.88 toks/s, output: 3.83 toks/s]
[2025-01-11 04:15:31,248][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.13s/it, est. speed input: 442.95 toks/s, output: 11.86 toks/s]
[2025-01-11 04:15:31,350][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:06,  3.53it/s, est. speed input: 1297.78 toks/s, output: 36.79 toks/s]
[2025-01-11 04:15:31,476][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  8.59it/s, est. speed input: 2523.90 toks/s, output: 78.17 toks/s]
[2025-01-11 04:15:31,636][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 11.29it/s, est. speed input: 3114.42 toks/s, output: 102.78 toks/s]
[2025-01-11 04:15:31,809][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 14.03it/s, est. speed input: 3656.91 toks/s, output: 131.22 toks/s]
[2025-01-11 04:15:31,960][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00, 16.06it/s, est. speed input: 4053.60 toks/s, output: 158.59 toks/s]
[2025-01-11 04:15:31,960][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.38it/s, est. speed input: 4053.60 toks/s, output: 158.59 toks/s]
WARNING 01-11 04:15:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:15:32,214][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:15:35,461][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:40,  3.25s/it, est. speed input: 200.85 toks/s, output: 4.31 toks/s]
[2025-01-11 04:15:36,425][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:57,  1.90s/it, est. speed input: 309.92 toks/s, output: 10.45 toks/s]
[2025-01-11 04:15:36,522][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.43it/s, est. speed input: 4895.88 toks/s, output: 227.06 toks/s]
WARNING 01-11 04:15:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:15:36,788][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:15:41,759][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.97s/it, est. speed input: 184.46 toks/s, output: 5.83 toks/s]
[2025-01-11 04:15:41,760][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.23it/s, est. speed input: 5742.75 toks/s, output: 180.81 toks/s]
WARNING 01-11 04:15:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:15:41,971][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:15:42,793][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 846.78 toks/s, output: 37.72 toks/s]
[2025-01-11 04:15:42,794][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 846.78 toks/s, output: 37.72 toks/s]
WARNING 01-11 04:15:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:15:43,209][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:15:43,983][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1181.59 toks/s, output: 37.49 toks/s]
[2025-01-11 04:15:43,983][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1181.59 toks/s, output: 37.49 toks/s]
[2025-01-11 04:15:46,995][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:15:47,047][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:15:48,699][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.65s/it]
[2025-01-11 04:15:50,337][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 04:15:51,929][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.62s/it]
[2025-01-11 04:15:52,495][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 04:15:52,495][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 04:16:12,569][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:16:12,621][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:16:14,670][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.05s/it]
[2025-01-11 04:16:16,304][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.80s/it]
[2025-01-11 04:16:17,906][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.71s/it]
[2025-01-11 04:16:18,422][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.24s/it]
[2025-01-11 04:16:18,422][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.45s/it]
WARNING 01-11 04:16:33 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:16:33 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:16:34 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:16:34 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:16:34,489][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:16:35,800][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:16:36,190][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 04:16:37,478][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:16:38,800][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:16:38,800][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:16:39 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:16:52 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:16:53 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:16:53 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:17:14 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:17:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:17:15,155][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:17:19,273][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.12s/it, est. speed input: 154.20 toks/s, output: 3.89 toks/s]
[2025-01-11 04:17:19,391][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.11s/it, est. speed input: 449.72 toks/s, output: 11.80 toks/s]
[2025-01-11 04:17:19,534][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:04<00:04,  4.85it/s, est. speed input: 1739.85 toks/s, output: 49.78 toks/s]
[2025-01-11 04:17:19,650][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  8.67it/s, est. speed input: 2683.80 toks/s, output: 82.97 toks/s]
[2025-01-11 04:17:19,804][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00, 11.42it/s, est. speed input: 3277.99 toks/s, output: 108.84 toks/s]
[2025-01-11 04:17:19,981][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 14.10it/s, est. speed input: 3815.83 toks/s, output: 139.87 toks/s]
[2025-01-11 04:17:20,084][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.49it/s, est. speed input: 4122.03 toks/s, output: 160.46 toks/s]
WARNING 01-11 04:17:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:17:20,327][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:17:24,615][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:12,  4.29s/it, est. speed input: 153.22 toks/s, output: 7.23 toks/s]
[2025-01-11 04:17:24,616][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.46it/s, est. speed input: 4916.13 toks/s, output: 231.30 toks/s]
WARNING 01-11 04:17:24 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:17:24,880][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:17:29,844][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.96s/it, est. speed input: 185.15 toks/s, output: 5.84 toks/s]
[2025-01-11 04:17:29,845][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.24it/s, est. speed input: 5745.91 toks/s, output: 181.09 toks/s]
WARNING 01-11 04:17:30 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:17:30,081][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:17:30,879][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 926.46 toks/s, output: 38.86 toks/s]
[2025-01-11 04:17:30,879][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 926.46 toks/s, output: 38.86 toks/s]
WARNING 01-11 04:17:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:17:31,278][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:17:32,058][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1202.54 toks/s, output: 37.18 toks/s]
[2025-01-11 04:17:32,058][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1202.54 toks/s, output: 37.18 toks/s]
[2025-01-11 04:17:35,035][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:17:35,087][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:17:36,774][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 04:17:38,429][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 04:17:40,053][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:17:40,627][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:17:40,627][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:18:00,850][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:18:00,902][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:18:03,411][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:07,  2.51s/it]
[2025-01-11 04:18:05,501][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.26s/it]
[2025-01-11 04:18:07,132][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:01,  1.97s/it]
[2025-01-11 04:18:07,755][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.44s/it]
[2025-01-11 04:18:07,756][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.71s/it]
WARNING 01-11 04:18:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:18:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:18:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:18:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:18:24,242][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:18:25,566][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 04:18:25,972][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 04:18:27,266][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:18:28,594][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:18:28,594][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:18:28 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:18:42 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:18:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:18:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:19:04 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:19:04 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:04,994][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:09,155][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:08,  4.16s/it, est. speed input: 152.61 toks/s, output: 3.85 toks/s]
[2025-01-11 04:19:09,271][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:22,  1.22it/s, est. speed input: 593.90 toks/s, output: 15.43 toks/s]
[2025-01-11 04:19:09,373][root][ERROR] - Processed prompts:  31%|###1      | 10/32 [00:04<00:05,  3.87it/s, est. speed input: 1450.38 toks/s, output: 40.66 toks/s]
[2025-01-11 04:19:09,501][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:01,  8.32it/s, est. speed input: 2536.14 toks/s, output: 78.10 toks/s]
[2025-01-11 04:19:09,696][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 10.78it/s, est. speed input: 3106.24 toks/s, output: 102.94 toks/s]
[2025-01-11 04:19:09,854][root][ERROR] - Processed prompts:  84%|########4 | 27/32 [00:04<00:00, 12.84it/s, est. speed input: 3528.06 toks/s, output: 126.35 toks/s]
[2025-01-11 04:19:09,898][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.53it/s, est. speed input: 4144.14 toks/s, output: 162.54 toks/s]
WARNING 01-11 04:19:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:10,122][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:11,042][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 762.75 toks/s, output: 41.35 toks/s]
[2025-01-11 04:19:11,042][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 762.75 toks/s, output: 41.35 toks/s]
WARNING 01-11 04:19:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:11,280][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:15,470][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:05,  4.19s/it, est. speed input: 156.56 toks/s, output: 7.40 toks/s]
[2025-01-11 04:19:15,471][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.40it/s, est. speed input: 4872.22 toks/s, output: 229.31 toks/s]
WARNING 01-11 04:19:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:15,732][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:20,683][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 185.41 toks/s, output: 5.86 toks/s]
[2025-01-11 04:19:20,684][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.26it/s, est. speed input: 5762.66 toks/s, output: 181.54 toks/s]
WARNING 01-11 04:19:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:20,899][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:21,706][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 832.58 toks/s, output: 38.41 toks/s]
[2025-01-11 04:19:21,706][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 832.58 toks/s, output: 38.41 toks/s]
WARNING 01-11 04:19:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:19:22,114][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:19:22,896][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1279.38 toks/s, output: 37.10 toks/s]
[2025-01-11 04:19:22,897][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1279.38 toks/s, output: 37.10 toks/s]
[2025-01-11 04:19:25,879][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:19:25,932][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:19:27,616][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.68s/it]
[2025-01-11 04:19:29,291][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 04:19:30,943][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 04:19:31,468][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:19:31,468][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:19:51,626][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:19:51,678][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:19:53,586][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 04:19:55,254][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 04:19:56,823][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 04:19:57,327][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:19:57,327][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:20:12 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:20:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:20:13 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:20:13 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:20:13,278][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:20:14,589][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:20:14,993][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:20:16,278][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:20:17,597][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:20:17,597][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:20:17 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:20:31 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:20:32 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:20:32 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:20:53 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:20:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:20:53,903][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:20:57,792][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:00,  3.89s/it, est. speed input: 163.28 toks/s, output: 4.37 toks/s]
[2025-01-11 04:20:57,899][root][ERROR] - Processed prompts:  38%|###7      | 12/32 [00:03<00:04,  4.14it/s, est. speed input: 1907.25 toks/s, output: 53.81 toks/s]
[2025-01-11 04:20:58,060][root][ERROR] - Processed prompts:  59%|#####9    | 19/32 [00:04<00:01,  7.09it/s, est. speed input: 2902.74 toks/s, output: 87.58 toks/s]
[2025-01-11 04:20:58,170][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 10.33it/s, est. speed input: 3720.32 toks/s, output: 119.05 toks/s]
[2025-01-11 04:20:58,415][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 12.87it/s, est. speed input: 4363.39 toks/s, output: 159.60 toks/s]
[2025-01-11 04:20:58,432][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.07it/s, est. speed input: 4487.00 toks/s, output: 167.38 toks/s]
WARNING 01-11 04:20:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:20:58,697][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:21:02,874][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.18s/it, est. speed input: 160.17 toks/s, output: 6.94 toks/s]
[2025-01-11 04:21:02,992][root][ERROR] - Processed prompts:   9%|9         | 3/32 [00:04<00:32,  1.13s/it, est. speed input: 461.95 toks/s, output: 20.72 toks/s]
[2025-01-11 04:21:03,027][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.39it/s, est. speed input: 4862.65 toks/s, output: 228.68 toks/s]
WARNING 01-11 04:21:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:21:03,294][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:21:08,410][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.12s/it, est. speed input: 179.64 toks/s, output: 5.67 toks/s]
[2025-01-11 04:21:08,411][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.25it/s, est. speed input: 5751.98 toks/s, output: 181.37 toks/s]
[2025-01-11 04:21:11,624][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:21:11,676][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:21:13,343][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.67s/it]
[2025-01-11 04:21:14,974][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 04:21:16,614][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.64s/it]
[2025-01-11 04:21:17,162][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:21:17,162][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 04:21:37,225][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:21:37,277][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:21:39,249][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.97s/it]
[2025-01-11 04:21:40,867][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.76s/it]
[2025-01-11 04:21:42,431][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 04:21:42,937][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:21:42,937][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
WARNING 01-11 04:21:58 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:21:58 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:21:58 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:21:58 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:21:58,896][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:22:00,202][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:22:00,577][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[2025-01-11 04:22:01,888][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:22:03,202][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:22:03,203][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:22:03 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:22:17 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:22:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:22:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:22:39 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:22:39 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:22:39,647][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:22:44,067][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:16,  4.42s/it, est. speed input: 143.69 toks/s, output: 4.07 toks/s]
[2025-01-11 04:22:44,175][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:11,  2.09it/s, est. speed input: 981.84 toks/s, output: 28.49 toks/s]
[2025-01-11 04:22:44,306][root][ERROR] - Processed prompts:  56%|#####6    | 18/32 [00:04<00:02,  6.61it/s, est. speed input: 2453.34 toks/s, output: 78.34 toks/s]
[2025-01-11 04:22:44,706][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:05<00:00,  8.24it/s, est. speed input: 3012.39 toks/s, output: 110.30 toks/s]
[2025-01-11 04:22:44,803][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.21it/s, est. speed input: 3941.53 toks/s, output: 166.82 toks/s]
WARNING 01-11 04:22:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:22:45,062][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:22:49,369][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:13,  4.31s/it, est. speed input: 152.32 toks/s, output: 7.20 toks/s]
[2025-01-11 04:22:49,370][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.43it/s, est. speed input: 4910.02 toks/s, output: 230.29 toks/s]
WARNING 01-11 04:22:49 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:22:49,624][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:22:54,603][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.98s/it, est. speed input: 184.39 toks/s, output: 5.82 toks/s]
[2025-01-11 04:22:54,604][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.23it/s, est. speed input: 5744.20 toks/s, output: 180.54 toks/s]
WARNING 01-11 04:22:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:22:54,809][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:22:55,621][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 899.84 toks/s, output: 38.21 toks/s]
[2025-01-11 04:22:55,621][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 899.84 toks/s, output: 38.21 toks/s]
WARNING 01-11 04:22:56 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:22:56,010][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:22:56,789][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1192.94 toks/s, output: 37.24 toks/s]
[2025-01-11 04:22:56,789][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1192.94 toks/s, output: 37.24 toks/s]
[2025-01-11 04:22:59,830][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:22:59,883][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:23:01,501][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 04:23:03,163][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.64s/it]
[2025-01-11 04:23:04,766][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 04:23:05,293][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:23:05,293][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 04:23:25,609][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:23:25,661][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:23:27,537][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.88s/it]
[2025-01-11 04:23:29,119][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 04:23:30,762][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 04:23:31,274][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:23:31,274][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 04:23:46 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:23:46 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:23:47 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:23:47 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:23:47,386][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:23:48,727][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 04:23:49,103][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:23:50,422][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:23:51,740][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:23:51,741][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:23:51 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:24:05 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:24:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:24:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:24:27 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:24:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:28,206][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:32,398][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:09,  4.19s/it, est. speed input: 151.48 toks/s, output: 3.82 toks/s]
[2025-01-11 04:24:32,511][root][ERROR] - Processed prompts:  16%|#5        | 5/32 [00:04<00:17,  1.54it/s, est. speed input: 737.58 toks/s, output: 19.05 toks/s]
[2025-01-11 04:24:32,646][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.42it/s, est. speed input: 2002.22 toks/s, output: 56.31 toks/s]
[2025-01-11 04:24:32,928][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00, 10.49it/s, est. speed input: 3362.29 toks/s, output: 105.90 toks/s]
[2025-01-11 04:24:33,054][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.05it/s, est. speed input: 3929.50 toks/s, output: 139.85 toks/s]
[2025-01-11 04:24:33,073][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.57it/s, est. speed input: 4174.83 toks/s, output: 154.50 toks/s]
WARNING 01-11 04:24:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:33,311][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:34,236][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 757.82 toks/s, output: 41.08 toks/s]
[2025-01-11 04:24:34,236][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 757.82 toks/s, output: 41.08 toks/s]
WARNING 01-11 04:24:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:34,482][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:38,544][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:01,  4.06s/it, est. speed input: 161.26 toks/s, output: 7.14 toks/s]
[2025-01-11 04:24:38,662][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:50,  1.74s/it, est. speed input: 313.67 toks/s, output: 14.36 toks/s]
[2025-01-11 04:24:38,680][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.39it/s, est. speed input: 4853.86 toks/s, output: 228.72 toks/s]
WARNING 01-11 04:24:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:38,952][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:43,911][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.96s/it, est. speed input: 185.12 toks/s, output: 5.85 toks/s]
[2025-01-11 04:24:43,912][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.25it/s, est. speed input: 5744.50 toks/s, output: 181.25 toks/s]
WARNING 01-11 04:24:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:44,124][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:44,931][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 833.36 toks/s, output: 38.44 toks/s]
[2025-01-11 04:24:44,931][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 833.36 toks/s, output: 38.44 toks/s]
WARNING 01-11 04:24:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:24:45,327][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:24:46,106][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1283.40 toks/s, output: 37.22 toks/s]
[2025-01-11 04:24:46,106][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1283.40 toks/s, output: 37.22 toks/s]
[2025-01-11 04:24:49,152][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:24:49,206][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:24:50,867][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.66s/it]
[2025-01-11 04:24:52,536][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 04:24:54,181][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 04:24:54,724][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:24:54,725][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:25:14,930][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:25:14,982][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:25:16,840][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.86s/it]
[2025-01-11 04:25:18,448][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-01-11 04:25:19,997][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.64s/it]
[2025-01-11 04:25:20,524][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 04:25:20,524][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
WARNING 01-11 04:25:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:25:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:25:36 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:25:36 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:25:36,515][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:25:37,856][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 04:25:38,232][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:25:39,521][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 04:25:40,839][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:25:40,839][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:25:41 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:25:54 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:25:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:25:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:26:16 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:26:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:26:17,272][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:26:21,028][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:56,  3.76s/it, est. speed input: 169.08 toks/s, output: 4.26 toks/s]
[2025-01-11 04:26:21,135][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:03<00:08,  2.81it/s, est. speed input: 1315.27 toks/s, output: 34.18 toks/s]
[2025-01-11 04:26:21,267][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:02,  6.05it/s, est. speed input: 2384.44 toks/s, output: 66.34 toks/s]
[2025-01-11 04:26:21,616][root][ERROR] - Processed prompts:  75%|#######5  | 24/32 [00:04<00:00,  9.92it/s, est. speed input: 3508.63 toks/s, output: 108.67 toks/s]
[2025-01-11 04:26:21,712][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.21it/s, est. speed input: 4576.45 toks/s, output: 170.94 toks/s]
WARNING 01-11 04:26:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:26:21,999][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:26:26,299][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:13,  4.30s/it, est. speed input: 152.57 toks/s, output: 7.21 toks/s]
[2025-01-11 04:26:26,299][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.44it/s, est. speed input: 4895.00 toks/s, output: 230.67 toks/s]
WARNING 01-11 04:26:26 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:26:26,556][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:26:31,657][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 180.00 toks/s, output: 5.69 toks/s]
[2025-01-11 04:26:31,658][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5769.72 toks/s, output: 181.93 toks/s]
[2025-01-11 04:26:34,893][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:26:34,946][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:26:37,244][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.30s/it]
[2025-01-11 04:26:39,228][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.11s/it]
[2025-01-11 04:26:41,485][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:06<00:02,  2.18s/it]
[2025-01-11 04:26:42,176][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.59s/it]
[2025-01-11 04:26:42,176][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:07<00:00,  1.81s/it]
[2025-01-11 04:27:02,894][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:27:02,945][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:27:04,590][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.64s/it]
[2025-01-11 04:27:06,202][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.63s/it]
[2025-01-11 04:27:07,760][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.59s/it]
[2025-01-11 04:27:08,283][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.17s/it]
[2025-01-11 04:27:08,283][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.33s/it]
WARNING 01-11 04:27:23 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:27:23 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:27:24 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:27:24 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:27:24,467][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:27:25,779][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[2025-01-11 04:27:26,161][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[2025-01-11 04:27:27,460][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:27:28,780][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
[2025-01-11 04:27:28,781][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
INFO 01-11 04:27:29 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:27:42 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:27:43 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:27:43 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:28:04 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:28:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:05,211][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:09,278][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:06,  4.07s/it, est. speed input: 156.17 toks/s, output: 3.93 toks/s]
[2025-01-11 04:28:09,411][root][ERROR] - Processed prompts:  53%|#####3    | 17/32 [00:04<00:02,  5.60it/s, est. speed input: 2570.43 toks/s, output: 68.10 toks/s]
[2025-01-11 04:28:09,527][root][ERROR] - Processed prompts:  88%|########7 | 28/32 [00:04<00:00, 10.31it/s, est. speed input: 4120.28 toks/s, output: 119.34 toks/s]
[2025-01-11 04:28:09,819][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.95it/s, est. speed input: 4410.47 toks/s, output: 140.65 toks/s]
WARNING 01-11 04:28:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:10,034][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:10,654][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 1107.18 toks/s, output: 32.28 toks/s]
[2025-01-11 04:28:10,654][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 1107.18 toks/s, output: 32.28 toks/s]
WARNING 01-11 04:28:10 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:10,899][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:14,954][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:01,  4.05s/it, est. speed input: 160.34 toks/s, output: 7.15 toks/s]
[2025-01-11 04:28:15,072][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:50,  1.74s/it, est. speed input: 312.33 toks/s, output: 14.38 toks/s]
[2025-01-11 04:28:15,106][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.37it/s, est. speed input: 4822.41 toks/s, output: 228.47 toks/s]
WARNING 01-11 04:28:15 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:15,388][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:20,315][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:27,  4.93s/it, est. speed input: 185.70 toks/s, output: 5.89 toks/s]
[2025-01-11 04:28:20,316][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.29it/s, est. speed input: 5763.47 toks/s, output: 182.42 toks/s]
WARNING 01-11 04:28:20 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:20,538][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:21,349][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 806.77 toks/s, output: 38.24 toks/s]
[2025-01-11 04:28:21,349][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 806.77 toks/s, output: 38.24 toks/s]
WARNING 01-11 04:28:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:28:21,744][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:28:22,521][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1244.35 toks/s, output: 37.32 toks/s]
[2025-01-11 04:28:22,521][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1244.35 toks/s, output: 37.32 toks/s]
[2025-01-11 04:28:25,604][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:28:25,657][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:28:27,421][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.76s/it]
[2025-01-11 04:28:29,047][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.68s/it]
[2025-01-11 04:28:30,684][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 04:28:31,206][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:28:31,206][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.39s/it]
[2025-01-11 04:28:51,218][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:28:51,270][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:28:53,158][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.89s/it]
[2025-01-11 04:28:54,782][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.73s/it]
[2025-01-11 04:28:56,360][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 04:28:56,897][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:28:56,897][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:29:12 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:29:12 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:29:12 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:29:12 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:29:12,761][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:29:14,078][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
[2025-01-11 04:29:14,479][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:29:15,801][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 04:29:17,128][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 04:29:17,129][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:29:17 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:29:31 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:29:31 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:29:31 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:29:53 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:29:53 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:29:53,931][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:29:58,079][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:08,  4.15s/it, est. speed input: 153.07 toks/s, output: 3.86 toks/s]
[2025-01-11 04:29:58,181][root][ERROR] - Processed prompts:  28%|##8       | 9/32 [00:04<00:07,  2.89it/s, est. speed input: 1344.70 toks/s, output: 34.35 toks/s]
[2025-01-11 04:29:58,320][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  4.95it/s, est. speed input: 2025.69 toks/s, output: 55.83 toks/s]
[2025-01-11 04:29:58,640][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 12.05it/s, est. speed input: 3911.00 toks/s, output: 124.46 toks/s]
[2025-01-11 04:29:58,676][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.74it/s, est. speed input: 4282.26 toks/s, output: 146.25 toks/s]
WARNING 01-11 04:29:58 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:29:58,931][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:30:03,144][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:10,  4.21s/it, est. speed input: 155.50 toks/s, output: 7.12 toks/s]
[2025-01-11 04:30:03,205][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.49it/s, est. speed input: 4909.78 toks/s, output: 231.88 toks/s]
WARNING 01-11 04:30:03 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:30:03,479][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:30:08,583][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:38,  5.10s/it, est. speed input: 180.07 toks/s, output: 5.68 toks/s]
[2025-01-11 04:30:08,583][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.27it/s, est. speed input: 5753.20 toks/s, output: 181.81 toks/s]
[2025-01-11 04:30:11,903][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:30:11,956][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:30:13,622][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.67s/it]
[2025-01-11 04:30:15,288][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 04:30:16,937][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.66s/it]
[2025-01-11 04:30:17,460][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:30:17,461][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:30:37,858][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:30:37,909][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:30:39,822][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.91s/it]
[2025-01-11 04:30:41,395][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.71s/it]
[2025-01-11 04:30:42,923][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.63s/it]
[2025-01-11 04:30:43,423][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.18s/it]
[2025-01-11 04:30:43,424][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 04:31:00 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:31:00 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:31:00 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:31:00 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:31:00,713][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:31:02,111][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
[2025-01-11 04:31:02,616][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
[2025-01-11 04:31:04,265][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.23s/it]
[2025-01-11 04:31:05,829][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 04:31:05,829][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.28s/it]
INFO 01-11 04:31:06 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:31:19 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:31:20 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:31:20 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:31:41 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:31:42 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:31:42,217][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:31:46,279][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.06s/it, est. speed input: 156.34 toks/s, output: 3.94 toks/s]
[2025-01-11 04:31:46,382][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:09,  2.61it/s, est. speed input: 1219.57 toks/s, output: 31.21 toks/s]
[2025-01-11 04:31:46,523][root][ERROR] - Processed prompts:  41%|####      | 13/32 [00:04<00:04,  4.72it/s, est. speed input: 1917.33 toks/s, output: 52.72 toks/s]
[2025-01-11 04:31:46,628][root][ERROR] - Processed prompts:  72%|#######1  | 23/32 [00:04<00:00, 10.48it/s, est. speed input: 3311.13 toks/s, output: 102.02 toks/s]
[2025-01-11 04:31:46,953][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 12.92it/s, est. speed input: 4022.62 toks/s, output: 142.53 toks/s]
[2025-01-11 04:31:46,970][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.73it/s, est. speed input: 4275.23 toks/s, output: 157.80 toks/s]
WARNING 01-11 04:31:47 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:31:47,244][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:31:51,533][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:12,  4.29s/it, est. speed input: 152.98 toks/s, output: 7.23 toks/s]
[2025-01-11 04:31:51,551][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.43it/s, est. speed input: 4885.96 toks/s, output: 230.61 toks/s]
WARNING 01-11 04:31:51 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:31:51,828][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:31:56,921][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.09s/it, est. speed input: 180.25 toks/s, output: 5.69 toks/s]
[2025-01-11 04:31:56,922][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5775.93 toks/s, output: 182.18 toks/s]
[2025-01-11 04:32:00,223][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:32:00,275][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:32:01,900][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.62s/it]
[2025-01-11 04:32:03,568][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 04:32:05,138][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.61s/it]
[2025-01-11 04:32:05,667][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:32:05,668][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.35s/it]
[2025-01-11 04:32:25,868][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:32:25,920][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:32:27,875][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.95s/it]
[2025-01-11 04:32:29,480][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.75s/it]
[2025-01-11 04:32:31,045][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.67s/it]
[2025-01-11 04:32:31,548][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:32:31,549][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.41s/it]
WARNING 01-11 04:32:47 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:32:47 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:32:47 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:32:47 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:32:48,013][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:32:49,358][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
[2025-01-11 04:32:49,737][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:32:51,041][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:32:52,368][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:32:52,368][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:32:52 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:33:06 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:33:06 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:33:06 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:33:28 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:33:28 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:33:28,831][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:33:32,816][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  3.98s/it, est. speed input: 159.38 toks/s, output: 4.02 toks/s]
[2025-01-11 04:33:32,920][root][ERROR] - Processed prompts:  25%|##5       | 8/32 [00:04<00:09,  2.66it/s, est. speed input: 1242.69 toks/s, output: 31.80 toks/s]
[2025-01-11 04:33:33,058][root][ERROR] - Processed prompts:  44%|####3     | 14/32 [00:04<00:03,  5.27it/s, est. speed input: 2103.70 toks/s, output: 58.21 toks/s]
[2025-01-11 04:33:33,369][root][ERROR] - Processed prompts:  94%|#########3| 30/32 [00:04<00:00, 13.13it/s, est. speed input: 4198.18 toks/s, output: 136.19 toks/s]
[2025-01-11 04:33:33,443][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.94it/s, est. speed input: 4406.66 toks/s, output: 150.72 toks/s]
WARNING 01-11 04:33:33 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:33:33,693][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:33:37,537][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:59,  3.84s/it, est. speed input: 170.67 toks/s, output: 6.24 toks/s]
[2025-01-11 04:33:37,839][root][ERROR] - Processed prompts:   6%|6         | 2/32 [00:04<00:52,  1.76s/it, est. speed input: 315.04 toks/s, output: 12.78 toks/s]
[2025-01-11 04:33:37,956][root][ERROR] - Processed prompts:  12%|#2        | 4/32 [00:04<00:19,  1.44it/s, est. speed input: 612.55 toks/s, output: 26.74 toks/s]
[2025-01-11 04:33:37,973][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.48it/s, est. speed input: 4902.72 toks/s, output: 229.67 toks/s]
WARNING 01-11 04:33:38 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:33:38,234][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:33:43,203][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:29,  4.97s/it, est. speed input: 184.15 toks/s, output: 5.84 toks/s]
[2025-01-11 04:33:43,204][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.24it/s, est. speed input: 5724.45 toks/s, output: 180.89 toks/s]
WARNING 01-11 04:33:43 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:33:43,443][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:33:44,247][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 883.55 toks/s, output: 38.58 toks/s]
[2025-01-11 04:33:44,247][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 883.55 toks/s, output: 38.58 toks/s]
WARNING 01-11 04:33:44 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:33:44,636][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:33:45,411][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1184.67 toks/s, output: 37.42 toks/s]
[2025-01-11 04:33:45,412][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 1184.67 toks/s, output: 37.42 toks/s]
[2025-01-11 04:33:48,490][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:33:48,545][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:33:50,154][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:04,  1.61s/it]
[2025-01-11 04:33:51,853][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.66s/it]
[2025-01-11 04:33:53,485][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:33:54,013][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:33:54,014][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.37s/it]
[2025-01-11 04:34:14,177][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:34:14,230][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:34:16,100][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.87s/it]
[2025-01-11 04:34:17,707][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 04:34:19,241][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.63s/it]
[2025-01-11 04:34:19,744][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:34:19,745][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
WARNING 01-11 04:34:35 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:34:35 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:34:35 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:34:36 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:34:36,610][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:34:37,935][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 04:34:38,313][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]
[2025-01-11 04:34:39,609][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
[2025-01-11 04:34:40,966][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 04:34:40,966][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:34:41 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:34:55 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:34:55 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:34:55 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:35:17 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:35:17 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:17,767][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:21,891][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:07,  4.12s/it, est. speed input: 154.01 toks/s, output: 3.88 toks/s]
[2025-01-11 04:35:22,026][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:03,  4.87it/s, est. speed input: 2236.76 toks/s, output: 58.47 toks/s]
[2025-01-11 04:35:22,129][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:04<00:01,  7.80it/s, est. speed input: 3203.26 toks/s, output: 89.65 toks/s]
[2025-01-11 04:35:22,394][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00, 12.16it/s, est. speed input: 4392.51 toks/s, output: 137.27 toks/s]
[2025-01-11 04:35:22,394][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.92it/s, est. speed input: 4392.51 toks/s, output: 137.27 toks/s]
WARNING 01-11 04:35:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:22,606][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:23,228][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 1103.61 toks/s, output: 32.17 toks/s]
[2025-01-11 04:35:23,228][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 1103.61 toks/s, output: 32.17 toks/s]
WARNING 01-11 04:35:23 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:23,475][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:27,228][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:03<01:52,  3.75s/it, est. speed input: 175.33 toks/s, output: 6.39 toks/s]
[2025-01-11 04:35:27,640][root][ERROR] - Processed prompts:   6%|6         | 2/31 [00:04<00:51,  1.79s/it, est. speed input: 315.53 toks/s, output: 13.21 toks/s]
[2025-01-11 04:35:27,660][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.41it/s, est. speed input: 4844.07 toks/s, output: 228.46 toks/s]
WARNING 01-11 04:35:27 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:27,923][root][ERROR] - Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:32,707][root][ERROR] - Processed prompts:   3%|3         | 1/30 [00:04<02:18,  4.78s/it, est. speed input: 191.92 toks/s, output: 6.06 toks/s]
[2025-01-11 04:35:32,708][root][ERROR] - Processed prompts: 100%|##########| 30/30 [00:04<00:00,  6.27it/s, est. speed input: 5741.76 toks/s, output: 181.85 toks/s]
WARNING 01-11 04:35:32 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:32,936][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:33,842][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.10it/s, est. speed input: 785.51 toks/s, output: 33.10 toks/s]
[2025-01-11 04:35:33,860][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.16it/s, est. speed input: 1478.77 toks/s, output: 66.03 toks/s]
WARNING 01-11 04:35:34 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:35:34,267][root][ERROR] - Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:35:35,192][root][ERROR] - Processed prompts:  50%|#####     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 1045.24 toks/s, output: 31.35 toks/s]
[2025-01-11 04:35:35,193][root][ERROR] - Processed prompts: 100%|##########| 2/2 [00:00<00:00,  2.16it/s, est. speed input: 2038.91 toks/s, output: 62.67 toks/s]
[2025-01-11 04:35:38,276][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:35:38,329][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:35:40,047][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.72s/it]
[2025-01-11 04:35:41,658][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.65s/it]
[2025-01-11 04:35:43,247][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.63s/it]
[2025-01-11 04:35:43,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.19s/it]
[2025-01-11 04:35:43,777][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.36s/it]
[2025-01-11 04:36:04,046][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:36:04,098][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:36:06,042][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.94s/it]
[2025-01-11 04:36:07,684][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 04:36:09,311][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 04:36:09,826][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 04:36:09,826][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 04:36:25 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:36:25 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:36:25 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:36:25 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:36:25,943][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:36:27,307][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
[2025-01-11 04:36:27,703][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.26it/s]
[2025-01-11 04:36:29,016][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
[2025-01-11 04:36:30,346][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
[2025-01-11 04:36:30,346][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
INFO 01-11 04:36:30 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:36:44 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:36:44 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:36:44 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:37:06 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:37:06 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:06,787][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:10,458][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<01:53,  3.67s/it, est. speed input: 173.02 toks/s, output: 4.36 toks/s]
[2025-01-11 04:37:10,598][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:03<00:03,  5.43it/s, est. speed input: 2499.61 toks/s, output: 66.39 toks/s]
[2025-01-11 04:37:10,702][root][ERROR] - Processed prompts:  69%|######8   | 22/32 [00:03<00:01,  8.64it/s, est. speed input: 3568.38 toks/s, output: 101.15 toks/s]
[2025-01-11 04:37:10,970][root][ERROR] - Processed prompts:  97%|#########6| 31/32 [00:04<00:00, 12.65it/s, est. speed input: 4706.58 toks/s, output: 146.56 toks/s]
[2025-01-11 04:37:11,004][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.59it/s, est. speed input: 4819.38 toks/s, output: 154.40 toks/s]
WARNING 01-11 04:37:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:11,221][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:11,834][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 1118.47 toks/s, output: 32.61 toks/s]
[2025-01-11 04:37:11,834][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 1118.47 toks/s, output: 32.61 toks/s]
WARNING 01-11 04:37:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:12,085][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:16,198][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:03,  4.11s/it, est. speed input: 158.07 toks/s, output: 7.30 toks/s]
[2025-01-11 04:37:16,256][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.43it/s, est. speed input: 4863.40 toks/s, output: 229.92 toks/s]
WARNING 01-11 04:37:16 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:16,535][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:21,463][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:27,  4.93s/it, est. speed input: 185.68 toks/s, output: 5.88 toks/s]
[2025-01-11 04:37:21,464][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.29it/s, est. speed input: 5763.32 toks/s, output: 182.40 toks/s]
WARNING 01-11 04:37:21 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:21,701][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:22,501][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 817.49 toks/s, output: 38.75 toks/s]
[2025-01-11 04:37:22,501][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 817.49 toks/s, output: 38.75 toks/s]
WARNING 01-11 04:37:22 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:37:22,884][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:37:23,667][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1233.91 toks/s, output: 37.00 toks/s]
[2025-01-11 04:37:23,668][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1233.91 toks/s, output: 37.00 toks/s]
[2025-01-11 04:37:26,900][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:37:26,953][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:37:28,652][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.70s/it]
[2025-01-11 04:37:30,390][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 04:37:32,021][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 04:37:32,550][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 04:37:32,550][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 04:37:52,766][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:37:52,818][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:37:54,718][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.90s/it]
[2025-01-11 04:37:56,349][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.74s/it]
[2025-01-11 04:37:57,907][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.66s/it]
[2025-01-11 04:37:58,416][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.20s/it]
[2025-01-11 04:37:58,416][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
WARNING 01-11 04:38:13 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:38:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:38:14 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:38:14 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:38:14,605][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:38:15,936][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 04:38:16,335][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 04:38:17,623][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 04:38:18,969][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:38:18,969][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:38:19 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:38:33 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:38:33 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:38:33 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:38:54 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:38:55 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:38:55,298][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:38:59,338][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.04s/it, est. speed input: 157.18 toks/s, output: 3.96 toks/s]
[2025-01-11 04:38:59,444][root][ERROR] - Processed prompts:  22%|##1       | 7/32 [00:04<00:10,  2.28it/s, est. speed input: 1071.98 toks/s, output: 27.49 toks/s]
[2025-01-11 04:38:59,577][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.26it/s, est. speed input: 2374.52 toks/s, output: 66.84 toks/s]
[2025-01-11 04:38:59,898][root][ERROR] - Processed prompts:  91%|######### | 29/32 [00:04<00:00, 12.19it/s, est. speed input: 4003.02 toks/s, output: 127.38 toks/s]
[2025-01-11 04:38:59,988][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.82it/s, est. speed input: 4332.82 toks/s, output: 149.05 toks/s]
WARNING 01-11 04:39:00 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:39:00,210][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:39:01,149][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 748.50 toks/s, output: 41.58 toks/s]
[2025-01-11 04:39:01,149][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 748.50 toks/s, output: 41.58 toks/s]
WARNING 01-11 04:39:01 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:39:01,391][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:39:05,511][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:03,  4.12s/it, est. speed input: 159.01 toks/s, output: 7.28 toks/s]
[2025-01-11 04:39:05,586][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  7.39it/s, est. speed input: 4843.18 toks/s, output: 228.84 toks/s]
WARNING 01-11 04:39:05 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:39:05,843][root][ERROR] - Processed prompts:   0%|          | 0/31 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:39:10,789][root][ERROR] - Processed prompts:   3%|3         | 1/31 [00:04<02:28,  4.95s/it, est. speed input: 185.00 toks/s, output: 5.86 toks/s]
[2025-01-11 04:39:10,790][root][ERROR] - Processed prompts: 100%|##########| 31/31 [00:04<00:00,  6.27it/s, est. speed input: 5748.89 toks/s, output: 181.74 toks/s]
WARNING 01-11 04:39:11 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:39:11,002][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:39:11,807][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 836.97 toks/s, output: 38.55 toks/s]
[2025-01-11 04:39:11,807][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 836.97 toks/s, output: 38.55 toks/s]
WARNING 01-11 04:39:12 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:39:12,193][root][ERROR] - Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:39:12,977][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 1277.69 toks/s, output: 36.98 toks/s]
[2025-01-11 04:39:12,978][root][ERROR] - Processed prompts: 100%|##########| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 1277.69 toks/s, output: 36.98 toks/s]
[2025-01-11 04:39:16,043][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:39:16,097][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:39:17,931][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.83s/it]
[2025-01-11 04:39:19,529][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.70s/it]
[2025-01-11 04:39:21,187][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.68s/it]
[2025-01-11 04:39:21,714][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:39:21,715][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.40s/it]
[2025-01-11 04:39:42,060][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:39:42,112][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:39:44,254][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:02<00:06,  2.14s/it]
[2025-01-11 04:39:46,274][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:04<00:04,  2.07s/it]
[2025-01-11 04:39:47,880][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.86s/it]
[2025-01-11 04:39:48,403][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.33s/it]
[2025-01-11 04:39:48,403][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:06<00:00,  1.57s/it]
WARNING 01-11 04:40:03 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:40:03 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:40:04 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:40:04 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:40:04,418][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:40:05,747][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
[2025-01-11 04:40:06,148][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
[2025-01-11 04:40:07,441][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
[2025-01-11 04:40:08,762][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:40:08,763][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:40:09 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:40:22 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:40:23 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:40:23 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:40:44 model_runner.py:1335] Graph capturing finished in 21 secs.
WARNING 01-11 04:40:45 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:40:45,179][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:40:49,175][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:03<02:03,  4.00s/it, est. speed input: 158.91 toks/s, output: 4.00 toks/s]
[2025-01-11 04:40:49,284][root][ERROR] - Processed prompts:  19%|#8        | 6/32 [00:04<00:13,  1.96it/s, est. speed input: 928.11 toks/s, output: 23.87 toks/s]
[2025-01-11 04:40:49,420][root][ERROR] - Processed prompts:  50%|#####     | 16/32 [00:04<00:02,  6.45it/s, est. speed input: 2395.89 toks/s, output: 68.39 toks/s]
[2025-01-11 04:40:49,789][root][ERROR] - Processed prompts:  81%|########1 | 26/32 [00:04<00:00, 10.48it/s, est. speed input: 3581.51 toks/s, output: 112.59 toks/s]
[2025-01-11 04:40:49,851][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.85it/s, est. speed input: 4349.51 toks/s, output: 158.18 toks/s]
WARNING 01-11 04:40:50 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:40:50,127][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:40:54,417][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:12,  4.29s/it, est. speed input: 152.22 toks/s, output: 7.23 toks/s]
[2025-01-11 04:40:54,418][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.46it/s, est. speed input: 4901.00 toks/s, output: 231.19 toks/s]
WARNING 01-11 04:40:54 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:40:54,695][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:40:59,788][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.09s/it, est. speed input: 179.68 toks/s, output: 5.69 toks/s]
[2025-01-11 04:40:59,789][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5774.38 toks/s, output: 182.20 toks/s]
[2025-01-11 04:41:03,075][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:41:03,127][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:41:04,821][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 04:41:06,565][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.72s/it]
[2025-01-11 04:41:08,238][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.70s/it]
[2025-01-11 04:41:08,805][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.25s/it]
[2025-01-11 04:41:08,805][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
[2025-01-11 04:41:29,058][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:41:29,110][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:41:31,043][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.93s/it]
[2025-01-11 04:41:32,729][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.79s/it]
[2025-01-11 04:41:34,296][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 04:41:34,826][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.23s/it]
[2025-01-11 04:41:34,826][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.43s/it]
WARNING 01-11 04:41:50 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 04:41:50 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 04:41:50 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 04:41:50 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 04:41:50,785][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:41:52,126][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[2025-01-11 04:41:52,508][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
[2025-01-11 04:41:53,802][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
[2025-01-11 04:41:55,138][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
[2025-01-11 04:41:55,138][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
INFO 01-11 04:41:55 model_runner.py:926] Loading model weights took 14.9614 GB
INFO 01-11 04:42:09 gpu_executor.py:122] # GPU blocks: 19931, # CPU blocks: 2048
INFO 01-11 04:42:09 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 04:42:09 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-11 04:42:31 model_runner.py:1335] Graph capturing finished in 22 secs.
WARNING 01-11 04:42:31 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:42:31,676][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:42:35,727][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:05,  4.05s/it, est. speed input: 156.74 toks/s, output: 3.95 toks/s]
[2025-01-11 04:42:35,868][root][ERROR] - Processed prompts:  47%|####6     | 15/32 [00:04<00:03,  4.94it/s, est. speed input: 2272.26 toks/s, output: 60.35 toks/s]
[2025-01-11 04:42:35,968][root][ERROR] - Processed prompts:  78%|#######8  | 25/32 [00:04<00:00,  9.29it/s, est. speed input: 3698.69 toks/s, output: 106.94 toks/s]
[2025-01-11 04:42:36,284][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  6.94it/s, est. speed input: 4409.45 toks/s, output: 143.22 toks/s]
WARNING 01-11 04:42:36 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_bob' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:42:36,530][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:42:40,765][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:04<02:11,  4.24s/it, est. speed input: 153.47 toks/s, output: 7.08 toks/s]
[2025-01-11 04:42:40,844][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:04<00:00,  7.42it/s, est. speed input: 4856.27 toks/s, output: 229.96 toks/s]
WARNING 01-11 04:42:41 tokenizer.py:174] No tokenizer found in /home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice, using base model tokenizer instead. (Exception: Can't load tokenizer for '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/mila/d/dereck.piche/llm_negotiation/outputs/2025-01-11/00-17-29/ad_alice' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.)
[2025-01-11 04:42:41,127][root][ERROR] - Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[2025-01-11 04:42:46,221][root][ERROR] - Processed prompts:   3%|3         | 1/32 [00:05<02:37,  5.09s/it, est. speed input: 179.64 toks/s, output: 5.69 toks/s]
[2025-01-11 04:42:46,222][root][ERROR] - Processed prompts: 100%|##########| 32/32 [00:05<00:00,  6.28it/s, est. speed input: 5757.43 toks/s, output: 182.15 toks/s]
[2025-01-11 04:42:49,430][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:42:49,483][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:42:51,173][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.69s/it]
[2025-01-11 04:42:52,824][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.67s/it]
[2025-01-11 04:42:54,459][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:04<00:01,  1.65s/it]
[2025-01-11 04:42:54,986][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.21s/it]
[2025-01-11 04:42:54,986][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.38s/it]
[2025-01-11 04:43:15,110][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2025-01-11 04:43:15,161][root][ERROR] - Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[2025-01-11 04:43:17,086][root][ERROR] - Loading checkpoint shards:  25%|##5       | 1/4 [00:01<00:05,  1.92s/it]
[2025-01-11 04:43:18,742][root][ERROR] - Loading checkpoint shards:  50%|#####     | 2/4 [00:03<00:03,  1.77s/it]
[2025-01-11 04:43:20,341][root][ERROR] - Loading checkpoint shards:  75%|#######5  | 3/4 [00:05<00:01,  1.69s/it]
[2025-01-11 04:43:20,843][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.22s/it]
[2025-01-11 04:43:20,843][root][ERROR] - Loading checkpoint shards: 100%|##########| 4/4 [00:05<00:00,  1.42s/it]
