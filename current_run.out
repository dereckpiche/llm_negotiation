WARNING 01-11 12:59:27 arg_utils.py:872] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-11 12:59:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)
INFO 01-11 12:59:29 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 01-11 12:59:29 weight_utils.py:236] Using model weights format ['*.safetensors']
[2025-01-11 12:59:29,772][root][ERROR] - Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2025-01-11 12:59:31,232][root][ERROR] - Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
[2025-01-11 12:59:31,610][root][ERROR] - Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
[2025-01-11 12:59:32,992][root][ERROR] - Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.08s/it]
[2025-01-11 12:59:34,389][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
[2025-01-11 12:59:34,390][root][ERROR] - Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
INFO 01-11 12:59:34 model_runner.py:926] Loading model weights took 14.9927 GB
INFO 01-11 13:00:17 gpu_executor.py:122] # GPU blocks: 3115, # CPU blocks: 2048
[2025-01-11 13:00:17,154][root][ERROR] - Error executing job with overrides: ['matches.dond_game_args.rounds_per_game=14', 'matches.run_matches_args.log_func_args.training_data_func=set_discounted_advalign_returns']
[2025-01-11 13:00:17,156][root][ERROR] - Traceback (most recent call last):
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/run.py", line 42, in main
    globals()[cfg.experiment.method](cfg)
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/experiments/dond_run_train.py", line 92, in dond_run_train
    run_matches(
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/generation/run_games.py", line 70, in run_matches
    model.prepare_adapter_eval(adapter_name)
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/llm_negotiation/src/models/hf_agent.py", line 194, in prepare_adapter_eval
    self.vllm_model = LLM(self.model_name, enable_lora=True, max_lora_rank=256)
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 177, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 538, in from_engine_args
    engine = cls(
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 319, in __init__
    self._initialize_kv_caches()
[2025-01-11 13:00:17,156][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 461, in _initialize_kv_caches
    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[2025-01-11 13:00:17,157][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in initialize_cache
    self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[2025-01-11 13:00:17,157][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/worker/worker.py", line 257, in initialize_cache
    raise_if_cache_size_invalid(num_gpu_blocks,
[2025-01-11 13:00:17,157][root][ERROR] - File "/home/mila/d/dereck.piche/negenv/lib/python3.10/site-packages/vllm/worker/worker.py", line 476, in raise_if_cache_size_invalid
    raise ValueError(
[2025-01-11 13:00:17,157][root][ERROR] - ValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (49840). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[2025-01-11 13:00:17,157][root][ERROR] - Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
