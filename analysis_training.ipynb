{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hydra-core\n",
    "%pip install pandas\n",
    "\n",
    "custom_path = input(\"Enter the path to your seed folder: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllm.training import produce_training_stats\n",
    "training_data = produce_training_stats.get_iterations_data(custom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = produce_training_stats.get_metric_paths(training_data)\n",
    "print(\"Example metric paths:\", paths[:10])\n",
    "value_mb = produce_training_stats.get_metric_iteration_list(training_data, [\"loss_mb_total\", \"value_mb_total\"])\n",
    "plt.plot([np.mean(c) for c in value_mb if c is not None])\n",
    "plt.title(\"Value MB loss (mean) per iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)\n",
    "training_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "kl_terms = produce_training_stats.get_metric_iteration_list(training_data, [\"mb_kl_loss_terms\"])\n",
    "kl_means = [np.mean(c) for c in kl_terms if c is not None]\n",
    "plt.plot(kl_means)\n",
    "plt.title(\"KL loss terms (mean) per iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mb = produce_training_stats.get_metric_iteration_list(training_data, [\"loss_mb_total\", \"value_mb_total\"])\n",
    "value_means = [np.mean(c) for c in value_mb if c is not None]\n",
    "plt.plot(value_means)\n",
    "plt.title(\"Policy Gradient Value Loss (mean) per iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get AdAlign Stats (Warning: Might be Affected by Padding)\n",
    "If the number of timesteps in your trajectories is not constant, then the return tensors will be padded \n",
    "with zero values and these statistics will be false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated metric names per new tally logging\n",
    "a1 = produce_training_stats.get_single_metric_vector(training_data, [\"regular_advantages\"], range(len(training_data)))\n",
    "a2 = produce_training_stats.get_single_metric_vector(training_data, [\"regular_advantages_other\"], range(len(training_data)))\n",
    "reg_scores = produce_training_stats.get_single_metric_vector(training_data, [\"raw_advantage_alignment_weights\"], range(len(training_data)))\n",
    "op_terms = produce_training_stats.get_single_metric_vector(training_data, [\"ad_align_opp_shaping_terms\"], range(len(training_data)))\n",
    "aa_scores = produce_training_stats.get_single_metric_vector(training_data, [\"final_advantage_alignment_credits\"], range(len(training_data)))\n",
    "\n",
    "n_bins = 25\n",
    "fig, axs = plt.subplots(1, 5, sharey=True, tight_layout=True)\n",
    "axs[0].hist(reg_scores, bins=n_bins)\n",
    "axs[0].set_xlabel('Raw Weights')\n",
    "axs[1].hist(op_terms, bins=n_bins)\n",
    "axs[1].set_xlabel('OP term')\n",
    "axs[2].hist(aa_scores, bins=n_bins)\n",
    "axs[2].set_xlabel('AdAlign credit')\n",
    "axs[3].hist(a1, bins=n_bins)\n",
    "axs[3].set_xlabel('a1')\n",
    "axs[4].hist(a2, bins=n_bins)\n",
    "axs[4].set_xlabel('a2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights Mean: \", np.mean(reg_scores))\n",
    "print(\"Weights STD: \", np.std(reg_scores))\n",
    "print(\"OP Mean: \", np.mean(op_terms))\n",
    "print(\"OP STD: \", np.std(op_terms))\n",
    "print(\"AA Mean: \", np.mean(aa_scores))\n",
    "print(\"AA STD: \", np.std(aa_scores))\n",
    "print(\"AA min: \", np.min(aa_scores))\n",
    "print(\"AA max: \", np.max(aa_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = reg_scores.size\n",
    "print(\"Corr. Coeff Reg & AA\", np.corrcoef(reg_scores[:N], aa_scores[:N])[0,1])\n",
    "print(\"Corr. Coeff OP & AA\", np.corrcoef(op_terms[:N], aa_scores[:N])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".analyse_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
