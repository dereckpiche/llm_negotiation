hydra:
  job:
    chdir: false


experiment:
  method: dond_run_train
  description: "Easy. They should be learning."
  nb_epochs: 10
  nb_matches_per_iteration: 32
  reinit_matches_each_it: true

matches:

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_matches_args:
    nb_parallel_matches: -1
    log_func: independant_players_logging
    log_func_args: 
      metrics_func: gather_dond_statistics
      metrics_func_args: 
        stats_to_log: [
        "agreement_percentage", 
        "points", 
        "points_on_agreement",
        "items_given_to_self"
        ]
      training_data_func: set_discounted_returns
      training_data_func_args: {}

  dond_game_args:
    nb_rounds_func: geometric_round_number
    nb_rounds_kwargs:
        p: 0.6
    max_turns: 1
    mode: basic
    random_setup_func: fixed_manual
    random_setup_kwargs: 
      items: ['coins']
      quantities: [10]
      val_starting_negotiator: [1]
      val_responding_negotiator: [1]
    role_assignator_func: fixed_role_assignator
    role_assignator_func_kwargs: {}
    finalization_visibility: True
    other_values_visibility: True

  players: 

    alice:

      dond_player_args:
        mod_adpt_id: 'llama/ad_alice'
        allow_reasoning: false
        max_retries: 1
        
    bob:

      dond_player_args:
        mod_adpt_id: 'llama/ad_bob'
        allow_reasoning: false
        max_retries: 1


training:
  keep_error_messages: False
  llama:
    adapters:
      ad_alice:
        train_func: train_reinforce_main
        train_func_args: {}


      ad_bob:
        train_func: train_reinforce_main
        train_func_args: {}


models: 

  llama:
    class: dummy_hf
    init_args:
      name: 'llama'
      adapter_names: ['ad_alice', 'ad_bob']
      max_model_length: 5000
      include_value_head: false
      device: "cuda"
      pretrained_args: 
        pretrained_model_name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
        torch_dtype: "bfloat16"
        device_map: "auto"
        attn_implementation: "flash_attention_2"
      bits_and_bytes_args: null
        #load_in_8bit: False
        #load_in_4bit: true
      lora_args:
        task_type: CAUSAL_LM
        r: 64
        lora_alpha: 32
        lora_dropout: 0.0
        target_modules: "all-linear"

      generation_args:
        max_new_tokens: 100
        do_sample: True
        temperature: 1.0
        top_k: 0.0
        top_p: 1.0
        repetition_penalty: 0.0
      keep_vllm_during_training: False
      keep_hf_during_training: True
      keep_hf_during_eval: False
      keep_vllm_during_eval: True
      eval_with: "vllm"
      train_with: "hf"

  








